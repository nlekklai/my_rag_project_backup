# assessments/mocking_assessment.py
"""
Mocking Assessment Utilities
‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏ö‡∏ö KM/Enabler Assessment ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏Å LLM ‡∏à‡∏£‡∏¥‡∏á
"""

import random
import logging
from typing import Dict, Any, List

logger = logging.getLogger(__name__)


# -------------------------------------------------------
# MOCK: Controlled LLM Evaluation (Deterministic)
# -------------------------------------------------------
def evaluate_with_llm_CONTROLLED_MOCK(
    statement: str,
    context: str,
    level: int = 1,
    sub_criteria_id: str = "UNKNOWN",
    statement_number: int = 1,
    **kwargs
) -> Dict[str, Any]:
    """
    Mock ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö (‡πÉ‡∏ä‡πâ‡πÅ‡∏ó‡∏ô LLM ‡∏à‡∏£‡∏¥‡∏á)
    ‡∏£‡∏∏‡πà‡∏ô Debug + Strict Mock ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö sub_criteria 1.2
    """
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö kwargs
    level = kwargs.get("level", level)
    sub_criteria_id = kwargs.get("sub_criteria_id", sub_criteria_id)
    statement_number = kwargs.get("statement_number", statement_number)

    # Trim & debug log
    sub_id = str(sub_criteria_id).strip()
    logger.info(f"[DEBUG MOCK INPUT] sub_criteria_id={sub_id}, level={level}, statement_number={statement_number}")

    # ‚úÖ Logic controlled mock
    if sub_id == "1.2":
        # L1-L3 ‡∏ú‡πà‡∏≤‡∏ô‡∏´‡∏°‡∏î
        if level in [1,2,3]:
            score = 1
        elif level == 4:
            # Fail statement 2 ‡∏Ç‡∏≠‡∏á L4
            score = 0 if statement_number == 2 else 1
        elif level == 5:
            # Fail statements 1 & 3 ‡∏Ç‡∏≠‡∏á L5
            score = 0 if statement_number in [1,3] else 1
        else:
            score = 0
    else:
        # Default mock logic
        if level == 1:
            score = 1
        elif level == 2:
            score = 1 if statement_number % 2 == 1 else 0
        elif level == 3:
            score = 1
        elif level in [4,5]:
            score = 0
        else:
            score = 0

    is_passed = score == 1

    mock_context_snippet = f"[MOCK CONTEXT SNIPPET] Evidence found for {sub_id} L{level} S{statement_number}." if is_passed else ""
    mock_sources = [
        {"source_name": f"mock_doc_{sub_id}_L{level}_S{statement_number}.pdf",
         "location": f"page_{10+statement_number}",
         "doc_id": f"DOC_{sub_id}"}
    ] if is_passed else []

    return {
        "sub_criteria_id": sub_id,
        "level": level,
        "statement_number": statement_number,
        "statement": statement,
        "context_retrieved_snippet": mock_context_snippet,
        "retrieved_sources_list": mock_sources,
        "llm_score": score,
        "score": score,
        "reason": f"MOCK reason for L{level} S{statement_number} ‚Üí {'PASS' if is_passed else 'FAIL'} (Controlled Mock)",
        "pass_status": is_passed,
        "status_th": "‡∏ú‡πà‡∏≤‡∏ô" if is_passed else "‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô",
        "llm_result": {"is_passed": is_passed, "score": float(score)}
    }



# -------------------------------------------------------
# MOCK: Retrieval
# -------------------------------------------------------
def retrieve_context_MOCK(statement: str, sub_criteria_id: str, level: int, statement_number: int, mapping_data=None, **kwargs) -> Dict[str, Any]:
    """
    Mock retrieval context ‡∏à‡∏≤‡∏Å Vectorstore (‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á)
    """
    fake_sources = [
        {"source_name": f"MOCK_DOC_{sub_criteria_id}_L{level}.pdf", "location": f"page_{10+statement_number}", "doc_id": f"DOC_{sub_criteria_id}"}
    ]
    # ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏î‡∏™‡∏≠‡∏ö Reranking Logic (‡∏´‡∏≤‡∏Å‡∏°‡∏µ)
    return {
        "top_evidences": [
            {"content": f"[MOCK EVIDENCE 1] Primary evidence for {sub_criteria_id} L{level} S{statement_number}.", "score": 0.9, "source": fake_sources[0]['source_name'], "doc_id": fake_sources[0]['doc_id'], "metadata": {"page_number": 10+statement_number}},
            {"content": f"[MOCK EVIDENCE 2] Secondary evidence for {sub_criteria_id} L{level} S{statement_number}.", "score": 0.7, "source": fake_sources[0]['source_name'], "doc_id": fake_sources[0]['doc_id'], "metadata": {"page_number": 11+statement_number}},
        ]
    }


# -------------------------------------------------------
# MOCK: Action Plan Generation
# -------------------------------------------------------
def create_structured_action_plan_MOCK(failed_statements_data: List[Dict[str, Any]], sub_id: str, target_level: int) -> List[Dict[str, Any]]:
    """
    Mock LLM Action Plan
    """
    actions = []
    
    # üö® FIX: ‡πÉ‡∏ä‡πâ Action Plan ‡∏ó‡∏µ‡πà‡∏á‡πà‡∏≤‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ö‡πà‡∏á‡∏ä‡∏µ‡πâ‡∏ñ‡∏∂‡∏á Gap 
    actions.append({
        "Statement_ID": "ALL_L1",
        "Failed_Level": target_level,
        "Recommendation": f"‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏´‡∏°‡πà‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Level {target_level} (‡πÅ‡∏•‡∏∞ Level ‡∏ó‡∏µ‡πà‡∏°‡∏µ Gap ‡∏≠‡∏∑‡πà‡∏ô‡πÜ: 1, 2, 3, 4, 5) ‡πÅ‡∏•‡∏∞‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤ Vector Store ‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏ô AI Assessment ‡πÉ‡∏ô‡πÇ‡∏´‡∏°‡∏î **FULLSCOPE** ‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏ß‡πà‡∏≤ Level ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå",
        "Target_Evidence_Type": "Rerunning Assessment & New Evidence",
        "Key_Metric": f"Overall Score ‡∏Ç‡∏≠‡∏á {sub_id} ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡πÅ‡∏•‡∏∞ Highest Full Level ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô L{target_level}"
    })
    
    # Action Plan ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô‡πÄ‡∏õ‡πá‡∏ô List ‡∏Ç‡∏≠‡∏á Phase (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö run_assessment.py)
    return [
        {
            "Phase": "2. AI Validation & Maintenance",
            "Goal": f"‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏Å‡∏≤‡∏£ Level-Up ‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö L{target_level}",
            "Actions": actions
        }
    ]


# -------------------------------------------------------
# MOCK: Summarize Context (‡πÉ‡∏ä‡πâ‡πÉ‡∏ô Evidence Summary)
# -------------------------------------------------------
def summarize_context_with_llm_MOCK(
    context: str, 
    sub_criteria_name: str, 
    level: int, 
    **kwargs # ‡∏£‡∏±‡∏ö Argument ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏Ñ‡∏≤‡∏î‡∏Ñ‡∏¥‡∏î‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
) -> Dict[str, Any]:
    """
    Mock ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡∏∏‡∏õ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î (L5)
    """
    
    sub_id = kwargs.get('sub_id', 'N/A')
    
    return {
        "summary": f"[MOCK SUMMARY] ‡∏™‡∏£‡∏∏‡∏õ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á {sub_criteria_name} ({sub_id}) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Level {level}. ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≥‡∏•‡∏≠‡∏á ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÉ‡∏ô Vector Store ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡∏ì‡∏ë‡πå {sub_id} Level {level}...",
        "suggestion_for_next_level": f"[MOCK SUGGESTION] ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏Å‡πá‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏Ç‡∏≠‡∏á L{level+1}"
    }


# -------------------------------------------------------
# MOCK: Set Control Mode
# -------------------------------------------------------
def set_mock_control_mode(mode: str = "default"):
    """
    ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ behavior ‡∏Ç‡∏≠‡∏á mock (‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢ pattern)
    """
    logger.info(f"[MOCK MODE] Using mock control mode = {mode}")
    random.seed(42)
    return True

 # ----------------------------------------------------
    ## üåü NEW FEATURE: Generate Action Plan (Mock Handler)
    # ----------------------------------------------------
def generate_action_plan_mock(
    sub_criteria_id: str,
    raw_llm_results: List[Dict[str, Any]] = None,
    final_subcriteria_results: List[Dict[str, Any]] = None,
) -> List[Dict]:
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á Action Plan ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Sub-Criteria ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î (‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô Mock ‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ self)
    ‡πÉ‡∏ä‡πâ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Action Plan ‡∏´‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏• LLM Mock ‡πÄ‡∏™‡∏£‡πá‡∏à
    """

    logger.info(f"[MOCK ACTION PLAN] Generating mock action plan for {sub_criteria_id}")

    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö input
    if not raw_llm_results:
        logger.warning("generate_action_plan_mock: raw_llm_results is empty or None")
        return []

    if not final_subcriteria_results:
        final_subcriteria_results = []

    # ‚úÖ ‡∏î‡∏∂‡∏á statement ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô
    failed_statements_data = [
        r for r in raw_llm_results
        if r.get("sub_criteria_id") == sub_criteria_id and not r.get("pass_status", False)
    ]

    # ‚úÖ ‡∏´‡∏≤ target_level ‡∏à‡∏≤‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏£‡∏∏‡∏õ
    sub_criteria_result = next(
        (r for r in final_subcriteria_results if r.get("sub_criteria_id") == sub_criteria_id),
        {}
    )
    target_level = sub_criteria_result.get("highest_full_level", 0) + 1

    logger.info(f"[MOCK ACTION PLAN] Found {len(failed_statements_data)} failed statements for {sub_criteria_id}, target_level={target_level}")

    # ‚úÖ ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ Mock Generator ‡∏à‡∏£‡∏¥‡∏á
    try:
        return create_structured_action_plan_MOCK(
            failed_statements_data=failed_statements_data,
            sub_id=sub_criteria_id,
            target_level=target_level
        )
    except Exception as e:
        logger.exception(f"[MOCK ACTION PLAN] Error creating action plan for {sub_criteria_id}: {e}")
        return []
