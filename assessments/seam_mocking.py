"""
Mocking Assessment Utilities
‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏ö‡∏ö KM/Enabler Assessment ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏Å LLM ‡∏à‡∏£‡∏¥‡∏á
(Revised for SEAM PDCA Engine)
"""

import logging
import random
from typing import Dict, Any, List, Optional, Type, TypeVar
from pydantic import BaseModel 

# üí° ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£ Import LcDocument ‡πÅ‡∏•‡∏∞ Config ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Mock Object ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏ï‡∏£‡∏á‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà Engine ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
try:
    from langchain_core.documents import Document as LcDocument 
    from config.global_vars import FINAL_K_RERANKED 
except ImportError:
    # Fallback to a simple dictionary and default value if imports fail
    LcDocument = dict 
    FINAL_K_RERANKED = 5 

# ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤ PDCA_PHASE_MAP ‡∏à‡∏≤‡∏Å seam_prompts ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÉ‡∏ô Mock Reason/Action Plan
try:
    from core.seam_prompts import PDCA_PHASE_MAP
except ImportError:
    PDCA_PHASE_MAP = {1: "Plan", 2: "Plan + Do", 3: "Plan + Do + Check", 4: "PDCA Cycle", 5: "Advanced"}

logger = logging.getLogger(__name__)

# Mock TypeVars for consistency with llm_data_utils
T = TypeVar("T", bound=BaseModel)


# -------------------------------------------------------
# MOCK: Controlled LLM Evaluation (Deterministic)
# Mocks: llm_data_utils.evaluate_with_llm
# -------------------------------------------------------
def evaluate_with_llm_CONTROLLED_MOCK(
    context: str, 
    sub_criteria_name: str, 
    level: int, 
    statement_text: str, 
    sub_id: str, 
    **kwargs
) -> Dict[str, Any]:
    """
    Mock ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö (Deterministic)
    ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏ï‡∏≤‡∏° Schema ‡∏Ç‡∏≠‡∏á StatementAssessment (score, reason, is_passed)
    """
    
    # Extract PDCA Phase (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£ Debug/Log)
    pdca_phase = kwargs.get('pdca_phase', PDCA_PHASE_MAP.get(level, f"L{level} Concept"))
    sub_id_clean = str(sub_id).strip()
    logger.info(f"[MOCK LLM] Evaluating sub={sub_id_clean}, level={level}, PDCA={pdca_phase}")

    # --- Controlled Logic ---
    score = 0
    if sub_id_clean == "1.2":
        # L1, L2, L3 ‡∏ú‡πà‡∏≤‡∏ô; L4, L5 ‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô (Highest Full Level ‡∏à‡∏∞‡∏à‡∏ö‡∏ó‡∏µ‡πà L3)
        if level in [1, 2, 3]:
            score = 1
        else:
            score = 0
    elif sub_id_clean == "3.1":
        # L1, L2 ‡∏ú‡πà‡∏≤‡∏ô; L3, L4, L5 ‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô (Highest Full Level ‡∏à‡∏∞‡∏à‡∏ö‡∏ó‡∏µ‡πà L2)
        if level in [1, 2]:
            score = 1
        else:
            score = 0
    else:
        # Default Logic: L1 ‡∏ú‡πà‡∏≤‡∏ô, ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô (Highest Full Level ‡∏à‡∏∞‡∏à‡∏ö‡∏ó‡∏µ‡πà L1)
        if level == 1:
            score = 1
        else:
            score = 0

    is_passed = score == 1
    reason = f"[MOCK] Statement passed the {pdca_phase} check (Controlled Mock). ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏ä‡∏µ‡πâ‡∏ß‡πà‡∏≤‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡∏ñ‡∏∂‡∏á L{level} ‡πÅ‡∏•‡πâ‡∏ß. Result: {'PASS' if is_passed else 'FAIL'}"

    # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà llm_data_utils.py ‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á
    return {
        "score": score,
        "reason": reason,
        "is_passed": is_passed, 
    }


# -------------------------------------------------------
# MOCK: Retrieval
# Mocks: llm_data_utils.retrieve_context_with_filter
# -------------------------------------------------------
# üéØ FIX: ‡πÄ‡∏û‡∏¥‡πà‡∏° Argument 'vsm_manager' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Signature ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö llm_data_utils.py
def retrieve_context_with_filter_MOCK(
    vsm_manager: Optional[Any], # üü¢ Argument ‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏ö VSM Instance
    query: str,
    collection_name: str,
    doc_uuid_filter: Optional[List[str]] = None,
    disable_semantic_filter: bool = False,
    top_k: int = FINAL_K_RERANKED 
) -> Dict[str, Any]:
    """
    Mock retrieval context ‡∏à‡∏≤‡∏Å Vectorstore (‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á)
    ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ Dict ‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà LLM Engine ‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á: {"top_evidences": [...], "aggregated_context": "..." }
    """
    
    # Mock does not use vsm_manager, but must accept it.
    sub_id = collection_name.split('_')[-1] 
    logger.info(f"[MOCK RAG] Retrieving {top_k} chunks for query on {sub_id}...")

    top_evidences = []
    aggregated_parts = []
    
    for i in range(top_k):
        metadata = {
            "stable_doc_uuid": f"mock-stable-uuid-{sub_id}-{i+1}",
            "file_name": f"MOCK_DOC_{sub_id}_Chunk_{i+1}.pdf", 
            "location": f"/path/to/docs/{sub_id}/chunk_{i+1}", 
            "chunk_uuid": f"mock-uuid-{sub_id}-{i+1}"
        }
        page_content = f"[MOCK CHUNK {i+1}] Relevant evidence for topic {sub_id} (Query: {query[:30]}...)"
        
        # ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ Evidence ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô Dict ‡∏ó‡∏µ‡πà‡∏°‡∏µ content ‡πÅ‡∏•‡∏∞ metadata
        top_evidences.append({"content": page_content, "metadata": metadata})
        aggregated_parts.append(page_content)

    aggregated_context = "\n---\n".join(aggregated_parts)

    return {
        "top_evidences": top_evidences,
        "aggregated_context": aggregated_context
    }


# -------------------------------------------------------
# MOCK: Action Plan Generation
# Mocks: llm_data_utils.create_structured_action_plan
# -------------------------------------------------------
def create_structured_action_plan_MOCK(
    failed_statements_data: List[Dict[str, Any]], 
    sub_id: str, 
    enabler: str, 
    target_level: int, 
    max_retries: int = 2 
) -> List[Dict[str, Any]]:
    """
    Mock LLM Action Plan. ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏ï‡∏≤‡∏° ActionPlanActions Schema (List ‡∏Ç‡∏≠‡∏á Phase)
    """
    logger.info(f"[MOCK ACTION PLAN] Generating mock plan for {sub_id} (Target L{target_level})")

    first_fail_reason = "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß"
    if failed_statements_data:
        first_fail_reason = failed_statements_data[0].get('reason', 'Missing reason').strip()
    
    # ‡πÉ‡∏ä‡πâ PDCA Phase ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢
    target_phase = PDCA_PHASE_MAP.get(target_level, f"Level {target_level} Requirements")
    
    # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏ï‡∏≤‡∏° Action Plan Schema
    return [
        {
            "Phase": f"1. Gap Closure & Planning ({target_phase})",
            "Goal": f"‡∏à‡∏±‡∏î‡∏ó‡∏≥‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏¥‡∏î Gap ‡∏ó‡∏µ‡πà L{target_level} ‡πÉ‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå {sub_id}",
            "Actions": [
                {
                    "Statement_ID": sub_id,
                    "Recommendation": f"‡∏ó‡∏ö‡∏ó‡∏ß‡∏ô Gap: {first_fail_reason[:50]}... ‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏ó‡∏≥‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÅ‡∏™‡∏î‡∏á‡∏ñ‡∏∂‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏ï‡∏≤‡∏°‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô {target_phase} ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô",
                    "Responsible": f"{enabler} Enabler Lead",
                    "Key_Metric": f"Evidence Quality Score L{target_level}",
                    "Tools_Templates": "Gap Analysis Template, Action Plan Form",
                    "Verification_Outcome": "‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏´‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡πÅ‡∏•‡∏∞‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤ Vector Store"
                }
            ]
        }
    ]


# -------------------------------------------------------
# MOCK: Summarize Context
# Mocks: llm_data_utils.summarize_context_with_llm
# -------------------------------------------------------
def summarize_context_with_llm_MOCK(
    context: str, 
    sub_criteria_name: str, 
    level: int, 
    sub_id: str, 
    schema: Optional[Type[T]] = None
) -> Dict[str, Any]:
    """
    Mock ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡∏∏‡∏õ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô (Evidence Summary)
    """
    logger.info(f"[MOCK SUMM] Summarizing evidence for {sub_id} L{level}")
    
    mock_context_len = context.count("[MOCK CHUNK") 
    
    return {
        "summary": f"[MOCK SUMMARY] ‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏à‡∏≥‡∏•‡∏≠‡∏á {mock_context_len} ‡∏ä‡∏¥‡πâ‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {sub_criteria_name} ({sub_id}) L{level} ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏ñ‡∏∂‡∏á‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô",
        "suggestion_for_next_level": f"[MOCK SUGGESTION] ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡πá‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏™‡∏î‡∏á‡∏ñ‡∏∂‡∏á **‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á (Check & Act Phase)** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö L{level+1}"
    }


# -------------------------------------------------------
# MOCK: Set Control Mode (Used by SEAMPDCAEngine)
# -------------------------------------------------------
def set_mock_control_mode(enable: bool):
    """
    Mock setting the global mock control flag.
    Note: The real mock flag is controlled in core/llm_data_utils.py
    """
    logger.info(f"[MOCK MODE] Setting mock control mode to {enable}")
    if enable:
        random.seed(42) # Ensure deterministic "random" for other components (if any)