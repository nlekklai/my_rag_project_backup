# -------------------- assessments/km_assessment.py (FINAL FULL VERSION WITH RUBRIC FIX) --------------------
import os
import json
import logging
import sys
from typing import List, Dict, Any, Optional, Union

# ‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç sys.path ‡πÉ‡∏ô __main__ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Import core ‡πÑ‡∏î‡πâ (‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÅ‡∏•‡πâ‡∏ß)
# NOTE: ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á
from core.vectorstore import load_all_vectorstores
from core.retrieval_utils import evaluate_with_llm, retrieve_context, set_mock_control_mode 

logger = logging.getLogger(__name__)
# NOTE: ‡∏õ‡∏£‡∏±‡∏ö level logging ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô INFO
logging.basicConfig(level=logging.INFO)


# Level Fractions ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Linear Interpolation (‡∏Ñ‡πà‡∏≤‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏Ç‡∏≠‡∏á 1/6, 2/6, 3/6, 4/6, 5/6)
DEFAULT_LEVEL_FRACTIONS = {
    "1": 0.16667,
    "2": 0.33333,
    "3": 0.50000,
    "4": 0.66667,
    "5": 0.83333,
    "MAX_LEVEL_FRACTION": 1.00000, 
    "0": 0.0 
}

# Default fallback rubric structure (‡πÉ‡∏ä‡πâ‡∏Ñ‡∏µ‡∏¢‡πå‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á)
DEFAULT_RUBRIC_STRUCTURE = {
    "KM_Maturity_Rubric": { 
        "levels": [
            {
                "level": i,
                "name": "Default",
                "criteria": {
                    f"subtopic_{j}": f"Default criteria for level {i}, subtopic {j}"
                    for j in range(1, 4)
                }
            }
            for i in range(1, 6)
        ]
    }
}


class KMAssessment:
    """
    Automated KM Maturity Assessment System
    ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ß‡∏∏‡∏í‡∏¥‡∏†‡∏≤‡∏ß‡∏∞ KM ‡∏Ç‡∏≠‡∏á‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£
    """

    # BASE_DIR ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô‡∏à‡∏≤‡∏Å Root (.. ‡∏Ñ‡∏∑‡∏≠ Root Directory)
    BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "evidence_checklist"))
    EVIDENCE_FILE = os.path.join(BASE_DIR, "km_evidence_statements_checklist.json")
    RUBRIC_FILE = os.path.join(BASE_DIR, "km_rating_criteria_rubric.json")
    LEVEL_FRACTIONS_FILE = os.path.join(BASE_DIR, "km_scoring_level_fractions.json")

    def __init__(self,
                 evidence_data: Optional[List] = None,
                 rubric_data: Optional[Dict] = None,
                 level_fractions: Optional[Dict] = None,
                 vectorstore_retriever=None):
        
        self.evidence_data = evidence_data or self._load_json_fallback(self.EVIDENCE_FILE, default=[])
        self.rubric_data = rubric_data or self._load_json_fallback(self.RUBRIC_FILE, default=DEFAULT_RUBRIC_STRUCTURE)
        self.level_fractions = level_fractions or self._load_json_fallback(self.LEVEL_FRACTIONS_FILE, default=DEFAULT_LEVEL_FRACTIONS)
        self.vectorstore_retriever = vectorstore_retriever
        
        self.raw_llm_results: List[Dict] = []
        self.final_subcriteria_results: List[Dict] = []
        
        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Global Rubric Map
        self.global_rubric_map: Dict[int, Dict[str, str]] = self._prepare_rubric_map()


    def _load_json_fallback(self, path: str, default: Any):
        """‡πÇ‡∏´‡∏•‡∏î JSON ‡∏´‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏°‡πà‡∏°‡∏µ ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ default"""
        if not os.path.isfile(path):
            logger.warning(f"[Warning] ‡πÑ‡∏ü‡∏•‡πå JSON ‡πÑ‡∏°‡πà‡∏û‡∏ö: {path}, ‡πÉ‡∏ä‡πâ default fallback")
            return default
        try:
            with open(path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"Failed to load JSON {path}: {e}")
            return default

    def _prepare_rubric_map(self) -> Dict[int, Dict[str, str]]:
        """
        ‡πÅ‡∏õ‡∏•‡∏á Global Rubric ‡πÄ‡∏õ‡πá‡∏ô Map ‡∏á‡πà‡∏≤‡∏¢‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ
        FIX: ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏µ‡∏¢‡πå 'KM_Maturity_Rubric' ‡πÅ‡∏ó‡∏ô 'Global_Maturity_Rubric'
        """
        rubric_map = {}
        
        # üü¢ FIX HERE: ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏µ‡∏¢‡πå‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡πÑ‡∏ü‡∏•‡πå JSON ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
        rubric_data_entry = self.rubric_data.get("KM_Maturity_Rubric")
        
        if self.rubric_data and isinstance(rubric_data_entry, dict):
            # ‡πÉ‡∏ä‡πâ rubric_data_entry ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏ô‡∏ã‡πâ‡∏≥
            for level_entry in rubric_data_entry.get("levels", []):
                level_num = level_entry.get("level")
                if level_num:
                    rubric_map[level_num] = level_entry.get("criteria", {})
        return rubric_map

    def _retrieve_context(self, statement: str) -> str:
        """‡πÉ‡∏ä‡πâ retrieve_context ‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á LLM/Context ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡πÄ‡∏õ‡πá‡∏ô String"""
        result = retrieve_context(statement, self.vectorstore_retriever)
        contexts = [e["content"] for e in result.get("top_evidences", [])]
        return "\n".join(contexts)

    def _compute_subcriteria_score(self, level_pass_ratios: Dict[str, float], sub_criteria_weight: float) -> Dict[str, Any]:
        """
        ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏ï‡∏≤‡∏° Linear Interpolation Logic
        """
        highest_full_level = 0
        progress_score = 0.0
        
        # 1. ‡∏´‡∏≤ Highest Fully Passed Level (1.0 ratio)
        for level in range(1, 6):
            level_str = str(level)
            if level_pass_ratios.get(level_str, 0.0) < 1.0:
                highest_full_level = level - 1 
                if highest_full_level < 0:
                    highest_full_level = 0
                break
            else:
                highest_full_level = level
        
        # 2. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Progress Score (Linear Interpolation)
        if highest_full_level == 5:
            progress_score = self.level_fractions.get("MAX_LEVEL_FRACTION", 1.0) * sub_criteria_weight
        elif highest_full_level < 5:
            base_fraction = self.level_fractions.get(str(highest_full_level) if highest_full_level > 0 else "0", 0.0)
            next_level = highest_full_level + 1 
            next_fraction = self.level_fractions.get(str(next_level), 0.0)
            progress_ratio = level_pass_ratios.get(str(next_level), 0.0)
            
            fraction_increase = (next_fraction - base_fraction) * progress_ratio
            total_fraction = base_fraction + fraction_increase
            progress_score = total_fraction * sub_criteria_weight
        
        # 3. ‡∏à‡∏±‡∏î Gap Analysis
        action_item = ""
        development_gap = False
        
        target_gap_level = highest_full_level + 1
        
        if target_gap_level <= 5: 
             development_gap = True
             ratio = level_pass_ratios.get(str(target_gap_level), 0.0)
             action_item = f"‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏£‡∏•‡∏∏‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô Level {target_gap_level} ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á (Pass Ratio: {ratio})"
        
        # 4. Return Results
        return {
            "highest_full_level": highest_full_level,
            "progress_score": round(progress_score, 2),
            "development_gap": development_gap,
            "action_item": action_item,
            "weight": sub_criteria_weight
        }


    def _process_subcriteria_results(self):
        """
        ‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå LLM ‡∏ï‡∏≤‡∏° Sub-Criteria ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
        """
        grouped_results: Dict[str, Dict] = {}
        for r in self.raw_llm_results:
            key = f"{r['enabler_id']}-{r['sub_criteria_id']}"
            if key not in grouped_results:
                enabler_data = next((e for e in self.evidence_data 
                                     if e.get("Enabler_ID") == r['enabler_id'] and 
                                        e.get("Sub_Criteria_ID") == r['sub_criteria_id']), {})
                
                grouped_results[key] = {
                    "enabler_id": r['enabler_id'],
                    "sub_criteria_id": r['sub_criteria_id'],
                    "sub_criteria_name": enabler_data.get("Sub_Criteria_Name_TH", "N/A"),
                    "weight": enabler_data.get("Weight", 1.0),
                    "raw_llm_scores": [],
                    "level_pass_ratios": {}, 
                    "num_statements_per_level": {} 
                }
            grouped_results[key]["raw_llm_scores"].append(r)

        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Pass Ratio
        for key, data in grouped_results.items():
            level_scores: Dict[int, List[int]] = {}
            for r in data["raw_llm_scores"]:
                level = r["level"]
                if level not in level_scores:
                    level_scores[level] = []
                level_scores[level].append(r["llm_score"])
            
            for level, scores in level_scores.items():
                total_statements = len(scores)
                passed_statements = sum(scores)
                
                level_str = str(level)
                
                data["level_pass_ratios"][level_str] = round(passed_statements / total_statements, 3)
                data["num_statements_per_level"][level_str] = total_statements

        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Final Score (Detailed Score)
        self.final_subcriteria_results = []
        for key, data in grouped_results.items():
            scoring_results = self._compute_subcriteria_score(
                level_pass_ratios=data["level_pass_ratios"],
                sub_criteria_weight=data["weight"]
            )
            data.update(scoring_results)
            self.final_subcriteria_results.append(data)


    def run_assessment(self) -> List[Dict]:
        """
        Run assessment across all levels & subtopics
        """
        self.raw_llm_results = [] 
        self.final_subcriteria_results = []

        from core.retrieval_utils import _MOCK_CONTROL_FLAG
        if _MOCK_CONTROL_FLAG:
             set_mock_control_mode(True) 
        
        for enabler in self.evidence_data:
            enabler_id = enabler.get("Enabler_ID")
            sub_criteria_id = enabler.get("Sub_Criteria_ID")
            sub_criteria_name = enabler.get("Sub_Criteria_Name_TH", "N/A")

            for level in range(1, 6):
                level_key = f"Level_{level}_Statements"
                statements: List[str] = enabler.get(level_key, [])
                
                if not statements:
                    continue 
                
                rubric_criteria = self.global_rubric_map.get(level, {})
                
                for i, statement in enumerate(statements):
                    subtopic_key = f"subtopic_{i+1}"
                    standard = rubric_criteria.get(subtopic_key, f"Default standard L{level} S{i+1}")
                    
                    context = self._retrieve_context(statement)
                    
                    result = evaluate_with_llm(
                        statement=statement,
                        context=context,
                        standard=standard
                    )
                    
                    self.raw_llm_results.append({
                        "enabler_id": enabler_id,
                        "sub_criteria_id": sub_criteria_id,
                        "sub_criteria_name": sub_criteria_name, 
                        "level": level,
                        "statement": statement,
                        "subtopic": subtopic_key,
                        "standard": standard,
                        "llm_score": result.get("score", 0), 
                        "reason": result.get("reason", ""),
                        "context_retrieved_snippet": context[:100] + "..." 
                    })
        
        self._process_subcriteria_results()
        
        return self.final_subcriteria_results

    def summarize_results(self) -> Dict[str, Dict]:
        """
        ‡∏™‡∏£‡∏∏‡∏õ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°‡∏à‡∏≤‡∏Å final_subcriteria_results
        """
        total_weight = sum(r["weight"] for r in self.final_subcriteria_results)
        total_score = sum(r["progress_score"] for r in self.final_subcriteria_results)
        num_subcriteria = len(self.final_subcriteria_results)
        
        return {
            "Overall": {
                "total_weighted_score": round(total_score, 2),
                "total_possible_weight": round(total_weight, 2),
                "overall_progress_percent": round((total_score / total_weight) * 100, 2) if total_weight > 0 else 0.0,
                "overall_maturity_score": round(total_score / num_subcriteria, 2) if num_subcriteria > 0 else 0.0
            },
            "SubCriteria_Breakdown": {
                r["sub_criteria_id"]: {
                    "name": r.get("sub_criteria_name", "N/A"),
                    "score": r["progress_score"],
                    "weight": r["weight"],
                    "highest_full_level": r["highest_full_level"],
                    "pass_ratios": r["level_pass_ratios"], 
                    "development_gap": r["development_gap"],
                    "action_item": r["action_item"]
                } for r in self.final_subcriteria_results
            }
        }