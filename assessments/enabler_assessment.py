import os
import json
import logging
import sys
import re 
from typing import List, Dict, Any, Optional, Union
import time 


# --- PATH SETUP (Must be executed first for imports to work) ---
try:
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
    if project_root not in sys.path:
        sys.path.append(project_root)
    
    # üö® FIX: ‡∏ï‡πâ‡∏≠‡∏á Import FINAL_K_RERANKED ‡∏à‡∏≤‡∏Å core.vectorstore
    from core.vectorstore import load_all_vectorstores, FINAL_K_RERANKED 
    # üö® FIX: ‡∏ï‡πâ‡∏≠‡∏á Import summarize_context_with_llm ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà
    from core.retrieval_utils import evaluate_with_llm, retrieve_context_with_filter, set_mock_control_mode, summarize_context_with_llm

    from core.assessment_schema import EvidenceSummary
    from core.action_plan_schema import ActionPlanActions

except ImportError as e:
    # ‡∏´‡∏≤‡∏Å‡∏£‡∏±‡∏ô‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà root project directory ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö path ‡πÄ‡∏û‡∏¥‡πà‡∏°
    print(f"FATAL ERROR: Failed to import required modules. Check sys.path and file structure. Error: {e}", file=sys.stderr)
    sys.exit(1)


logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


# Level Fractions ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Linear Interpolation
DEFAULT_LEVEL_FRACTIONS = {
    "1": 0.16667,
    "2": 0.33333,
    "3": 0.50000,
    "4": 0.66667,
    "5": 0.83333,
    "MAX_LEVEL_FRACTION": 1.00000, 
    "0": 0.0 
}

# Default fallback rubric structure
DEFAULT_RUBRIC_STRUCTURE = {
    "Default_Maturity_Rubric": { 
        "levels": [
            {
                "level": i,
                "name": "Default",
                "criteria": {
                    f"subtopic_{j}": f"Default criteria for level {i}, subtopic {j}"
                    for j in range(1, 4)
                }
            }
            for i in range(1, 6)
        ]
    }
}


class EnablerAssessment:
    """
    Automated Enabler Maturity Assessment System
    ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ß‡∏∏‡∏í‡∏¥‡∏†‡∏≤‡∏ß‡∏∞‡∏Ç‡∏≠‡∏á Enabler ‡πÉ‡∏î‡πÜ (KM, SCM, DT ‡∏Ø‡∏•‡∏Ø)
    """

    BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "evidence_checklist"))
    
    # Context length limit ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ Statement
    MAX_CONTEXT_LENGTH = 2500 


    def __init__(self,
                 enabler_abbr: str, 
                 evidence_data: Optional[List] = None,
                 rubric_data: Optional[Dict] = None,
                 level_fractions: Optional[Dict] = None,
                 evidence_mapping_data: Optional[Dict] = None, # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Mapping File
                 vectorstore_retriever=None,
                 # Argument ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏° Filter (‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÉ‡∏ô _get_metadata_filter ‡πÅ‡∏ï‡πà‡∏Ñ‡∏á‡πÑ‡∏ß‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô)
                 use_retrieval_filter: bool = False, 
                 target_sub_id: Optional[str] = None, # e.g., '1.1'
                 # Mock/Control LLM Function Override
                 mock_llm_eval_func=None): # Default: core.retrieval_utils.evaluate_with_llm
        
        self.enabler_abbr = enabler_abbr.lower()
        self.enabler_rubric_key = f"{self.enabler_abbr.upper()}_Maturity_Rubric"
        self.vectorstore_retriever = vectorstore_retriever
        
        # DYNAMIC FILENAMES
        self.EVIDENCE_FILE = os.path.join(self.BASE_DIR, f"{self.enabler_abbr}_evidence_statements_checklist.json")
        self.RUBRIC_FILE = os.path.join(self.BASE_DIR, f"{self.enabler_abbr}_rating_criteria_rubric.json")
        self.LEVEL_FRACTIONS_FILE = os.path.join(self.BASE_DIR, f"{self.enabler_abbr}_scoring_level_fractions.json")
        self.MAPPING_FILE = os.path.join(self.BASE_DIR, f"{self.enabler_abbr}_evidence_mapping.json")

        # LOAD DATA
        self.evidence_data = evidence_data or self._load_json_fallback(self.EVIDENCE_FILE, default=[])
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ rubric_data ‡∏°‡∏µ‡∏Ñ‡∏µ‡∏¢‡πå Enabler ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏à‡∏∞‡πÉ‡∏ä‡πâ Default Fallback
        default_rubric = {self.enabler_rubric_key: DEFAULT_RUBRIC_STRUCTURE["Default_Maturity_Rubric"]}
        self.rubric_data = rubric_data or self._load_json_fallback(self.RUBRIC_FILE, default=default_rubric)
        self.level_fractions = level_fractions or self._load_json_fallback(self.LEVEL_FRACTIONS_FILE, default=DEFAULT_LEVEL_FRACTIONS)
        self.evidence_mapping_data = evidence_mapping_data or self._load_json_fallback(self.MAPPING_FILE, default={})
        
        # ‡πÄ‡∏Å‡πá‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ Filter
        self.use_retrieval_filter = use_retrieval_filter
        self.target_sub_id = target_sub_id
        
        # ‡πÄ‡∏Å‡πá‡∏ö Mock Function
        self.mock_llm_eval_func = mock_llm_eval_func 

        self.raw_llm_results: List[Dict] = []
        self.final_subcriteria_results: List[Dict] = []
        
        self.global_rubric_map: Dict[int, Dict[str, str]] = self._prepare_rubric_map()


    def _load_json_fallback(self, path: str, default: Any):
        """Loads JSON ‡∏´‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏°‡πà‡∏°‡∏µ ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ default"""
        if not os.path.isfile(path):
            logger.warning(f"[Warning] JSON file not found for enabler '{self.enabler_abbr}': {path}, using default fallback.")
            return default
        try:
            with open(path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"Failed to load JSON {path}: {e}")
            return default

    def _prepare_rubric_map(self) -> Dict[int, Dict[str, str]]:
        """
        ‡πÅ‡∏õ‡∏•‡∏á Global Rubric ‡πÄ‡∏õ‡πá‡∏ô Map ‡∏á‡πà‡∏≤‡∏¢‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Ñ‡∏µ‡∏¢‡πå Enabler ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
        """
        rubric_map = {}
        
        rubric_data_entry = self.rubric_data.get(self.enabler_rubric_key)
        
        if self.rubric_data and isinstance(rubric_data_entry, dict):
            for level_entry in rubric_data_entry.get("levels", []):
                level_num = level_entry.get("level")
                if level_num:
                    rubric_map[level_num] = level_entry.get("criteria", {})
        else:
             # ‡∏Å‡∏≤‡∏£‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏≠‡∏≤‡∏à‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏î‡πâ‡∏´‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå rubric_data ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏µ‡∏¢‡πå Enabler ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ‡πÅ‡∏ï‡πà‡πÉ‡∏ä‡πâ Default Rubric ‡πÅ‡∏ó‡∏ô
             logger.warning(f"‚ö†Ô∏è Rubric key '{self.enabler_rubric_key}' not found in loaded rubric data. Using default/fallback structure.")
             
             # ‡∏™‡∏£‡πâ‡∏≤‡∏á Rubric Map ‡∏à‡∏≤‡∏Å DEFAULT_RUBRIC_STRUCTURE ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
             if self.enabler_rubric_key not in self.rubric_data:
                # ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ default key ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏µ‡∏¢‡πå‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡∏≠‡∏á Enabler
                default_data = DEFAULT_RUBRIC_STRUCTURE["Default_Maturity_Rubric"]
                for level_entry in default_data.get("levels", []):
                     level_num = level_entry.get("level")
                     if level_num:
                         rubric_map[level_num] = level_entry.get("criteria", {})


        return rubric_map


    def _compute_subcriteria_score(self, level_pass_ratios: Dict[str, float], sub_criteria_weight: float) -> Dict[str, Any]:
        """
        ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏ï‡∏≤‡∏° Linear Interpolation Logic (‡∏Ñ‡∏á Logic Maturity Model ‡πÑ‡∏ß‡πâ)
        """
        highest_full_level = 0
        progress_score = 0.0
        
        # 1. ‡∏´‡∏≤ Highest Fully Passed Level (1.0 ratio) - Logic Maturity Model
        for level in range(1, 6):
            level_str = str(level)
            # üö® FIX 1: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Ratio ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Key String
            # ‡∏ñ‡πâ‡∏≤ Level ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏Ñ‡∏£‡∏ö 100%
            if level_pass_ratios.get(level_str, 0.0) < 1.0: 
                highest_full_level = level - 1 
                if highest_full_level < 0:
                    highest_full_level = 0
                break # ‡∏´‡∏¢‡∏∏‡∏î‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏à‡∏≠ Level ‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô
            else:
                highest_full_level = level # ‡∏ñ‡πâ‡∏≤ Level ‡∏ú‡πà‡∏≤‡∏ô‡∏Ñ‡∏£‡∏ö ‡πÉ‡∏´‡πâ‡∏ï‡∏±‡πâ‡∏á‡πÄ‡∏õ‡πá‡∏ô Level ‡∏ô‡∏µ‡πâ‡πÅ‡∏•‡πâ‡∏ß‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏ï‡πà‡∏≠
        
        # 2. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Progress Score (Linear Interpolation)
        if highest_full_level == 5:
            progress_score = self.level_fractions.get("MAX_LEVEL_FRACTION", 1.0) * sub_criteria_weight
        else: # highest_full_level < 5
            # Level ‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡πÅ‡∏•‡πâ‡∏ß (‡∏ê‡∏≤‡∏ô)
            base_fraction = self.level_fractions.get(str(highest_full_level) if highest_full_level > 0 else "0", 0.0)
            
            # Level ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏°‡∏µ Gap ‡∏Ñ‡∏∑‡∏≠ Level ‡∏ï‡πà‡∏≠‡πÑ‡∏õ
            gap_level = highest_full_level + 1 
            gap_fraction = self.level_fractions.get(str(gap_level), 0.0)
            
            # üö® FIX 2: ‡∏î‡∏∂‡∏á progress_ratio ‡∏à‡∏≤‡∏Å Level ‡∏ó‡∏µ‡πà‡∏°‡∏µ Gap
            progress_ratio = level_pass_ratios.get(str(gap_level), 0.0)
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏à‡∏≤‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏∑‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏Ç‡∏≠‡∏á Level ‡∏ó‡∏µ‡πà‡∏°‡∏µ Gap
            fraction_increase = (gap_fraction - base_fraction) * progress_ratio
            total_fraction = base_fraction + fraction_increase
            progress_score = total_fraction * sub_criteria_weight
        
        # 3. ‡∏à‡∏±‡∏î Gap Analysis ‡πÅ‡∏•‡∏∞ Action Item (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô)
        action_item = ""
        development_gap = False
        target_gap_level = highest_full_level + 1
        
        if target_gap_level <= 5: 
             development_gap = True
             ratio = level_pass_ratios.get(str(target_gap_level), 0.0)
             
             # Action Item ‡∏à‡∏∞‡πÇ‡∏ü‡∏Å‡∏±‡∏™‡πÑ‡∏õ‡∏ó‡∏µ‡πà Gap ‡πÅ‡∏£‡∏Å‡∏™‡∏∏‡∏î
             action_item = f"‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏£‡∏•‡∏∏‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô Level {target_gap_level} ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á (Pass Ratio: {ratio})"
        
        # 4. Return Results
        return {
            "highest_full_level": highest_full_level,
            "progress_score": round(progress_score, 2),
            "development_gap": development_gap,
            "action_item": action_item,
            "weight": sub_criteria_weight
        }


    def _get_metadata_filter(self) -> Optional[Dict]:
        """
        ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏ñ‡∏π‡∏Å‡∏ó‡∏¥‡πâ‡∏á‡πÑ‡∏ß‡πâ‡πÅ‡∏ï‡πà‡∏à‡∏∞ return None ‡πÄ‡∏™‡∏°‡∏≠ ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ Filter Logic ‡∏ñ‡∏π‡∏Å‡∏¢‡πâ‡∏≤‡∏¢‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö document ID
        ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å Mapping File ‡πÉ‡∏ô _retrieve_context ‡πÅ‡∏ó‡∏ô
        """
        return None 


    def _retrieve_context(self, query: str, sub_criteria_id: str, level: int, mapping_data: Optional[Dict] = None, statement_number: int = 0) -> Dict[str, Any]:
        """
        ‡∏î‡∏∂‡∏á Context ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Filter ‡∏à‡∏≤‡∏Å evidence mapping ‡πÅ‡∏•‡∏∞ Metadata Filter ‡∏ï‡∏≤‡∏° Sub ID ‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏°‡∏≤
        """
        effective_mapping_data = mapping_data if mapping_data is not None else self.evidence_mapping_data
        
        if not self.vectorstore_retriever and mapping_data is None:
            logger.warning("Vectorstore retriever is None and not in Mock Mode. Skipping RAG retrieval.")
            # Return empty structure if RAG is skipped
            return {"top_evidences": []} 

        # 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏µ‡∏¢‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Mapping: "1.1_L1", "1.1_L2", ...
        mapping_key = f"{sub_criteria_id}_L{level}"
        
        # 2. ‡∏î‡∏∂‡∏á Filter IDs (‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å Clean ‡πÅ‡∏•‡πâ‡∏ß) ‡∏à‡∏≤‡∏Å effective_mapping_data
        filter_ids: List[str] = effective_mapping_data.get(mapping_key, {}).get("filter_ids", [])
        
        
        # --- LOGIC ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö REAL MODE (mapping_data is None) ---
        if mapping_data is None: 
            if not filter_ids:
                logger.warning(f"No filter IDs found for {mapping_key}. Retrieving context without doc_id restriction.")

            # 4. ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ RAG Retrieval (‡∏™‡πà‡∏á filter_ids ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å mapping ‡πÑ‡∏õ‡∏ó‡∏µ‡πà retrieve_context_with_filter)
            result = retrieve_context_with_filter(
                query=query, 
                retriever=self.vectorstore_retriever, 
                metadata_filter=filter_ids # ‡∏™‡πà‡∏á filter_ids (doc_id) ‡πÑ‡∏õ‡∏ó‡∏µ‡πà RAG
            )
            
            # 5. ‡∏™‡πà‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô Dictionary ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ
            return result

        # --- LOGIC ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö MOCK MODE ---
        # ‡∏ñ‡πâ‡∏≤‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô Mock Mode ‡πÅ‡∏ï‡πà‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å Patch ‡∏à‡∏∞ return ‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á
        return {"top_evidences": []} 


    def _process_subcriteria_results(self):
        """
        ‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå LLM ‡∏ï‡∏≤‡∏° Sub-Criteria ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ (‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç Key ‡πÉ‡∏ô Grouping)
        """
        grouped_results: Dict[str, Dict] = {}
        for r in self.raw_llm_results:
            # ‡πÉ‡∏ä‡πâ Sub-Criteria ID ‡πÄ‡∏õ‡πá‡∏ô Key ‡∏´‡∏•‡∏±‡∏Å‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°
            key = r['sub_criteria_id'] 
            if key not in grouped_results:
                enabler_data = next((e for e in self.evidence_data 
                                     if e.get("Sub_Criteria_ID") == r['sub_criteria_id']), {}) 
                
                grouped_results[key] = {
                    "enabler_id": self.enabler_abbr.upper(),
                    "sub_criteria_id": r['sub_criteria_id'],
                    "sub_criteria_name": enabler_data.get("Sub_Criteria_Name_TH", "N/A"),
                    "weight": enabler_data.get("Weight", 1.0),
                    "raw_llm_scores": [],
                    "level_pass_ratios": {}, 
                    "num_statements_per_level": {} 
                }
            grouped_results[key]["raw_llm_scores"].append(r)

        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Pass Ratio
        for key, data in grouped_results.items():
            level_statements: Dict[int, List[Dict]] = {} # üö® FIX 3: ‡πÄ‡∏Å‡πá‡∏ö‡∏ó‡∏±‡πâ‡∏á Dict ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á pass_status
            for r in data["raw_llm_scores"]:
                level = r["level"]
                if level not in level_statements:
                    level_statements[level] = []
                level_statements[level].append(r) # üö® FIX 4: ‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå LLM ‡∏ó‡∏±‡πâ‡∏á‡∏Å‡πâ‡∏≠‡∏ô
            
            for level, results in level_statements.items():
                total_statements = len(results)
                
                # üö® FIX 5: ‡∏ô‡∏±‡∏ö '‡∏ú‡πà‡∏≤‡∏ô' ‡∏à‡∏≤‡∏Å 'llm_score' ‡∏ó‡∏µ‡πà‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏ñ‡∏π‡∏Å‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô 1 ‡πÄ‡∏°‡∏∑‡πà‡∏≠ 'pass_status' ‡πÄ‡∏õ‡πá‡∏ô True
                # (‡∏ï‡∏≤‡∏° logic ‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÉ‡∏ô retrieval_utils.py)
                passed_statements = sum(r.get("llm_score", 0) for r in results) 
                
                level_str = str(level)
                
                data["level_pass_ratios"][level_str] = round(passed_statements / total_statements, 3)
                data["num_statements_per_level"][level_str] = total_statements

        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Final Score (Detailed Score)
        self.final_subcriteria_results = []
        for key, data in grouped_results.items():
            scoring_results = self._compute_subcriteria_score(
                level_pass_ratios=data["level_pass_ratios"],
                sub_criteria_weight=data["weight"]
            )
            data.update(scoring_results)
            self.final_subcriteria_results.append(data)


    def run_assessment(self) -> List[Dict]:
        """
        Run assessment across all levels & subtopics
        """
        self.raw_llm_results = [] 
        self.final_subcriteria_results = []
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£ Patch ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á Mapping Data ‡πÑ‡∏õ‡πÉ‡∏´‡πâ Mock)
        is_mock_mode = getattr(self._retrieve_context, '__name__', 'N/A') == 'retrieve_context_MOCK'
        mapping_data_for_mock = self.evidence_mapping_data if is_mock_mode else None
        
        # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ä‡πâ LLM Evaluation Function (Mock ‡∏´‡∏£‡∏∑‡∏≠ Real)
        llm_eval_func = self.mock_llm_eval_func if self.mock_llm_eval_func else evaluate_with_llm

        
        for enabler in self.evidence_data:
            enabler_id = enabler.get("Enabler_ID")
            sub_criteria_id = enabler.get("Sub_Criteria_ID")
            sub_criteria_name = enabler.get("Sub_Criteria_Name_TH", "N/A")

            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Sub ID ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            if self.target_sub_id and self.target_sub_id != sub_criteria_id:
                continue

            for level in range(1, 6):
                level_key = f"Level_{level}_Statements"
                statements: List[str] = enabler.get(level_key, [])
                
                if not statements:
                    continue 
                
                rubric_criteria = self.global_rubric_map.get(level, {})
                
                for i, statement in enumerate(statements):
                    # üí° NOTE: subtopic key ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏∂‡∏á‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏à‡∏≤‡∏Å rubric_criteria
                    subtopic_key = f"subtopic_{i+1}"
                    # ‡∏î‡∏∂‡∏á‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô: ‡πÉ‡∏ä‡πâ criteria ‡∏ó‡∏µ‡πà i+1 ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ default fallback
                    standard = rubric_criteria.get(subtopic_key, f"Default standard L{level} S{i+1}")
                    
                    # üö® FIX 1: ‡∏™‡∏£‡πâ‡∏≤‡∏á Query String ‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏° Statement ‡πÅ‡∏•‡∏∞ Sub Criteria Name ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ RAG ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô
                    query_string = f"{statement} ({sub_criteria_name})"
                    
                    # 1. ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å retrieval_result
                    retrieval_result = self._retrieve_context(
                        query=query_string, # ‡∏™‡πà‡∏á Query String ‡πÉ‡∏´‡∏°‡πà
                        sub_criteria_id=sub_criteria_id, 
                        level=level,
                        mapping_data=mapping_data_for_mock, 
                        statement_number=i + 1
                    )
                    
                    # 2. ‡∏Ç‡∏¢‡∏≤‡∏¢ Context String ‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡∏£‡∏ß‡∏° Content ‡∏à‡∏≤‡∏Å Top N Reranked Documents
                    context_list = []
                    context_length = 0
                    # üí° NEW: Initialize list to store source/location data
                    retrieved_sources_list = [] 
                    context = "" # Initialize context string
                    
                    if isinstance(retrieval_result, dict):
                        top_evidence = retrieval_result.get("top_evidences", [])
                        
                        # ‡πÉ‡∏ä‡πâ FINAL_K_RERANKED ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏£‡∏ß‡∏°
                        for doc in top_evidence[:FINAL_K_RERANKED]: 
                            doc_content = doc.get("content", "")
                            
                            # üí° NEW: Extract Source Information
                            source_name = doc.get("source", "N/A (No Source Tag)")
                            # Assume 'page_number' is stored in metadata or directly. Use 'doc_id' as fallback.
                            location = doc.get("metadata", {}).get("page_number", doc.get("doc_id", "N/A"))
                            # Format location string
                            location_str = f"Page {location}" if isinstance(location, int) else location
                            
                            # üí° NEW: Store Source Data for traceability
                            # ‡πÉ‡∏ä‡πâ Doc ID ‡πÅ‡∏•‡∏∞ Page Number ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏∞‡∏ö‡∏∏‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
                            doc_id = doc.get("doc_id", "N/A")
                            retrieved_sources_list.append({
                                "source_name": source_name,
                                "doc_id": doc_id,
                                "location": location_str
                            })
                            
                            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô MAX_CONTEXT_LENGTH ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                            if context_length + len(doc_content) <= self.MAX_CONTEXT_LENGTH:
                                context_list.append(doc_content)
                                context_length += len(doc_content)
                            else:
                                # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏Å‡∏¥‡∏ô ‡πÉ‡∏´‡πâ‡∏ï‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏Ç‡∏≠‡∏á Content ‡∏ä‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏≠‡∏≠‡∏Å
                                remaining_len = self.MAX_CONTEXT_LENGTH - context_length
                                if remaining_len > 0:
                                    context_list.append(doc_content[:remaining_len])
                                context_length = self.MAX_CONTEXT_LENGTH
                                break # ‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏£‡∏ß‡∏°‡πÄ‡∏°‡∏∑‡πà‡∏≠ Context ‡πÄ‡∏ï‡πá‡∏°
                                
                        # ‡∏£‡∏ß‡∏°‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡∏±‡∏î‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏ï‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
                        context = "\n---\n".join(context_list)
                    
                    # 3. Call the selected evaluation function
                    result = llm_eval_func(
                        statement=statement,
                        context=context, # ‡∏™‡πà‡∏á String Context ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏Ç‡∏¢‡∏≤‡∏¢
                        standard=standard,
                        #  üö® FIX 3: ‡∏™‡πà‡∏á level, sub_criteria_id ‡πÅ‡∏•‡∏∞ statement_number ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô kwargs
                        # level=level, 
                        # sub_criteria_id=sub_criteria_id,
                        # statement_number=i + 1
                    )
                    
                    # 4. Deduplicate sources before saving (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡πÅ‡∏™‡∏î‡∏á‡∏ã‡πâ‡∏≥‡πÉ‡∏ô‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô)
                    unique_sources = []
                    seen = set()
                    for src in retrieved_sources_list:
                        # ‡πÉ‡∏ä‡πâ doc_id ‡πÅ‡∏•‡∏∞ location ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏µ‡∏¢‡πå‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡πâ‡∏≥
                        # ‡πÉ‡∏ô‡∏ó‡∏≤‡∏á‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥: Location ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏õ‡πá‡∏ô Page Number/Section
                        key = (src['doc_id'], src['location']) 
                        if key not in seen:
                            seen.add(key)
                            unique_sources.append(src)
                    
            # 5. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
                    
                    # üö® FIX LOGIC: ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤ Context/Sources/Status ‡∏ï‡∏≤‡∏° Mode
                    if is_mock_mode:
                        # ‡πÉ‡∏ô Mock Mode: ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Context/Sources/Status/Score ‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏à‡∏≤‡∏Å Mock Function ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
                        final_score = result.get("llm_score", 0) # ‡πÉ‡∏ä‡πâ llm_score ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô 0/1
                        final_reason = result.get("reason", "")
                        final_sources = result.get("retrieved_sources_list", []) # üëà ‡πÉ‡∏ä‡πâ Mock Sources
                        final_context_snippet = result.get("context_retrieved_snippet", "") # üëà ‡πÉ‡∏ä‡πâ Mock Context
                        final_pass_status = result.get("pass_status", False)
                        final_status_th = result.get("status_th", "‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô")
                    else:
                        # ‡πÉ‡∏ô Real Mode: ‡πÉ‡∏ä‡πâ Context/Sources ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å RAG (Step 2)
                        final_score = result.get("score", 0) # ‡πÉ‡∏ä‡πâ score ‡∏à‡∏≤‡∏Å LLM ‡∏à‡∏£‡∏¥‡∏á
                        final_reason = result.get("reason", "")
                        final_sources = unique_sources # ‡∏à‡∏≤‡∏Å Step 4 (RAG)
                        final_context_snippet = context[:120] + "..." if context else ""
                        final_pass_status = final_score == 1
                        final_status_th = "‡∏ú‡πà‡∏≤‡∏ô" if final_pass_status else "‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô"


                    self.raw_llm_results.append({
                        "enabler_id": self.enabler_abbr.upper(),
                        "sub_criteria_id": sub_criteria_id,
                        "sub_criteria_name": sub_criteria_name, 
                        "level": level,
                        "statement_number": i + 1, 
                        "statement": statement,
                        "subtopic": subtopic_key,
                        "standard": standard,
                        # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ï‡∏≤‡∏° Mode
                        "llm_score": final_score, 
                        "reason": final_reason,
                        "retrieved_sources_list": final_sources, # üëà ‡πÉ‡∏ä‡πâ final_sources
                        "context_retrieved_snippet": final_context_snippet, # üëà ‡πÉ‡∏ä‡πâ final_context_snippet
                        "pass_status": final_pass_status,
                        "status_th": final_status_th,
                        "statement_id": "N/A" # ‡πÄ‡∏û‡∏¥‡πà‡∏° placeholder ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ID ‡∏ó‡∏µ‡πà‡∏Ç‡∏≤‡∏î‡πÑ‡∏õ
                    })
        
        self._process_subcriteria_results()
        
        return self.final_subcriteria_results
    
    # ----------------------------------------------------
    # üåü NEW FEATURE: Generate Evidence Summary
    # ----------------------------------------------------
    def generate_evidence_summary_for_level(self, sub_criteria_id: str, level: int) -> str:
        """
        ‡∏£‡∏ß‡∏°‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å Statement ‡πÉ‡∏ô Sub-Criteria/Level ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î ‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ LLM ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢
        """
        # 1. ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Statement Data
        enabler_data = next((e for e in self.evidence_data 
                             if e.get("Sub_Criteria_ID") == sub_criteria_id), None)
        
        if not enabler_data:
            logger.error(f"Sub-Criteria ID {sub_criteria_id} not found in evidence data.")
            return "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏¢‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î"

        level_key = f"Level_{level}_Statements"
        statements: List[str] = enabler_data.get(level_key, [])
        sub_criteria_name = enabler_data.get("Sub_Criteria_Name_TH", "N/A")

        if not statements:
            return f"‡πÑ‡∏°‡πà‡∏û‡∏ö Statements ‡πÉ‡∏ô Level {level} ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡∏ì‡∏ë‡πå {sub_criteria_id}"

        aggregated_context_list = []
        total_context_length = 0
        
        # 2. ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏î‡∏∂‡∏á Context ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏∏‡∏Å Statement ‡πÉ‡∏ô Level ‡∏ô‡∏±‡πâ‡∏ô (‡∏£‡∏ß‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)
        for i, statement in enumerate(statements):
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á Query ‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏° Statement ‡πÅ‡∏•‡∏∞ Sub Criteria Name
            query_string = f"{statement} ({sub_criteria_name})"
            
            # ‡∏î‡∏∂‡∏á Context ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Filter ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Sub-Criteria/Level ‡∏ô‡∏µ‡πâ
            retrieval_result = self._retrieve_context(
                query=query_string,
                sub_criteria_id=sub_criteria_id,
                level=level,
                statement_number=i + 1
            )
            
            if isinstance(retrieval_result, dict):
                top_evidence = retrieval_result.get("top_evidences", [])
                
                # ‡∏£‡∏ß‡∏° Context ‡∏à‡∏≤‡∏Å Top N evidences (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÉ‡∏ô run_assessment)
                for doc in top_evidence[:FINAL_K_RERANKED]: 
                    doc_content = doc.get("content", "")
                    
                    if total_context_length + len(doc_content) <= self.MAX_CONTEXT_LENGTH:
                        aggregated_context_list.append(doc_content)
                        total_context_length += len(doc_content)
                    else:
                        remaining_len = self.MAX_CONTEXT_LENGTH - total_context_length
                        if remaining_len > 0:
                            # ‡∏ï‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏Ç‡∏≠‡∏á Content ‡∏ä‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏≠‡∏≠‡∏Å
                            aggregated_context_list.append(doc_content[:remaining_len])
                        total_context_length = self.MAX_CONTEXT_LENGTH
                        break 
        
        if not aggregated_context_list:
            return f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÉ‡∏ô Vector Store ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡∏ì‡∏ë‡πå {sub_criteria_id} Level {level}"
        
        # ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏° Context ‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡πÑ‡∏î‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (‡πÉ‡∏ä‡πâ dict.fromkeys ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏ö‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô‡∏Å‡πà‡∏≠‡∏ô join)
        final_context = "\n---\n".join(list(dict.fromkeys(aggregated_context_list)))
        
        # 3. ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å LLM ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏™‡∏£‡∏∏‡∏õ
        try:
            summary_result = summarize_context_with_llm(
                context=final_context,
                sub_criteria_name=sub_criteria_name,
                level=level,
                sub_id=sub_criteria_id,
                schema=EvidenceSummary
            )
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö return type
            if isinstance(summary_result, dict):
                return summary_result.get("summary", "LLM ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÑ‡∏î‡πâ")
            elif isinstance(summary_result, str):
                return summary_result
            else:
                return "LLM return type ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á"
            
        except Exception as e:
            logger.error(f"Failed to generate summary with LLM: {e}")
            return f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ LLM ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {e}"
    # ----------------------------------------------------
    
    def summarize_results(self) -> Dict[str, Dict]:
        """
        ‡∏™‡∏£‡∏∏‡∏õ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°‡∏à‡∏≤‡∏Å final_subcriteria_results
        """
        
        if not self.final_subcriteria_results:
             return {
                 "Overall": {
                    "enabler": self.enabler_abbr.upper(),
                    "total_weighted_score": 0.0,
                    "total_possible_weight": 0.0,
                    "overall_progress_percent": 0.0,
                    "overall_maturity_score": 0.0
                 },
                 "SubCriteria_Breakdown": {}
             }
        
        total_weight = sum(r["weight"] for r in self.final_subcriteria_results)
        total_score = sum(r["progress_score"] for r in self.final_subcriteria_results)
        
        return {
            "Overall": {
                "enabler": self.enabler_abbr.upper(),
                "total_weighted_score": round(total_score, 2),
                "total_possible_weight": round(total_weight, 2),
                "overall_progress_percent": round((total_score / total_weight) * 100, 2) if total_weight > 0 else 0.0,
                "overall_maturity_score": round(total_score / total_weight, 2) if total_weight > 0 else 0.0
            },
            "SubCriteria_Breakdown": {
                r["sub_criteria_id"]: {
                    "name": r.get("sub_criteria_name", "N/A"),
                    "score": r["progress_score"],
                    "weight": r["weight"],
                    "highest_full_level": r["highest_full_level"],
                    "pass_ratios": r["level_pass_ratios"], 
                    "development_gap": r["development_gap"],
                    "action_item": r["action_item"]
                } for r in self.final_subcriteria_results
            }
        }
