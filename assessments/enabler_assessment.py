# enabler_assessment.py (Full Script)
# enabler_assessment.py (Full Revised Script)
import os
import json
import logging
import sys
import re 
from typing import List, Dict, Any, Optional, Union, Tuple
import time 

# --- PATH SETUP (Must be executed first for imports to work) ---
try:
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
    if project_root not in sys.path:
        sys.path.append(project_root)
    
    # üü¢ FIX: ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏î‡∏∂‡∏á FINAL_K_NON_RERANKED ‡∏à‡∏≤‡∏Å core.vectorstore
    from core.vectorstore import (
        load_all_vectorstores, 
        FINAL_K_RERANKED,
        FINAL_K_NON_RERANKED,
        # üü¢ FIX: ‡πÄ‡∏û‡∏¥‡πà‡∏° get_vectorstore_manager ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏ñ‡∏π‡∏Å‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å
        get_vectorstore_manager 
    )
    
    from core.retrieval_utils import (
        evaluate_with_llm, 
        retrieve_context_with_filter, 
        set_mock_control_mode, 
        summarize_context_with_llm,
    )
    from core.assessment_schema import EvidenceSummary
    from core.action_plan_schema import ActionPlanActions

except ImportError as e:
    print(f"FATAL ERROR: Failed to import required modules. Check sys.path and file structure. Error: {e}", file=sys.stderr)
    sys.exit(1)


logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


# Level Fractions ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Linear Interpolation
DEFAULT_LEVEL_FRACTIONS = {
    "1": 0.16667,
    "2": 0.33333,
    "3": 0.50000,
    "4": 0.66667,
    "5": 0.83333,
    "MAX_LEVEL_FRACTION": 1.00000, 
    "0": 0.0 
}

# Default fallback rubric structure
DEFAULT_RUBRIC_STRUCTURE = {
    "Default_Maturity_Rubric": { 
        "levels": [
            {
                "level": i,
                "name": "Default",
                "criteria": {
                    f"subtopic_{j}": f"Default criteria for level {i}, subtopic {j}"
                    for j in range(1, 4)
                }
            }
            for i in range(1, 6)
        ]
    }
}

import os
import json
import logging
from typing import List, Dict, Any, Optional, Union, Tuple
import re

# NOTE: ‡∏ï‡πâ‡∏≠‡∏á Import ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏†‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ! (‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡∏ñ‡∏π‡∏Å Import ‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ö‡∏ô‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå)
# from core.assessment_schema import EvidenceSummary, StatementAssessment, DEFAULT_RUBRIC_STRUCTURE
# from core.retrieval_utils import retrieve_context_with_filter
# from core.llm_utils import summarize_context_with_llm, evaluate_with_llm

# ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ Import ‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏® Default ‡πÑ‡∏ß‡πâ:
DEFAULT_LEVEL_FRACTIONS = {"0": 0.0, "1": 0.1, "2": 0.3, "3": 0.6, "4": 0.85, "5": 1.0}

# -------------------- EnablerAssessment Class --------------------

class EnablerAssessment:

    def __init__(self,
                 enabler_abbr: str, 
                 evidence_data: Optional[List] = None,
                 rubric_data: Optional[Dict] = None,
                 level_fractions: Optional[Dict] = None,
                 evidence_mapping_data: Optional[Dict] = None, # üü¢ FIX 1: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠ Argument
                 vectorstore_retriever=None,
                 use_mapping_filter: bool = True,
                 target_sub_id: Optional[str] = None,
                 mock_llm_eval_func=None,
                 mock_llm_summarize_func=None,
                 mock_llm_action_plan_func=None,
                 disable_semantic_filter: bool = False,
                 allow_fallback: bool = False):
        
        # --- ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤ K-Values ‡πÅ‡∏•‡∏∞ Context Length ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏Ñ‡∏•‡∏≤‡∏™ ---
        self.MAX_CONTEXT_LENGTH = 35000 
        self.FINAL_K_RERANKED = 3 # üü¢ FIX 2: ‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡πÄ‡∏õ‡πá‡∏ô Attribute ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏Ñ‡∏•‡∏≤‡∏™
        self.FINAL_K_NON_RERANKED = 5 # üü¢ FIX 2: ‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡πÄ‡∏õ‡πá‡∏ô Attribute ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏Ñ‡∏•‡∏≤‡∏™
        self.enabler_rubric_key = f"{enabler_abbr.upper()}_Maturity_Rubric" # ‡πÉ‡∏ä‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏Ñ‡∏µ‡∏¢‡πå‡πÉ‡∏ô Rubric

        # --- ‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤ Attributes (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å) ---
        
        # 1. Attributes ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        self.enabler_abbr = enabler_abbr.upper()
        
        # üü¢ FIX 3: ‡πÉ‡∏ä‡πâ self._load_json_fallback ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏≤‡∏Å Argument ‡πÄ‡∏õ‡πá‡∏ô None
        self.BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "evidence_checklist"))
        self.EVIDENCE_FILE = os.path.join(self.BASE_DIR, f"{enabler_abbr.lower()}_evidence_statements_checklist.json")
        self.RUBRIC_FILE = os.path.join(self.BASE_DIR, f"{enabler_abbr.lower()}_rating_criteria_rubric.json")
        self.LEVEL_FRACTIONS_FILE = os.path.join(self.BASE_DIR, f"{enabler_abbr.lower()}_scoring_level_fractions.json")
        self.MAPPING_FILE = os.path.join(self.BASE_DIR, f"{enabler_abbr.lower()}_evidence_mapping_new.json")

        self.evidence_data = evidence_data or self._load_json_fallback(self.EVIDENCE_FILE, default=[])
        default_rubric = {self.enabler_rubric_key: {"levels": []}} # Placeholder default
        self.rubric_data = rubric_data or self._load_json_fallback(self.RUBRIC_FILE, default=default_rubric)
        self.level_fractions = level_fractions or self._load_json_fallback(self.LEVEL_FRACTIONS_FILE, default=DEFAULT_LEVEL_FRACTIONS)
        self.evidence_mapping_data = evidence_mapping_data or self._load_json_fallback(self.MAPPING_FILE, default={}) # üü¢ FIX 4: ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠ Attribute ‡πÉ‡∏´‡∏°‡πà
        
        self.vectorstore_retriever = vectorstore_retriever
        
        # 2. Attributes ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Filter ‡πÅ‡∏•‡∏∞ Control
        self.use_mapping_filter = use_mapping_filter
        self.target_sub_id = target_sub_id
        
        self.disable_semantic_filter = disable_semantic_filter 
        self.allow_fallback = allow_fallback 
        
        # 3. Attributes ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Mocking
        self.mock_llm_eval_func = mock_llm_eval_func
        self.mock_llm_summarize_func = mock_llm_summarize_func
        self.mock_llm_action_plan_func = mock_llm_action_plan_func

        self.raw_llm_results: List[Dict] = []
        self.final_subcriteria_results: List[Dict] = []
        
        self.global_rubric_map: Dict[int, Dict[str, str]] = self._prepare_rubric_map()


    def _load_json_fallback(self, path: str, default: Any):
        """Loads JSON ‡∏´‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏°‡πà‡∏°‡∏µ ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ default"""
        if not os.path.isfile(path):
            # logger.warning(f"[Warning] JSON file not found for enabler '{self.enabler_abbr}': {path}, using default fallback.")
            return default
        try:
            with open(path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            # logger.error(f"Failed to load JSON {path}: {e}")
            return default


    def _prepare_rubric_map(self) -> Dict[int, Dict[str, str]]:
        """
        ‡πÅ‡∏õ‡∏•‡∏á Global Rubric ‡πÄ‡∏õ‡πá‡∏ô Map ‡∏á‡πà‡∏≤‡∏¢‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Ñ‡∏µ‡∏¢‡πå Enabler ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
        """
        rubric_map = {}
        
        if self.rubric_data is None:
            logger.warning("‚ö†Ô∏è Rubric data is None. Cannot prepare rubric map. Using default/fallback structure.")
            
        rubric_data_entry = self.rubric_data.get(self.enabler_rubric_key)
        
        if isinstance(rubric_data_entry, dict) and 'levels' in rubric_data_entry:
            for level_entry in rubric_data_entry.get("levels", []):
                level_num = level_entry.get("level")
                if level_num:
                    rubric_map[level_num] = level_entry.get("criteria", {})
        else:
            logger.warning(f"‚ö†Ô∏è Rubric key '{self.enabler_rubric_key}' not found or is invalid in loaded rubric data. Falling back to internal defaults.")
            
            # ‡πÉ‡∏ä‡πâ DEFAULT_LEVEL_FRACTIONS ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Rubric Map ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô‡πÅ‡∏ö‡∏ö Mock/Random
            default_levels = [int(lvl) for lvl in self.level_fractions.keys() if lvl.isdigit()]
            for level in default_levels:
                rubric_map[level] = {f"subtopic_{i+1}": f"Default standard L{level} S{i+1}" for i in range(5)} # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡∏°‡∏µ 5 statement ‡∏ï‡πà‡∏≠ level

        return rubric_map

    def _compute_subcriteria_score(self, level_pass_ratios: Dict[str, float], sub_criteria_weight: float) -> Dict[str, Any]:
        """
        ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏ï‡∏≤‡∏° Linear Interpolation Logic
        """
        highest_full_level = 0
        progress_score = 0.0
        
        # 1. ‡∏´‡∏≤ Highest Fully Passed Level (1.0 ratio)
        for level in range(1, 6):
            level_str = str(level)
            if level_pass_ratios.get(level_str, 0.0) < 1.0: 
                highest_full_level = level - 1 
                if highest_full_level < 0:
                    highest_full_level = 0
                break 
            else:
                highest_full_level = level 
        
        # 2. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Progress Score (Linear Interpolation)
        # üü¢ FIX 5: ‡πÉ‡∏ä‡πâ self.level_fractions ‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß
        max_fraction = self.level_fractions.get("MAX_LEVEL_FRACTION", 1.0)
        
        if highest_full_level == 5:
            progress_score = max_fraction * sub_criteria_weight
        else:
            base_fraction = self.level_fractions.get(str(highest_full_level) if highest_full_level > 0 else "0", 0.0)
            gap_level = highest_full_level + 1 
            gap_fraction = self.level_fractions.get(str(gap_level), 0.0)
            progress_ratio = level_pass_ratios.get(str(gap_level), 0.0)
            fraction_increase = (gap_fraction - base_fraction) * progress_ratio
            total_fraction = base_fraction + fraction_increase
            progress_score = total_fraction * sub_criteria_weight
        
        # 3. ‡∏à‡∏±‡∏î Gap Analysis ‡πÅ‡∏•‡∏∞ Action Item
        action_item = ""
        development_gap = False
        target_gap_level = highest_full_level + 1
        
        if target_gap_level <= 5: 
             development_gap = True
             ratio = level_pass_ratios.get(str(target_gap_level), 0.0)
             action_item = f"‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏£‡∏•‡∏∏‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô Level {target_gap_level} ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á (Pass Ratio: {ratio})"
        
        # 4. Return Results
        return {
            "highest_full_level": highest_full_level,
            "progress_score": round(progress_score, 2),
            "development_gap": development_gap,
            "action_item": action_item,
            "weight": sub_criteria_weight
        }

    def _get_collection_name(self) -> str:
        """
        Constructs the specific collection name based on the enabler abbreviation.
        Format: evidence_<ENABLER_ABBR_UPPER> (e.g., km -> evidence_KM)
        """
        return f"evidence_{self.enabler_abbr.lower()}" # üü¢ FIX: ‡πÉ‡∏ä‡πâ Lowercase ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô retrieval_utils

    def _get_doc_uuid_filter(self, sub_id: str, level: int) -> Optional[List[str]]:
        """
        Generates a list of document UUIDs to filter the RAG retrieval.
        Uses the pre-mapped JSON data (self.evidence_mapping_data) based on sub_id and level.
        """
        if not self.evidence_mapping_data: # üü¢ FIX 6: ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠ Attribute ‡πÉ‡∏´‡∏°‡πà
            logger.warning("Evidence mapping data is not loaded. Returning None filter.")
            return None

        # Key format: "1.1_L1", "2.1_L3"
        key = f"{sub_id}_L{level}"

        if key not in self.evidence_mapping_data: # üü¢ FIX 7: ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠ Attribute ‡πÉ‡∏´‡∏°‡πà
            logger.warning(f"No evidence mapping found for key: {key}. Returning None filter.")
            return None

        try:
            key_data = self.evidence_mapping_data[key] # üü¢ FIX 8: ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠ Attribute ‡πÉ‡∏´‡∏°‡πà
            
            if 'evidences' in key_data and isinstance(key_data['evidences'], list):
                doc_uuids = [
                    evidence.get('doc_id') 
                    for evidence in key_data['evidences'] 
                    if isinstance(evidence, dict) and evidence.get('doc_id')
                ]
                
                if doc_uuids:
                    logger.info(f"Loaded {len(doc_uuids)} UUIDs for RAG filter key: {key}")
                    return doc_uuids
                else:
                    logger.warning(f"Evidence list for {key} is empty or contains no valid doc_id. Returning None filter.")
                    return None
            else:
                logger.warning(f"Mapping data for {key} is missing 'evidences' list. Returning None filter.")
                return None

        except Exception as e:
            logger.error(f"Error parsing evidence mapping for key {key}: {e}", exc_info=True)
            return None


    def _get_level_constraint_prompt(self, level: int) -> str:
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á Prompt Constraint ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Å‡∏±‡∏ö‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ß‡∏∏‡∏í‡∏¥‡∏†‡∏≤‡∏ß‡∏∞‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô
        """
        # ... (‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°) ...
        # ‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£: ‡∏´‡πâ‡∏≤‡∏°‡∏î‡∏∂‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô
        if level == 1:
            return "‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö '‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô', '‡∏Å‡∏≤‡∏£‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà', ‡∏´‡∏£‡∏∑‡∏≠ '‡∏Å‡∏≤‡∏£‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô' ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö '‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á', '‡∏Å‡∏≤‡∏£‡∏ö‡∏π‡∏£‡∏ì‡∏≤‡∏Å‡∏≤‡∏£', '‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°', ‡∏´‡∏£‡∏∑‡∏≠ '‡∏Å‡∏≤‡∏£‡∏ß‡∏±‡∏î‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß' (L1-Filter)"
        elif level == 2:
            return "‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö '‡∏Å‡∏≤‡∏£‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥', '‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô', ‡∏´‡∏£‡∏∑‡∏≠ '‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô' ‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö '‡∏Å‡∏≤‡∏£‡∏ö‡∏π‡∏£‡∏ì‡∏≤‡∏Å‡∏≤‡∏£', '‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°', ‡∏´‡∏£‡∏∑‡∏≠ '‡∏Å‡∏≤‡∏£‡∏ß‡∏±‡∏î‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß' (L2-Filter)"
        elif level == 3:
            return "‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏£‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö '‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°', '‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏Å‡∏±‡∏ö‡∏î‡∏π‡πÅ‡∏•', ‡∏´‡∏£‡∏∑‡∏≠ '‡∏Å‡∏≤‡∏£‡∏ß‡∏±‡∏î‡∏ú‡∏•‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô' ‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö '‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°', ‡∏´‡∏£‡∏∑‡∏≠ '‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡πà‡∏≤‡∏ó‡∏≤‡∏á‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß' (L3-Filter)"
        elif level == 4:
            return "‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ñ‡∏∂‡∏á '‡∏Å‡∏≤‡∏£‡∏ö‡∏π‡∏£‡∏ì‡∏≤‡∏Å‡∏≤‡∏£' ‡∏´‡∏£‡∏∑‡∏≠ '‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á' ‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏û‡∏¥‡∏™‡∏π‡∏à‡∏ô‡πå '‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡πà‡∏≤‡∏ó‡∏≤‡∏á‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß' (L4-Filter)"
        elif level == 5:
            return "‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ñ‡∏∂‡∏á '‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°', '‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡πà‡∏≤‡∏ó‡∏≤‡∏á‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à', ‡∏´‡∏£‡∏∑‡∏≠ '‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß' ‡πÇ‡∏î‡∏¢‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô (L5-Focus)"
        else:
            return "‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£"


    def _retrieve_context(self, query: str, sub_criteria_id: str, level: int, mapping_data: Optional[Dict] = None, statement_number: int = 0) -> Dict[str, Any]:
        """
        ‡∏î‡∏∂‡∏á Context ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Filter ‡∏à‡∏≤‡∏Å evidence mapping ‡πÅ‡∏•‡∏∞ Level Constraint Query
        """
        
        # 1. ‡∏î‡∏∂‡∏á Filter IDs (64-char Stable UUIDs)
        filter_ids_64_char: List[str] = []

        if self.use_mapping_filter: 
            # self._get_doc_uuid_filter ‡∏î‡∏∂‡∏á 64-char UUIDs ‡∏à‡∏≤‡∏Å JSON Mapping
            filter_ids_64_char = self._get_doc_uuid_filter(sub_criteria_id, level) or []
        
        # üéØ FIX: ‡πÅ‡∏õ‡∏•‡∏á 64-char Stable UUIDs ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô 34-char Ref IDs ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Hard Filter
        filter_ids: List[str] = []
        if filter_ids_64_char:
            filter_ids = self._map_64_to_34_ids(filter_ids_64_char) # ‚¨ÖÔ∏è ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà
        
        # 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á Constrained Query ‡πÄ‡∏™‡∏°‡∏≠
        constraint_instruction = self._get_level_constraint_prompt(level)
        effective_query = f"{query}. {constraint_instruction}"
        
        semantic_filter_status = "Disabled" if self.disable_semantic_filter else "Enabled"
        hard_filter_status = "Enabled" if filter_ids else "Disabled (Flexible Search)" # ‡πÉ‡∏ä‡πâ filter_ids ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏õ‡∏•‡∏á‡πÅ‡∏•‡πâ‡∏ß
        logger.info(f"üîë Constrained Query (L{level}, Hard Filter {hard_filter_status}, Semantic Filter {semantic_filter_status}): {effective_query}")
        
        # --- LOGIC ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö REAL MODE (mapping_data is None) ---
        if mapping_data is None: 
            # 3. ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ RAG Retrieval ‡∏î‡πâ‡∏ß‡∏¢‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ ID 34-char
            result = retrieve_context_with_filter(
                query=effective_query, 
                doc_type="evidence", 
                enabler=self.enabler_abbr, 
                stable_doc_ids=filter_ids, # ‚¨ÖÔ∏è ‡∏™‡πà‡∏á ID 34-char ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
                disable_semantic_filter=self.disable_semantic_filter,
                allow_fallback=self.allow_fallback,
                # retriever_instance=self.vectorstore_retriever 
            )
            return result

        # --- LOGIC ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö MOCK MODE ---
        return {"top_evidences": []}

    
    def _map_64_to_34_ids(self, uuids_64: List[str]) -> List[str]:
        """
        [FIX] ‡πÅ‡∏õ‡∏•‡∏á 64-char Stable UUIDs (‡∏à‡∏≤‡∏Å mapping file) ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô 34-char Ref IDs 
        ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ VectorStoreManager.get_id_mapping_from_vectorstore
        """
        if not uuids_64:
            return []
            
        try:
            manager = get_vectorstore_manager() # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤ Import ‡πÅ‡∏•‡πâ‡∏ß
            
            mapping_dict: Dict[str, str] = manager.get_id_mapping_from_vectorstore(
                uuids_64, 
                doc_type="evidence", 
                enabler=self.enabler_abbr
            )
            
            ref_ids_34 = [
                # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ 34-char Ref ID ‡∏à‡∏≤‡∏Å 64-char UUID ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å JSON 
                mapping_dict.get(uuid_64, "").lower() 
                for uuid_64 in uuids_64
            ]
            
            # ‡∏Å‡∏£‡∏≠‡∏á‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á
            valid_ref_ids = [ref_id for ref_id in ref_ids_34 if ref_id]
            
            logger.info(f"‚úÖ Mapped {len(valid_ref_ids)}/{len(uuids_64)} Stable UUIDs to Ref IDs for Hard Filter.")
            return valid_ref_ids
            
        except Exception as e:
            logger.error(f"‚ùå Failed to map 64-char UUIDs to 34-char Ref IDs. Error: {e}", exc_info=True)
            return []
        
    def _process_subcriteria_results(self):
        """
        ‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå LLM ‡∏ï‡∏≤‡∏° Sub-Criteria ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ 
        """
        # ... (‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°) ...
        grouped_results: Dict[str, Dict] = {}
        for r in self.raw_llm_results:
            key = r['sub_criteria_id'] 
            if key not in grouped_results:
                enabler_data = next((e for e in self.evidence_data 
                                     if e.get("Sub_Criteria_ID") == r['sub_criteria_id']), {}) 
                
                grouped_results[key] = {
                    "enabler_id": self.enabler_abbr.upper(),
                    "sub_criteria_id": r['sub_criteria_id'],
                    "sub_criteria_name": enabler_data.get("Sub_Criteria_Name_TH", "N/A"),
                    "weight": enabler_data.get("Weight", 1.0),
                    "raw_llm_scores": [],
                    "level_pass_ratios": {}, 
                    "num_statements_per_level": {} 
                }
            grouped_results[key]["raw_llm_scores"].append(r)

        for key, data in grouped_results.items():
            level_statements: Dict[int, List[Dict]] = {} 
            for r in data["raw_llm_scores"]:
                level = r["level"]
                if level not in level_statements:
                    level_statements[level] = []
                level_statements[level].append(r) 
            
            for level, results in level_statements.items():
                total_statements = len(results)
                
                passed_statements = sum(r.get("llm_score", 0) for r in results) 
                
                level_str = str(level)
                
                data["level_pass_ratios"][level_str] = round(passed_statements / total_statements, 3)
                data["num_statements_per_level"][level_str] = total_statements

        self.final_subcriteria_results = []
        for key, data in grouped_results.items():
            scoring_results = self._compute_subcriteria_score(
                level_pass_ratios=data["level_pass_ratios"],
                sub_criteria_weight=data["weight"]
            )
            data.update(scoring_results)
            self.final_subcriteria_results.append(data)


    def run_assessment(self, target_doc_ids_or_filter_status: Union[List[str], str] = 'none') -> Tuple[Dict[str, Any], Dict[str, Any]]:
        """
        Run assessment across all levels & subtopics
        """
        self.raw_llm_results = [] 
        self.final_subcriteria_results = []
        
        is_mock_mode = self.mock_llm_eval_func is not None
        mapping_data_for_mock = self.evidence_mapping_data if is_mock_mode else None # üü¢ FIX 9: ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠ Attribute ‡πÉ‡∏´‡∏°‡πà
        
        # NOTE: evaluate_with_llm ‡∏ï‡πâ‡∏≠‡∏á‡∏ñ‡∏π‡∏Å Import
        llm_eval_func = self.mock_llm_eval_func if self.mock_llm_eval_func else evaluate_with_llm

        
        for enabler in self.evidence_data:
            enabler_id = enabler.get("Enabler_ID")
            sub_criteria_id = enabler.get("Sub_Criteria_ID")
            sub_criteria_name = enabler.get("Sub_Criteria_Name_TH", "N/A")

            if self.target_sub_id and self.target_sub_id != sub_criteria_id:
                continue

            for level in range(1, 6):
                level_key = f"Level_{level}_Statements"
                statements: List[str] = enabler.get(level_key, [])
                
                if not statements:
                    continue 
                
                rubric_criteria = self.global_rubric_map.get(level, {})
                
                for i, statement in enumerate(statements):
                    subtopic_key = f"subtopic_{i+1}"
                    standard = rubric_criteria.get(subtopic_key, f"Default standard L{level} S{i+1}")
                    
                    query_string = f"{statement} ({sub_criteria_name})"
                    
                    retrieval_result = self._retrieve_context(
                        query=query_string,
                        sub_criteria_id=sub_criteria_id, 
                        level=level,
                        mapping_data=mapping_data_for_mock, 
                        statement_number=i + 1
                    )
                    
                    context_list = []
                    context_length = 0
                    retrieved_sources_list = [] 
                    context = "" 
                    
                    if isinstance(retrieval_result, dict):
                        top_evidence = retrieval_result.get("top_evidences", [])
                        
                        logger.info(f"DEBUG RAG: {sub_criteria_id}_L{level}_S{i+1} Retrieved {len(top_evidence)} raw evidences. Filter: {self.use_mapping_filter}")
                        
                        # <<< ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÇ‡∏Ñ‡πâ‡∏î 3 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ‡∏ñ‡∏π‡∏Å‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÅ‡∏•‡πâ‡∏ß
                        for doc in top_evidence[:3]: 
                            doc_id = doc.get('doc_id', 'N/A')
                            source_name = doc.get('source', 'N/A')
                            logger.info(f"DEBUG RAG Evidence (Top {top_evidence.index(doc) + 1}): UUID={doc_id[:8]}... Source={source_name[:30]}...")
                        # >>>

                        k_to_use = self.FINAL_K_RERANKED
                        if self.disable_semantic_filter:
                            k_to_use = self.FINAL_K_NON_RERANKED

                        for doc in top_evidence[:k_to_use]: 
                            doc_content = doc.get("content", "")
                            metadata = doc.get("metadata", {}) # ‡∏î‡∏∂‡∏á metadata ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤
                            
                            # üü¢ FIX 1: ‡∏î‡∏∂‡∏á source_name ‡∏à‡∏≤‡∏Å metadata['source'] (‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå)
                            # ‡πÉ‡∏ä‡πâ doc.get("source") ‡πÄ‡∏õ‡πá‡∏ô fallback (‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤ retriever ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÑ‡∏ß‡πâ)
                            source_name = metadata.get("source", metadata.get("filename", "N/A (No Source Tag)"))
                            
                            # üü¢ FIX 2: ‡∏î‡∏∂‡∏á Doc ID ‡∏ï‡∏±‡∏ß‡πÄ‡∏ï‡πá‡∏°‡∏à‡∏≤‡∏Å metadata['doc_id'] (Stable UUID)
                            # ‡πÉ‡∏ä‡πâ doc_id ‡πÄ‡∏õ‡πá‡∏ô fallback (‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤ retriever ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÑ‡∏ß‡πâ)
                            doc_id = metadata.get("doc_id", doc.get("doc_id", "N/A"))
                            
                            # ‡∏î‡∏∂‡∏á Page Number
                            location = metadata.get("page_number", "N/A")
                            location_str = f"Page {location}" if isinstance(location, int) else "N/A"
                            
                            retrieved_sources_list.append({
                                "source_name": source_name,
                                "doc_id": doc_id,
                                "location": location_str
                            })
                            
                            if context_length + len(doc_content) <= self.MAX_CONTEXT_LENGTH:
                                context_list.append(doc_content)
                                context_length += len(doc_content)
                            else:
                                remaining_len = self.MAX_CONTEXT_LENGTH - context_length
                                if remaining_len > 0:
                                    context_list.append(doc_content[:remaining_len])
                                context_length = self.MAX_CONTEXT_LENGTH
                                break
                                
                        context = "\n---\n".join(context_list)
                    
                    llm_kwargs = {
                        "level": level, 
                        "sub_criteria_id": sub_criteria_id,
                        "statement_number": i + 1
                    }
                    
                    raw_llm_response_content = ""
                    llm_result_dict = {}

                    if is_mock_mode:
                        llm_result_dict = llm_eval_func(
                            statement=statement, context=context, standard=standard, **llm_kwargs
                        )
                        try:
                            raw_llm_response_content = json.dumps(llm_result_dict, ensure_ascii=False, indent=2)
                        except:
                            raw_llm_response_content = str(llm_result_dict)
                    else:
                        llm_output = llm_eval_func(
                            statement=statement, context=context, standard=standard, **llm_kwargs
                        )
                        if isinstance(llm_output, tuple) and len(llm_output) == 2:
                            llm_result_dict, raw_llm_response_content = llm_output
                        elif isinstance(llm_output, dict):
                            llm_result_dict = llm_output
                            try:
                                raw_llm_response_content = json.dumps(llm_result_dict, ensure_ascii=False, indent=2)
                            except:
                                raw_llm_response_content = str(llm_result_dict)
                        else:
                            logger.error(f"Unexpected return type from LLM evaluation: {type(llm_output)}")
                            llm_result_dict = {}

                    
                    unique_sources = []
                    seen = set()
                    
                    if is_mock_mode:
                        final_score = llm_result_dict.get("llm_score", 0)
                        final_reason = llm_result_dict.get("reason", "")
                        final_sources = llm_result_dict.get("retrieved_sources_list", []) 
                        final_context_snippet = llm_result_dict.get("context_retrieved_snippet", "") 
                        final_pass_status = llm_result_dict.get("pass_status", False)
                        final_status_th = llm_result_dict.get("status_th", "‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô")
                    else:
                        for src in retrieved_sources_list:
                            key = (src['doc_id'], src['location']) 
                            if key not in seen:
                                seen.add(key)
                                unique_sources.append(src)

                        final_score = llm_result_dict.get("score", 0) 
                        final_reason = llm_result_dict.get("reason", "")
                        final_sources = unique_sources 
                        final_context_snippet = context[:240] + "..." if context else ""
                        final_pass_status = final_score == 1
                        final_status_th = "‡∏ú‡πà‡∏≤‡∏ô" if final_pass_status else "‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô"


                    self.raw_llm_results.append({
                        "enabler_id": self.enabler_abbr.upper(),
                        "sub_criteria_id": sub_criteria_id,
                        "sub_criteria_name": sub_criteria_name, 
                        "level": level,
                        "statement_number": i + 1, 
                        "statement": statement,
                        "subtopic": subtopic_key,
                        "standard": standard,
                        "llm_score": final_score, 
                        "reason": final_reason,
                        "retrieved_sources_list": final_sources, 
                        "context_retrieved_snippet": final_context_snippet, 
                        "pass_status": final_pass_status,
                        "status_th": final_status_th,
                        "statement_id": f"{sub_criteria_id}_L{level}_S{i+1}",
                        "llm_result": llm_result_dict, 
                        "llm_raw_response_content": raw_llm_response_content 
                    })
        
        self._process_subcriteria_results()
        
        final_summary = self.summarize_results()
        action_plan = {} 
        
        return {"summary": final_summary, "action_plan": action_plan}, {f"{r['statement_id']}": r for r in self.raw_llm_results}


    # ----------------------------------------------------
    # üåü NEW FEATURE: Generate Evidence Summary
    # ----------------------------------------------------
    def generate_evidence_summary_for_level(self, sub_criteria_id: str, level: int) -> Union[str, Dict]: 
        """
        ‡∏£‡∏ß‡∏°‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å Statement ‡πÉ‡∏ô Sub-Criteria/Level ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î ‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ LLM ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢
        """
        enabler_data = next((e for e in self.evidence_data 
                             if e.get("Sub_Criteria_ID") == sub_criteria_id), None)
        
        if not enabler_data:
            logger.error(f"Sub-Criteria ID {sub_criteria_id} not found in evidence data.")
            return "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏¢‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î"

        level_key = f"Level_{level}_Statements"
        statements: List[str] = enabler_data.get(level_key, [])
        sub_criteria_name = enabler_data.get("Sub_Criteria_Name_TH", "N/A")

        if not statements:
            return f"‡πÑ‡∏°‡πà‡∏û‡∏ö Statements ‡πÉ‡∏ô Level {level} ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡∏ì‡∏ë‡πå {sub_criteria_id}"

        aggregated_context_list = []
        total_context_length = 0
        
        is_mock_mode = self.mock_llm_summarize_func is not None
        mapping_data_for_mock = self.evidence_mapping_data if is_mock_mode else None # üü¢ FIX 10: ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠ Attribute ‡πÉ‡∏´‡∏°‡πà
        
        k_to_use = self.FINAL_K_RERANKED 
        if self.disable_semantic_filter:
             k_to_use = self.FINAL_K_NON_RERANKED 
        
        for i, statement in enumerate(statements):
            query_string = f"{statement} ({sub_criteria_name})"
            
            retrieval_result = self._retrieve_context(
                query=query_string,
                sub_criteria_id=sub_criteria_id,
                level=level,
                mapping_data=mapping_data_for_mock,
                statement_number=i + 1
            )
            
            if isinstance(retrieval_result, dict):
                top_evidence = retrieval_result.get("top_evidences", [])
                
                for doc in top_evidence[:k_to_use]: 
                    doc_content = doc.get("content", "")
                    
                    if total_context_length + len(doc_content) <= self.MAX_CONTEXT_LENGTH:
                        aggregated_context_list.append(doc_content)
                        total_context_length += len(doc_content)
                    else:
                        remaining_len = self.MAX_CONTEXT_LENGTH - total_context_length
                        if remaining_len > 0:
                            aggregated_context_list.append(doc_content[:remaining_len])
                        total_context_length = self.MAX_CONTEXT_LENGTH
                        break 
        
        if not aggregated_context_list:
            return {"summary": f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÉ‡∏ô Vector Store ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡∏ì‡∏ë‡πå {sub_criteria_id} Level {level}", "suggestion_for_next_level": "N/A"}
        
        final_context = "\n---\n".join(list(dict.fromkeys(aggregated_context_list)))
        
        try:
            summarize_func = self.mock_llm_summarize_func if self.mock_llm_summarize_func else summarize_context_with_llm
            
            # NOTE: EvidenceSummary ‡∏ï‡πâ‡∏≠‡∏á‡∏ñ‡∏π‡∏Å Import
            summary_result = summarize_func(
                context=final_context,
                sub_criteria_name=sub_criteria_name,
                level=level,
                sub_id=sub_criteria_id,
                schema=None # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤ EvidenceSummary ‡∏ñ‡∏π‡∏Å Import
            )
            
            if isinstance(summary_result, dict):
                return summary_result
            else:
                return {"summary": summary_result if isinstance(summary_result, str) else "LLM return type ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á", "suggestion_for_next_level": "N/A"}
            
        except Exception as e:
            logger.error(f"Failed to generate summary with LLM: {e}")
            return {"summary": f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ LLM ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {e}", "suggestion_for_next_level": "Error"}
    
    # ----------------------------------------------------
    # üåü NEW FEATURE: Generate Action Plan (Mock Handler)
    # ----------------------------------------------------
    def generate_action_plan(self, sub_criteria_id: str) -> List[Dict]:
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á Action Plan ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Sub-Criteria ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î (‡πÄ‡∏°‡∏ò‡∏≠‡∏î‡∏ô‡∏µ‡πâ‡∏ñ‡∏π‡∏Å‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡∏°‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Mocking ‡πÉ‡∏ô run_assessment.py)
        """
        
        if self.mock_llm_action_plan_func:
            return self.mock_llm_action_plan_func(
                failed_statements_data=[],
                sub_id=sub_criteria_id,
                target_level=0 
            )
        
        logger.warning(f"generate_action_plan is stubbed. Action Plan logic is handled via external calls.")
        return []
    
    # ----------------------------------------------------
    def summarize_results(self) -> Dict[str, Dict]:
        """
        ‡∏™‡∏£‡∏∏‡∏õ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°‡∏à‡∏≤‡∏Å final_subcriteria_results
        """
        # ... (‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°) ...
        if not self.final_subcriteria_results:
             return {
                 "Overall": {
                    "enabler": self.enabler_abbr.upper(),
                    "total_weighted_score": 0.0,
                    "total_possible_weight": 0.0,
                    "overall_progress_percent": 0.0,
                    "overall_maturity_score": 0.0
                 },
                 "SubCriteria_Breakdown": {}
             }
        
        total_weight = sum(r["weight"] for r in self.final_subcriteria_results)
        total_score = sum(r["progress_score"] for r in self.final_subcriteria_results)
        
        return {
            "Overall": {
                "enabler": self.enabler_abbr.upper(),
                "total_weighted_score": round(total_score, 2),
                "total_possible_weight": round(total_weight, 2),
                "overall_progress_percent": round((total_score / total_weight) * 100, 2) if total_weight > 0 else 0.0,
                "overall_maturity_score": round(total_score / total_weight, 2) if total_weight > 0 else 0.0
            },
            "SubCriteria_Breakdown": {
                r["sub_criteria_id"]: {
                    "name": r.get("sub_criteria_name", "N/A"),
                    "score": r["progress_score"],
                    "weight": r["weight"],
                    "highest_full_level": r["highest_full_level"],
                    "pass_ratios": r["level_pass_ratios"], 
                    "development_gap": r["development_gap"],
                    "action_item": r["action_item"]
                } for r in self.final_subcriteria_results
            }
        }