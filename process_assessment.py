# process_assessment.py
import argparse
import os
import sys
import time
import json
import logging
from typing import Dict, Any, List

# --- Core Imports ---
# üö® CRITICAL: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏Å‡∏≤‡∏£ Import ‡πÉ‡∏´‡πâ‡∏ä‡∏µ‡πâ‡πÑ‡∏õ‡∏ó‡∏µ‡πà core/run_assessment.py
try:
    from core.run_assessment import run_assessment_process 
except ImportError as e:
    print(f"Error importing run_assessment_process: {e}")
    print("Please ensure core/run_assessment.py exists and the project root is in Python path.")
    sys.exit(1)

# -----------------------------
# --- Logging Setup ---
# -----------------------------
# ‡πÉ‡∏ä‡πâ basicConfig ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ logging ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÉ‡∏ô CLI script
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("process_assessment")
logger.setLevel(logging.INFO)

# -----------------------------
# --- Main CLI Entry Point ---
# -----------------------------

if __name__ == "__main__":
    
    # Start global timer
    start_time_global = time.perf_counter()
    
    # 1. Setup Argparse (‡πÉ‡∏ä‡πâ Logic ‡πÄ‡∏î‡∏¥‡∏°‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ö Argument)
    parser = argparse.ArgumentParser(description="Automated Enabler Maturity Assessment System (CLI Runner).")
    parser.add_argument("--mode", choices=["mock", "random", "real"], default="real", help="Assessment mode ('mock', 'random', 'real').")
    parser.add_argument("--enabler", type=str, default="KM", help="Enabler abbreviation (e.g., 'KM', 'SCM').")
    parser.add_argument("--sub", type=str, default="all", help="Filter to a specific Sub-Criteria ID (e.g., '1.1') or 'all'.")
    parser.add_argument("--filter", action="store_true", help="Enable strict metadata filtering based on mapping data for RAG.")
    parser.add_argument("--export", action="store_true", help="Export the final summary results to a JSON file in the 'results' directory.")
    args = parser.parse_args()
    
    logger.info(f"CLI: Starting Assessment (Enabler: {args.enabler}, Sub: {args.sub}, Mode: {args.mode})")

    try:
        # üö® CRITICAL CHANGE: ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà (core/run_assessment.py)
        final_summary = run_assessment_process(
            enabler=args.enabler,
            sub_criteria_id=args.sub,
            mode=args.mode,
            filter_mode=args.filter,
            export=args.export
        )
        
        # 2. Output Summary (Print Final JSON to terminal)
        print("\n\n=============================================")
        print(f"===== FINAL ASSESSMENT SUMMARY ({args.enabler}/{args.sub}) =====")
        print("=============================================")
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö JSON ‡∏ó‡∏µ‡πà‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢
        print(json.dumps(final_summary, indent=4, ensure_ascii=False))

        # 3. Output Detailed Results (‡∏•‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ print_detailed_results ‡∏≠‡∏≠‡∏Å)
        # ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÇ‡∏î‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏ñ‡∏π‡∏Å‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÇ‡∏î‡∏¢ run_assessment_process/print_detailed_results ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡πÅ‡∏•‡πâ‡∏ß
        
        # 4. Output Export Path (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£ Export)
        if args.export and 'export_path_used' in final_summary:
            logger.info(f"Report exported to: {final_summary['export_path_used']}")


    except Exception as e:
        logger.critical(f"A fatal error occurred during the assessment process: {e}")
        # Exit with error code
        sys.exit(1)


    # Stop global timer and print total
    end_time_global = time.perf_counter()
    global_duration = end_time_global - start_time_global
    
    # ‡∏î‡∏∂‡∏á‡πÄ‡∏ß‡∏•‡∏≤ Execution Time ‡∏ó‡∏µ‡πà‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÉ‡∏ô core/run_assessment.py ‡∏°‡∏≤‡πÉ‡∏ä‡πâ
    if 'Execution_Time' in final_summary:
        global_duration = final_summary['Execution_Time'].get('total', global_duration)
        
    print(f"\n[‚è±Ô∏è TOTAL PROCESS TIME] All processes completed in: {global_duration:.2f} seconds.")