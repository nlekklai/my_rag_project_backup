import argparse
import os
import sys
import time
import json
import logging
from typing import Dict, Any, List

# --- Core Imports ---
# üö® CRITICAL: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏Å‡∏≤‡∏£ Import ‡πÉ‡∏´‡πâ‡∏ä‡∏µ‡πâ‡πÑ‡∏õ‡∏ó‡∏µ‡πà core/run_assessment.py
try:
    from core.run_assessment import run_assessment_process 
except ImportError as e:
    print(f"Error importing run_assessment_process: {e}")
    print("Please ensure core/run_assessment.py exists and the project root is in Python path.")
    sys.exit(1)

# -----------------------------
# --- Logging Setup ---
# -----------------------------
# ‡πÉ‡∏ä‡πâ basicConfig ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ logging ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÉ‡∏ô CLI script
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("process_assessment")
logger.setLevel(logging.INFO)

# -----------------------------
# --- Helper Functions (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö CLI Output) ---
# -----------------------------

def print_detailed_results(raw_llm_results: List[Dict[str, Any]]):
    """
    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô LLM ‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
    (‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å Logic ‡πÄ‡∏î‡∏¥‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡πÉ‡∏ô CLI)
    """
    print("\n\n===== DETAILED LLM EVALUATION RESULTS =====")
    
    if not raw_llm_results:
        print("No raw LLM results recorded (maybe run in REAL/MOCK mode without export, or an error occurred).")
        return

    # ‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ï‡∏≤‡∏° Sub-Criteria ‡πÅ‡∏•‡∏∞ Level
    grouped_results: Dict[str, Dict[str, List[Dict]]] = {}
    for result in raw_llm_results:
        sub_id = result.get('sub_criteria_id', 'N/A')
        level = f"Level {result.get('level', 'N/A')}"
        
        if sub_id not in grouped_results:
            grouped_results[sub_id] = {}
        if level not in grouped_results[sub_id]:
            grouped_results[sub_id][level] = []
            
        grouped_results[sub_id][level].append(result)

    for sub_id, levels in sorted(grouped_results.items()):
        print(f"\n--- Sub-Criteria ID: {sub_id} ---")
        for level, results in sorted(levels.items()):
            passed = sum(r.get('score', 0) for r in results)
            total = len(results)
            pass_ratio = passed / total if total > 0 else 0
            
            print(f"  > {level} ({passed}/{total}, Pass Ratio: {pass_ratio:.2f})")
            for i, r in enumerate(results):
                status = "‚úÖ PASS" if r.get('score') == 1 else "‚ùå FAIL"
                print(f"    - S{i+1}: {status} | Statement: {r.get('statement', '')[:80]}...")
                print(f"      [Reason]: {r.get('reason', 'No reason provided')}")
                
# -----------------------------
# --- Main CLI Entry Point ---
# -----------------------------

if __name__ == "__main__":
    
    # Start global timer
    start_time_global = time.perf_counter()
    
    # 1. Setup Argparse (‡πÉ‡∏ä‡πâ Logic ‡πÄ‡∏î‡∏¥‡∏°‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ö Argument)
    parser = argparse.ArgumentParser(description="Automated Enabler Maturity Assessment System (CLI Runner).")
    parser.add_argument("--mode", choices=["mock", "random", "real"], default="real", help="Assessment mode ('mock', 'random', 'real').")
    parser.add_argument("--enabler", type=str, default="KM", help="Enabler abbreviation (e.g., 'KM', 'SCM').")
    parser.add_argument("--sub", type=str, default="all", help="Filter to a specific Sub-Criteria ID (e.g., '1.1') or 'all'.")
    parser.add_argument("--filter", action="store_true", help="Enable strict metadata filtering based on mapping data for RAG.")
    parser.add_argument("--export", action="store_true", help="Export the final summary results to a JSON file in the 'results' directory.")
    args = parser.parse_args()
    
    logger.info(f"CLI: Starting Assessment (Enabler: {args.enabler}, Sub: {args.sub}, Mode: {args.mode})")

    try:
        # üö® CRITICAL CHANGE: ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà (core/run_assessment.py)
        final_summary = run_assessment_process(
            enabler=args.enabler,
            sub_criteria_id=args.sub,
            mode=args.mode,
            filter_mode=args.filter,
            export=args.export
        )
        
        # 2. Output Summary (Print Final JSON to terminal)
        print("\n\n=============================================")
        print(f"===== FINAL ASSESSMENT SUMMARY ({args.enabler}/{args.sub}) =====")
        print("=============================================")
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö JSON ‡∏ó‡∏µ‡πà‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢
        print(json.dumps(final_summary, indent=4, ensure_ascii=False))

        # 3. Output Detailed Results (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ raw data)
        if 'raw_llm_results' in final_summary and args.mode != "random":
            print_detailed_results(final_summary['raw_llm_results'])
        
        # 4. Output Export Path (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£ Export)
        if args.export and 'export_path_used' in final_summary:
            logger.info(f"Report exported to: {final_summary['export_path_used']}")


    except Exception as e:
        logger.critical(f"A fatal error occurred during the assessment process: {e}")
        # Exit with error code
        sys.exit(1)


    # Stop global timer and print total
    end_time_global = time.perf_counter()
    global_duration = end_time_global - start_time_global
    
    # ‡∏î‡∏∂‡∏á‡πÄ‡∏ß‡∏•‡∏≤ Execution Time ‡∏ó‡∏µ‡πà‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÉ‡∏ô core/run_assessment.py ‡∏°‡∏≤‡πÉ‡∏ä‡πâ
    if 'Execution_Time' in final_summary:
        global_duration = final_summary['Execution_Time'].get('total', global_duration)
        
    print(f"\n[‚è±Ô∏è TOTAL PROCESS TIME] All processes completed in: {global_duration:.2f} seconds.")