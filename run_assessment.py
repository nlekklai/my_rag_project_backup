# -------------------- run_assessment.py (FINAL FIXED VERSION) --------------------
import os
import json
import logging
import sys
import argparse
import random
from typing import List, Dict, Any

# 1. Setup Logging (Need to set it up here since it's the main entry point)
logger = logging.getLogger(__name__)
# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î logging format ‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- CORRECT IMPORTS ---
try:
    # Ensure the parent directory is in sys.path for correct relative imports
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
    if project_root not in sys.path:
        sys.path.append(project_root)
    
    # Import the main class from the assessments module
    from assessments.km_assessment import KMAssessment
    
    # ‚ùó IMPORTANT: Import the module, not the function, for successful patching
    import core.retrieval_utils
    from core.retrieval_utils import set_mock_control_mode
    
except ImportError as e:
    logger.error(f"FATAL ERROR: Failed to import required modules. Check sys.path and file structure. Error: {e}")
    sys.exit(1)


# -------------------- Mock Evaluation Logic --------------------
# Counter ‡∏ñ‡∏π‡∏Å‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô Global ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ
_MOCK_EVALUATION_COUNTER = 0

def evaluate_with_llm_CONTROLLED_MOCK(statement: str, context: str, standard: str) -> Dict[str, Any]:
    """Returns controlled scores: L1(1,1,1), L2(1,1,0), L3-L5(0)"""
    global _MOCK_EVALUATION_COUNTER
    _MOCK_EVALUATION_COUNTER += 1
    
    # üéØ ULTIMATE MOCK LOGIC: (L1: 100%, L2: 66% Pass)
    # Sub-Criteria 1.1 ‡∏°‡∏µ 3 Statements/Level (‡∏£‡∏ß‡∏° 15 Statements)
    
    score = 0
    
    if _MOCK_EVALUATION_COUNTER <= 3: # L1 Statements (Counter 1, 2, 3)
        score = 1
    elif _MOCK_EVALUATION_COUNTER in [4, 5]: # L2 Statements (Counter 4, 5)
        score = 1
    else: # L2 S3 (Counter 6) ‡πÅ‡∏•‡∏∞ L3-L5 (Counter > 6)
        score = 0 # FORCED FAIL for easy verification
    
    logger.debug(f"MOCK COUNT: {_MOCK_EVALUATION_COUNTER} | SCORE: {score} | STMT: '{statement[:20]}...'")
    
    return {"score": score, "reason": f"CONTROLLED MOCK Score {score}"}

def retrieve_context_MOCK(statement: str) -> str:
    """Mock retrieval function returns empty string for speed."""
    # ‡πÉ‡∏ô Mock Mode ‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ Snippet ‡∏™‡∏±‡πâ‡∏ô‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Context ‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤
    return "MOCK CONTEXT SNIPPET: The organization has a formal KM policy approved by the CEO."

# -------------------- Detailed Output Function --------------------

def print_detailed_results(raw_llm_results: List[Dict]):
    """
    ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô LLM ‡∏£‡∏≤‡∏¢ Statement ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
    """
    if not raw_llm_results:
        logger.info("\n[Detailed Results] No raw LLM results found to display.")
        return

    # ‚ùó ‡∏™‡∏•‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡πÉ‡∏´‡πâ Detailed ‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏•‡∏±‡∏á Summary
    print("\n\n=========================================================================================")
    print("        DETAILED LLM EVALUATION RESULTS (Statement-by-Statement)")
    print("=========================================================================================")

    # ‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ï‡∏≤‡∏° Sub-Criteria ID ‡πÅ‡∏•‡∏∞ Level ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢
    grouped: Dict[str, List[Dict]] = {}
    for r in raw_llm_results:
        key = f"{r['sub_criteria_id']}: L{r['level']}"
        if key not in grouped:
            grouped[key] = []
        grouped[key].append(r)
    
    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ï‡∏≤‡∏° Sub ID ‡πÅ‡∏•‡∏∞ Level
    sorted_keys = sorted(grouped.keys())

    for key in sorted_keys:
        # ‡πÉ‡∏ä‡πâ Sub-Criteria Name (‡∏ñ‡πâ‡∏≤‡∏î‡∏∂‡∏á‡πÑ‡∏î‡πâ)
        sub_name = next((r['sub_criteria_name'] for r in grouped[key] if 'sub_criteria_name' in r), 'N/A')
        print(f"\n--- Sub-Criteria / Level: {key} - {sub_name} (Total: {len(grouped[key])} Statements) ---")
        
        for i, r in enumerate(grouped[key]):
            # ‡∏ï‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢‡πÉ‡∏ô Console
            statement_snippet = r['statement'][:120] + "..." if len(r['statement']) > 120 else r['statement']
            reason_snippet = r['reason'][:120] + "..." if len(r['reason']) > 120 else r['reason']
            # ‡πÉ‡∏ô km_assessment.py ‡πÄ‡∏£‡∏≤‡πÄ‡∏Å‡πá‡∏ö context_retrieved_snippet ‡πÑ‡∏ß‡πâ
            context_snippet = r.get('context_retrieved_snippet', 'N/A')
            
            # ‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•
            score_text = "‚úÖ PASS (1)" if r['llm_score'] == 1 else "‚ùå FAIL (0)"
            
            print(f"\n[STMT {i+1}] Result: {score_text}")
            print(f"  > Statement: {statement_snippet}")
            print(f"  > Standard:  {r['standard'][:120] + '...' if len(r['standard']) > 120 else r['standard']}")
            print(f"  > Context:   {context_snippet}")
            print(f"  > Reason:    {reason_snippet}")
            
    print("\n=========================================================================================")


# -------------------- Main CLI Entry Point --------------------
if __name__ == "__main__":
    
    # 1. Setup Argparse
    parser = argparse.ArgumentParser(description="Automated KM Maturity Assessment System.")
    parser.add_argument("--mode", 
                        choices=["mock", "random", "real"], 
                        default="random",
                        help="Assessment mode: 'mock' (controlled scores for 1.1), 'random' (random 0/1 scores), or 'real' (requires actual RAG setup).")
    parser.add_argument("--sub", 
                        type=str, 
                        default="all",
                        help="Filter to a specific Sub-Criteria ID (e.g., '1.1'). Default is 'all'.")
    parser.add_argument("--export", 
                        action="store_true",
                        help="Export the final summary results to a JSON file.")
    
    args = parser.parse_args()
    
    retriever = None 
    
    # 2. Setup LLM/RAG Mode (Mode-specific setup)
    if args.mode == "mock":
        # üü¢ FIX 1: ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Global Flag ‡πÅ‡∏•‡∏∞‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï Counter ‡πÉ‡∏ô core/retrieval_utils
        set_mock_control_mode(True)
        # üü¢ FIX 2: Patch evaluation function ‡πÉ‡∏ô‡πÇ‡∏°‡∏î‡∏π‡∏• core.retrieval_utils ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
        original_eval_func = core.retrieval_utils.evaluate_with_llm
        core.retrieval_utils.evaluate_with_llm = evaluate_with_llm_CONTROLLED_MOCK
        logger.info(f"Using CONTROLLED MOCK Mode on Sub-Criteria: {args.sub}.")
    
    elif args.mode == "random":
        logger.info("Using RANDOM LLM Scores (0/1) for full test.")
        pass

    elif args.mode == "real":
        try:
            from core.vectorstore import load_all_vectorstores 
            retriever = load_all_vectorstores()
            logger.info("Using REAL RAG/LLM mode.")
        except Exception as e:
            logger.warning(f"Error loading vectorstore for REAL mode: {e}. Falling back to RANDOM.")
            args.mode = "random"


    # 3. Load & Filter Evidence Data
    try:
        km_loader = KMAssessment(vectorstore_retriever=retriever)
    except Exception as e:
        logger.error(f"Error during KMAssessment init: {e}")
        sys.exit(1)

    if not km_loader.evidence_data:
        logger.error("‚ùå KMAssessment failed to load evidence data. Cannot run test.")
        sys.exit(1)
        
    filtered_evidence = km_loader.evidence_data
    if args.sub != "all":
        filtered_evidence = [
            e for e in km_loader.evidence_data 
            if e.get("Sub_Criteria_ID") == args.sub
        ]
        if not filtered_evidence:
            logger.error(f"‚ùå Sub-Criteria ID '{args.sub}' not found in JSON.")
            sys.exit(1)
        
    # 4. Create Final KMAssessment Object (with filtered data)
    km = KMAssessment(
        evidence_data=filtered_evidence,
        rubric_data=km_loader.rubric_data,
        level_fractions=km_loader.level_fractions,
        vectorstore_retriever=retriever
    )
    
    print(f"‚úÖ Loaded {len(filtered_evidence)} Sub-Criteria (Filtered to {args.sub}) for assessment.")
    
    
    # 5. Run Assessment
    try:
        # Patch retrieval for speed in mock/random mode
        if args.mode == "mock" or args.mode == "random":
            km._retrieve_context = retrieve_context_MOCK 
        
        results = km.run_assessment()
        
    finally:
        # üü¢ FIX 3: Clean up patch
        if args.mode == "mock":
            core.retrieval_utils.evaluate_with_llm = original_eval_func
        
    summary = km.summarize_results()

    
    # 6. Output Summary (Code is the same as provided)
    print("\n=====================================================")
    print(f"      SUMMARY OF SCORING RESULTS ({args.mode.upper()} MODE) ")
    print("=====================================================")
    print(f"Overall Maturity Score (Avg.): {summary['Overall']['overall_maturity_score']}")
    print(f"Total Score (Weighted): {summary['Overall']['total_weighted_score']}/{summary['Overall']['total_possible_weight']} (Progress: {summary['Overall']['overall_progress_percent']}%)")
    print("\n-----------------------------------------------------")
    
    for sub_id, data in summary['SubCriteria_Breakdown'].items():
        print(f"| {sub_id}: {data['name']}")
        print(f"| - Score: {data['score']}/{data['weight']} | Full Lvl: L{data['highest_full_level']} | Gap: {'YES' if data['development_gap'] else 'NO'}")
        print(f"| - Ratios (L1-L5): {data['pass_ratios']}")
        if data['development_gap']:
            print(f"| - Action: {data['action_item']}")
        print("-----------------------------------------------------")
        
    # 7. Export JSON 
    if args.export:
        EXPORT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), "results"))
        try:
            os.makedirs(EXPORT_DIR, exist_ok=True)
            logger.info(f"Using export directory: {EXPORT_DIR}")
        except OSError as e:
            logger.error(f"‚ùå Failed to create results directory {EXPORT_DIR}: {e}")
            sys.exit(1)
        
        EXPORT_FILENAME = f"km_assessment_summary_{args.sub}_{args.mode}_{os.urandom(4).hex()}.json"
        FULL_EXPORT_PATH = os.path.join(EXPORT_DIR, EXPORT_FILENAME)

        try:
            with open(FULL_EXPORT_PATH, 'w', encoding='utf-8') as f:
                json.dump(summary, f, ensure_ascii=False, indent=4)
            print(f"\n‚úÖ Successfully exported summary to {FULL_EXPORT_PATH}")
        except Exception as e:
            logger.error(f"‚ùå Failed to export JSON summary to {FULL_EXPORT_PATH}: {e}")
            
    # 7.5. ‚ùó ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
    # NOTE: ‡πÄ‡∏£‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ó‡∏µ‡πà‡∏ó‡πâ‡∏≤‡∏¢‡∏™‡∏∏‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Summary ‡∏Ç‡∏∂‡πâ‡∏ô‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏™‡∏°‡∏≠
    print_detailed_results(km.raw_llm_results)