import os
import json
import sys
import argparse
from pathlib import Path
from typing import List, Optional, Dict, Any
from pydantic import BaseModel, Field
import logging

# --- LangChain/Loader Imports ---
from langchain_community.document_loaders import UnstructuredPDFLoader
from langchain_core.output_parsers import PydanticOutputParser
from langchain.output_parsers import OutputFixingParser
from langchain_core.prompts import PromptTemplate

# --- LLM Instance Loading ‡πÅ‡∏•‡∏∞ CONFIG (‡∏Ñ‡∏á‡πÄ‡∏î‡∏¥‡∏°) ---
try:
    from models.llm import llm as llm_instance
except Exception:
    llm_instance = None
    
try:
    current_dir = Path(__file__).parent.parent
    sys.path.append(str(current_dir)) 
    from config import global_vars 
    PROJECT_ROOT = Path(global_vars.PROJECT_ROOT)
    SUPPORTED_ENABLERS = global_vars.SUPPORTED_ENABLERS 
except (ImportError, AttributeError):
    PROJECT_ROOT = Path(os.getcwd())
    SUPPORTED_ENABLERS = ["KM"] 

logger = logging.getLogger(__name__)

# --- 1. DEFINING THE STRUCTURED OUTPUT (Pydantic Schema) ---
class SubCriteria(BaseModel):
    """‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏¢‡πà‡∏≠‡∏¢ (Sub-Criteria) ‡πÄ‡∏ä‡πà‡∏ô KM 1.1"""
    Enabler_ID: str = Field(description="‡∏£‡∏´‡∏±‡∏™ Enabler ‡∏´‡∏•‡∏±‡∏Å (‡πÄ‡∏ä‡πà‡∏ô '1', '2' ‡∏´‡∏£‡∏∑‡∏≠ 'KM')")
    Sub_Criteria_ID: str = Field(description="‡∏£‡∏´‡∏±‡∏™‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏¢‡πà‡∏≠‡∏¢ (‡πÄ‡∏ä‡πà‡∏ô '1.1', '2.1')")
    Sub_Criteria_Name_TH: str = Field(description="‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏¢‡πà‡∏≠‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢")
    Weight: Optional[float] = Field(description="‡∏Ñ‡πà‡∏≤‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏Ç‡∏≠‡∏á‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏ô‡∏µ‡πâ (‡∏ñ‡πâ‡∏≤‡∏™‡∏Å‡∏±‡∏î‡πÑ‡∏î‡πâ, ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô 0.0)")
    
    Level_1_Statements: List[str] = Field(description="‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏¢‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏à‡∏≤‡∏Å‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠ (‡πÑ‡∏°‡πà‡∏¢‡πà‡∏≠) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Level 1")
    Level_2_Statements: List[str] = Field(description="‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏¢‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏à‡∏≤‡∏Å‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠ (‡πÑ‡∏°‡πà‡∏¢‡πà‡∏≠) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Level 2")
    Level_3_Statements: List[str] = Field(description="‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏¢‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏à‡∏≤‡∏Å‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠ (‡πÑ‡∏°‡πà‡∏¢‡πà‡∏≠) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Level 3")
    Level_4_Statements: List[str] = Field(description="‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏¢‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏à‡∏≤‡∏Å‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠ (‡πÑ‡∏°‡πà‡∏¢‡πà‡∏≠) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Level 4")
    Level_5_Statements: List[str] = Field(description="‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏¢‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏à‡∏≤‡∏Å‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠ (‡πÑ‡∏°‡πà‡∏¢‡πà‡∏≠) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Level 5")
    
class EnablerStatementList(BaseModel):
    """‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ Statement ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Ç‡∏≠‡∏á Enabler ‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏ï‡∏±‡∏ß"""
    statement_list: List[SubCriteria] = Field(description="‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏™‡∏Å‡∏±‡∏î‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠")


def extract_single_enabler_statements(target_enabler: str):
    """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Statement ‡∏Ç‡∏≠‡∏á Enabler ‡∏ï‡∏±‡∏ß‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏à‡∏≤‡∏Å PDF ‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á"""
    
    # --- DYNAMIC PATH GENERATION ---
    pdf_filename = f"SE-AM_{target_enabler}.pdf"
    pdf_path = PROJECT_ROOT / "data" / "seam" / pdf_filename
    output_enabler_name = target_enabler.lower().replace("&", "").replace("-", "")
    output_filename = f"official_{output_enabler_name}_statements.json"
    output_path = PROJECT_ROOT / "evidence_checklist" / output_filename
    
    # --- LOGGING AND VALIDATION ---
    logger.info("-" * 60)
    logger.info(f"  Starting Extraction for ENABLER: {target_enabler}")
    logger.info(f"  Source File: {pdf_path.name}")
    logger.info("-" * 60)

    if llm_instance is None:
        logger.error("üõë Cannot proceed: LLM instance is not available.")
        return
    if not pdf_path.exists():
        logger.error(f"‚ùå Error: File not found at the expected path: {pdf_path.resolve()}")
        return

    # 1. ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡∏™‡∏Å‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡∏¥‡∏ö
    try:
        loader = UnstructuredPDFLoader(str(pdf_path), mode="elements")
        docs = loader.load()
    except Exception as e:
        logger.error(f"‚ùå Error loading PDF {pdf_path.name}: {e}")
        return

    full_text = "\n\n".join([d.page_content for d in docs if len(d.page_content.strip()) > 10])

    if not full_text:
        logger.error(f"‚ùå Error: Extracted text from {pdf_path.name} is empty.")
        return
    
    logger.info(f"‚úÖ Extracted {len(full_text)} characters. Sending to LLM...")

    # 3. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Prompt ‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ LLM
    try:
        # üü¢ 3.1 ‡∏™‡∏£‡πâ‡∏≤‡∏á Pydantic Output Parser
        parser = PydanticOutputParser(pydantic_object=EnablerStatementList)
        format_instructions = parser.get_format_instructions()
        
        # --- PROMPT INSTRUCTION (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏ô‡πâ‡∏ô‡∏¢‡πâ‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå) ---
        system_instruction = (
            f"‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡∏£‡∏±‡∏ê‡∏ß‡∏¥‡∏™‡∏≤‡∏´‡∏Å‡∏¥‡∏à (SE-AM) ‡∏†‡∏≤‡∏£‡∏Å‡∏¥‡∏à‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏™‡∏Å‡∏±‡∏î‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô "
            f"Enabler: {target_enabler} **‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÇ‡∏î‡∏¢‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå** ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡∏¥‡∏ö "
            "‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏¢‡∏¥‡πà‡∏á‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏Å‡∏±‡∏î‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡∏ó‡∏∏‡∏Å Sub-Criteria (‡πÄ‡∏ä‡πà‡∏ô KM 1.1, 2.1, 3.1, 4.1, 5.1, 6.1) "
            "Statement ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠ (‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏¢‡πà‡∏≠) ‡πÅ‡∏•‡∏∞ **‡∏ï‡πâ‡∏≠‡∏á‡∏à‡∏±‡∏î‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô JSON ‡∏ï‡∏≤‡∏° Schema ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÑ‡∏ß‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏Ñ‡∏£‡πà‡∏á‡∏Ñ‡∏£‡∏±‡∏î‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ:** \n\n"
        )
        
        # üü¢ 3.2 ‡∏™‡∏£‡πâ‡∏≤‡∏á Prompt Template
        prompt = PromptTemplate(
            template="{system_instruction}{format_instructions}\n\n[RAW TEXT]:\n{raw_text}",
            input_variables=["raw_text"],
            partial_variables={
                "system_instruction": system_instruction,
                "format_instructions": format_instructions
            }
        )
        
        # üü¢ 3.3 ‡∏™‡∏£‡πâ‡∏≤‡∏á Chain (‡πÉ‡∏ä‡πâ OutputFixingParser)
        fixing_parser = OutputFixingParser.from_llm(parser=parser, llm=llm_instance)
        
        # üü¢ 3.4 ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ LLM ‡πÅ‡∏•‡∏∞ Parse ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        full_prompt_text = prompt.format(raw_text=full_text)
        
        llm_output = llm_instance.invoke(full_prompt_text)
        
        # üü¢ 3.5 Parse ‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        result_pydantic = fixing_parser.parse(llm_output)
        
        # 4. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        final_json_data = result_pydantic.model_dump() 
        
        # 5. ‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô JSON ‡∏ó‡∏µ‡πà‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with output_path.open('w', encoding='utf-8') as f:
            # ‡πÉ‡∏ä‡πâ indent=2 ‡πÉ‡∏ô json.dump()
            json.dump(final_json_data['statement_list'], f, indent=2, ensure_ascii=False) 

        logger.info("-" * 60)
        logger.info(f"‚ú® Success! Extracted {len(final_json_data['statement_list'])} sub-criteria saved to: {output_path.resolve()}")
        logger.info("-" * 60)
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Failed during LLM processing for {target_enabler}: {e}", exc_info=True)
        logger.error("*** NOTE: LLM is likely truncating the output due to large context. Check Ollama context size (num_ctx) or use a larger model. ***")
        return False
        
        
def extract_all_enabler_statements():
    """‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Statement ‡∏Ç‡∏≠‡∏á Enabler ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö"""
    
    if not SUPPORTED_ENABLERS:
        logger.error("üõë SUPPORTED_ENABLERS list is empty. Cannot proceed.")
        return

    logger.info(f"Starting batch extraction for Enablers: {SUPPORTED_ENABLERS}")
    
    for enabler in SUPPORTED_ENABLERS:
        extract_single_enabler_statements(enabler)
        
    logger.info("Batch extraction process complete.")


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    parser = argparse.ArgumentParser(
        description="Extracts SE-AM Statements from PDF to JSON format."
    )
    parser.add_argument(
        '--enabler', 
        type=str, 
        nargs='?', 
        default=None, 
        help='Specify a single Enabler (e.g., KM, CG, HCM) to process. If omitted, all supported Enablers will be processed.'
    )
    
    args = parser.parse_args()
    
    if args.enabler:
        target_enabler = args.enabler.upper()
        if target_enabler in SUPPORTED_ENABLERS:
            extract_single_enabler_statements(target_enabler)
        else:
            logger.error(f"‚ùå Enabler '{target_enabler}' is not supported in the configuration.")
    else:
        extract_all_enabler_statements()