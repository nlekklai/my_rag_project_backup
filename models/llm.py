# -*- coding: utf-8 -*-
# models/llm.py - Production Version (Ollama Unified Connector)
# ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á Local (Mac 8B) ‡πÅ‡∏•‡∏∞ Cloud (NVIDIA 70B) ‡∏ú‡πà‡∏≤‡∏ô Ollama

import logging
import os
from typing import Optional, Any, Final
from langchain_core.language_models.llms import BaseLLM

# --- Conditional Imports ---
try:
    from langchain_ollama import OllamaLLM
except ImportError:
    logger = logging.getLogger(__name__)
    logger.error("‚ùå 'langchain-ollama' not found. Please install it using 'pip install langchain-ollama'")
    OllamaLLM = None

logger = logging.getLogger(__name__)

# --- Configuration Constants (‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å Global Config) ---
from config.global_vars import (
    DEFAULT_LLM_MODEL_NAME, 
    LLM_CONTEXT_WINDOW, 
    LLM_TEMPERATURE,
    RAG_RUN_MODE
)

# -----------------------------------------------------
# üéØ Global LLM Instance (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Backward Compatibility)
# -----------------------------------------------------
# ‡πÄ‡∏Å‡πá‡∏ö‡∏≠‡∏¥‡∏ô‡∏™‡πÅ‡∏ï‡∏ô‡∏ã‡πå‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ö‡∏™‡πà‡∏ß‡∏ô‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏ö
llm: Optional[Any] = None 

# -----------------------------------------------------
# üõ†Ô∏è Factory Logic: create_llm_instance
# -----------------------------------------------------

def create_llm_instance(
    model_name: Optional[str] = None,
    temperature: float = LLM_TEMPERATURE,
    context_window: Optional[int] = None
) -> Optional[Any]:
    """
    Initializes and returns the appropriate Ollama LLM instance.
    ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏ö‡∏ô Mac (localhost) ‡πÅ‡∏•‡∏∞ Cloud (IP/URL ‡∏ú‡πà‡∏≤‡∏ô .env)
    """
    global llm 
    
    # 1. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡πà‡∏≤ Default ‡∏ï‡∏≤‡∏°‡πÇ‡∏´‡∏°‡∏î (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏∞‡∏ö‡∏∏ Model ‡∏´‡∏£‡∏∑‡∏≠ Context ‡∏°‡∏≤)
    selected_model = model_name or DEFAULT_LLM_MODEL_NAME
    selected_ctx = context_window or LLM_CONTEXT_WINDOW
    
    # 2. ‡∏î‡∏∂‡∏á Base URL ‡∏à‡∏≤‡∏Å Environment Variable
    # - ‡∏ö‡∏ô Mac/Local: http://localhost:11434 (‡∏´‡∏£‡∏∑‡∏≠ http://host.docker.internal:11434 ‡πÉ‡∏ô Docker)
    # - ‡∏ö‡∏ô Cloud: http://<server-ip>:11434
    base_url = os.environ.get("OLLAMA_BASE_URL", "http://localhost:11434")
    
    logger.warning(f"‚ö†Ô∏è Initializing LLM in {RAG_RUN_MODE} mode")
    logger.info(f"üìç Target Model: {selected_model}")
    logger.info(f"üîó Ollama URL: {base_url}")
    logger.info(f"üß† Context Window: {selected_ctx}")

    try:
        if OllamaLLM is None:
            raise ImportError("langchain-ollama is required for this project.")

        # 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á Instance ‡∏Ç‡∏≠‡∏á OllamaLLM
        # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ timeout ‡πÑ‡∏ß‡πâ‡∏™‡∏π‡∏á‡∏´‡∏ô‡πà‡∏≠‡∏¢ (600s) ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ 70B ‡∏ö‡∏ô Cloud ‡∏≠‡∏≤‡∏à‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ô‡∏≤‡∏ô‡πÉ‡∏ô‡∏ö‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
        llm_instance = OllamaLLM(
            model=selected_model,
            temperature=temperature,
            num_ctx=selected_ctx,
            base_url=base_url,
            timeout=600,
            num_predict=4096
            # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏ó‡∏µ‡πà Ollama ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà
        )

        logger.info(f"‚úÖ LLM Instance created successfully: {selected_model}")
        
        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡πâ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£ Global ‡∏´‡∏≤‡∏Å‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î
        if llm is None:
            llm = llm_instance
            
        return llm_instance

    except Exception as e:
        logger.error(f"‚ùå Failed to initialize Ollama LLM: {e}")
        return None

# -----------------------------------------------------
# üß™ Health Check (Optional)
# -----------------------------------------------------
def check_llm_connection() -> bool:
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô‡∏ß‡πà‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö Ollama Server ‡πÑ‡∏î‡πâ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà"""
    if llm:
        try:
            # ‡∏•‡∏≠‡∏á‡∏™‡πà‡∏á‡∏Ñ‡∏≥‡∏Ç‡∏≠‡∏î‡πà‡∏ß‡∏ô‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ä‡πá‡∏Ñ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞
            # llm.invoke("Hi") 
            return True
        except:
            return False
    return False