# models/llm.py
import logging
import os
from typing import Optional, Final
from langchain_core.language_models.llms import BaseLLM

# --- Conditional Imports for Specific Backend ---
# Local Ollama Backend
try:
    from langchain_ollama import OllamaLLM
except ImportError:
    OllamaLLM = None

# Cloud Backend (Placeholder/OpenAI)
try:
    import openai
except ImportError:
    openai = None

logger = logging.getLogger(__name__)

# --- CONFIGURATION CONSTANTS (‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å Global Config ‡πÅ‡∏•‡∏∞ Environment) ---
# ‡πÄ‡∏£‡∏≤‡∏à‡∏∞ import RAG_RUN_MODE, LLM_MODEL_NAME, ‡∏Ø‡∏•‡∏Ø ‡∏°‡∏≤‡∏à‡∏≤‡∏Å global_vars
from config.global_vars import (
    LLM_MODEL_NAME, 
    LLM_CONTEXT_WINDOW, 
    LLM_TEMPERATURE,
    RAG_RUN_MODE # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏ô global_vars.py
)

# ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö CLOUD Mode (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
CLOUD_LLM_MODEL: Final[str] = "gpt-4o" 
OPENAI_API_KEY: Final[str] = os.environ.get("OPENAI_API_KEY", "")


# -----------------------------------------------------
# üéØ Global LLM Instance (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Backward Compatibility)
# -----------------------------------------------------
llm: Optional[BaseLLM] = None 

# -----------------------------------------------------
# üß© Placeholder Class ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Cloud (‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô)
# -----------------------------------------------------
class CloudLLMPlaceholder(BaseLLM):
    """Placeholder for the Cloud LLM connector (e.g., OpenAI)."""
    model_name: str = CLOUD_LLM_MODEL

    def _generate(self, prompts: list[str], stop: Optional[list[str]] = None, **kwargs) -> str:
        logger.warning(f"--- Simulating CLOUD call to {self.model_name} ---")
        # ‡∏à‡∏≥‡∏•‡∏≠‡∏á JSON output ‡∏ó‡∏µ‡πà Assessment Engine ‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á
        return '{"summary": "Simulation result", "score": 5, "explanation": "This is a cloud simulation.", "evidence_map": []}' 

    @property
    def _llm_type(self) -> str:
        return "cloud-placeholder"
    
    def _call(self, prompt: str, stop: Optional[list[str]] = None, **kwargs) -> str:
        return self._generate([prompt], stop=stop, **kwargs)


# -----------------------------------------------------
# üõ†Ô∏è Factory Logic: ‡∏™‡∏£‡πâ‡∏≤‡∏á Instance ‡∏ï‡∏≤‡∏°‡πÇ‡∏´‡∏°‡∏î
# -----------------------------------------------------

def create_llm_instance(
    model_name: str = LLM_MODEL_NAME,
    temperature: float = LLM_TEMPERATURE,
    context_window: int = LLM_CONTEXT_WINDOW
) -> Optional[BaseLLM]:
    """
    Initializes and returns the appropriate LLM instance based on RAG_RUN_MODE.
    """
    global llm 
    
    selected_model = CLOUD_LLM_MODEL if RAG_RUN_MODE == "CLOUD" else model_name
    
    logger.warning(f"‚ö†Ô∏è Initializing LLM in {RAG_RUN_MODE} mode with model: {selected_model}")

    try:
        llm_instance = None
        
        # --- 1. CLOUD MODE ---
        if RAG_RUN_MODE == "CLOUD":
            if not OPENAI_API_KEY and not os.environ.get('OPENAI_API_KEY'):
                logger.error("‚ùå CLOUD mode requires OPENAI_API_KEY to be set.")
                return None
            
            llm_instance = CloudLLMPlaceholder(
                model_name=selected_model,
                temperature=temperature
            )
        
        # --- 2. LOCAL_OLLAMA MODE ---
        elif RAG_RUN_MODE == "LOCAL_OLLAMA":
            if OllamaLLM is None:
                raise ImportError("langchain-ollama is required for LOCAL_OLLAMA mode.")

            llm_instance = OllamaLLM(
                model=selected_model,
                temperature=temperature,
                context_window=context_window
            )
        
        else:
            raise ValueError(f"Unknown RAG_RUN_MODE: {RAG_RUN_MODE}. Must be 'LOCAL_OLLAMA' or 'CLOUD'.")


        logger.info(f"‚úÖ LLM Instance created successfully: {selected_model} (Mode: {RAG_RUN_MODE})")
        
        # 3. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡πâ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£ Global
        if llm is None:
            llm = llm_instance
            logger.debug("Global 'llm' variable set for backward compatibility.")
            
        return llm_instance

    except Exception as e:
        logger.error(f"‚ùå Failed to initialize LLM in {RAG_RUN_MODE} mode: {e}")
        return None
    
    
# import logging
# from typing import Optional, Final
# from langchain_ollama import OllamaLLM
# from langchain_core.language_models.llms import BaseLLM

# logger = logging.getLogger(__name__)

# # --- CONFIGURATION CONSTANTS ---
# # LLM_MODEL: Final[str] = "mistral:latest"
# # LLM_MODEL: Final[str] = LLM_MODEL_NAME
# from config.global_vars import LLM_MODEL_NAME, LLM_CONTEXT_WINDOW, LLM_TEMPERATURE

# # -----------------------------------------------------
# # üéØ Global LLM Instance (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Legacy Code ‡∏¢‡∏±‡∏á‡∏Ñ‡∏á Import ‡πÑ‡∏î‡πâ)
# # -----------------------------------------------------
# llm: Optional[BaseLLM] = None 

# def create_llm_instance(
#     model_name: str = LLM_MODEL_NAME,
#     temperature: float = LLM_TEMPERATURE,
#     context_window: int = LLM_CONTEXT_WINDOW
# ) -> Optional[BaseLLM]:
#     """
#     Initializes and returns a new Ollama LLM instance. 
#     It also sets the global 'llm' variable if it's currently None (for compatibility).
#     """
#     global llm # ‚¨ÖÔ∏è ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£ Global
    
#     # 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á Instance ‡πÉ‡∏´‡∏°‡πà
#     try:
#         logger.warning(f"‚ö†Ô∏è Initializing LLM: model={model_name}, temperature={temperature}")
#         llm_instance = OllamaLLM(
#             model=model_name,
#             temperature=temperature,
#             context_window=context_window
#         )
#         logger.info(f"‚úÖ LLM Instance created successfully: {model_name} (Temp: {temperature})")
        
#         # 2. üü¢ ‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏° Global: ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡πâ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£ Global ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏≠‡∏¢‡∏π‡πà
#         if llm is None:
#             llm = llm_instance
#             logger.debug("Global 'llm' variable set for backward compatibility.")
            
#         return llm_instance

#     except Exception as e:
#         logger.error(f"‚ùå Failed to initialize Ollama LLM: {e}")
#         return None

# models/llm.py

# import logging
# from typing import Optional, Final
# import os
# from langchain_core.language_models.llms import BaseLLM

# # --- Conditional Imports for Specific Backend ---
# # ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤ Dependencies ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô

# # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LOCAL Mode (LlamaCpp)
# try:
#     from langchain_community.llms import LlamaCpp
# except ImportError:
#     LlamaCpp = None

# # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö CLOUD Mode (OpenAI/Placeholder)
# try:
#     import openai
# except ImportError:
#     openai = None


# logger = logging.getLogger(__name__)

# # --- CONFIGURATION CONSTANTS (‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å ENV ‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Default) ---
# # ‡∏ï‡∏±‡∏ß‡∏™‡∏•‡∏±‡∏ö‡πÇ‡∏´‡∏°‡∏î‡∏´‡∏•‡∏±‡∏Å: 'LOCAL' ‡∏´‡∏£‡∏∑‡∏≠ 'CLOUD'
# RUN_MODE: Final[str] = os.environ.get("RAG_RUN_MODE", "LOCAL") 

# # ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LOCAL Mode
# LLM_MODEL: Final[str] = "llama3.1:8b" # ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô
# # üõë ‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏î‡πâ‡∏ß‡∏¢ Path ‡πÑ‡∏ü‡∏•‡πå GGUF ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á (4.9G) ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏û‡∏ö‡πÉ‡∏ô ~/.ollama/models/blobs/
# GGUF_FILE_PATH: Final[str] = "/Users/oddnaphat/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29" 
# N_GPU_LAYERS: Final[int] = -1 # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ GPU/MPS

# # ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö CLOUD Mode
# CLOUD_LLM_MODEL: Final[str] = "gpt-4o"  
# OPENAI_API_KEY: Final[str] = os.environ.get("OPENAI_API_KEY", "")

# LLM_TEMPERATURE: Final[float] = 0.0
# LLM_CONTEXT_WINDOW: Final[int] = 8192

# # -----------------------------------------------------
# # üéØ Global LLM Instance (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Backward Compatibility)
# # -----------------------------------------------------
# llm: Optional[BaseLLM] = None 

# # -----------------------------------------------------
# # üß© Placeholder Class ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Cloud (‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô)
# # -----------------------------------------------------
# class CloudLLMPlaceholder(BaseLLM):
#     """
#     Placeholder class for the OpenAI/Cloud LLM connector. 
#     It mimics BaseLLM behavior but calls the OpenAI API internally.
#     """
#     model_name: str = CLOUD_LLM_MODEL

#     def _call(self, prompt: str, stop: Optional[list[str]] = None) -> str:
#         # NOTE: ‡πÉ‡∏ô Production, ‡πÇ‡∏Ñ‡πâ‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ openai.chat.completions.create
#         logger.warning(f"--- Simulating CLOUD call to {self.model_name} ---")
#         return '{"summary": "Simulation result", "score": 5, "evidence_map": []}' # ‡∏à‡∏≥‡∏•‡∏≠‡∏á JSON output

#     @property
#     def _llm_type(self) -> str:
#         return "cloud-placeholder"

# # -----------------------------------------------------
# # üõ†Ô∏è Factory Logic: ‡∏™‡∏£‡πâ‡∏≤‡∏á Instance ‡∏ï‡∏≤‡∏°‡πÇ‡∏´‡∏°‡∏î
# # -----------------------------------------------------

# def create_llm_instance(
#     model_name: str = LLM_MODEL,
#     temperature: float = LLM_TEMPERATURE,
#     context_window: int = LLM_CONTEXT_WINDOW
# ) -> Optional[BaseLLM]:
#     """
#     Initializes and returns the appropriate LLM instance based on RUN_MODE (Factory Pattern).
#     """
#     global llm 
    
#     selected_model = CLOUD_LLM_MODEL if RUN_MODE == "CLOUD" else model_name
    
#     logger.warning(f"‚ö†Ô∏è Initializing LLM in {RUN_MODE} mode with model: {selected_model}")

#     try:
#         llm_instance = None
        
#         # --- 1. CLOUD MODE (GPT-4o / Production Test) ---
#         if RUN_MODE == "CLOUD":
#             if not OPENAI_API_KEY and not os.environ.get('OPENAI_API_KEY'):
#                 logger.error("‚ùå CLOUD mode requires OPENAI_API_KEY to be set.")
#                 return None
            
#             llm_instance = CloudLLMPlaceholder(
#                 model_name=selected_model,
#                 temperature=temperature
#             )
        
#         # --- 2. LOCAL MODE (LlamaCpp / Local Dev) ---
#         elif RUN_MODE == "LOCAL":
#             if LlamaCpp is None:
#                 raise ImportError("llama-cpp-python is required for LOCAL mode (GGUF backend).")

#             logger.info(f"üíæ Loading GGUF model from: {GGUF_FILE_PATH}")
#             llm_instance = LlamaCpp(
#                 model_path=GGUF_FILE_PATH,
#                 n_gpu_layers=N_GPU_LAYERS,
#                 n_ctx=context_window,
#                 temperature=temperature,
#                 n_threads=0,
#                 verbose=False
#             )
        
#         else:
#             raise ValueError(f"Unknown RUN_MODE: {RUN_MODE}. Must be 'LOCAL' or 'CLOUD'.")


#         logger.info(f"‚úÖ LLM Instance created successfully: {selected_model} (Mode: {RUN_MODE})")
        
#         # 3. üü¢ ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡πâ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£ Global
#         if llm is None:
#             llm = llm_instance
#             logger.debug("Global 'llm' variable set for backward compatibility.")
            
#         return llm_instance

#     except Exception as e:
#         logger.error(f"‚ùå Failed to initialize LLM in {RUN_MODE} mode: {e}")
#         return None