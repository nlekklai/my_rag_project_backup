# models/llm.py
import logging
from typing import Optional, Final
from langchain_ollama import OllamaLLM
from langchain_core.language_models.llms import BaseLLM

logger = logging.getLogger(__name__)

# --- CONFIGURATION CONSTANTS ---
LLM_MODEL: Final[str] = "llama3.1:8b"
LLM_TEMPERATURE: Final[float] = 0.0
LLM_CONTEXT_WINDOW: Final[int] = 4096

# -----------------------------------------------------
# üéØ Global LLM Instance (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Legacy Code ‡∏¢‡∏±‡∏á‡∏Ñ‡∏á Import ‡πÑ‡∏î‡πâ)
# -----------------------------------------------------
llm: Optional[BaseLLM] = None 

def create_llm_instance(
    model_name: str = LLM_MODEL,
    temperature: float = LLM_TEMPERATURE,
    context_window: int = LLM_CONTEXT_WINDOW
) -> Optional[BaseLLM]:
    """
    Initializes and returns a new Ollama LLM instance. 
    It also sets the global 'llm' variable if it's currently None (for compatibility).
    """
    global llm # ‚¨ÖÔ∏è ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£ Global
    
    # 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á Instance ‡πÉ‡∏´‡∏°‡πà
    try:
        logger.warning(f"‚ö†Ô∏è Initializing LLM: model={model_name}, temperature={temperature}")
        llm_instance = OllamaLLM(
            model=model_name,
            temperature=temperature,
            context_window=context_window
        )
        logger.info(f"‚úÖ LLM Instance created successfully: {model_name} (Temp: {temperature})")
        
        # 2. üü¢ ‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏° Global: ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡πâ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£ Global ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏≠‡∏¢‡∏π‡πà
        if llm is None:
            llm = llm_instance
            logger.debug("Global 'llm' variable set for backward compatibility.")
            
        return llm_instance

    except Exception as e:
        logger.error(f"‚ùå Failed to initialize Ollama LLM: {e}")
        return None

