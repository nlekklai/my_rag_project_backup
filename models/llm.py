# -*- coding: utf-8 -*-
# models/llm.py - Production Version (Ollama Unified Connector)
# ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á Local (Mac 8B) ‡πÅ‡∏•‡∏∞ Cloud (NVIDIA 70B) ‡∏ú‡πà‡∏≤‡∏ô Ollama

import logging
import os
from typing import Optional, Any, Final
from langchain_core.language_models.llms import BaseLLM

# --- Conditional Imports ---
try:
    from langchain_ollama import OllamaLLM
except ImportError:
    logger = logging.getLogger(__name__)
    logger.error("‚ùå 'langchain-ollama' not found. Please install it using 'pip install langchain-ollama'")
    OllamaLLM = None

logger = logging.getLogger(__name__)

# --- Configuration Constants (‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å Global Config) ---
from config.global_vars import (
    DEFAULT_LLM_MODEL_NAME, 
    LLM_CONTEXT_WINDOW, 
    LLM_TEMPERATURE,
    RAG_RUN_MODE,
    LLM_NUM_PREDICT
)

# -----------------------------------------------------
# üéØ Global LLM Instance (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Backward Compatibility)
# -----------------------------------------------------
# ‡πÄ‡∏Å‡πá‡∏ö‡∏≠‡∏¥‡∏ô‡∏™‡πÅ‡∏ï‡∏ô‡∏ã‡πå‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ö‡∏™‡πà‡∏ß‡∏ô‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏ö
llm: Optional[Any] = None 

# -----------------------------------------------------
# üõ†Ô∏è Factory Logic: create_llm_instance
# -----------------------------------------------------

def create_llm_instance(
    model_name: Optional[str] = None,
    temperature: float = LLM_TEMPERATURE,
    context_window: Optional[int] = None
) -> Optional[Any]:
    global llm 
    
    selected_model = model_name or DEFAULT_LLM_MODEL_NAME
    selected_ctx = context_window or LLM_CONTEXT_WINDOW
    
    base_url = os.environ.get("OLLAMA_BASE_URL", "http://localhost:11434")
    
    # ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡∏•‡∏¥‡∏°‡∏¥‡∏ï Token ‡∏à‡∏≤‡∏Å Global Vars (Mac=2048, Server=4096)
    selected_predict = LLM_NUM_PREDICT 

    logger.warning(f"‚ö†Ô∏è Initializing LLM in {RAG_RUN_MODE} mode")
    logger.info(f"üìç Model: {selected_model} | Predict: {selected_predict}")

    try:
        if OllamaLLM is None:
            raise ImportError("langchain-ollama is required.")

        # üéØ ‡∏õ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°
        llm_instance = OllamaLLM(
            model=selected_model,
            temperature=temperature,
            num_ctx=selected_ctx,
            base_url=base_url,
            timeout=600,
            # num_predict ‡∏Ñ‡∏∑‡∏≠‡∏´‡∏±‡∏ß‡πÉ‡∏à‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ JSON ‡∏ï‡∏±‡∏î‡∏à‡∏ö
            num_predict=selected_predict 
        )

        logger.info(f"‚úÖ LLM Instance created successfully")
        if llm is None:
            llm = llm_instance
        return llm_instance

    except Exception as e:
        logger.error(f"‚ùå Failed to initialize Ollama LLM: {e}")
        return None

# -----------------------------------------------------
# üß™ Health Check (Optional)
# -----------------------------------------------------
def check_llm_connection() -> bool:
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô‡∏ß‡πà‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö Ollama Server ‡πÑ‡∏î‡πâ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà"""
    if llm:
        try:
            # ‡∏•‡∏≠‡∏á‡∏™‡πà‡∏á‡∏Ñ‡∏≥‡∏Ç‡∏≠‡∏î‡πà‡∏ß‡∏ô‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ä‡πá‡∏Ñ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞
            # llm.invoke("Hi") 
            return True
        except:
            return False
    return False