from fastapi import APIRouter, UploadFile, File, Form, HTTPException, BackgroundTasks, Path, Query, Depends, status
from fastapi.responses import FileResponse
from starlette.concurrency import run_in_threadpool
from pydantic import BaseModel
from typing import List, Optional, Union, Tuple, Dict, Any # üìå ‡πÄ‡∏û‡∏¥‡πà‡∏° Dict, Any
from datetime import datetime, timezone
import logging, os, sys, uuid
from pathlib import Path as SysPath

# -----------------------------
# --- Project Modules ---
# -----------------------------
try:
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
    if project_root not in sys.path:
        sys.path.append(project_root)

    from config.global_vars import (
        SUPPORTED_DOC_TYPES,
        DEFAULT_ENABLER, 
        EVIDENCE_DOC_TYPES
    )
    from core.ingest import (
        process_document,
        list_documents,
        delete_document_by_uuid,
        # DocInfo, # üìå ‡∏ñ‡πâ‡∏≤ DocInfo ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏ä‡πâ‡πÅ‡∏•‡πâ‡∏ß‡∏Å‡πá‡∏•‡∏ö‡∏≠‡∏≠‡∏Å ‡∏´‡∏£‡∏∑‡∏≠‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á
    )
    # --- NEW IMPORT for Auth ---
    from routers.auth_router import UserMe, get_current_user
    # ---------------------------
    
    # üü¢ FIX: Import Path Utility ‡πÅ‡∏•‡∏∞ Central Logic ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏¢‡πâ‡∏≤‡∏¢‡∏°‡∏≤
    from utils.path_utils import get_document_source_dir, get_normalized_metadata 

except ImportError as e:
    print(f"‚ùå Import error: {e}")
    raise

# -----------------------------
# --- Router & Logger ---
# -----------------------------
upload_router = APIRouter(prefix="/api", tags=["Upload / Ingest"])

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
if not any(isinstance(h, logging.StreamHandler) for h in logger.handlers):
    handler = logging.StreamHandler()
    handler.setFormatter(logging.Formatter("%(asctime)s - %(levelname)s - %(message)s"))
    logger.addHandler(handler)

# -----------------------------
# --- Models ---
# -----------------------------
class UploadResponse(BaseModel):
    doc_id: str
    status: str
    filename: str
    doc_type: str
    file_type: Optional[str] = None
    upload_date: Optional[str] = None
    message: Optional[str] = None
    chunk_count: Optional[int] = None
    size: Optional[float] = None
    enabler: Optional[str] = None
    tenant: Optional[str] = None
    year: Optional[Union[int, str]] = None 

# -----------------------------
# --- Helper Function for File Path (USES CENTRAL LOGIC) ---
# -----------------------------
def get_save_dir(doc_type: str, tenant: str, year: Optional[Union[int, str]], enabler: Optional[str] = None) -> str:
    """
    Constructs the segregated directory path for saving files using Path Utility.
    The actual path is normalized based on doc_type (Global vs. Evidence).
    """
    
    # üìå ‡πÉ‡∏ä‡πâ Central Logic ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏Ñ‡πà‡∏≤ Year/Enabler ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å Normalize ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Path
    normalized_year, normalized_enabler = get_normalized_metadata(
        doc_type=doc_type,
        year_input=year, 
        enabler_input=enabler,
        default_enabler=DEFAULT_ENABLER
    )
    
    return get_document_source_dir(
        tenant=tenant,
        # ‡πÉ‡∏ä‡πâ normalized values ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö path construction
        year=normalized_year, 
        enabler=normalized_enabler, 
        doc_type=doc_type
    )

# -----------------------------
# --- Upload with background processing ---
# -----------------------------
@upload_router.post("/upload/{doc_type}", response_model=UploadResponse)
async def upload_document(
    doc_type: str = Path(..., description=f"Document type. Must be one of: {SUPPORTED_DOC_TYPES}"),
    file: UploadFile = File(..., description="Document file to upload"),
    enabler: Optional[str] = Form(None, description="Enabler code (used for evidence doc_type)"),
    background_tasks: BackgroundTasks = None,
    current_user: UserMe = Depends(get_current_user), # <-- User Dependency
):
    # --- LOGGING DEBUG INFO ---
    user_id_display = getattr(current_user, 'id', 'N/A')
    logger.info(
        f"USER CONTEXT (Upload): ID={user_id_display}, Tenant={current_user.tenant}, Year={current_user.year} (Type: {type(current_user.year)})"
    )
    # --------------------------
    
    if doc_type not in SUPPORTED_DOC_TYPES:
        raise HTTPException(400, detail=f"Invalid doc_type. Must be one of: {SUPPORTED_DOC_TYPES}")
        
    # üìå ‡πÉ‡∏ä‡πâ Logic Normalized Metadata ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å (‡∏õ‡∏µ‡πÅ‡∏•‡∏∞ Enabler ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÉ‡∏ô Mapping DB)
    normalized_year, normalized_enabler = get_normalized_metadata(
        doc_type=doc_type,
        year_input=current_user.year,
        enabler_input=enabler,
        default_enabler=DEFAULT_ENABLER
    )

    # Folder for file storage (‡∏™‡πà‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏°‡πÑ‡∏õ‡πÉ‡∏´‡πâ get_save_dir ‡∏ã‡∏∂‡πà‡∏á‡∏à‡∏∞ Normalize ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô)
    save_dir = get_save_dir(doc_type, current_user.tenant, current_user.year, enabler)
    os.makedirs(save_dir, exist_ok=True)

    timestamp = datetime.now(timezone.utc).strftime("%Y%m%d%H%M%S")
    sanitized_filename = SysPath(file.filename).name
    if not sanitized_filename:
        sanitized_filename = f"uploaded_{timestamp}.tmp"

    file_path = os.path.join(save_dir, f"{timestamp}_{uuid.uuid4().hex}_{sanitized_filename}")
    mock_doc_id = f"temp-{uuid.uuid4().hex}"

    try:
        contents = await file.read()
        await run_in_threadpool(lambda: open(file_path, "wb").write(contents))

        # Process document in background
        if background_tasks:
            background_tasks.add_task(
                process_document,
                file_path=file_path,
                file_name=file.filename,
                stable_doc_uuid=mock_doc_id,
                doc_type=doc_type,
                tenant=current_user.tenant,
                # üìå ‡∏™‡πà‡∏á‡∏Ñ‡πà‡∏≤ Normalized ‡πÑ‡∏õ‡πÉ‡∏´‡πâ process_document ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÉ‡∏ô Mapping DB
                year=normalized_year, 
                enabler=normalized_enabler,
            )

        return UploadResponse(
            doc_id=mock_doc_id,
            status="Processing",
            filename=file.filename,
            doc_type=doc_type,
            file_type=os.path.splitext(file.filename)[1],
            upload_date=datetime.now(timezone.utc).isoformat(),
            message="Document accepted for background processing.",
            tenant=current_user.tenant,
            year=current_user.year,
            enabler=normalized_enabler,
        )
    except Exception as e:
        logger.error(f"Upload failed: {e}")
        if os.path.exists(file_path):
            os.remove(file_path)
        raise HTTPException(500, detail=f"Upload failed: {e}")

# -----------------------------
# --- List uploaded documents (CLEANED) ---
# -----------------------------
@upload_router.get("/uploads/{doc_type}", response_model=List[UploadResponse])
async def list_uploads_by_type(
    doc_type: str, 
    filter_year: Optional[int] = Query(None, alias="year", description="Filter by year (overrides user's year)"),
    filter_enabler: Optional[str] = Query(None, alias="enabler", description="Filter by enabler code (e.g. KM)"),
    current_user: UserMe = Depends(get_current_user) # <-- User Dependency
):
    # --- LOGGING DEBUG INFO ---
    user_id_display = getattr(current_user, 'id', 'N/A')
    target_year_log = str(filter_year) if filter_year is not None else str(current_user.year)
    logger.info(
        f"USER CONTEXT (List Uploads): ID={user_id_display}, DocType={doc_type}, Tenant={current_user.tenant}, Year={target_year_log} (Filtering with STR year)"
    )
    # --------------------------
    
    # üìå ‡πÉ‡∏ä‡πâ CENTRAL LOGIC ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏Ñ‡πà‡∏≤ Year/Enabler ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Query
    year_to_fetch, enabler_to_fetch = get_normalized_metadata(
        doc_type=doc_type,
        year_input=filter_year, 
        enabler_input=filter_enabler,
        default_enabler=DEFAULT_ENABLER
    )
    
    tenant_to_fetch = current_user.tenant
    
    # Support "all"
    doc_types_to_fetch = SUPPORTED_DOC_TYPES if doc_type.lower() == "all" else [doc_type]
    
    # List documents for the user's specific tenant and year
    # doc_data: List[Dict[str, Any]]
    doc_data: List[Dict[str, Any]] = await run_in_threadpool(
        lambda: list_documents(
            doc_types=doc_types_to_fetch, 
            tenant=tenant_to_fetch,
            year=year_to_fetch, 
            enabler=enabler_to_fetch 
        )
    )
    uploads: List[UploadResponse] = []

    # üü¢ FIX: ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏ú‡πà‡∏≤‡∏ô doc_data ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏Å .values() ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏õ‡πá‡∏ô List[Dict] ‡πÅ‡∏•‡πâ‡∏ß)
    # üü¢ FIX: ‡πÉ‡∏ä‡πâ Key ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏°‡∏≤‡∏à‡∏≤‡∏Å list_documents ('UUID', 'File Name', 'Status', 'Chunks', 'Doc Type', 'Enabler', 'Year')
    for doc_info in doc_data: 
        status = "Pending"
        doc_status = doc_info.get("Status", "").lower() # üìå ‡πÉ‡∏ä‡πâ Key "Status"
        doc_filename = doc_info.get("File Name") # üìå ‡πÉ‡∏ä‡πâ Key "File Name"
        doc_chunk_count = doc_info.get("Chunks", 0) # üìå ‡πÉ‡∏ä‡πâ Key "Chunks"
        
        if doc_status == "failed":
            status = "Failed"
        elif doc_chunk_count > 0 or doc_status == "ingested":
            status = "Ingested"
        elif doc_status == "processing":
            status = "Processing"
        elif doc_status == "missing":
            status = "Pending"

        # NOTE: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• size ‡πÅ‡∏•‡∏∞ upload_date ‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏ñ‡∏π‡∏Å‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å list_documents ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏Å‡πâ‡πÑ‡∏°‡πà‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ
        size_mb = doc_info.get("size", 0) / (1024*1024) if doc_info.get("size") else None

        uploads.append(
            UploadResponse(
                doc_id=doc_info.get("UUID") or "N/A", # üìå ‡πÉ‡∏ä‡πâ Key "UUID"
                filename=doc_filename,
                doc_type=doc_info.get("Doc Type", "").lower(), # üìå ‡πÉ‡∏ä‡πâ Key "Doc Type"
                file_type=os.path.splitext(doc_filename)[1] if doc_filename else None,
                status=status,
                upload_date=doc_info.get("upload_date"), 
                chunk_count=doc_chunk_count, 
                size=size_mb,
                enabler=doc_info.get("Enabler") or "-", # üìå ‡πÉ‡∏ä‡πâ Key "Enabler"
                tenant=doc_info.get("tenant") or current_user.tenant,
                year=doc_info.get("Year") or current_user.year, # üìå ‡πÉ‡∏ä‡πâ Key "Year"
            )
        )

    # Sort by doc_type then filename
    return sorted(uploads, key=lambda x: (x.doc_type, x.filename))

# -----------------------------
# --- Manual Ingest API ---
# -----------------------------
@upload_router.post("/ingest")
async def ingest_document(
    file: UploadFile = File(..., description="File to ingest"),
    doc_type: str = Form(..., description=f"Document type: {', '.join(SUPPORTED_DOC_TYPES)}"),
    enabler: Optional[str] = Form(None, description="Enabler code (used for evidence)"),
    current_user: UserMe = Depends(get_current_user), # <-- User Dependency
):
    # --- LOGGING DEBUG INFO ---
    user_id_display = getattr(current_user, 'id', 'N/A')
    logger.info(
        f"USER CONTEXT (Ingest): ID={user_id_display}, DocType={doc_type}, Tenant={current_user.tenant}, Year={current_user.year}"
    )
    # --------------------------

    if doc_type not in SUPPORTED_DOC_TYPES:
        raise HTTPException(400, detail=f"Invalid doc_type: {doc_type}")

    # üìå ‡πÉ‡∏ä‡πâ Logic Normalized Metadata ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å
    normalized_year, normalized_enabler = get_normalized_metadata(
        doc_type=doc_type,
        year_input=current_user.year,
        enabler_input=enabler,
        default_enabler=DEFAULT_ENABLER
    )
    
    # Use segregated save directory (‡∏™‡πà‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏°‡πÑ‡∏õ‡πÉ‡∏´‡πâ get_save_dir ‡∏ã‡∏∂‡πà‡∏á‡∏à‡∏∞ Normalize ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô)
    save_dir = get_save_dir(doc_type, current_user.tenant, current_user.year, enabler)
    os.makedirs(save_dir, exist_ok=True)

    # ‡πÉ‡∏ä‡πâ SysPath ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡πâ‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏¢‡∏¥‡πà‡∏á‡∏Ç‡∏∂‡πâ‡∏ô
    file_path = os.path.join(save_dir, SysPath(file.filename).name)

    try:
        contents = await file.read()
        await run_in_threadpool(lambda: open(file_path, "wb").write(contents))

        # Ingest document
        chunks, stable_doc_uuid, doc_type_result = await run_in_threadpool(
            lambda: process_document(
                file_path=file_path, 
                file_name=SysPath(file.filename).name, 
                stable_doc_uuid=str(uuid.uuid4().hex), 
                doc_type=doc_type, 
                enabler=normalized_enabler, # üìå ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Normalized
                tenant=current_user.tenant, 
                year=normalized_year # üìå ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Normalized
            )
        )
        
        return {
             "status": "success", 
             "doc_info": {
                 "doc_id": stable_doc_uuid,
                 "doc_type": doc_type_result,
                 "chunk_count": len(chunks) if chunks else 0
             }
        }
    except Exception as e:
        logger.error(f"Ingest failed: {e}")
        if os.path.exists(file_path):
            os.remove(file_path)
        raise HTTPException(500, detail=str(e))

# -----------------------------
# --- Get all documents (CLEANED) ---
# -----------------------------
@upload_router.get("/documents", response_model=List[UploadResponse])
async def get_documents(
    doc_type: Optional[str] = Query(None, description=f"Filter by doc_type: {', '.join(SUPPORTED_DOC_TYPES)}"),
    enabler: Optional[str] = Query(None, description="Filter by enabler (e.g. KM)"),
    filter_year: Optional[int] = Query(None, alias="year", description="Filter by year (overrides user's year)"),
    current_user: UserMe = Depends(get_current_user), # <-- User Dependency
):
    # --- LOGGING DEBUG INFO ---
    user_id_display = getattr(current_user, 'id', 'N/A')
    logger.info(
        f"USER CONTEXT (Get Docs): ID={user_id_display}, FilterDoc={doc_type}, Tenant={current_user.tenant}, Year={current_user.year} (Filtering with STR year)"
    )
    # --------------------------

    doc_types_to_fetch = [doc_type] if doc_type and doc_type.lower() != "all" else None
    
    # üìå ‡πÉ‡∏ä‡πâ CENTRAL LOGIC ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏Ñ‡πà‡∏≤ Year/Enabler ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Query
    doc_type_for_filter = doc_type if doc_type else EVIDENCE_DOC_TYPES 
    
    year_to_fetch, enabler_to_fetch = get_normalized_metadata(
        doc_type=doc_type_for_filter,
        year_input=filter_year, 
        enabler_input=enabler,
        default_enabler=DEFAULT_ENABLER
    )
    
    tenant_to_fetch = current_user.tenant

    # List documents for the user's specific tenant and year
    # doc_data: List[Dict[str, Any]]
    doc_data: List[Dict[str, Any]] = await run_in_threadpool(
        lambda: list_documents(
            doc_types=doc_types_to_fetch, 
            enabler=enabler_to_fetch, 
            tenant=tenant_to_fetch,
            year=year_to_fetch 
        )
    )
    uploads: List[UploadResponse] = []

    # üü¢ FIX: ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏ú‡πà‡∏≤‡∏ô doc_data ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏Å .values() ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏õ‡πá‡∏ô List[Dict] ‡πÅ‡∏•‡πâ‡∏ß)
    # üü¢ FIX: ‡πÉ‡∏ä‡πâ Key ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏°‡∏≤‡∏à‡∏≤‡∏Å list_documents ('UUID', 'File Name', 'Status', 'Chunks', 'Doc Type', 'Enabler', 'Year')
    for doc_info in doc_data:
        status = "Pending"
        doc_status = doc_info.get("Status", "").lower()
        doc_filename = doc_info.get("File Name")
        doc_chunk_count = doc_info.get("Chunks", 0)
        
        if doc_status == "failed":
            status = "Failed"
        elif doc_chunk_count > 0 or doc_status == "ingested":
            status = "Ingested"
        elif doc_status == "processing":
            status = "Processing"
        elif doc_status == "missing":
            status = "Pending"

        size_mb = doc_info.get("size", 0) / (1024*1024) if doc_info.get("size") else None

        uploads.append(
            UploadResponse(
                doc_id=doc_info.get("UUID") or "N/A", # üìå ‡πÉ‡∏ä‡πâ Key "UUID"
                filename=doc_filename,
                doc_type=doc_info.get("Doc Type", "").lower(), # üìå ‡πÉ‡∏ä‡πâ Key "Doc Type"
                file_type=os.path.splitext(doc_filename)[1] if doc_filename else None,
                status=status,
                upload_date=doc_info.get("upload_date"),
                chunk_count=doc_chunk_count, # üìå ‡πÉ‡∏ä‡πâ Key "Chunks"
                size=size_mb,
                enabler=doc_info.get("Enabler") or "-", # üìå ‡πÉ‡∏ä‡πâ Key "Enabler"
                tenant=doc_info.get("tenant") or current_user.tenant,
                year=doc_info.get("Year") or current_user.year, # üìå ‡πÉ‡∏ä‡πâ Key "Year"
            )
        )

    return sorted(uploads, key=lambda x: (x.doc_type, x.filename))

# -----------------------------
# --- Delete document ---
# -----------------------------
@upload_router.delete("/documents/{doc_id}")
async def delete_document(
    doc_id: str,
    # üí° REVISED: ‡πÄ‡∏û‡∏¥‡πà‡∏° doc_type ‡πÅ‡∏•‡∏∞ enabler ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Å‡∏≤‡∏£‡∏•‡∏ö‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Ç‡∏≠‡∏á Path
    doc_type: Optional[str] = Query(None, description=f"Document type of the file being deleted"),
    enabler: Optional[str] = Query(None, description="Enabler code (used for evidence)"),
    current_user: UserMe = Depends(get_current_user) # <-- User Dependency
):
    # --- LOGGING DEBUG INFO ---
    user_id_display = getattr(current_user, 'id', 'N/A')
    logger.info(
        f"USER CONTEXT (Delete Doc): ID={user_id_display}, DocID={doc_id}, Tenant={current_user.tenant}, Year={current_user.year}, DocType={doc_type}, Enabler={enabler}"
    )
    # --------------------------
    
    # 1. üí° REVISED: ‡πÉ‡∏ä‡πâ get_normalized_metadata ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏õ‡∏µ‡πÅ‡∏•‡∏∞ enabler ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Mapping
    doc_type_for_lookup = doc_type if doc_type else EVIDENCE_DOC_TYPES # ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏ ‡πÉ‡∏´‡πâ‡πÄ‡∏î‡∏≤‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô Evidence

    normalized_year, normalized_enabler = get_normalized_metadata(
        doc_type=doc_type_for_lookup,
        year_input=current_user.year,
        enabler_input=enabler,
        default_enabler=DEFAULT_ENABLER
    )

    success = await run_in_threadpool(
        lambda: delete_document_by_uuid(
            doc_id, 
            tenant=current_user.tenant,
            # üìå ‡∏™‡πà‡∏á‡∏Ñ‡πà‡∏≤ Normalized ‡πÑ‡∏õ‡πÉ‡∏´‡πâ delete_document_by_uuid
            year=normalized_year, 
            enabler=normalized_enabler
        )
    )
    if not success:
        raise HTTPException(
            status.HTTP_404_NOT_FOUND, 
            detail=(
                f"Document {doc_id} not found or access denied. "
                f"(Tenant: {current_user.tenant}, Context: {normalized_year}/{normalized_enabler})"
            )
        )
    return {"status": "success", "message": f"Document {doc_id} deleted successfully."}

# -----------------------------
# --- Download uploaded file ---
# -----------------------------
@upload_router.get("/upload/{doc_type}/{file_id}/download")
async def download_upload(
    doc_type: str, 
    file_id: str,
    current_user: UserMe = Depends(get_current_user) # <-- User Dependency
):
    # --- LOGGING DEBUG INFO ---
    user_id_display = getattr(current_user, 'id', 'N/A')
    logger.info(
        f"USER CONTEXT (Download): ID={user_id_display}, DocType={doc_type}, FileID={file_id}, Tenant={current_user.tenant}, Year={current_user.year}"
    )
    # --------------------------

    # üìå ‡πÉ‡∏ä‡πâ CENTRAL LOGIC ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏Ñ‡πà‡∏≤ Year/Enabler ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Query
    year_to_fetch, enabler_to_fetch = get_normalized_metadata(
        doc_type=doc_type,
        year_input=current_user.year,
        enabler_input=None, # ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á enabler ‡∏à‡∏≤‡∏Å Query
        default_enabler=DEFAULT_ENABLER
    )

    # list_documents ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ List[Dict] ‡πÅ‡∏ï‡πà‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏î‡πâ‡∏ß‡∏¢ file_id (UUID)
    doc_data: List[Dict[str, Any]] = await run_in_threadpool(
        lambda: list_documents(
            doc_types=[doc_type], 
            tenant=current_user.tenant,
            year=year_to_fetch,
            enabler=enabler_to_fetch
        )
    )
    
    # üìå FIX: ‡πÅ‡∏õ‡∏•‡∏á List[Dict] ‡πÄ‡∏õ‡πá‡∏ô Dict[UUID, DocInfo] ‡πÄ‡∏û‡∏∑‡πà‡∏≠ Lookup ‡∏î‡πâ‡∏ß‡∏¢ file_id 
    doc_map: Dict[str, Any] = next(
        (doc for doc in doc_data if doc.get("UUID") == file_id), 
        {}
    )
    
    # NOTE: list_documents ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ "filepath" ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á (‡∏ï‡∏≤‡∏° core/ingest.py ‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î)
    # ‡∏Ñ‡∏∏‡∏ì‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç list_documents ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ filepath ‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤‡∏î‡πâ‡∏ß‡∏¢
    # ‡πÅ‡∏ï‡πà‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô download_upload ‡∏¢‡∏±‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ .get("filepath") ‡πÅ‡∏•‡∏∞ .get("filename") ‡∏≠‡∏¢‡∏π‡πà
    # ‡∏ã‡∏∂‡πà‡∏á‡πÇ‡∏î‡∏¢‡∏õ‡∏Å‡∏ï‡∏¥‡πÅ‡∏•‡πâ‡∏ß‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô Mapping Database ‡πÅ‡∏ï‡πà list_documents ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏î‡∏∂‡∏á‡∏°‡∏≤
    
    # üí° ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ filepath ‡πÉ‡∏ô doc_map
    if not doc_map:
        raise HTTPException(
            status.HTTP_404_NOT_FOUND, 
            detail="Document ID not found or access denied."
        )

    # 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Path ‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á‡πÑ‡∏ü‡∏•‡πå (‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏î‡πâ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç list_documents ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ "filepath" ‡πÅ‡∏•‡∏∞ "filename" ‡∏î‡πâ‡∏ß‡∏¢)
    # üõë ‡∏ì ‡∏à‡∏∏‡∏î‡∏ô‡∏µ‡πâ ‡πÇ‡∏Ñ‡πâ‡∏î‡∏à‡∏∞‡∏û‡∏±‡∏á‡∏ñ‡πâ‡∏≤ list_documents ‡πÉ‡∏ô core/ingest.py ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ "filepath" ‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤
    filepath = doc_map.get("filepath")
    filename = doc_map.get("File Name") or "download.bin" # ‡πÉ‡∏ä‡πâ File Name ‡∏à‡∏≤‡∏Å list_documents

    if not filepath or not os.path.exists(filepath):
        logger.error(f"File path missing for doc_id {file_id}: {filepath}")
        raise HTTPException(status.HTTP_404_NOT_FOUND, detail="File not found on disk or mapping incomplete.")

    return FileResponse(filepath, filename=filename)