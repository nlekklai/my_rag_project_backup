# -*- coding: utf-8 -*-
# routers/llm_router.py - Enterprise Enhanced (Markdown Focus)

import os
import logging
import uuid
import asyncio
import json
import re
from typing import List, Optional, Any, Dict

from fastapi import APIRouter, Form, HTTPException, Depends
from pydantic import BaseModel, Field

from langchain_core.messages import SystemMessage, HumanMessage
from langchain_core.documents import Document as LcDocument

# --- Core & Utils Imports ---
from core.history_utils import async_save_message
from core.llm_data_utils import retrieve_context_for_endpoint
from core.vectorstore import get_vectorstore_manager
# ‡∏¢‡∏±‡∏á‡∏Ñ‡∏á import ‡πÑ‡∏ß‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡πà‡∏ß‡∏ô /query ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ score/reason
from core.json_extractor import _robust_extract_json 

# --- Prompts ---
from core.rag_prompts import (
    SYSTEM_QA_INSTRUCTION, 
    SYSTEM_ANALYSIS_INSTRUCTION,
    SYSTEM_COMPARE_INSTRUCTION,
    COMPARE_PROMPT # ‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤‡πÉ‡∏ô rag_prompts.py ‡πÅ‡∏Å‡πâ‡πÄ‡∏õ‡πá‡∏ô Markdown Table ‡πÅ‡∏•‡πâ‡∏ß
)
from core.llm_guardrails import detect_intent, build_prompt 

# --- Models & Config ---
from models.llm import create_llm_instance
from routers.auth_router import UserMe, get_current_user
from config.global_vars import (
    EVIDENCE_DOC_TYPES,
    DEFAULT_ENABLER,
    DEFAULT_LLM_MODEL_NAME,
    LLM_TEMPERATURE,
    QUERY_FINAL_K
)

logger = logging.getLogger(__name__)
llm_router = APIRouter(prefix="/api", tags=["LLM"])

# ===================================================================
# Models ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Request ‡πÅ‡∏•‡∏∞ Response
# ===================================================================
class QuerySource(BaseModel):
    source_id: str
    file_name: str
    chunk_text: str
    chunk_id: Optional[str] = None
    score: float

class QueryResponse(BaseModel):
    answer: str
    sources: List[QuerySource] = Field(default_factory=list)
    conversation_id: str
    result: Optional[Dict[str, Any]] = None

# ===================================================================
# 1. /query (General QA)
# ===================================================================
@llm_router.post("/query", response_model=QueryResponse)
async def query_llm(
    question: str = Form(...),
    doc_types: Optional[List[str]] = Form(None),
    doc_ids: Optional[List[str]] = Form(None),
    enabler: Optional[str] = Form(None),
    subject: Optional[str] = Form(None),
    conversation_id: Optional[str] = Form(None),
    current_user: UserMe = Depends(get_current_user),
):
    llm = create_llm_instance(model_name=DEFAULT_LLM_MODEL_NAME, temperature=LLM_TEMPERATURE)
    conv_id = conversation_id or str(uuid.uuid4())
    
    q_lower = question.lower()
    is_compare = any(word in q_lower for word in ["compare", "‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö", "‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô", "‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á", "vs"])
    
    if is_compare:
        return await compare_llm(question=question, doc_ids=doc_ids, doc_types=doc_types, current_user=current_user)

    vsm = get_vectorstore_manager(tenant=current_user.tenant)
    all_chunks = await _get_context_chunks(question, doc_types or [EVIDENCE_DOC_TYPES], set(doc_ids) if doc_ids else None, enabler or DEFAULT_ENABLER, subject, vsm, current_user)
    
    context_text = "\n\n".join([f"Source [{c.metadata.get('source')}]: {c.page_content}" for c in all_chunks])
    intent = detect_intent(question)
    base_instruction = SYSTEM_ANALYSIS_INSTRUCTION if intent.get("is_analysis") else SYSTEM_QA_INSTRUCTION
    
    messages = [
        SystemMessage(content=f"ALWAYS ANSWER IN THAI.\n{base_instruction}"),
        HumanMessage(content=build_prompt(context_text, question, intent))
    ]

    raw_res = await asyncio.to_thread(llm.invoke, messages)
    answer = raw_res.content if hasattr(raw_res, 'content') else str(raw_res)
    
    await async_save_message(conv_id, "user", question)
    await async_save_message(conv_id, "ai", answer)

    return QueryResponse(answer=answer.strip(), sources=_map_sources(all_chunks), conversation_id=conv_id)

# ===================================================================
# 2. /compare (Markdown Table Version - Optimized for Enterprise)
# ===================================================================
@llm_router.post("/compare", response_model=QueryResponse)
async def compare_llm(
    question: str = Form(...),
    doc_ids: List[str] = Form(...),
    doc_types: Optional[List[str]] = Form(None),
    current_user: UserMe = Depends(get_current_user),
):
    # 1. Validation: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
    if not doc_ids or len(doc_ids) < 2:
        raise HTTPException(
            status_code=400, 
            detail="‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 2 ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"
        )

    llm = create_llm_instance(model_name=DEFAULT_LLM_MODEL_NAME, temperature=LLM_TEMPERATURE)
    vsm = get_vectorstore_manager(tenant=current_user.tenant)
    
    # 2. ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö Strict Filter (‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô)
    async def fetch_doc(d_id, index):
        # ‡∏Ç‡∏¢‡∏≤‡∏¢ Query ‡πÉ‡∏´‡πâ‡∏™‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ User ‡∏û‡∏¥‡∏°‡∏û‡πå‡∏°‡∏≤‡∏™‡∏±‡πâ‡∏ô‡πÜ (‡πÄ‡∏ä‡πà‡∏ô‡∏û‡∏¥‡∏°‡∏û‡πå‡πÅ‡∏Ñ‡πà "KM")
        enhanced_query = question
        if len(question.strip()) < 10:
            enhanced_query = f"‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö {question} ‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ô‡∏µ‡πâ"

        res = await asyncio.to_thread(
            retrieve_context_for_endpoint,
            vectorstore_manager=vsm,
            query=enhanced_query, # ‡πÉ‡∏ä‡πâ query ‡∏ó‡∏µ‡πà‡∏Ç‡∏¢‡∏≤‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏•‡πâ‡∏ß
            tenant=current_user.tenant,
            year=current_user.year,
            stable_doc_ids={d_id}, 
            doc_type=doc_types[0] if doc_types else EVIDENCE_DOC_TYPES,
            enabler=DEFAULT_ENABLER,
            k_to_retrieve=40, # üéØ ‡∏î‡∏∂‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡∏à‡∏≤‡∏Å 15 ‡πÄ‡∏õ‡πá‡∏ô 40 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏±‡πâ‡∏á‡πÑ‡∏ü‡∏•‡πå
            # ‡∏•‡∏ö strict_filter ‡∏≠‡∏≠‡∏Å‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Error 
            # ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÉ‡∏ô llm_data_utils.py ‡πÄ‡∏£‡∏≤‡∏°‡∏µ Double-Gate Filter ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß
        )
        
        evidences = res.get("top_evidences", []) if isinstance(res, dict) else []
        
        # ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏≤‡∏Å Metadata (‡∏ñ‡πâ‡∏≤‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ ID ‡πÅ‡∏ó‡∏ô)
        file_name = "Unknown File"
        if evidences:
            first_meta = evidences[0]
            file_name = first_meta.get("source") or first_meta.get("file_name") or f"ID: {d_id}"
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Context Block ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
        content_text = "\n".join([f"- {e['text']}" for e in evidences])
        
        formatted_content = f"### [‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà {index}]: {file_name}\n"
        if not content_text:
            formatted_content += "(‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ)\n"
        else:
            formatted_content += f"{content_text}\n"
            
        return {
            "formatted_content": formatted_content,
            "evidences": evidences
        }

    # 3. ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô‡πÅ‡∏ö‡∏ö Parallel
    fetch_results = await asyncio.gather(*[fetch_doc(d_id, i+1) for i, d_id in enumerate(doc_ids)])
    
    doc_contents = [r["formatted_content"] for r in fetch_results]
    comparison_sources = []
    for r in fetch_results:
        comparison_sources.extend(r["evidences"])

    # 4. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Prompt ‡πÇ‡∏î‡∏¢‡πÉ‡∏™‡πà Context ‡∏ó‡∏µ‡πà‡πÅ‡∏¢‡∏Å‡∏™‡πà‡∏ß‡∏ô‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
    # ‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤ COMPARE_PROMPT ‡πÉ‡∏ô rag_prompts.py ‡∏°‡∏µ '‡∏Å‡∏é‡πÄ‡∏´‡∏•‡πá‡∏Å' ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏ä‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏†‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å
    user_compare_content = COMPARE_PROMPT.format(
        documents_content="\n\n".join(doc_contents), 
        query=question
    )

    # 5. ‡∏™‡∏±‡πà‡∏á‡∏á‡∏≤‡∏ô LLM
    messages = [
        SystemMessage(content=(
            "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ô‡∏±‡∏Å‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• SE-AM ‡∏°‡∏∑‡∏≠‡∏≠‡∏≤‡∏ä‡∏µ‡∏û "
            "‡∏à‡∏á‡∏ï‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö Markdown Table ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô "
            "‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏î‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏≠‡∏Å‡πÄ‡∏´‡∏ô‡∏∑‡∏≠‡∏à‡∏≤‡∏Å Context ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡πÑ‡∏ß‡πâ "
            "‡∏´‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏û‡∏≠‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏∏‡∏ß‡πà‡∏≤ '‡πÑ‡∏°‡πà‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•' ‡πÉ‡∏ô‡∏ä‡πà‡∏≠‡∏á‡∏ô‡∏±‡πâ‡∏ô‡πÜ"
        )),
        HumanMessage(content=user_compare_content)
    ]

    try:
        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ invoke ‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ response
        raw_res_obj = await asyncio.to_thread(llm.invoke, messages)
        answer = raw_res_obj.content if hasattr(raw_res_obj, 'content') else str(raw_res_obj)
        
        # 6. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤
        conv_id = str(uuid.uuid4())
        await async_save_message(conv_id, "user", f"[‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£] {question}")
        await async_save_message(conv_id, "ai", answer)

        # 7. ‡∏™‡πà‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏•‡∏±‡∏ö (Markdown Table ‡∏à‡∏∞‡πÄ‡∏£‡∏ô‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡∏ó‡∏µ‡πà‡∏´‡∏ô‡πâ‡∏≤‡∏ö‡πâ‡∏≤‡∏ô)
        return QueryResponse(
            answer=answer.strip(),
            sources=_map_sources_from_list(comparison_sources),
            conversation_id=conv_id,
            result=None # ‡∏õ‡∏¥‡∏î JSON Parse ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£
        )

    except Exception as e:
        logger.error(f"Error in compare_llm: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail="‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö")

# ===================================================================
# Helpers
# ===================================================================

def _map_sources_from_list(evidences):
    # ‡∏Å‡∏≥‡∏à‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô‡∏Ç‡∏≠‡∏á source ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö
    seen = set()
    unique_sources = []
    for e in evidences:
        if e.get("doc_id") not in seen:
            unique_sources.append(e)
            seen.add(e.get("doc_id"))
    
    return [QuerySource(
        source_id=str(e.get("doc_id", "unknown")),
        file_name=e.get("source", "Unknown"),
        chunk_text=e.get("text", "")[:200] + "...", # ‡∏¢‡πà‡∏≠ text ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏Ç‡∏ô‡∏≤‡∏î response
        chunk_id=e.get("chunk_uuid"),
        score=float(e.get("score", 0))
    ) for e in unique_sources][:10]

async def _get_context_chunks(question, doc_types, stable_doc_ids, enabler, subject, vsm, user):
    tasks = [
        asyncio.to_thread(
            retrieve_context_for_endpoint,
            vectorstore_manager=vsm, query=question, doc_type=dt,
            enabler=enabler, stable_doc_ids=stable_doc_ids,
            tenant=user.tenant, year=user.year, subject=subject
        ) for dt in doc_types
    ]
    
    results = await asyncio.gather(*tasks, return_exceptions=True)
    all_chunks = []
    for res in results:
        if isinstance(res, dict) and "top_evidences" in res:
            for ev in res["top_evidences"]:
                all_chunks.append(LcDocument(
                    page_content=ev["text"],
                    metadata={
                        "score": ev.get("score", 0),
                        "doc_id": ev.get("doc_id"),
                        "source": ev.get("source"),
                        "chunk_uuid": ev.get("chunk_uuid")
                    }
                ))
    
    all_chunks.sort(key=lambda x: x.metadata.get("score", 0), reverse=True)
    return all_chunks[:QUERY_FINAL_K]

def _map_sources(chunks):
    return [QuerySource(
        source_id=str(c.metadata.get("doc_id", "unknown")),
        file_name=c.metadata.get("source", "Unknown"),
        chunk_text=c.page_content,
        chunk_id=c.metadata.get("chunk_uuid"),
        score=float(c.metadata.get("score", 0))
    ) for c in chunks]