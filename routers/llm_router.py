# routers/llm_router.py
import logging
import uuid
import asyncio
from typing import List, Optional

from fastapi import APIRouter, Form, HTTPException, Request, Depends
from fastapi.concurrency import run_in_threadpool
from pydantic import BaseModel, Field

# LangChain
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_core.documents import Document as LcDocument
from langchain_core.output_parsers import PydanticOutputParser

# Project imports (‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏•‡πâ‡∏ß)
from core.history_utils import async_save_message, async_load_conversation_history
# üí° FIX: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô retrieve_context_with_filter ‡πÄ‡∏õ‡πá‡∏ô retrieve_context_for_endpoint
from core.llm_data_utils import retrieve_context_for_endpoint, retrieve_context_by_doc_ids
from core.vectorstore import get_vectorstore_manager
from core.rag_prompts import (
    SYSTEM_QA_INSTRUCTION,
    QA_PROMPT,
    SYSTEM_COMPARE_INSTRUCTION,
    COMPARE_PROMPT
)
from core.llm_guardrails import detect_intent, build_prompt
from models.llm import create_llm_instance
# üü¢ FIX: ‡∏ï‡πâ‡∏≠‡∏á Import UserMe ‡πÅ‡∏•‡∏∞ get_current_user ‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Dependency Injection
from routers.auth_router import UserMe, get_current_user 

from config.global_vars import (
    DEFAULT_ENABLER,
    EVIDENCE_DOC_TYPES,
    FINAL_K_RERANKED,
    QUERY_INITIAL_K,
    QUERY_FINAL_K,
    DEFAULT_LLM_MODEL_NAME
    # üí• ‡∏•‡∏ö DATA_DIR ‡πÅ‡∏•‡∏∞ VECTORSTORE_DIR ‡∏≠‡∏≠‡∏Å‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÉ‡∏ô Router ‡∏ô‡∏µ‡πâ‡πÅ‡∏•‡πâ‡∏ß
)

logger = logging.getLogger(__name__)
llm_router = APIRouter(prefix="/api", tags=["LLM"])


# =============================
#    Pydantic Models
# =============================
class QuerySource(BaseModel):
    source_id: str
    file_name: str
    chunk_text: str
    chunk_id: Optional[str] = None
    score: float

class QueryResponse(BaseModel):
    answer: str
    sources: List[QuerySource] = Field(default_factory=list)
    conversation_id: str


# ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö /compare (‡πÉ‡∏ä‡πâ Pydantic Parser ‚Üí ‡πÅ‡∏°‡πà‡∏ô 100%)
class ComparisonItem(BaseModel):
    metric: str
    doc1: str
    doc2: str
    delta: str
    remark: Optional[str] = ""

class ComparisonOutput(BaseModel):
    metrics: List[ComparisonItem]
    overall_summary: str

class CompareResponse(BaseModel):
    result: ComparisonOutput
    status: str = "success"


# =============================
#    /query ‚Üí RAG ‡∏™‡∏∏‡∏î‡∏¢‡∏≠‡∏î (‡πÄ‡∏£‡πá‡∏ß + ‡πÅ‡∏°‡πà‡∏ô + ‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢)
# =============================
@llm_router.post("/query", response_model=QueryResponse)
async def query_llm(
    request: Request,
    question: str = Form(...),
    doc_ids: Optional[List[str]] = Form(None),
    doc_types: Optional[List[str]] = Form(None),
    enabler: Optional[str] = Form(None),
    subject: Optional[str] = Form(None), # üü¢ ‡πÄ‡∏û‡∏¥‡πà‡∏° subject argument
    conversation_id: Optional[str] = Form(None),
    current_user: UserMe = Depends(get_current_user), # <--- üü¢ FIX: ‡πÄ‡∏û‡∏¥‡πà‡∏° User Dependency
):
    llm = create_llm_instance(model_name=DEFAULT_LLM_MODEL_NAME, temperature=0.0)
    if not llm:
        raise HTTPException(status_code=503, detail="LLM service unavailable")

    # üìå ‡∏î‡∏∂‡∏á‡∏ö‡∏£‡∏¥‡∏ö‡∏ó Tenant ‡πÅ‡∏•‡∏∞ Year ‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
    tenant_context = current_user.tenant
    year_context = current_user.year
    
    conversation_id = conversation_id or str(uuid.uuid4())
    # ‡πÉ‡∏ä‡πâ enabler ‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏°‡∏≤ ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ DEFAULT_ENABLER ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ (‡πÄ‡∏ä‡πà‡∏ô 'KM')
    enabler = enabler or DEFAULT_ENABLER 
    doc_types = doc_types or [EVIDENCE_DOC_TYPES] # üí° FIX: ‡∏ï‡πâ‡∏≠‡∏á‡∏™‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô list ‡πÄ‡∏™‡∏°‡∏≠
    doc_ids = doc_ids or []
    
    # --- LOGGING DEBUG INFO ---
    user_id_display = getattr(current_user, 'id', 'N/A')
    logger.info(
        f"USER CONTEXT (Query): ID={user_id_display}, Tenant={tenant_context}, Year={year_context}, DocTypes={doc_types}"
    )
    # --------------------------

    vsm = get_vectorstore_manager()

    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å + ‡πÇ‡∏´‡∏•‡∏î history ‡πÅ‡∏ö‡∏ö async (‡πÑ‡∏°‡πà‡πÅ‡∏Ç‡πà‡∏á‡∏Å‡∏±‡∏ô‡∏≠‡∏µ‡∏Å‡∏ï‡πà‡∏≠‡πÑ‡∏õ)
    await async_save_message(conversation_id, "user", question)
    history_messages = await async_load_conversation_history(conversation_id)

    # ‡πÉ‡∏ä‡πâ guardrails ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏´‡πâ‡∏â‡∏•‡∏≤‡∏î‡∏™‡∏∏‡∏î ‡πÜ
    intent = detect_intent(question)

    # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö parallel ‚Üí ‡πÄ‡∏£‡πá‡∏ß‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡∏™‡∏≤‡∏°‡πÇ‡∏•‡∏Å
    all_chunks: List[LcDocument] = []
    if vsm:
        # üí° ‡∏™‡∏£‡πâ‡∏≤‡∏á Set ‡∏•‡πà‡∏ß‡∏á‡∏´‡∏ô‡πâ‡∏≤ (Set ‡∏Ç‡∏≠‡∏á Stable Doc IDs)
        final_doc_set = set(doc_ids) if doc_ids else set() 
        
        tasks = [
            run_in_threadpool(
                # üéØ FIX: ‡πÉ‡∏ä‡πâ retrieve_context_for_endpoint ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏ä‡πâ Hard Filter
                retrieve_context_for_endpoint,
                query=question,
                doc_type=d_type,
                enabler=enabler,
                subject=subject, # üü¢ ‡∏™‡πà‡∏á subject ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô kwargs
                vectorstore_manager=vsm,
                stable_doc_ids=final_doc_set,
                k_to_retrieve=QUERY_INITIAL_K,
                k_to_rerank=QUERY_FINAL_K,
                # üü¢ FIX: ‡∏™‡πà‡∏á Tenant ‡πÅ‡∏•‡∏∞ Year ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô Retrieval
                tenant=tenant_context,
                year=year_context
                # ---------------------------------------------
            )
            for d_type in doc_types
        ]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        for result in results:
            if isinstance(result, Exception):
                logger.warning(f"Retrieval failed for a doc_type: {result}")
                continue
            for ev in result.get("top_evidences", []):
                # üí° Note: retrieve_context_for_endpoint ‡πÑ‡∏°‡πà‡∏°‡∏µ "score" ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡πÉ‡∏ô top_evidences
                # ‡πÅ‡∏ï‡πà‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ logic ‡∏Ç‡∏≠‡∏á Reranker ‡πÉ‡∏ô‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏±‡πâ‡∏ô‡πÅ‡∏•‡πâ‡∏ß ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô score ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô 0 ‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡πâ‡∏≠‡∏á‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å metadata ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏†‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏á
                # ‡πÉ‡∏ô‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πâ ‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ score ‡πÄ‡∏õ‡πá‡∏ô 1.0 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏∏‡∏Å chunk ‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£ Rerank ‡πÅ‡∏•‡πâ‡∏ß (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ Sorting ‡πÅ‡∏¢‡πà‡∏•‡∏á)
                score = ev.get("score", 1.0)
                all_chunks.append(LcDocument(
                    page_content=ev["text"],
                    metadata={
                        "score": float(score),
                        "stable_doc_uuid": ev.get("doc_id"),
                        "chunk_uuid": ev.get("chunk_uuid"),
                        "file_name": ev.get("source", "Unknown Document"),
                        "doc_type": ev.get("doc_type"),
                    }
                ))

    # Fallback: Pure LLM (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ vectorstore)
    if not all_chunks:
        messages = [
            SystemMessage(content=SYSTEM_QA_INSTRUCTION),
            *history_messages,
            HumanMessage(content=question)
        ]
        response = await run_in_threadpool(llm.invoke, messages)
        answer = getattr(response, "content", str(response)).strip()
        await async_save_message(conversation_id, "ai", answer)
        return QueryResponse(answer=answer, sources=[], conversation_id=conversation_id)

    # RAG Mode ‚Üí ‡πÉ‡∏ä‡πâ prompt ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏£‡∏±‡∏Å
    # Note: ‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ FINAL_K_RERANKED ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Source ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ LLM
    top_chunks = sorted(all_chunks, key=lambda x: x.metadata.get("score", 0), reverse=True)[:FINAL_K_RERANKED]

    context = "\n\n---\n\n".join([
        f"Source [{doc.metadata['file_name']} | Score: {doc.metadata['score']:.3f}]:\n{doc.page_content[:3500]}"
        for doc in top_chunks
    ])

    user_prompt = build_prompt(context, question, intent)
    messages = [
        SystemMessage(content=SYSTEM_QA_INSTRUCTION),
        *history_messages,
        HumanMessage(content=user_prompt)
    ]

    response = await run_in_threadpool(llm.invoke, messages)
    answer = getattr(response, "content", str(response)).strip()
    await async_save_message(conversation_id, "ai", answer)

    sources = [
        QuerySource(
            source_id=doc.metadata.get("stable_doc_uuid", "unknown"),
            file_name=doc.metadata.get("file_name", "Unknown Document"),
            chunk_text=doc.page_content,
            chunk_id=doc.metadata.get("chunk_uuid"),
            score=doc.metadata.get("score", 0.0)
        )
        for doc in top_chunks
    ]
    
    # üí° LOG FIX: ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÉ‡∏´‡πâ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Multiple IDs
    doc_ids_summary = f"Filter IDs: {len(doc_ids)}"
    
    if doc_ids and vsm and doc_types:
        try:
            # ‡∏î‡∏∂‡∏á Metadata ‡∏Ç‡∏≠‡∏á Doc ID ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
            doc_metadata = await run_in_threadpool(
                retrieve_context_by_doc_ids,
                doc_uuids=doc_ids,
                doc_type=doc_types[0], # ‡πÉ‡∏ä‡πâ doc_type ‡πÅ‡∏£‡∏Å‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
                enabler=enabler,
                vectorstore_manager=vsm,
                # üü¢ FIX: ‡∏™‡πà‡∏á Tenant ‡πÅ‡∏•‡∏∞ Year ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô Retrieval
                tenant=tenant_context,
                year=year_context
                # ---------------------------------------------
            )
            
            # ‡∏™‡∏Å‡∏±‡∏î‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô
            file_names = set()
            for ev in doc_metadata.get("top_evidences", []):
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Metadata ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏á Doc ID ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏à‡∏£‡∏¥‡∏á‡πÜ
                if ev.get("doc_id") in doc_ids: 
                    file_names.add(ev.get("source", "Unknown File"))
            
            file_names_list = sorted(list(file_names))
            num_files = len(file_names_list)
            
            if num_files > 0:
                # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏• 2 ‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏£‡∏Å‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
                display_names = file_names_list[:2]
                names_summary = ", ".join(display_names)
                
                if num_files > 2:
                    names_summary += f" (+{num_files - 2} files)"
                    
                doc_ids_summary = f"Filter IDs: {len(doc_ids)} ({num_files} files) | Files: {names_summary}"
            # ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå (num_files=0) ‡∏à‡∏∞‡πÉ‡∏ä‡πâ doc_ids_summary ‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô

        except Exception as e:
            logger.warning(f"Could not retrieve file names for logging: {e}")
            # ‡πÅ‡∏™‡∏î‡∏á UUIDs ‡πÅ‡∏ó‡∏ô‡∏´‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏î Error
            doc_ids_list = doc_ids[:2] if len(doc_ids) > 2 else doc_ids
            doc_ids_summary = f"Filter IDs: {len(doc_ids)} (Log Error: {e.__class__.__name__})"

    logger.info(f"RAG Query Success | conv:{conversation_id[:8]} | chunks:{len(top_chunks)} | intent:{intent} | {doc_ids_summary}")
    
    return QueryResponse(answer=answer, sources=sources, conversation_id=conversation_id)


# =============================
#    /compare ‚Üí ‡πÉ‡∏ä‡πâ Pydantic Parser ‚Üí ‡πÑ‡∏°‡πà‡∏û‡∏±‡∏á‡∏≠‡∏µ‡∏Å‡∏ï‡πà‡∏≠‡πÑ‡∏õ!
# ==================================
@llm_router.post("/compare", response_model=CompareResponse)
async def compare_documents(
    doc1_id: str = Form(...),
    doc2_id: str = Form(...),
    final_query: str = Form("‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á‡∏â‡∏ö‡∏±‡∏ö‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î"),
    doc_type: str = Form("document"),
    enabler: str = Form("KM"),
    current_user: UserMe = Depends(get_current_user), # <--- üü¢ FIX: ‡πÄ‡∏û‡∏¥‡πà‡∏° User Dependency
):
    llm = create_llm_instance(model_name=DEFAULT_LLM_MODEL_NAME, temperature=0.0)
    vsm = get_vectorstore_manager()
    if not vsm:
        raise HTTPException(503, "Vector store not available")

    # üìå ‡∏î‡∏∂‡∏á‡∏ö‡∏£‡∏¥‡∏ö‡∏ó Tenant ‡πÅ‡∏•‡∏∞ Year ‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
    tenant_context = current_user.tenant
    year_context = current_user.year

    docs = await run_in_threadpool(
        retrieve_context_by_doc_ids,
        doc_uuids=[doc1_id, doc2_id],
        doc_type=doc_type,
        enabler=enabler,
        vectorstore_manager=vsm,
        # üü¢ FIX: ‡∏™‡πà‡∏á Tenant ‡πÅ‡∏•‡∏∞ Year ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô Retrieval
        tenant=tenant_context,
        year=year_context
        # ---------------------------------------------
    )

    evidences = docs.get("top_evidences", [])
    if len(evidences) < 2:
        raise HTTPException(404, "One or both documents not found")

    doc_map = {}
    for ev in evidences:
        # Note: retrieve_context_by_doc_ids ‡∏à‡∏∞‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÉ‡∏ô key "text"
        doc_map.setdefault(ev["doc_id"], []).append(ev["text"]) 

    doc1_text = "\n\n".join(doc_map.get(doc1_id, []))[:18000]
    doc2_text = "\n\n".join(doc_map.get(doc2_id, []))[:18000]

    if not doc1_text or not doc2_text:
        raise HTTPException(404, "Document content is empty")

    # ‡πÉ‡∏ä‡πâ Pydantic Parser ‚Üí ‡πÅ‡∏°‡πà‡∏ô 100%
    parser = PydanticOutputParser(pydantic_object=ComparisonOutput)
    format_instructions = parser.get_format_instructions()

    prompt = COMPARE_PROMPT.format(
        doc1_content=doc1_text,
        doc2_content=doc2_text,
        query=final_query
    ) + "\n\n" + format_instructions

    messages = [
        SystemMessage(content=SYSTEM_COMPARE_INSTRUCTION),
        HumanMessage(content=prompt)
    ]

    response = await run_in_threadpool(llm.invoke, messages)
    raw_output = getattr(response, "content", str(response)).strip()

    try:
        parsed = parser.parse(raw_output)
    except Exception as e:
        logger.error(f"Comparison parser failed:\n{raw_output}\nError: {e}")
        raise HTTPException(500, "Failed to parse comparison result from LLM")

    return CompareResponse(result=parsed)