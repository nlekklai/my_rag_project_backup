# -*- coding: utf-8 -*-
# routers/assessment_router.py
# Production Final Version - 20 ‡∏ò‡∏±‡∏ô‡∏ß‡∏≤‡∏Ñ‡∏° 2568 (Fixed parameter order + stable UUID + full assessment flow)

import os
import uuid
import json
import asyncio
import logging
import mimetypes
from datetime import datetime
from typing import Optional, Dict, Any, Union, List

from fastapi import APIRouter, BackgroundTasks, HTTPException, Depends, Query
from fastapi.responses import FileResponse
from pydantic import BaseModel

import tempfile
from docx import Document
from docx.shared import Pt, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH
from docx.enum.style import WD_STYLE_TYPE

from docx.shared import Pt, RGBColor, Inches
from docx.oxml.ns import qn

from routers.auth_router import UserMe, get_current_user
from utils.path_utils import (
    _n, 
    get_tenant_year_export_root, 
    load_doc_id_mapping, 
    get_document_file_path,
    get_vectorstore_collection_path,
    get_vectorstore_tenant_root_path
    )

from core.seam_assessment import SEAMPDCAEngine, AssessmentConfig
from core.vectorstore import load_all_vectorstores
from models.llm import create_llm_instance
from config.global_vars import EVIDENCE_DOC_TYPES, DEFAULT_LLM_MODEL_NAME, DEFAULT_YEAR
import pytz

logger = logging.getLogger(__name__)
assessment_router = APIRouter(prefix="/api/assess", tags=["Assessment"])

ACTIVE_TASKS: Dict[str, Any] = {}

class StartAssessmentRequest(BaseModel):
    tenant: str
    year: Union[int, str]
    enabler: str
    sub_criteria: Optional[str] = "all"
    sequential_mode: bool = True

# ------------------- Permission Helper -------------------
def check_user_permission(user: UserMe, tenant: str, enabler: str):
    if _n(user.tenant) != _n(tenant):
        raise HTTPException(status_code=403, detail="Tenant mismatch")
    if user.enablers and enabler.upper() not in [e.upper() for e in user.enablers]:
        raise HTTPException(status_code=403, detail=f"Enabler '{enabler}' not allowed")

# ------------------- Helpers -------------------
def parse_safe_date(raw_date_str: Any, file_path: str) -> str:
    tz = pytz.timezone('Asia/Bangkok') # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Timezone ‡πÑ‡∏ó‡∏¢
    
    if raw_date_str and isinstance(raw_date_str, str):
        try:
            # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö %Y%m%d_%H%M%S (‡πÄ‡∏ä‡πà‡∏ô‡∏à‡∏≤‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå)
            if "_" in raw_date_str:
                dt = datetime.strptime(raw_date_str, "%Y%m%d_%H%M%S")
                # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ß‡∏•‡∏≤‡πÑ‡∏ó‡∏¢
                return tz.localize(dt).isoformat()
        except:
            pass

    try:
        # ‡∏î‡∏∂‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÑ‡∏ü‡∏•‡πå‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å Disk
        mtime = os.path.getmtime(file_path)
        dt = datetime.fromtimestamp(mtime, tz) # ‡∏£‡∏∞‡∏ö‡∏∏ Timezone ‡∏ï‡∏≠‡∏ô‡∏î‡∏∂‡∏á timestamp
        return dt.isoformat()
    except:
        # ‡∏Å‡∏£‡∏ì‡∏µ‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô Thai Timezone
        return datetime.now(tz).isoformat()

def _find_assessment_file(search_id: str, current_user: UserMe) -> str:
    # 1. ‡∏´‡∏≤ root ‡∏Ç‡∏≠‡∏á tenant
    # ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡∏õ‡∏µ 2568 ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏ï‡∏±‡πâ‡∏á‡∏ï‡πâ‡∏ô‡∏Å‡πà‡∏≠‡∏ô
    sample_path = get_tenant_year_export_root(current_user.tenant, "2568")
    tenant_export_root = os.path.dirname(sample_path)
    
    norm_search = _n(search_id).lower()

    # 2. ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Path ‡∏™‡∏≥‡∏£‡∏≠‡∏á (‡∏Å‡∏£‡∏ì‡∏µ‡∏£‡∏±‡∏ô‡∏ö‡∏ô Linux/Docker ‡πÅ‡∏•‡πâ‡∏ß /app/ ‡∏´‡∏≤‡∏¢‡πÑ‡∏õ)
    search_paths = [tenant_export_root]
    if tenant_export_root.startswith("/app/"):
        search_paths.append(tenant_export_root.replace("/app/", "", 1))

    for s_path in search_paths:
        if os.path.exists(s_path):
            for root, _, files in os.walk(s_path):
                for f in files:
                    if f.endswith(".json") and norm_search in _n(f).lower():
                        return os.path.join(root, f)
                    
    raise HTTPException(status_code=404, detail=f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô ID: {search_id}")


@assessment_router.get("/evidence/{doc_type}/{document_uuid}")
async def serve_evidence_file(
    document_uuid: str,
    doc_type: str,
    tenant: str,
    year: str = None,
    enabler: str = None,
    current_user: UserMe = Depends(get_current_user)
):
    check_user_permission(current_user, tenant, enabler or "KM")

    file_info = get_document_file_path(
        document_uuid=document_uuid,
        tenant=tenant,
        year=year,
        enabler=enabler,
        doc_type_name=doc_type
    )

    if not file_info:
        raise HTTPException(status_code=404, detail="File not found")

    file_path = file_info["file_path"]
    
    # ‡∏î‡∏∂‡∏á‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•‡πÑ‡∏ü‡∏•‡πå
    ext = os.path.splitext(file_path)[1].lower()
    
    # üõ°Ô∏è Force MIME Type ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Mac/Safari
    mime_map = {
        ".pdf": "application/pdf",
        ".jpg": "image/jpeg",
        ".jpeg": "image/jpeg",
        ".png": "image/png",
    }
    
    mime_type = mime_map.get(ext) or mimetypes.guess_type(file_path)[0] or "application/octet-stream"

    # ‡∏™‡πà‡∏á FileResponse
    response = FileResponse(
        path=file_path,
        media_type=mime_type,
        content_disposition_type="inline"
    )

    # üí° ‡∏´‡∏±‡∏ß‡πÉ‡∏à‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Mac/Safari:
    # 1. ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ Browser ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å Path ‡∏ã‡∏∂‡πà‡∏á‡∏ö‡∏≤‡∏á‡∏ó‡∏µ‡∏°‡∏µ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÅ‡∏•‡πâ‡∏ß‡∏ó‡∏≥‡πÉ‡∏´‡πâ Header ‡πÄ‡∏û‡∏µ‡πâ‡∏¢‡∏ô
    # 2. ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö Header ‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
    response.headers["Content-Type"] = mime_type
    response.headers["Accept-Ranges"] = "bytes" 
    
    # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô PDF ‡∏ö‡∏ô Mac ‡πÉ‡∏´‡πâ‡πÄ‡∏ï‡∏¥‡∏° Cache-Control ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Viewer ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô
    if ext == ".pdf":
        response.headers["Cache-Control"] = "no-cache, no-store, must-revalidate"

    return response


@assessment_router.get("/view-document")
async def view_document(filename: str, page: Optional[str] = "1", current_user: UserMe = Depends(get_current_user)):
    """ Endpoint ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡∏¥‡∏î‡πÑ‡∏ü‡∏•‡πå PDF ‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏ """
    # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏Ç‡∏≠‡∏á Tenant
    import os
    from utils.path_utils import get_tenant_year_import_root
    
    # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Å‡πá‡∏ö‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå import/EVIDENCE_DOC
    base_path = os.path.join(get_tenant_year_import_root(current_user.tenant, current_user.year), "EVIDENCE_DOC")
    file_path = os.path.join(base_path, filename)

    if not os.path.exists(file_path):
        raise HTTPException(status_code=404, detail=f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£: {filename}")

    # ‡∏™‡πà‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Browser ‡πÄ‡∏õ‡∏¥‡∏î (‡∏£‡∏∞‡∏ö‡∏∏‡∏´‡∏ô‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢ #page=X ‡πÉ‡∏ô‡∏ù‡∏±‡πà‡∏á Frontend)
    return FileResponse(file_path, media_type="application/pdf")


def _transform_result_for_ui(raw_data: Dict[str, Any], current_user: Any = None) -> Dict[str, Any]:
    """
    ‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå:
    - ‡∏ô‡∏¥‡∏¢‡∏≤‡∏° enabler_name ‡πÅ‡∏•‡∏∞ overall_level ‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
    - ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì pdca_coverage ‡πÅ‡∏•‡∏∞ avg_confidence_per_level ‡πÉ‡∏´‡πâ UI ‡∏ô‡∏≥‡πÑ‡∏õ‡∏Å‡∏≤‡∏á Accordion ‡πÑ‡∏î‡πâ
    - ‡∏à‡∏±‡∏î‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Roadmap (Actions/Steps) ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏ï‡∏≤‡∏° UI
    """
    summary = raw_data.get("summary", {})
    sub_results = raw_data.get("sub_criteria_results", [])

    processed_sub_criteria = []
    radar_data = []

    # --- 1. ‡∏™‡∏Å‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Overall (‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç enabler_name ‡πÅ‡∏•‡∏∞ overall_level) ---
    enabler_name = (summary.get("enabler") or "N/A").upper()
    overall_level = summary.get("Overall Maturity Level (Weighted)") or f"L{summary.get('highest_pass_level_overall', 0)}"
    
    total_score = round(float(summary.get("Total Weighted Score Achieved") or 0.0), 2)
    full_score_all = round(float(summary.get("Total Possible Weight") or 40.0), 2)
    total_expected = int(summary.get("total_subcriteria") or 12)
    passed_count = int(summary.get("total_subcriteria_assessed") or len(sub_results))
    completion_rate = (passed_count / total_expected * 100) if total_expected > 0 else 0.0

    for res in sub_results:
        cid = res.get("sub_criteria_id", "N/A")
        cname = res.get("sub_criteria_name", f"‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏¢‡πà‡∏≠‡∏¢ {cid}")
        highest_pass = int(res.get("highest_full_level") or 0)
        raw_levels_list = res.get("raw_results_ref", [])
        
        # --- 2. PDCA Matrix & Coverage Calculation ---
        pdca_matrix = []
        pdca_coverage = {} 
        avg_conf_per_lv = {}
        raw_levels_map = {item.get("level"): item for item in raw_levels_list}
        
        for lv_idx in range(1, 6):
            lv_info = raw_levels_map.get(lv_idx)
            is_passed = lv_info.get("is_passed", False) if lv_info else (lv_idx <= highest_pass)
            
            # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Mode ‡∏™‡∏µ‡πÉ‡∏´‡πâ UI
            eval_mode = "NORMAL"
            if is_passed and lv_idx > highest_pass:
                eval_mode = "GAP_ONLY" # ‡∏™‡∏µ‡∏ô‡πâ‡∏≥‡πÄ‡∏á‡∏¥‡∏ô Potential
            elif not is_passed and lv_info:
                eval_mode = "FAILED" # ‡∏™‡∏µ‡πÄ‡∏ó‡∏≤‡πÄ‡∏Ç‡πâ‡∏° (‡∏ï‡∏£‡∏ß‡∏à‡πÅ‡∏•‡πâ‡∏ß‡∏ï‡∏Å)
            elif not is_passed:
                eval_mode = "INACTIVE" # ‡∏™‡∏µ‡πÄ‡∏ó‡∏≤‡∏à‡∏≤‡∏á (‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô)

            pdca_raw = lv_info.get("pdca_breakdown", {}) if lv_info else {}
            pdca_final = {k: (1 if float(pdca_raw.get(k, 0)) > 0 else 0) for k in ["P", "D", "C", "A"]}
            
            # ‡∏Å‡∏£‡∏ì‡∏µ‡∏ú‡πà‡∏≤‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö PDCA ‡πÄ‡∏ï‡πá‡∏°
            if not lv_info and lv_idx <= highest_pass:
                pdca_final = {"P": 1, "D": 1, "C": 1, "A": 1}

            pdca_matrix.append({
                "level": lv_idx,
                "is_passed": is_passed,
                "evaluation_mode": eval_mode,
                "pdca": pdca_final,
                "reason": lv_info.get("reason") or ("‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô" if lv_idx <= highest_pass else "‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏ñ‡∏∂‡∏á‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô")
            })

            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì % ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Progress Bar ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏•‡πÄ‡∏ß‡∏•
            covered_count = sum(pdca_final.values())
            pdca_coverage[str(lv_idx)] = {"percentage": (covered_count / 4) * 100}

        # --- 3. Sources & Confidence per Level ---
        grouped_sources = {str(lv): [] for lv in range(1, 6)}
        all_scores = []
        
        for lv_idx in range(1, 6):
            lv_scores = []
            lv_refs = [r for r in raw_levels_list if r.get("level") == lv_idx]
            for ref in lv_refs:
                sources = ref.get("temp_map_for_level", []) or [ref]
                for s in sources:
                    meta = s.get('metadata', {})
                    d_uuid = s.get('document_uuid') or meta.get('doc_id')
                    if not d_uuid: continue
                    
                    raw_s = meta.get("rerank_score") or s.get("rerank_score") or 0.0
                    score_val = 0.895 if float(raw_s) >= 1.0 else float(raw_s)
                    if score_val > 0: 
                        all_scores.append(score_val)
                        lv_scores.append(score_val)

                    grouped_sources[str(lv_idx)].append({
                        "filename": s.get('filename') or meta.get('filename') or "Evidence Document",
                        "page": str(s.get('page_number') or meta.get('page') or "1"),
                        "text": s.get("text", "")[:300],
                        "rerank_score": round(score_val * 100, 1), # ‡∏™‡πà‡∏á % ‡πÉ‡∏´‡πâ UI
                        "document_uuid": d_uuid,
                        "pdca_tag": str(s.get("pdca_tag") or meta.get("pdca_tag", "N/A")).upper()
                    })
            
            # ‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏£‡∏≤‡∏¢‡πÄ‡∏•‡πÄ‡∏ß‡∏•
            avg_conf_per_lv[str(lv_idx)] = (sum(lv_scores)/len(lv_scores)*100) if lv_scores else 0

        # --- 4. Roadmap Structure ---
        ui_roadmap = []
        raw_plans = res.get("action_plan") or []
        for p in raw_plans:
            phase_actions = []
            current_actions = p.get("actions") or p.get("Actions") or []
            for act in current_actions:
                phase_actions.append({
                    "level": str(act.get("level") or act.get("failed_level") or (highest_pass + 1)),
                    "recommendation": act.get("recommendation") or act.get("Recommendation") or "",
                    "steps": act.get("steps") or act.get("Steps") or []
                })
            ui_roadmap.append({
                "phase": p.get("phase") or p.get("Phase") or "‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô‡∏û‡∏±‡∏í‡∏ô‡∏≤",
                "actions": phase_actions
            })

        # --- 5. Final Sub-Criteria Logic ---
        # ‡∏´‡∏≤‡πÄ‡∏•‡πÄ‡∏ß‡∏•‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà "‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•" (‡πÅ‡∏°‡πâ‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£)
        potential_levels = [r.get('level') for r in raw_levels_list if r.get('is_passed')]
        potential_level = max(potential_levels + [highest_pass])

        processed_sub_criteria.append({
            "code": cid,
            "name": cname,
            "level": f"L{highest_pass}",
            "potential_level": f"L{potential_level}",
            "is_gap_analysis": potential_level > highest_pass,
            "pdca_matrix": pdca_matrix,
            "pdca_coverage": pdca_coverage,
            "avg_confidence_per_level": avg_conf_per_lv,
            "roadmap": ui_roadmap,
            "grouped_sources": grouped_sources,
            "summary_thai": (res.get("summary_thai") or "").strip(),
            "gap": (res.get("gap_analysis") or "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤").strip(),
            "confidence_score": round((sum(all_scores)/len(all_scores)*100) if all_scores else 0, 1)
        })
        radar_data.append({"axis": cid, "value": highest_pass})

    return {
        "status": "COMPLETED",
        "record_id": raw_data.get("record_id", "unknown"),
        "tenant": str(summary.get("tenant", "PEA")).upper(),
        "year": str(summary.get("year", "2568")),
        "enabler": enabler_name,
        "level": overall_level,
        "score": total_score,
        "full_score": full_score_all,
        "metrics": {
            "total_criteria": total_expected,
            "passed_criteria": passed_count,
            "completion_rate": round(completion_rate, 2)
        },
        "radar_data": radar_data,
        "sub_criteria": processed_sub_criteria
    }

def create_docx_report_similar_to_ui(ui_data: dict) -> Document:
    doc = Document()

    # --- ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡∏Å‡∏£‡∏∞‡∏î‡∏≤‡∏© ---
    section = doc.sections[0]
    section.top_margin = Inches(0.8)
    section.bottom_margin = Inches(0.8)
    section.left_margin = Inches(1.0)
    section.right_margin = Inches(1.0)

    # --- ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ä‡πà‡∏ß‡∏¢‡∏ï‡∏±‡πâ‡∏á‡∏ü‡∏≠‡∏ô‡∏ï‡πå‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ---
    def set_thai_font(run, name='TH Sarabun New', size=14, bold=False, color=None):
        run.font.name = name
        run._element.rPr.rFonts.set(qn('w:eastAsia'), name)
        run.font.size = Pt(size)
        run.bold = bold
        if color:
            run.font.color.rgb = color

    # --- ‡∏™‡πÑ‡∏ï‡∏•‡πå‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏´‡∏•‡∏±‡∏Å ---
    if 'Report Title' not in doc.styles:
        title_style = doc.styles.add_style('Report Title', WD_STYLE_TYPE.PARAGRAPH)
        title_style.font.name = 'TH Sarabun New'
        title_style._element.rPr.rFonts.set(qn('w:eastAsia'), 'TH Sarabun New')
        title_style.font.size = Pt(28)
        title_style.font.bold = True
        title_style.font.color.rgb = RGBColor(30, 58, 138)
        title_style.paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER
        title_style.paragraph_format.space_after = Pt(30)

    # --- ‡∏´‡∏±‡∏ß‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏´‡∏•‡∏±‡∏Å ---
    title_p = doc.add_paragraph(f"{ui_data['enabler']} ASSESSMENT REPORT", style='Report Title')

    # --- ‡∏™‡∏£‡∏∏‡∏õ‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏° (‡πÅ‡∏ö‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏™‡∏ß‡∏¢ ‡πÜ) ---
    summary_table = doc.add_table(rows=5, cols=2)
    summary_table.style = 'Table Grid'
    summary_table.autofit = False
    summary_table.columns[0].width = Inches(2.5)
    summary_table.columns[1].width = Inches(4.0)

    summary_data = [
        ("Record ID", ui_data['record_id']),
        ("‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô", ui_data['tenant']),
        ("‡∏õ‡∏µ‡∏á‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì", ui_data['year']),
        ("‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°", ui_data['level']),
        ("‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏° / ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏ï‡πá‡∏°", f"{ui_data['score']} / {ui_data['full_score']}"),
        ("‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡πÄ‡∏Å‡∏ì‡∏ë‡πå", f"{ui_data['metrics']['completion_rate']:.1f}%")
    ]

    for label, value in summary_data:
        row = summary_table.add_row().cells
        row[0].text = label
        row[1].text = value
        set_thai_font(row[0].paragraphs[0].runs[0], size=13, bold=True)
        set_thai_font(row[1].paragraphs[0].runs[0], size=13)

    doc.add_page_break()

    sub_criteria = ui_data['sub_criteria']

    # --- ‡∏Å‡∏£‡∏ì‡∏µ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô ALL sub-criteria ---
    if len(sub_criteria) > 1 or (len(sub_criteria) == 1 and sub_criteria[0]['code'] == "ALL"):
        # ‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏£‡∏Å: ‡∏™‡∏£‡∏∏‡∏õ‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        doc.add_heading("‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏° (‡∏ó‡∏∏‡∏Å‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏¢‡πà‡∏≠‡∏¢)", level=1)
        set_thai_font(doc.paragraphs[-1].runs[0], size=20, bold=True, color=RGBColor(30, 58, 138))

        # ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏Å‡∏ì‡∏ë‡πå
        overall_table = doc.add_table(rows=1, cols=5)
        overall_table.style = 'Table Grid'
        hdr = overall_table.rows[0].cells
        headers = ['‡∏£‡∏´‡∏±‡∏™‡πÄ‡∏Å‡∏ì‡∏ë‡πå', '‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏Å‡∏ì‡∏ë‡πå', '‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô', '‡∏®‡∏±‡∏Å‡∏¢‡∏†‡∏≤‡∏û', '‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô']
        for cell, text in zip(hdr, headers):
            cell.text = text
            run = cell.paragraphs[0].runs[0]
            set_thai_font(run, size=12, bold=True)
            cell.paragraphs[0].alignment = WD_ALIGN_PARAGRAPH.CENTER

        for item in sub_criteria:
            row = overall_table.add_row().cells
            row[0].text = item['code']
            row[1].text = item['name']
            row[2].text = item['level']
            row[3].text = item['potential_level'] if item['potential_level'] != item['level'] else "-"
            row[4].text = f"{item['score']} / {item['full_score']}"

        doc.add_page_break()

    # --- ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏¢‡πà‡∏≠‡∏¢ ---
    for item in sub_criteria:
        # ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÄ‡∏Å‡∏ì‡∏ë‡πå
        heading = doc.add_heading(f"{item['code']} {item['name']}", level=1)
        set_thai_font(heading.runs[0], size=18, bold=True, color=RGBColor(30, 58, 138))

        # ‡∏£‡∏∞‡∏î‡∏±‡∏ö + Potential + Bottleneck
        level_text = f"‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô: {item['level']}"
        if item['potential_level'] != item['level']:
            level_text += f" ‚Üí {item['potential_level']} (‡∏°‡∏µ‡∏®‡∏±‡∏Å‡∏¢‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤)"
        if item['is_gap_analysis']:
            level_text += " ‚ö†Ô∏è ‡∏°‡∏µ‡∏à‡∏∏‡∏î‡∏ï‡∏¥‡∏î‡∏Ç‡∏±‡∏î (Bottleneck)"

        level_p = doc.add_paragraph(level_text)
        set_thai_font(level_p.runs[0], size=14, bold=True)
        level_p.paragraph_format.space_after = Pt(15)

        # ‡∏ï‡∏≤‡∏£‡∏≤‡∏á PDCA Coverage + Confidence
        doc.add_paragraph("‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏° PDCA ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ï‡∏≤‡∏°‡∏£‡∏∞‡∏î‡∏±‡∏ö", style='Heading 3')

        table = doc.add_table(rows=1, cols=4)
        table.style = 'Table Grid'

        hdr_cells = table.rows[0].cells
        headers = ['‡∏£‡∏∞‡∏î‡∏±‡∏ö', '‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏° PDCA', '‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞', '‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢']
        for cell, text in zip(hdr_cells, headers):
            cell.text = text
            run = cell.paragraphs[0].runs[0]
            set_thai_font(run, size=12, bold=True)
            cell.paragraphs[0].alignment = WD_ALIGN_PARAGRAPH.CENTER

        current_lvl = int(item['level'].replace('L', ''))
        for lvl in range(1, 6):
            cov = item['pdca_coverage'].get(lvl, {'percentage': 0})
            pct = round(cov['percentage'])
            avg_conf = item['avg_confidence_per_level'].get(lvl, 0)
            conf_pct = round(avg_conf * 100) if avg_conf > 0 else 0

            status = ""
            if lvl == current_lvl:
                status = "‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô"
            elif lvl > current_lvl and pct > 0:
                status = "‡∏°‡∏µ‡∏®‡∏±‡∏Å‡∏¢‡∏†‡∏≤‡∏û"

            row_cells = table.add_row().cells
            row_cells[0].text = f"L{lvl}"
            row_cells[1].text = f"{pct}%"
            row_cells[2].text = status
            row_cells[3].text = f"{conf_pct}%" if avg_conf > 0 else "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô"

        doc.add_paragraph()

        # ‡∏™‡∏£‡∏∏‡∏õ‡∏à‡∏∏‡∏î‡πÅ‡∏Ç‡πá‡∏á
        if item.get('summary_thai'):
            doc.add_paragraph("‡∏™‡∏£‡∏∏‡∏õ‡∏à‡∏∏‡∏î‡πÅ‡∏Ç‡πá‡∏á‡∏à‡∏≤‡∏Å AI", style='Heading 3')
            summary_p = doc.add_paragraph(item['summary_thai'])
            summary_p.paragraph_format.left_indent = Inches(0.3)

        # ‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏±‡∏í‡∏ô‡∏≤
        if item.get('gap'):
            doc.add_paragraph("‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏±‡∏í‡∏ô‡∏≤ (Critical Gaps)", style='Heading 3')
            gap_p = doc.add_paragraph(item['gap'])
            gap_p.paragraph_format.left_indent = Inches(0.3)

        # ‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤ (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö Schema)
        if item.get('roadmap'):
            doc.add_paragraph("‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÄ‡∏ä‡∏¥‡∏á‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå", style='Heading 3')
            for phase in item['roadmap']:
                phase_p = doc.add_paragraph(phase['phase'])
                set_thai_font(phase_p.runs[0], size=14, bold=True)

                if phase.get('goal'):
                    goal_p = doc.add_paragraph(f"‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢: {phase['goal']}")
                    goal_p.paragraph_format.left_indent = Inches(0.5)

                # ‚úÖ ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å 'tasks' ‡πÄ‡∏õ‡πá‡∏ô 'actions'
                for act in phase.get('actions', []):
                    task_p = doc.add_paragraph(
                        f"‚Ä¢ ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ {act['level']}: {act['recommendation']}"
                    )
                    set_thai_font(task_p.runs[0], bold=True)

                    for step in act.get('steps', []):
                        # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏ö‡∏≠‡∏£‡πå step ‡πÉ‡∏´‡πâ‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°
                        s_idx = step.get('step', '-')
                        s_desc = step.get('description', '')
                        resp = step.get('responsible', '‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á')
                        
                        step_p = doc.add_paragraph(f"   {s_idx}. {s_desc} ({resp})")
                        step_p.paragraph_format.left_indent = Inches(1.0)

        # ‡∏™‡∏£‡∏∏‡∏õ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô
        doc.add_paragraph("‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏ô‡∏±‡∏ö‡∏™‡∏ô‡∏∏‡∏ô", style='Heading 3')
        total = sum(len(files) for files in item['grouped_sources'].values() if files)
        total_p = doc.add_paragraph(f"‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {total} ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£")
        set_thai_font(total_p.runs[0], bold=True)

        for lv, files in item['grouped_sources'].items():
            if files:
                doc.add_paragraph(f"‚Ä¢ Level {lv}: {len(files)} ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£")

        # ‡πÄ‡∏ß‡πâ‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏´‡∏•‡∏±‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏Å‡∏ì‡∏ë‡πå
        if item != sub_criteria[-1]:  # ‡πÑ‡∏°‡πà‡πÄ‡∏ß‡πâ‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏´‡∏•‡∏±‡∏á‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
            doc.add_page_break()

    return doc
# ------------------- API Endpoints -------------------
@assessment_router.get("/status/{record_id}")
async def get_assessment_status(record_id: str, current_user: UserMe = Depends(get_current_user)):
    # 1. ‡πÄ‡∏ä‡πá‡∏Ñ‡πÉ‡∏ô Memory ‡∏Å‡πà‡∏≠‡∏ô (‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏£‡∏±‡∏ô)
    if record_id in ACTIVE_TASKS:
        return ACTIVE_TASKS[record_id]

    # 2. ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô Memory ‡πÉ‡∏´‡πâ‡πÑ‡∏õ‡∏´‡∏≤‡πÉ‡∏ô Disk (‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß)
    # ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏™‡πÅ‡∏Å‡∏ô‡∏´‡∏≤‡∏ó‡∏∏‡∏Å‡∏õ‡∏µ‡πÉ‡∏´‡πâ‡πÄ‡∏≠‡∏á
    file_path = _find_assessment_file(record_id, current_user)
    
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            raw_data = json.load(f)

        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Enabler ‡∏°‡∏≤‡πÄ‡∏ä‡πá‡∏Ñ Permission
        summary = raw_data.get("summary", {})
        enabler = (summary.get("enabler") or "KM").upper()
        tenant = summary.get("tenant") or current_user.tenant
        
        check_user_permission(current_user, tenant, enabler)

        # ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ UI
        return _transform_result_for_ui(raw_data, current_user)
    except Exception as e:
        logger.error(f"Error loading status for {record_id}: {e}")
        raise HTTPException(status_code=500, detail="‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÑ‡∏î‡πâ")

@assessment_router.get("/history")
async def get_assessment_history(
    tenant: str, 
    year: Optional[str] = Query(None), # ‡πÅ‡∏Å‡πâ‡∏à‡∏≤‡∏Å Union ‡πÄ‡∏õ‡πá‡∏ô Optional ‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ default ‡πÄ‡∏õ‡πá‡∏ô None
    current_user: UserMe = Depends(get_current_user)
):
    # 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£
    if _n(tenant) != _n(current_user.tenant):
        raise HTTPException(status_code=403, detail="Permission Denied")

    history_list = []
    
    # 2. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á "‡∏õ‡∏µ" ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
    # ‡∏ñ‡πâ‡∏≤ Frontend ‡πÑ‡∏°‡πà‡∏™‡πà‡∏á‡∏°‡∏≤ ‡∏´‡∏£‡∏∑‡∏≠‡∏™‡πà‡∏á‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô "all" ‡πÉ‡∏´‡πâ‡∏™‡πÅ‡∏Å‡∏ô‡∏ó‡∏∏‡∏Å‡∏õ‡∏µ
    search_years = []
    
    # ‡∏´‡∏≤ Root Path ‡∏Ç‡∏≠‡∏á Tenant ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏ß‡πà‡∏≤‡∏°‡∏µ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏õ‡∏µ‡πÑ‡∏´‡∏ô‡∏ö‡πâ‡∏≤‡∏á
    # ‡πÇ‡∏î‡∏¢‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏Ç‡∏≠‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏õ‡∏µ 2568 (‡∏´‡∏£‡∏∑‡∏≠‡∏õ‡∏µ‡πÉ‡∏î‡∏Å‡πá‡πÑ‡∏î‡πâ)
    sample_path = get_tenant_year_export_root(tenant, "2568")
    tenant_export_root = os.path.dirname(sample_path)

    if not year or str(year).lower() == "all":
        if os.path.exists(tenant_export_root):
            # ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏¢‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç (‡∏õ‡∏µ‡∏á‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì) ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
            search_years = [d for d in os.listdir(tenant_export_root) if d.isdigit()]
        else:
            search_years = []
    else:
        search_years = [str(year)]

    # 3. ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏™‡πÅ‡∏Å‡∏ô‡πÑ‡∏ü‡∏•‡πå JSON ‡∏ï‡∏≤‡∏°‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠
    for y in search_years:
        export_root = get_tenant_year_export_root(tenant, y)
        
        if not os.path.exists(export_root):
            continue

        for root, _, files in os.walk(export_root):
            for f in files:
                if f.lower().endswith(".json"):
                    try:
                        file_path = os.path.join(root, f)
                        with open(file_path, "r", encoding="utf-8") as jf:
                            data = json.load(jf)
                            summary = data.get("summary", {})
                            enabler = (summary.get("enabler") or "KM").upper()
                            
                            # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå‡∏£‡∏≤‡∏¢ Enabler (‡∏ñ‡πâ‡∏≤‡∏û‡∏±‡∏á‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ‡πÑ‡∏õ)
                            try:
                                check_user_permission(current_user, tenant, enabler)
                            except:
                                continue

                            history_list.append({
                                "record_id": data.get("record_id") or summary.get("record_id") or f.rsplit('.', 1)[0],
                                "date": parse_safe_date(summary.get("export_timestamp"), file_path),
                                "tenant": tenant,
                                "year": y,
                                "enabler": enabler,
                                "scope": summary.get("sub_criteria_id", "ALL"),
                                "level": f"L{summary.get('highest_pass_level_overall', summary.get('highest_pass_level', 0))}",
                                "score": round(float(summary.get("Total Weighted Score Achieved", summary.get("achieved_weight", 0.0))), 2),
                                "status": "COMPLETED"
                            })
                    except Exception as e:
                        logger.error(f"Error reading history file {f} in year {y}: {e}")

    # 4. ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà (‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏õ‡πÄ‡∏Å‡πà‡∏≤)
    return {"items": sorted(history_list, key=lambda x: x['date'], reverse=True)}

@assessment_router.post("/start")
async def start_assessment(
    request: StartAssessmentRequest, 
    background_tasks: BackgroundTasks, 
    current_user: UserMe = Depends(get_current_user)
):
    """
    Endpoint ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏õ‡∏µ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏≠‡∏¥‡∏™‡∏£‡∏∞
    - ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡∏õ‡∏µ‡∏à‡∏≤‡∏Å Request ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å
    - ‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Path ‡πÅ‡∏ö‡∏ö‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Docker/Local Path)
    """
    # 1. ‡∏à‡∏±‡∏î‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ñ‡πà‡∏≤ Parameter
    enabler_uc = request.enabler.upper()
    
    # --- ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á Logic ‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏õ‡∏µ (Priority: Request > User Profile > Default) ---
    raw_year = request.year if request.year else (current_user.year or DEFAULT_YEAR)
    target_year = str(raw_year).strip()
    
    target_sub = str(request.sub_criteria).strip().lower() if request.sub_criteria else "all"

    # 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå
    check_user_permission(current_user, request.tenant, enabler_uc)

    # ‡∏´‡∏≤ Path ‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏ö‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á
    vs_path = get_vectorstore_collection_path(
        tenant=request.tenant,
        year=target_year,
        doc_type="evidence",
        enabler=enabler_uc
    )

    # üõ°Ô∏è FIX: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô‡∏Ç‡∏≠‡∏á Path (‡∏Å‡∏£‡∏ì‡∏µ‡∏£‡∏±‡∏ô‡∏ö‡∏ô Server ‡∏ó‡∏µ‡πà Path ‡∏≠‡∏≤‡∏à‡∏ï‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å‡πÉ‡∏ô Container)
    resolved_vs_path = vs_path
    if not os.path.exists(resolved_vs_path) and vs_path.startswith("/app/"):
        # ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡πÅ‡∏ö‡∏ö‡∏ï‡∏±‡∏î /app/ ‡∏≠‡∏≠‡∏Å (Local mode)
        alt_path = vs_path.replace("/app/", "", 1)
        if os.path.exists(alt_path):
            resolved_vs_path = alt_path

    # A. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏õ‡∏µ‡∏ô‡∏±‡πâ‡∏ô‡πÜ ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á‡πÑ‡∏´‡∏°
    if not os.path.exists(resolved_vs_path):
        vs_tenant_root = get_vectorstore_tenant_root_path(request.tenant)
        # ‡∏•‡∏≠‡∏á‡∏™‡πÅ‡∏Å‡∏ô‡∏´‡∏≤ Path ‡∏à‡∏£‡∏¥‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ User
        real_root = vs_tenant_root.replace("/app/", "", 1) if not os.path.exists(vs_tenant_root) else vs_tenant_root
        
        available_info = ""
        if os.path.exists(real_root):
            years = [d for d in os.listdir(real_root) if os.path.isdir(os.path.join(real_root, d))]
            if years:
                available_info = f" ‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö‡∏Ñ‡∏∑‡∏≠: {', '.join(years)}"
            else:
                available_info = " ‡∏£‡∏∞‡∏ö‡∏ö‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏µ‡πÉ‡∏î‡πÜ ‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"
        
        logger.error(f"‚ùå Path Not Found: {vs_path} (Resolved: {resolved_vs_path})")
        raise HTTPException(
            status_code=400, 
            detail=f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• {enabler_uc} ‡∏Ç‡∏≠‡∏á‡∏õ‡∏µ {target_year}.{available_info}"
        )

    # B. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≤‡∏á‡πÉ‡∏ô (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ß‡πà‡∏≤‡∏á)
    # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ó‡∏±‡πâ‡∏á chroma.sqlite3 ‡∏´‡∏£‡∏∑‡∏≠‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå UUID ‡∏Ç‡∏≠‡∏á Chroma
    db_file = os.path.join(resolved_vs_path, "chroma.sqlite3")
    has_subdirs = any(os.path.isdir(os.path.join(resolved_vs_path, d)) for d in os.listdir(resolved_vs_path)) if os.path.exists(resolved_vs_path) else False
    
    if not os.path.exists(db_file) and not has_subdirs:
        raise HTTPException(
            status_code=400, 
            detail=f"‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏µ {target_year} ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å Ingest ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤)"
        )

    # --------------------------------------------------------

    # 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á Record ID
    record_id = uuid.uuid4().hex[:12]
    
    # 4. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á ACTIVE_TASKS
    ACTIVE_TASKS[record_id] = {
        "status": "RUNNING",
        "record_id": record_id,
        "tenant": request.tenant,
        "year": target_year,
        "enabler": enabler_uc,
        "progress_message": f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô {enabler_uc} ‡∏õ‡∏µ {target_year}..."
    }

    # 5. ‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤ Background Task
    background_tasks.add_task(
        run_assessment_engine_task,
        record_id=record_id,
        tenant=request.tenant,
        year=target_year,
        enabler=enabler_uc,
        sub_id=target_sub,
        sequential=request.sequential_mode
    )

    logger.info(f"üöÄ Started Assessment: {record_id} | Year: {target_year} | Path: {resolved_vs_path}")
    return {"record_id": record_id, "status": "RUNNING"}

async def run_assessment_engine_task(
    record_id: str, 
    tenant: str, 
    year: str,  # ‡πÅ‡∏Å‡πâ Type Hint ‡πÄ‡∏õ‡πá‡∏ô str
    enabler: str, 
    sub_id: str, 
    sequential: bool
):
    try:
        str_year = year # ‡πÉ‡∏ä‡πâ‡∏á‡πà‡∏≤‡∏¢‡πÜ ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏õ‡∏•‡∏á
        logger.info(f"üöÄ [TASK START] Record: {record_id} | Enabler: {enabler} | Sub-ID: {sub_id} | Year: {str_year}")

        # 1. Load Vectorstores (‡πÉ‡∏ä‡πâ str_year ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡∏´‡∏≤ Path)
        vsm = await asyncio.to_thread(
            load_all_vectorstores,
            doc_types=EVIDENCE_DOC_TYPES,
            enabler_filter=enabler,
            tenant=tenant,
            year=str_year
        )
        
        # 2. Load Document Mapping
        doc_map_raw = await asyncio.to_thread(
            load_doc_id_mapping, 
            EVIDENCE_DOC_TYPES, 
            tenant, 
            str_year, 
            enabler
        )
        doc_map = {d_id: d.get("file_name", d_id) for d_id, d in doc_map_raw.items()}

        # 3. Create LLM & Engine
        llm = await asyncio.to_thread(create_llm_instance, model_name=DEFAULT_LLM_MODEL_NAME, temperature=0.0)
        config = AssessmentConfig(enabler=enabler, tenant=tenant, year=str_year, force_sequential=sequential)

        engine = SEAMPDCAEngine(
            config=config,
            llm_instance=llm,
            logger_instance=logger,
            doc_type=EVIDENCE_DOC_TYPES,
            vectorstore_manager=vsm,
            document_map=doc_map
        )

        # 4. Execution
        result = await asyncio.to_thread(
            engine.run_assessment, 
            target_sub_id=sub_id, 
            export=True, 
            vectorstore_manager=vsm, 
            sequential=sequential, 
            record_id=record_id,
            document_map=doc_map
        )

        if isinstance(result, dict) and result.get("status") == "FAILED":
            error_msg = result.get("error_message", "Engine reported an error")
            logger.error(f"‚ùå [TASK FAILED] {record_id}: {error_msg}")
            if record_id in ACTIVE_TASKS:
                ACTIVE_TASKS[record_id]["status"] = "FAILED"
                ACTIVE_TASKS[record_id]["error_message"] = error_msg
            return

        if record_id in ACTIVE_TASKS:
            del ACTIVE_TASKS[record_id]
            logger.info(f"‚úÖ [TASK COMPLETED] Record: {record_id}")
            
    except Exception as e:
        logger.error(f"üí• [TASK CRASH] Record {record_id}: {str(e)}", exc_info=True)
        if record_id in ACTIVE_TASKS:
            ACTIVE_TASKS[record_id]["status"] = "FAILED"
            ACTIVE_TASKS[record_id]["error_message"] = f"Internal Server Error: {str(e)}"


@assessment_router.get("/download/{record_id}/{file_type}")
async def download_assessment_file(
    record_id: str,
    file_type: str,
    current_user: UserMe = Depends(get_current_user)
):
    logger.info(f"Download request: record_id={record_id}, file_type={file_type}")

    # 1. ‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå JSON
    json_path = _find_assessment_file(record_id, current_user)

    # 2. ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    with open(json_path, "r", encoding="utf-8") as f:
        raw_data = json.load(f)

    # 3. ‡∏ï‡∏£‡∏ß‡∏à permission
    enabler = (raw_data.get("summary", {}).get("enabler") or "KM").upper()
    check_user_permission(current_user, current_user.tenant, enabler)

    file_type = file_type.lower()

    # 4. JSON
    if file_type == "json":
        return FileResponse(
            path=json_path,
            filename=f"assessment-{record_id}.json",
            media_type="application/json"
        )

    # 5. Word Report
    elif file_type in ["word", "docx"]:
        logger.info(f"Generating on-the-fly Word report for {record_id}")

        ui_data = _transform_result_for_ui(raw_data)
        doc = create_docx_report_similar_to_ui(ui_data)

        with tempfile.NamedTemporaryFile(delete=False, suffix=".docx") as tmp:
            doc.save(tmp.name)
            temp_path = tmp.name

        logger.info(f"Word report generated: {os.path.basename(temp_path)}")

        return FileResponse(
            path=temp_path,
            filename=f"{ui_data['enabler']}_Assessment_Report_{record_id}.docx",
            media_type="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
            background=lambda: os.remove(temp_path)  # ‚úÖ ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á import BackgroundTask
        )

    else:
        raise HTTPException(status_code=400, detail="‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞ json ‡πÅ‡∏•‡∏∞ word")