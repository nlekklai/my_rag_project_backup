# -*- coding: utf-8 -*-
# routers/assessment_router.py
# Production Final Version - 2026 Optimized for DB Persistence & Professional Reporting

import os
import uuid
import json
import asyncio
import logging
import mimetypes
import tempfile
import pytz
from datetime import datetime
from typing import Optional, Dict, Any, Union, List

from fastapi import APIRouter, BackgroundTasks, HTTPException, Depends, Query
from fastapi.responses import FileResponse
from pydantic import BaseModel

# --- Docx Imports (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á Report) ---
from docx import Document
from docx.shared import Pt, RGBColor, Inches
from docx.enum.text import WD_ALIGN_PARAGRAPH
from docx.oxml.ns import qn

# --- Project Imports ---
from routers.auth_router import get_current_user, check_user_permission, UserMe
from utils.path_utils import (
    _n, 
    get_tenant_year_export_root, 
    load_doc_id_mapping, 
    get_document_file_path,
    get_vectorstore_collection_path,
    get_vectorstore_tenant_root_path
)

from core.seam_assessment import SEAMPDCAEngine, AssessmentConfig
from core.vectorstore import load_all_vectorstores
from models.llm import create_llm_instance
from config.global_vars import (
    EVIDENCE_DOC_TYPES, 
    DEFAULT_LLM_MODEL_NAME, 
    DEFAULT_YEAR, 
    DEFAULT_TENANT,
    DATA_STORE_ROOT
)

# üéØ Database Components (SQLite Persistence)
from database import (
    SessionLocal, 
    AssessmentTaskTable, 
    AssessmentResultTable,
    db_update_task_status,
    db_finish_task
)

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Logger ‡πÅ‡∏•‡∏∞ Router
logger = logging.getLogger(__name__)
assessment_router = APIRouter(prefix="/api/assess", tags=["Assessment"])

# --- Request Models ---
class StartAssessmentRequest(BaseModel):
    tenant: str
    year: Optional[Union[int, str]] = None
    enabler: str = "KM"
    sub_criteria: Optional[str] = "all"
    sequential_mode: bool = False

# ------------------------------------------------------------------
# [Helpers]
# ------------------------------------------------------------------
def parse_safe_date(raw_date_str: Any, file_path: str) -> str:
    """‡πÅ‡∏õ‡∏•‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏≤‡∏Å String ‡∏´‡∏£‡∏∑‡∏≠ File Metadata ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô ISO Format (Bangkok Time)"""
    tz = pytz.timezone('Asia/Bangkok')
    if raw_date_str and isinstance(raw_date_str, str):
        try:
            # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö format yyyymmdd_hhmmss
            if "_" in raw_date_str:
                dt = datetime.strptime(raw_date_str, "%Y%m%d_%H%M%S")
                return tz.localize(dt).isoformat()
        except: pass
    
    # Fallback: ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÑ‡∏ü‡∏•‡πå‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
    try:
        mtime = os.path.getmtime(file_path)
        dt = datetime.fromtimestamp(mtime, tz)
        return dt.isoformat()
    except:
        return datetime.now(tz).isoformat()


def safe_float(value):
    try:
        if isinstance(value, str):
            value = value.replace('%', '') # ‡∏•‡∏ö % ‡∏≠‡∏≠‡∏Å‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
        return float(value)
    except:
        return 0.0
    

@assessment_router.get("/evidence/{doc_type}/{document_uuid}")
async def serve_evidence_file(
    document_uuid: str,
    doc_type: str,
    tenant: str,
    year: str = None,
    enabler: str = None,
    current_user: UserMe = Depends(get_current_user)
):
    check_user_permission(current_user, tenant, enabler or "KM")

    file_info = get_document_file_path(
        document_uuid=document_uuid,
        tenant=tenant,
        year=year,
        enabler=enabler,
        doc_type_name=doc_type
    )

    if not file_info:
        raise HTTPException(status_code=404, detail="File not found")

    file_path = file_info["file_path"]
    
    # ‡∏î‡∏∂‡∏á‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•‡πÑ‡∏ü‡∏•‡πå
    ext = os.path.splitext(file_path)[1].lower()
    
    # üõ°Ô∏è Force MIME Type ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Mac/Safari
    mime_map = {
        ".pdf": "application/pdf",
        ".jpg": "image/jpeg",
        ".jpeg": "image/jpeg",
        ".png": "image/png",
    }
    
    mime_type = mime_map.get(ext) or mimetypes.guess_type(file_path)[0] or "application/octet-stream"

    # ‡∏™‡πà‡∏á FileResponse
    response = FileResponse(
        path=file_path,
        media_type=mime_type,
        content_disposition_type="inline"
    )

    # üí° ‡∏´‡∏±‡∏ß‡πÉ‡∏à‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Mac/Safari:
    # 1. ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ Browser ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å Path ‡∏ã‡∏∂‡πà‡∏á‡∏ö‡∏≤‡∏á‡∏ó‡∏µ‡∏°‡∏µ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÅ‡∏•‡πâ‡∏ß‡∏ó‡∏≥‡πÉ‡∏´‡πâ Header ‡πÄ‡∏û‡∏µ‡πâ‡∏¢‡∏ô
    # 2. ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö Header ‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
    response.headers["Content-Type"] = mime_type
    response.headers["Accept-Ranges"] = "bytes" 
    
    # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô PDF ‡∏ö‡∏ô Mac ‡πÉ‡∏´‡πâ‡πÄ‡∏ï‡∏¥‡∏° Cache-Control ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Viewer ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô
    if ext == ".pdf":
        response.headers["Cache-Control"] = "no-cache, no-store, must-revalidate"

    return response


@assessment_router.get("/view-document")
async def view_document(
    filename: str, 
    document_uuid: Optional[str] = None, 
    current_user: UserMe = Depends(get_current_user)
):
    """ Endpoint ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡∏¥‡∏î‡πÑ‡∏ü‡∏•‡πå PDF ‡πÇ‡∏î‡∏¢‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ó‡∏µ‡πà‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Ç‡∏∂‡πâ‡∏ô """
    
    file_path = None

    # 1. ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏´‡∏≤‡∏à‡∏≤‡∏Å UUID ‡∏Å‡πà‡∏≠‡∏ô (‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ 100% ‡πÅ‡∏°‡πâ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏∞‡∏ã‡πâ‡∏≥)
    if document_uuid:
        file_info = get_document_file_path(
            document_uuid=document_uuid,
            tenant=current_user.tenant,
            year=current_user.year,
            enabler="KM", # ‡∏´‡∏£‡∏∑‡∏≠‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å Query Param
            doc_type_name="evidence"
        )
        if file_info:
            file_path = file_info["file_path"]

    # 2. ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ UUID ‡πÉ‡∏´‡πâ‡∏´‡∏≤‡∏à‡∏≤‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå (‡πÉ‡∏ä‡πâ Logic ‡πÄ‡∏î‡∏¥‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì)
    if not file_path:
        # ‡πÉ‡∏ä‡πâ get_document_source_dir ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡πÉ‡∏ô path_utils ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß
        from utils.path_utils import get_document_source_dir, resolve_filepath_to_absolute
        
        base_path = get_document_source_dir(
            tenant=current_user.tenant,
            year=current_user.year,
            enabler="KM",
            doc_type="evidence"
        )
        file_path = resolve_filepath_to_absolute(os.path.join(base_path, filename))

    if not file_path or not os.path.exists(file_path):
        logger.error(f"‚ùå File not found: {file_path}")
        raise HTTPException(status_code=404, detail=f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ö‡∏ô‡πÄ‡∏ã‡∏¥‡∏£‡πå‡∏ü‡πÄ‡∏ß‡∏≠‡∏£‡πå")

    # ‡∏™‡πà‡∏á‡πÑ‡∏ü‡∏•‡πå PDF ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ
    return FileResponse(file_path, media_type="application/pdf")


def _transform_result_for_ui(raw_data: Dict[str, Any], current_user: Any = None) -> Dict[str, Any]:
    """
    [PRODUCTION READY - FIXED VERSION for Level 1 realism]
    - AI STRENGTH SUMMARY: ‡πÄ‡∏û‡∏¥‡πà‡∏° prefix ‡∏ï‡∏≤‡∏°‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á + ‡∏õ‡∏£‡∏±‡∏ö‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
    - ‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏î‡∏π "‡πÄ‡∏Å‡∏¥‡∏ô‡∏à‡∏£‡∏¥‡∏á" ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ú‡πà‡∏≤‡∏ô‡πÅ‡∏Ñ‡πà L1
    - Roadmap: Steps ‡πÄ‡∏õ‡πá‡∏ô Object ‡∏Ñ‡∏£‡∏ö 4 fields
    - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏≤‡∏î‡∏´‡∏≤‡∏¢ + fallback ‡∏ó‡∏µ‡πà‡∏™‡∏°‡πÄ‡∏´‡∏ï‡∏∏‡∏™‡∏°‡∏ú‡∏•
    """
    summary = raw_data.get("summary", {})
    sub_results = raw_data.get("sub_criteria_results", []) or []

    processed_sub_criteria: List[Dict[str, Any]] = []
    radar_data: List[Dict[str, Any]] = []

    # --- 1. Header & Global Metrics ---
    enabler_name = (summary.get("enabler") or "KM").upper()
    overall_level = str(summary.get("Overall Maturity Level (Weighted)") or
                        f"L{summary.get('highest_pass_level_overall', 0)}")

    total_score = round(safe_float(summary.get("Total Weighted Score Achieved")), 2)
    full_score_all = round(float(summary.get("Total Possible Weight") or 40.0), 2)
    total_expected = int(summary.get("total_subcriteria") or 12)
    passed_count = int(summary.get("total_subcriteria_assessed") or len(sub_results))
    completion_rate = (passed_count / total_expected * 100) if total_expected > 0 else 0.0

    for res in sub_results:
        cid = res.get("sub_criteria_id", "N/A")
        cname = res.get("sub_criteria_name", f"‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏¢‡πà‡∏≠‡∏¢ {cid}")
        highest_pass = int(res.get("highest_full_level") or 0)
        raw_levels_list = res.get("raw_results_ref", []) or []

        # --- 2. Audit Confidence ---
        raw_audit_conf = res.get("audit_confidence") or {}
        if not raw_audit_conf and raw_levels_list:
            raw_audit_conf = raw_levels_list[-1].get("audit_confidence") or {}

        ui_audit_confidence = {
            "level": raw_audit_conf.get("level", "LOW"),
            "source_count": int(raw_audit_conf.get("source_count", 0)),
            "traceability_score": float(raw_audit_conf.get("traceability_score", 0.0)),
            "consistency_check": bool(raw_audit_conf.get("consistency_check", True)),
            "reason": raw_audit_conf.get("reason", "‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ï‡∏≤‡∏°‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô SE-AM")
        }

        # --- 3. PDCA Matrix & Coverage ---
        pdca_matrix = []
        pdca_coverage = {str(lv): {"percentage": 0} for lv in range(1, 6)}
        raw_levels_map = {item.get("level"): item for item in raw_levels_list}

        for lv_idx in range(1, 6):
            lv_info = raw_levels_map.get(lv_idx)
            is_passed = lv_info.get("is_passed", False) if lv_info else (lv_idx <= highest_pass)

            eval_mode = "NORMAL"
            if is_passed and lv_idx > highest_pass:
                eval_mode = "GAP_ONLY"
            elif not is_passed and lv_info:
                eval_mode = "FAILED"
            elif not is_passed:
                eval_mode = "INACTIVE"

            pdca_raw = lv_info.get("pdca_breakdown", {}) if lv_info else {}
            pdca_final = {k: (1 if float(pdca_raw.get(k, 0)) > 0 else 0) for k in ["P", "D", "C", "A"]}

            if not lv_info and lv_idx <= highest_pass:
                pdca_final = {"P": 1, "D": 1, "C": 1, "A": 1}

            pdca_matrix.append({
                "level": lv_idx,
                "is_passed": is_passed,
                "evaluation_mode": eval_mode,
                "pdca": pdca_final
            })

            pdca_coverage[str(lv_idx)]["percentage"] = (sum(pdca_final.values()) / 4) * 100

        # --- 4. Grouped Evidence & Confidence ---
        grouped_sources = {str(lv): [] for lv in range(1, 6)}
        all_scores = []
        avg_confidence_per_level = {}

        for lv_idx in range(1, 6):
            lv_scores = []
            lv_refs = [r for r in raw_levels_list if r.get("level") == lv_idx]
            for ref in lv_refs:
                sources = ref.get("temp_map_for_level", []) or [ref]
                for s in sources:
                    meta = s.get("metadata", {})
                    d_uuid = s.get("document_uuid") or meta.get("stable_doc_uuid") or s.get("doc_id")
                    if not d_uuid:
                        continue

                    score_val = float(s.get("rerank_score") or meta.get("rerank_score") or s.get("score") or 0.0)
                    if score_val > 0:
                        all_scores.append(score_val)
                        lv_scores.append(score_val)

                    pdca_tag = s.get("pdca_tag") or meta.get("pdca_tag") or "OTHER"

                    grouped_sources[str(lv_idx)].append({
                        "filename": s.get("filename") or meta.get("source") or "Evidence Document",
                        "page": str(meta.get("page") or meta.get("page_label") or "1"),
                        "text": (s.get("text") or "")[:300] + ("..." if len(s.get("text") or "") > 300 else ""),
                        "rerank_score": round(score_val * 100, 1),
                        "document_uuid": d_uuid,
                        "pdca_tag": str(pdca_tag).upper(),
                        "doc_type": s.get("doc_type", "evidence")
                    })

            avg_confidence_per_level[str(lv_idx)] = round((sum(lv_scores) / len(lv_scores) * 100), 1) if lv_scores else 0.0

        # --- 5. Roadmap ---
        ui_roadmap = []
        all_gaps = []
        raw_plans = res.get("action_plan") or []

        for p in raw_plans:
            phase_name = p.get("Phase") or p.get("phase") or "Phase ‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤"
            goal = p.get("Goal") or p.get("goal") or "‡∏õ‡∏¥‡∏î‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏¢‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡πÄ‡∏Å‡∏ì‡∏ë‡πå"

            phase_actions = []
            actions_list = p.get("Actions") or p.get("actions") or []

            for act in actions_list:
                recommendation = act.get("Recommendation") or act.get("recommendation") or "‡∏Ñ‡∏ß‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå"
                failed_level = str(act.get("Failed_Level") or act.get("failed_level") or (highest_pass + 1))
                all_gaps.append(f"**L{failed_level}**: {recommendation}")

                formatted_steps = []
                raw_steps = act.get("Steps") or act.get("steps") or []

                for s_idx, s in enumerate(raw_steps):
                    if isinstance(s, dict):
                        formatted_steps.append({
                            "step": s.get("Step") or s.get("step") or (s_idx + 1),
                            "description": s.get("Description") or s.get("description") or "‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≤‡∏°‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥",
                            "responsible": s.get("Responsible") or s.get("responsible") or "‡∏Ñ‡∏ì‡∏∞‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á",
                            "verification_outcome": s.get("Verification_Outcome") or s.get("verification_outcome") or "‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô"
                        })
                    else:
                        formatted_steps.append({
                            "step": s_idx + 1,
                            "description": str(s),
                            "responsible": "‡∏Ñ‡∏ì‡∏∞‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á",
                            "verification_outcome": "‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô"
                        })

                phase_actions.append({
                    "failed_level": failed_level,
                    "recommendation": recommendation,
                    "target_evidence_type": act.get("Target_Evidence_Type") or "Report/Policy/Document",
                    "steps": formatted_steps
                })

            ui_roadmap.append({
                "phase": phase_name,
                "goal": goal,
                "actions": phase_actions
            })

        # --- 6. üéØ AI STRENGTH SUMMARY - ‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏´‡πâ‡∏™‡∏°‡∏à‡∏£‡∏¥‡∏á‡∏ï‡∏≤‡∏°‡∏£‡∏∞‡∏î‡∏±‡∏ö ---
        base_reason = ui_audit_confidence["reason"].strip()

        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î prefix ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏ï‡∏≤‡∏°‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á
        level_num = highest_pass
        if level_num == 1:
            prefix = f"‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô (L{level_num}): ‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô"
            adjusted_reason = base_reason.replace("‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡∏ï‡∏≤‡∏°‡∏ß‡∏á‡∏à‡∏£ PDCA", "‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô")
        elif level_num == 2:
            prefix = f"‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô (L{level_num}): "
            adjusted_reason = base_reason
        elif level_num == 3:
            prefix = f"‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏û‡∏±‡∏í‡∏ô‡∏≤ (L{level_num}): "
            adjusted_reason = base_reason
        else:
            prefix = f"‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á (L{level_num}): "
            adjusted_reason = base_reason

        strength_summary = f"{prefix} {adjusted_reason}"

        # ‡∏ï‡πà‡∏≠‡∏î‡πâ‡∏ß‡∏¢ summary_thai ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå
        content_analysis = res.get("summary_thai", "").strip()
        if (content_analysis and
            content_analysis != "‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô" and
            len(content_analysis) > 20 and
            content_analysis not in strength_summary):
            strength_summary += f" {content_analysis}"

        # Fallback ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
        if not strength_summary or len(strength_summary) < 20:
            strength_summary = f"‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö L{level_num}: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô"

        # --- 7. Final Sub-Criteria Mapping ---
        potential_level = max(
            [r.get("level") for r in raw_levels_list if r.get("is_passed")] + [highest_pass, 0]
        )
        current_score = float(raw_levels_list[-1].get("score") or 0.0) if raw_levels_list else (highest_pass * 0.2)

        processed_sub_criteria.append({
            "code": cid,
            "name": cname,
            "level": f"L{highest_pass}",
            "score": round(current_score, 1),
            "potential_level": f"L{potential_level}",
            "is_gap_analysis": potential_level > highest_pass,
            "pdca_matrix": pdca_matrix,
            "pdca_coverage": pdca_coverage,
            "avg_confidence_per_level": avg_confidence_per_level,
            "audit_confidence": ui_audit_confidence,
            "roadmap": ui_roadmap,
            "grouped_sources": grouped_sources,
            "summary_thai": strength_summary,  # <--- ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏à‡∏£‡∏¥‡∏á‡∏Ç‡∏∂‡πâ‡∏ô
            "gap": "\n\n".join(all_gaps) if all_gaps else "‡∏ö‡∏£‡∏£‡∏•‡∏∏‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô",
            "confidence_score": round((sum(all_scores) / len(all_scores) * 100) if all_scores else 0, 1)
        })

        radar_data.append({"axis": cid, "value": highest_pass})

    # --- Final Return ---
    return {
        "status": "COMPLETED",
        "record_id": raw_data.get("record_id", "unknown"),
        "tenant": str(summary.get("tenant", DEFAULT_TENANT)).upper(),
        "year": str(summary.get("year", DEFAULT_YEAR)),
        "enabler": enabler_name,
        "level": overall_level,
        "score": total_score,
        "full_score": full_score_all,
        "metrics": {
            "total_criteria": total_expected,
            "passed_criteria": passed_count,
            "completion_rate": round(completion_rate, 2)
        },
        "radar_data": radar_data,
        "sub_criteria": processed_sub_criteria
    }

def create_docx_report_similar_to_ui(ui_data: dict) -> Document:
    doc = Document()

    # --- ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡∏Å‡∏£‡∏∞‡∏î‡∏≤‡∏© ---
    section = doc.sections[0]
    section.top_margin = Inches(0.5)
    section.bottom_margin = Inches(0.5)
    section.left_margin = Inches(0.8)
    section.right_margin = Inches(0.8)

    def set_thai_font(run, name='TH Sarabun New', size=14, bold=False, color=None):
        run.font.name = name
        run._element.rPr.rFonts.set(qn('w:eastAsia'), name)
        run.font.size = Pt(size)
        run.bold = bold
        if color:
            run.font.color.rgb = color

    # --- 1. ‡∏´‡∏ô‡πâ‡∏≤‡∏õ‡∏Å / ‡∏´‡∏±‡∏ß‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô ---
    title_p = doc.add_paragraph()
    title_p.alignment = WD_ALIGN_PARAGRAPH.CENTER
    run = title_p.add_run(f"{ui_data.get('enabler', 'KM')} ASSESSMENT REPORT\n")
    set_thai_font(run, size=24, bold=True, color=RGBColor(30, 58, 138))
    
    # ‡∏™‡∏£‡∏∏‡∏õ‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°
    summary_table = doc.add_table(rows=0, cols=2)
    summary_table.alignment = WD_ALIGN_PARAGRAPH.CENTER
    
    summary_data = [
        ("Record ID", ui_data.get('record_id', '-')),
        ("‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô", ui_data.get('tenant', '-')),
        ("‡∏õ‡∏µ‡∏á‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì", ui_data.get('year', '-')),
        ("‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°", f"L{ui_data.get('level', '0')}"),
        ("‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏° / ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏ï‡πá‡∏°", f"{ui_data.get('score', 0)} / {ui_data.get('full_score', 40)}"),
        ("‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô (Completion)", f"{ui_data.get('metrics', {}).get('completion_rate', 0):.1f}%")
    ]

    for label, value in summary_data:
        row = summary_table.add_row().cells
        set_thai_font(row[0].paragraphs[0].add_run(label), size=14, bold=True)
        set_thai_font(row[1].paragraphs[0].add_run(str(value)), size=14)

    doc.add_page_break()

    # --- 2. ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏£‡∏≤‡∏¢‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏¢‡πà‡∏≠‡∏¢ ---
    sub_criteria = ui_data.get('sub_criteria', [])
    for item in sub_criteria:
        # ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÄ‡∏Å‡∏ì‡∏ë‡πå
        h = doc.add_paragraph()
        run = h.add_run(f"‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏¢‡πà‡∏≠‡∏¢ {item.get('code', '')}: {item.get('name', '')}")
        set_thai_font(run, size=18, bold=True, color=RGBColor(30, 58, 138))

        # --- ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏°: Audit Confidence Metrics (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏ö‡∏ô UI) ---
        conf_table = doc.add_table(rows=1, cols=3)
        conf_table.style = 'Table Grid'
        cells = conf_table.rows[0].cells
        
        # ‡∏Å‡∏•‡πà‡∏≠‡∏á 1: Independence
        p1 = cells[0].paragraphs[0]
        p1.alignment = WD_ALIGN_PARAGRAPH.CENTER
        set_thai_font(p1.add_run("Independence"), size=10, bold=True)
        p1.add_run(f"\n{item.get('audit_confidence', {}).get('source_count', 0)} Files").font.size = Pt(14)
        
        # ‡∏Å‡∏•‡πà‡∏≠‡∏á 2: Traceability
        p2 = cells[1].paragraphs[0]
        p2.alignment = WD_ALIGN_PARAGRAPH.CENTER
        set_thai_font(p2.add_run("Traceability"), size=10, bold=True)
        trace_val = int(item.get('audit_confidence', {}).get('traceability_score', 0) * 100)
        p2.add_run(f"\n{trace_val}%").font.size = Pt(14)
        
        # ‡∏Å‡∏•‡πà‡∏≠‡∏á 3: Consistency
        p3 = cells[2].paragraphs[0]
        p3.alignment = WD_ALIGN_PARAGRAPH.CENTER
        set_thai_font(p3.add_run("Consistency"), size=10, bold=True)
        consist_txt = "VERIFIED" if item.get('audit_confidence', {}).get('consistency_check') else "CONFLICT"
        p3.add_run(f"\n{consist_txt}").font.size = Pt(14)

        doc.add_paragraph() # ‡πÄ‡∏ß‡πâ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î

        # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Ç‡∏≠‡∏á AI (Strength & Gap)
        # Strength
        s_title = doc.add_paragraph()
        set_thai_font(s_title.add_run("‡∏ö‡∏ó‡∏™‡∏£‡∏∏‡∏õ‡∏à‡∏∏‡∏î‡πÅ‡∏Ç‡πá‡∏á (AI Strength Summary):"), size=14, bold=True, color=RGBColor(22, 101, 52))
        set_thai_font(doc.add_paragraph(item.get('summary_thai', '-')).runs[0], size=13)

        # Gap
        g_title = doc.add_paragraph()
        set_thai_font(g_title.add_run("‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á (Critical Gaps):"), size=14, bold=True, color=RGBColor(154, 52, 18))
        set_thai_font(doc.add_paragraph(item.get('gap', '‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏ö‡∏Å‡∏û‡∏£‡πà‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç')).runs[0], size=13)

        # Roadmap (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
        if item.get('roadmap'):
            r_title = doc.add_paragraph()
            set_thai_font(r_title.add_run("Roadmap ‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÄ‡∏ä‡∏¥‡∏á‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå:"), size=14, bold=True, color=RGBColor(30, 58, 138))
            
            for phase in item['roadmap']:
                p_text = f"‡∏£‡∏∞‡∏¢‡∏∞: {phase.get('phase', '')}"
                phase_p = doc.add_paragraph(style='List Bullet')
                set_thai_font(phase_p.add_run(p_text), size=13, bold=True)

                for act in phase.get('actions', []):
                    # Recommendation
                    act_p = doc.add_paragraph(style='List Bullet 2')
                    set_thai_font(act_p.add_run(f"‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ L{act.get('level')}: {act.get('recommendation')}"), size=12, bold=True)
                    
                    # Steps
                    for step in act.get('steps', []):
                        step_p = doc.add_paragraph(style='List Bullet 3')
                        set_thai_font(step_p.add_run(str(step)), size=11)

        doc.add_page_break()

    return doc

# ==================== API ENDPOINT: GET Status / Get Data ====================
@assessment_router.get("/status/{record_id}")
async def get_assessment_status(
    record_id: str, 
    current_user: UserMe = Depends(get_current_user)
):
    """
    Endpoint ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏£‡∏≤‡∏¢ Record:
    1. ‡∏ñ‡πâ‡∏≤‡∏á‡∏≤‡∏ô‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏£‡∏±‡∏ô (ACTIVE_TASKS) ‡∏à‡∏∞‡∏™‡πà‡∏á Progress ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ
    2. ‡∏ñ‡πâ‡∏≤‡∏á‡∏≤‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß ‡∏à‡∏∞‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå JSON ‡∏ö‡∏ô Disk ‡πÅ‡∏•‡πâ‡∏ß‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà Transform ‡πÅ‡∏•‡πâ‡∏ß‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ
    """
    
    # 1. ‡πÄ‡∏ä‡πá‡∏Ñ‡πÉ‡∏ô Memory ‡∏Å‡πà‡∏≠‡∏ô (‡∏Å‡∏£‡∏ì‡∏µ‡∏á‡∏≤‡∏ô‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏£‡∏±‡∏ô - Polling)
    # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤ ACTIVE_TASKS ‡∏Ñ‡∏∑‡∏≠ Dict ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏á‡∏≤‡∏ô‡πÉ‡∏ô RAM
    if record_id in globals().get("ACTIVE_TASKS", {}):
        return globals()["ACTIVE_TASKS"][record_id]

    # 2. ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô Memory ‡πÉ‡∏´‡πâ‡πÑ‡∏õ‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß‡∏ö‡∏ô Disk
    file_path = _find_assessment_file(record_id, current_user)
    
    try:
        # 3. ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå JSON ‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
        with open(file_path, "r", encoding="utf-8") as f:
            raw_data = json.load(f)

        # 4. ‡∏î‡∏∂‡∏á Metadata ‡∏°‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á (Tenant Isolation)
        summary = raw_data.get("summary", {})
        file_enabler = (summary.get("enabler") or "KM").upper()
        file_tenant = summary.get("tenant") or current_user.tenant
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ User ‡∏°‡∏µ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå‡∏î‡∏π‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏≠‡∏á Tenant ‡πÅ‡∏•‡∏∞ Enabler ‡∏ô‡∏µ‡πâ‡∏à‡∏£‡∏¥‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        check_user_permission(current_user, file_tenant, file_enabler)

        # 5. üî• ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Transform) ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô Format ‡∏ó‡∏µ‡πà UI ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•
        # ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡πÉ‡∏™‡πà current_user ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢‡∏ï‡∏≤‡∏° Signature ‡πÄ‡∏î‡∏¥‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
        ui_result = _transform_result_for_ui(raw_data, current_user)
        
        # üõ°Ô∏è ‡∏à‡∏∏‡∏î‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏™‡πà status ‡πÅ‡∏•‡∏∞ record_id ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏™‡∏°‡∏≠ 
        # ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Frontend ‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡∏ï‡πâ‡∏≠‡∏á "‡πÄ‡∏•‡∏¥‡∏Å‡πÇ‡∏´‡∏•‡∏î" ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏õ‡∏´‡∏ô‡πâ‡∏≤ Result ‡πÑ‡∏î‡πâ‡πÅ‡∏•‡πâ‡∏ß
        ui_result["status"] = "COMPLETED"
        ui_result["record_id"] = record_id
        
        logger.info(f"üöÄ [Status] Returning COMPLETED status for: {record_id}")
        return ui_result

    except json.JSONDecodeError:
        logger.error(f"üí• [Status] Invalid JSON file format at: {file_path}")
        raise HTTPException(status_code=500, detail="‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢ ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏õ‡∏¥‡∏î‡πÑ‡∏î‡πâ")
    except Exception as e:
        logger.error(f"üí• [Status] Error processing result for {record_id}: {str(e)}")
        # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô UI ‡∏ô‡∏¥‡πà‡∏á‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡∏ö‡∏≠‡∏Å Error ‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
        raise HTTPException(status_code=500, detail=f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡∏ú‡∏•: {str(e)}")
    
@assessment_router.get("/history")
async def get_assessment_history(
    tenant: str, 
    year: Optional[str] = Query(None),
    enabler: Optional[str] = Query(None),
    current_user: UserMe = Depends(get_current_user)
):
    """
    Full Revised History Endpoint:
    ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÉ‡∏´‡πâ‡∏≠‡πà‡∏≤‡∏ô‡∏Ñ‡πà‡∏≤ Level ‡∏à‡∏≤‡∏Å "Overall Maturity Level (Weighted)" ‡∏´‡∏£‡∏∑‡∏≠ "highest_pass_level"
    ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
    """
    # 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£
    check_user_permission(current_user, tenant)

    history_list = []
    from config.global_vars import DATA_STORE_ROOT
    
    # 2. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Path (Tenant & Exports)
    norm_tenant = _n(tenant)
    tenant_export_root = os.path.join(DATA_STORE_ROOT, norm_tenant, "exports")
    
    # Fallback ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏±‡∏ô Local
    if not os.path.exists(tenant_export_root):
        alt_path = os.path.join("data_store", norm_tenant, "exports")
        if os.path.exists(alt_path): 
            tenant_export_root = alt_path

    if not os.path.exists(tenant_export_root):
        logger.warning(f"‚ö†Ô∏è [History] ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏≠‡∏á {norm_tenant}")
        return {"items": []}

    # 3. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Filter
    user_allowed_enablers = [e.upper() for e in current_user.enablers]
    target_enabler = enabler.upper() if enabler else None

    # 4. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ä‡πà‡∏ß‡∏á‡∏õ‡∏µ
    if not year or str(year).lower() == "all":
        search_years = [d for d in os.listdir(tenant_export_root) if d.isdigit()]
    else:
        search_years = [str(year)]

    # 5. ‡∏™‡πÅ‡∏Å‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡∏∞‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    for y in search_years:
        year_path = os.path.join(tenant_export_root, y)
        if not os.path.exists(year_path): continue

        for root, _, files in os.walk(year_path):
            for f in files:
                if f.lower().endswith(".json"):
                    try:
                        file_path = os.path.join(root, f)
                        with open(file_path, "r", encoding="utf-8") as jf:
                            data = json.load(jf)
                            summary = data.get("summary", {})
                            
                            # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Enabler ‡πÅ‡∏•‡∏∞ Scope
                            file_enabler = (summary.get("enabler") or "KM").upper()
                            scope = (summary.get("sub_criteria_id") or "ALL").upper()
                            
                            # üõ°Ô∏è ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á Enabler
                            if file_enabler not in user_allowed_enablers:
                                continue

                            # üéØ ‡∏Å‡∏£‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà User ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
                            if target_enabler and file_enabler != target_enabler:
                                continue

                            # --- üõ†Ô∏è Logic ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Level (‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏° JSON ‡∏à‡∏£‡∏¥‡∏á) ---
                            display_level = "-"
                            
                            if scope != "ALL":
                                # 1. ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å "Overall Maturity Level (Weighted)" (‡πÄ‡∏ä‡πà‡∏ô "L1")
                                raw_weighted_level = summary.get("Overall Maturity Level (Weighted)")
                                # 2. ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å "highest_pass_level" (‡πÄ‡∏ä‡πà‡∏ô 1)
                                raw_highest_level = summary.get("highest_pass_level")

                                if raw_weighted_level:
                                    display_level = str(raw_weighted_level)
                                elif raw_highest_level is not None:
                                    display_level = f"L{raw_highest_level}"
                                else:
                                    # Fallback: ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏à‡∏≤‡∏Å Score ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ü‡∏¥‡∏•‡∏î‡πå‡∏Ç‡πâ‡∏≤‡∏á‡∏ï‡πâ‡∏ô
                                    score_val = float(summary.get("Total Weighted Score Achieved") or 0)
                                    if score_val >= 0.8: display_level = "L5"
                                    elif score_val >= 0.6: display_level = "L4"
                                    elif score_val >= 0.4: display_level = "L3"
                                    elif score_val >= 0.2: display_level = "L2"
                                    elif score_val > 0: display_level = "L1"
                            
                            # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô (Score)
                            total_score = round(float(summary.get("Total Weighted Score Achieved") or 0.0), 2)
                            # --------------------------------------------------

                            history_list.append({
                                "record_id": data.get("record_id") or f.replace(".json", ""),
                                "date": parse_safe_date(summary.get("export_timestamp"), file_path),
                                "tenant": tenant,
                                "year": y,
                                "enabler": file_enabler,
                                "scope": scope,
                                "level": display_level,
                                "score": total_score,
                                "status": "COMPLETED"
                            })
                    except Exception as e:
                        logger.error(f"‚ùå Error parsing {f}: {e}")

    # 6. ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏à‡∏≤‡∏Å‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡∏Å‡πà‡∏≠‡∏ô
    return {"items": sorted(history_list, key=lambda x: x['date'], reverse=True)}


# ------------------------------------------------------------------
# 1. Start Assessment Endpoint
# ------------------------------------------------------------------
@assessment_router.post("/start")
async def start_assessment(
    request: StartAssessmentRequest, 
    background_tasks: BackgroundTasks, 
    current_user: UserMe = Depends(get_current_user)
):
    enabler_uc = request.enabler.upper()
    target_year = str(request.year if request.year else (current_user.year or DEFAULT_YEAR)).strip()
    target_sub = str(request.sub_criteria).strip().lower() if request.sub_criteria else "all"

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå
    check_user_permission(current_user, request.tenant, enabler_uc)

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Path ‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Chroma)
    vs_path = get_vectorstore_collection_path(request.tenant, target_year, "evidence", enabler_uc)
    if not os.path.exists(vs_path):
        raise HTTPException(status_code=400, detail=f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• {enabler_uc} ‡∏õ‡∏µ {target_year}")

    # 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á Record ID ‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á Database
    record_id = uuid.uuid4().hex[:12]
    
    db = SessionLocal()
    try:
        new_task = AssessmentTaskTable(
            record_id=record_id,
            user_id=current_user.id,
            tenant=request.tenant,
            year=target_year,
            enabler=enabler_uc,
            sub_criteria=target_sub,
            status="RUNNING",
            progress_percent=5,
            progress_message="‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Vector Store..."
        )
        db.add(new_task)
        db.commit()
    except Exception as e:
        logger.error(f"DB Error: {e}")
        raise HTTPException(status_code=500, detail="‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á Task ‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ")
    finally:
        db.close()

    # 4. ‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤ Background Task
    background_tasks.add_task(
        run_assessment_engine_task,
        record_id=record_id,
        tenant=request.tenant,
        year=target_year,
        enabler=enabler_uc,
        sub_id=target_sub,
        sequential=request.sequential_mode
    )

    return {"record_id": record_id, "status": "RUNNING"}


# ------------------------------------------------------------------
# 1. Start Assessment Endpoint
# ------------------------------------------------------------------
@assessment_router.post("/start")
async def start_assessment(
    request: StartAssessmentRequest, 
    background_tasks: BackgroundTasks, 
    current_user: UserMe = Depends(get_current_user)
):
    enabler_uc = request.enabler.upper()
    target_year = str(request.year if request.year else (current_user.year or DEFAULT_YEAR)).strip()
    target_sub = str(request.sub_criteria).strip().lower() if request.sub_criteria else "all"

    # 1. Permission & Data Integrity Check
    check_user_permission(current_user, request.tenant, enabler_uc)

    vs_path = get_vectorstore_collection_path(request.tenant, target_year, "evidence", enabler_uc)
    if not os.path.exists(vs_path):
        raise HTTPException(status_code=400, detail=f"Data Store ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {enabler_uc}/{target_year} ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á")

    # 2. Generate Traceable Record ID
    record_id = uuid.uuid4().hex[:12]
    
    # 3. Persistent Task Entry
    db = SessionLocal()
    try:
        new_task = AssessmentTaskTable(
            record_id=record_id,
            user_id=current_user.id,
            tenant=request.tenant,
            year=target_year,
            enabler=enabler_uc,
            sub_criteria=target_sub,
            status="RUNNING",
            progress_percent=5,
            progress_message="‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡∏¥‡∏ß‡∏á‡∏≤‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô..."
        )
        db.add(new_task)
        db.commit()
    except Exception as e:
        logger.error(f"‚ùå Initial DB Error: {e}")
        raise HTTPException(status_code=500, detail="‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏á‡∏≤‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÑ‡∏î‡πâ")
    finally:
        db.close()

    # 4. Delegate to Background Worker
    background_tasks.add_task(
        run_assessment_engine_task,
        record_id=record_id,
        tenant=request.tenant,
        year=target_year,
        enabler=enabler_uc,
        sub_id=target_sub,
        sequential=request.sequential_mode # True ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Mac
    )

    return {
        "record_id": record_id, 
        "status": "RUNNING", 
        "message": f"‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô {enabler_uc} ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß"
    }

# ------------------------------------------------------------------
# 2. Background Task Engine (Robust Implementation)
# ------------------------------------------------------------------
async def run_assessment_engine_task(
    record_id: str, tenant: str, year: str, enabler: str, sub_id: str, sequential: bool
):
    """
    Worker ‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£ ‡∏£‡∏±‡∏ô AI Engine ‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•
    ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÉ‡∏ô‡πÇ‡∏´‡∏°‡∏î Non-blocking ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ asyncio.to_thread ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö CPU-bound tasks
    """
    try:
        logger.info(f"‚öôÔ∏è [Task {record_id}] Processing Started...")
        
        # --- Step 1: Resource Hydration ---
        db_update_task_status(record_id, 10, "‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Vector Database ‡πÅ‡∏•‡∏∞‡πÇ‡∏´‡∏•‡∏î Mapping...")
        
        vsm = await asyncio.to_thread(
            load_all_vectorstores, tenant, year, None, EVIDENCE_DOC_TYPES, enabler
        )
        
        doc_map_raw = await asyncio.to_thread(
            load_doc_id_mapping, EVIDENCE_DOC_TYPES, tenant, year, enabler
        )
        # ‡∏õ‡∏£‡∏±‡∏ö Mapping ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Engine
        doc_map = {d_id: d.get("file_name", d_id) for d_id, d in doc_map_raw.items()}

        # --- Step 2: Engine & Model Setup ---
        db_update_task_status(record_id, 20, f"‡πÇ‡∏´‡∏•‡∏î AI Model ({DEFAULT_LLM_MODEL_NAME})...")
        
        llm = await asyncio.to_thread(
            create_llm_instance, model_name=DEFAULT_LLM_MODEL_NAME, temperature=0.0
        )
        
        config = AssessmentConfig(
            enabler=enabler, tenant=tenant, year=year, 
            force_sequential=sequential,
            export_path=None # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ default ‡∏ï‡∏≤‡∏°‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á folder
        )
        
        engine = SEAMPDCAEngine(
            config=config, 
            llm_instance=llm, 
            logger_instance=logger, 
            doc_type=EVIDENCE_DOC_TYPES, 
            vectorstore_manager=vsm, 
            document_map=doc_map
        )

        # --- Step 3: Core Assessment (The Heavy Part) ---
        db_update_task_status(record_id, 35, "AI ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô (RAG Assessment)...")
        
        # ‡∏£‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏´‡∏•‡∏±‡∏Å (‡∏£‡∏±‡∏ô‡∏¢‡∏≤‡∏ß‡∏ô‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î)
        result = await asyncio.to_thread(
            engine.run_assessment, 
            target_sub_id=sub_id, 
            export=True, 
            record_id=record_id,
            vectorstore_manager=vsm,
            sequential=sequential
        )

        # --- Step 4: Finalize & Persistence ---
        if isinstance(result, dict) and result.get("status") == "FAILED":
            error_msg = result.get("error_message", "AI Engine Error")
            db_update_task_status(record_id, 0, f"‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {error_msg}", status="FAILED")
        else:
            # ‡πÉ‡∏ä‡πâ db_finish_task ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏ß‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å JSON ‡∏Å‡πâ‡∏≠‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
            await asyncio.to_thread(db_finish_task, record_id, result)
            db_update_task_status(record_id, 100, "‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå", status="COMPLETED")
            logger.info(f"‚úÖ [Task {record_id}] Finished Successfully")
            
    except Exception as e:
        logger.error(f"üí• [Task {record_id}] Critical Failure: {str(e)}", exc_info=True)
        db_update_task_status(record_id, 0, f"‡∏£‡∏∞‡∏ö‡∏ö‡∏Ç‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏á: {str(e)}", status="FAILED")

def _find_assessment_file(search_id: str, current_user: UserMe) -> str:
    """
    [HYBRID SEARCH v2026.2] ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÅ‡∏ö‡∏ö 2 ‡∏ä‡∏±‡πâ‡∏ô
    1. Fast-Track: ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏à‡∏≤‡∏Å Database (AssessmentResultTable)
    2. Fallback: ‡∏™‡πÅ‡∏Å‡∏ô‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏ö‡∏ô Disk (‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏ü‡∏•‡πå‡∏ñ‡∏π‡∏Å‡∏¢‡πâ‡∏≤‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡∏£‡∏±‡∏ô‡∏à‡∏≤‡∏Å CLI)
    """
    
    norm_tenant = _n(current_user.tenant)
    norm_search = _n(search_id).lower()

    # --- ‡∏ä‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà 1: ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏à‡∏≤‡∏Å Database (Fastest) ---
    db = SessionLocal()
    try:
        # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏´‡∏≤ Record ‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö search_id (record_id)
        res_record = db.query(AssessmentResultTable).filter(
            AssessmentResultTable.record_id == search_id
        ).first()
        
        if res_record and res_record.full_result_json:
            # ‡∏ñ‡πâ‡∏≤‡πÉ‡∏ô DB ‡∏°‡∏µ path ‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ß‡πâ ‡πÉ‡∏´‡πâ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏¢‡∏±‡∏á‡∏≠‡∏¢‡∏π‡πà‡πÑ‡∏´‡∏°
            try:
                data = json.loads(res_record.full_result_json)
                db_path = data.get("export_path_used") or data.get("metadata", {}).get("full_path")
                if db_path and os.path.exists(db_path):
                    logger.info(f"‚ö° [Search] DB Hit! Found path: {db_path}")
                    return db_path
            except:
                pass # ‡∏ñ‡πâ‡∏≤ JSON ‡∏û‡∏±‡∏á‡πÉ‡∏´‡πâ‡πÑ‡∏õ‡∏™‡πÅ‡∏Å‡∏ô Disk ‡∏ï‡πà‡∏≠
    finally:
        db.close()

    # --- ‡∏ä‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà 2: Robust Disk Scanning (Fallback) ---
    # ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏ß‡πâ
    search_paths = [
        os.path.join(DATA_STORE_ROOT, norm_tenant, "exports"),
        os.path.join("data_store", norm_tenant, "exports"),
        "/app/data_store/{}/exports".format(norm_tenant) # Docker Path
    ]
    
    logger.info(f"üîç [Search] DB Miss. Scanning Disk for ID: {norm_search}...")

    for s_path in search_paths:
        if not os.path.exists(s_path):
            continue
            
        for root, _, files in os.walk(s_path):
            for f in files:
                # üü¢ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå JSON ‡πÅ‡∏•‡∏∞‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤ ID ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á ID ‡πÄ‡∏ï‡πá‡∏° ‡πÅ‡∏•‡∏∞ ID ‡πÅ‡∏ö‡∏ö‡∏¢‡πà‡∏≠ (Prefix matching)
                norm_filename = _n(f).lower()
                if norm_filename.endswith(".json") and norm_search in norm_filename:
                    # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ú‡∏¥‡∏î Tenant (Security Check)
                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÉ‡∏ô Path ‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠ Tenant ‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á
                    if norm_tenant.lower() in _n(root).lower() or "exports" in root:
                        found_path = os.path.join(root, f)
                        logger.info(f"‚úÖ [Search] Disk Scan Success: {found_path}")
                        return found_path
                    
    # --- ‡∏Å‡∏£‡∏ì‡∏µ‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡πÄ‡∏•‡∏¢ ---
    logger.error(f"‚ùå [Search] Total Failure for ID: {norm_search}")
    raise HTTPException(
        status_code=404, 
        detail=f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô (ID: {search_id}) ‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏•‡∏≠‡∏á‡∏£‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á"
    )

# ------------------------------------------------------------------
# 3. Task List API (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏´‡∏ô‡πâ‡∏≤ UI ‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏°‡∏≤‡πÇ‡∏ä‡∏ß‡πå)
# ------------------------------------------------------------------
@assessment_router.get("/tasks")
async def get_assessment_tasks(current_user: UserMe = Depends(get_current_user)):
    db = SessionLocal()
    try:
        # ‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡∏≠‡∏á Tenant ‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÉ‡∏´‡∏°‡πà‡∏™‡∏∏‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡∏Å‡πà‡∏≠‡∏ô
        tasks = db.query(AssessmentTaskTable).filter(
            AssessmentTaskTable.tenant == current_user.tenant
        ).order_by(AssessmentTaskTable.created_at.desc()).limit(20).all()
        
        return {"tasks": tasks}
    finally:
        db.close()


# ------------------------------------------------------------------
# 4. Download API (Full Revised)
# ------------------------------------------------------------------
@assessment_router.get("/download/{record_id}/{file_type}")
async def download_assessment_file(
    record_id: str,
    file_type: str,
    background_tasks: BackgroundTasks,
    current_user: UserMe = Depends(get_current_user)
):
    """
    API ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏ú‡∏•‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á (JSON ‡∏´‡∏£‡∏∑‡∏≠ Word)
    """
    logger.info(f"üì• Download request: record_id={record_id}, type={file_type} by {current_user.email}")

    # 1. ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö (JSON)
    json_path = _find_assessment_file(record_id, current_user)

    # 2. ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Permission ‡∏£‡∏∞‡∏î‡∏±‡∏ö Enabler ‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á
    try:
        with open(json_path, "r", encoding="utf-8") as f:
            raw_data = json.load(f)
    except Exception as e:
        logger.error(f"Error reading JSON: {e}")
        raise HTTPException(status_code=500, detail="‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ")

    # 3. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á Enabler (‡πÄ‡∏ä‡πà‡∏ô User PEA-KM ‡∏´‡πâ‡∏≤‡∏°‡πÇ‡∏´‡∏•‡∏î PEA-IT)
    enabler = (raw_data.get("summary", {}).get("enabler") or "KM").upper()
    check_user_permission(current_user, current_user.tenant, enabler)

    file_type = file_type.lower()

    # --- ‡∏Å‡∏£‡∏ì‡∏µ‡∏Ç‡∏≠‡πÑ‡∏ü‡∏•‡πå JSON ---
    if file_type == "json":
        return FileResponse(
            path=json_path,
            filename=f"SEAM_Result_{enabler}_{record_id}.json",
            media_type="application/json"
        )

    # --- ‡∏Å‡∏£‡∏ì‡∏µ‡∏Ç‡∏≠‡πÑ‡∏ü‡∏•‡πå Word (DOCX) ---
    elif file_type in ["word", "docx"]:
        logger.info(f"üìÑ Generating Word report for {record_id}...")

        # ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        ui_data = _transform_result_for_ui(raw_data)
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Document (‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏à‡∏≤‡∏Å‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡πÉ‡∏ô gen_report.py ‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô)
        try:
            doc = create_docx_report_similar_to_ui(ui_data)
        except ImportError:
            # Fallback ‡∏´‡∏≤‡∏Å‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ó‡∏≥‡∏ï‡∏±‡∏ß‡∏™‡∏£‡πâ‡∏≤‡∏á Report
            raise HTTPException(status_code=501, detail="‡∏£‡∏∞‡∏ö‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå Word ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á")

        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß (Temporary File)
        with tempfile.NamedTemporaryFile(delete=False, suffix=".docx") as tmp:
            doc.save(tmp.name)
            temp_path = tmp.name

        logger.info(f"‚úÖ Word report generated at: {temp_path}")

        # ‡πÉ‡∏ä‡πâ Background Task ‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏¥‡πâ‡∏á‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ User ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß
        background_tasks.add_task(os.remove, temp_path)

        return FileResponse(
            path=temp_path,
            filename=f"SEAM_Report_{enabler}_{record_id}.docx",
            media_type="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
        )

    else:
        raise HTTPException(status_code=400, detail="‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö json, word)")