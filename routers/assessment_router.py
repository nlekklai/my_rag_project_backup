# routers/assessment_router.py
import os
import uuid
import logging
import json
from datetime import datetime, timezone
from typing import Optional, Dict, Any

from fastapi import APIRouter, BackgroundTasks, HTTPException, Path
from fastapi.responses import JSONResponse, FileResponse
from pydantic import BaseModel, Field

# ------------------- Core & LLM -------------------
from core.seam_assessment import SEAMPDCAEngine, AssessmentConfig
from models.llm import create_llm_instance

from config.global_vars import LLM_MODEL_NAME

logger = logging.getLogger(__name__)

assessment_router = APIRouter(prefix="/api/assess", tags=["Assessment"])

# ------------------- Pydantic Models -------------------
class StartAssessmentRequest(BaseModel):
    enabler: str = Field(..., example="KM")
    sub_criteria_id: Optional[str] = Field(None, example="1.2")
    sequential: bool = Field(True, description="‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏õ‡∏¥‡∏î")

class AssessmentStatus(BaseModel):
    record_id: str
    enabler: str
    sub_criteria_id: str
    sequential: bool
    status: str
    started_at: str
    finished_at: Optional[str] = None
    overall_score: Optional[float] = None
    highest_level: Optional[int] = None
    export_path: Optional[str] = None
    message: str = "Assessment in progress..."

# ------------------- In-memory Store -------------------
ASSESSMENT_RECORDS: Dict[str, AssessmentStatus] = {}

# ------------------- Background Runner -------------------
async def _run_assessment_background(record_id: str, request: StartAssessmentRequest):
    record = ASSESSMENT_RECORDS[record_id]
    try:
        logger.info(f"Assessment STARTED ‚Üí {record_id} | {request.enabler} | Sequential: {request.sequential}")

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á config
        config = AssessmentConfig(
            enabler=request.enabler.upper(),
            target_level=5,
            mock_mode="none",
            force_sequential=False,
            model_name=LLM_MODEL_NAME,
            temperature=0.0
        )

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á engine
        engine = SEAMPDCAEngine(
            config=config,
            llm_instance=create_llm_instance(model_name=LLM_MODEL_NAME, temperature=0.0)
        )

        # ‡∏£‡∏±‡∏ô assessment
        
        # V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V V
        # üß™ HARDCODE TEST: ‡πÉ‡∏ä‡πâ "1.2" ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏î‡∏™‡∏≠‡∏ö Engine ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á (‡∏ï‡πâ‡∏≠‡∏á‡∏•‡∏ö‡∏≠‡∏≠‡∏Å‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÄ‡∏™‡∏£‡πá‡∏à)
        target_id_to_use = "1.2"
        # ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^
        
        # ‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á (‡πÉ‡∏´‡πâ‡∏•‡∏ö Hardcode ‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ‡πÇ‡∏Ñ‡πâ‡∏î‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Client ‡πÄ‡∏™‡∏£‡πá‡∏à)
        # target_id_to_use = (
        #     request.sub_criteria_id.strip() 
        #     if request.sub_criteria_id and request.sub_criteria_id.strip()
        #     else "all"
        # )

        result = engine.run_assessment(
            target_sub_id=target_id_to_use,
            export=True,
            sequential=request.sequential
        )

        # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï record
        export_path = result.get("export_path_used")
        if not export_path or not os.path.exists(export_path):
            raise Exception("Export file was not created")

        overall = result.get("Overall", {}) or {}
        record.status = "COMPLETED"
        record.finished_at = datetime.now(timezone.utc).isoformat()
        record.overall_score = overall.get("overall_maturity_score", 0.0)
        record.highest_level = overall.get("overall_maturity_level", 0)
        record.export_path = export_path
        record.message = "Assessment completed successfully"

        logger.info(f"Assessment COMPLETED ‚Üí {record_id}")

    except Exception as e:
        logger.exception(f"Assessment FAILED ‚Üí {record_id}")
        record.status = "FAILED"
        record.message = f"Error: {str(e)} ‡∏ä‡∏¥‡πâ‡∏ô‡∏™‡πà‡∏ß‡∏ô (sub_criteria_id) ‡∏ñ‡∏π‡∏Å Hardcode ‡πÑ‡∏ß‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏î‡∏™‡∏≠‡∏ö"

# ------------------- API Endpoints -------------------
@assessment_router.post("/start", response_model=AssessmentStatus)
async def start_assessment(request: StartAssessmentRequest, background_tasks: BackgroundTasks):
    llm = create_llm_instance(model_name=LLM_MODEL_NAME, temperature=0.0)
    if not llm:
        raise HTTPException(status_code=503, detail="LLM service unavailable")

    record_id = uuid.uuid4().hex[:12]
    os.makedirs("exports", exist_ok=True)

    # üü¢ FIX: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö String ‡∏ß‡πà‡∏≤‡∏á/None ‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Record)
    sub_id_for_record = (
        request.sub_criteria_id.strip() 
        if request.sub_criteria_id and request.sub_criteria_id.strip()
        else "all"
    )

    record = AssessmentStatus(
        record_id=record_id,
        enabler=request.enabler.upper(),
        sub_criteria_id=sub_id_for_record,
        sequential=request.sequential,
        status="RUNNING",
        started_at=datetime.now(timezone.utc).isoformat(),
        message="‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏Å‡∏ß‡πà‡∏≤ 300 ‡∏â‡∏ö‡∏±‡∏ö‡∏î‡πâ‡∏ß‡∏¢ AI..."
    )
    ASSESSMENT_RECORDS[record_id] = record

    background_tasks.add_task(_run_assessment_background, record_id, request)

    return record

@assessment_router.get("/status/{record_id}", response_model=AssessmentStatus)
async def get_status(record_id: str = Path(..., description="Record ID ‡∏à‡∏≤‡∏Å /start")):
    record = ASSESSMENT_RECORDS.get(record_id)
    if not record:
        raise HTTPException(status_code=404, detail="Record not found")
    return record

@assessment_router.get("/results/{record_id}")
async def get_results_json(record_id: str = Path(...)):
    record = ASSESSMENT_RECORDS.get(record_id)
    if not record:
        raise HTTPException(status_code=404, detail="Record not found")
    if record.status != "COMPLETED":
        raise HTTPException(status_code=425, detail=f"‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏™‡∏£‡πá‡∏à (‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞: {record.status})")
    if not record.export_path or not os.path.exists(record.export_path):
        raise HTTPException(status_code=404, detail="‡πÑ‡∏ü‡∏•‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏´‡∏≤‡∏¢")

    with open(record.export_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    return JSONResponse(content=data)

@assessment_router.get("/download/{record_id}")
async def download_result_file(record_id: str = Path(...)):
    record = ASSESSMENT_RECORDS.get(record_id)
    if not record or record.status != "COMPLETED" or not record.export_path:
        raise HTTPException(status_code=404, detail="Result not ready")
    return FileResponse(
        path=record.export_path,
        media_type="application/json",
        filename=os.path.basename(record.export_path)
    )

@assessment_router.get("/history")
async def get_assessment_history(enabler: Optional[str] = None):
    items = list(ASSESSMENT_RECORDS.values())
    if enabler:
        items = [i for i in items if i.enabler == enabler.upper()]
    return sorted(items, key=lambda x: x.started_at, reverse=True)