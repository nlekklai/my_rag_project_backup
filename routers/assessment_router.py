# -*- coding: utf-8 -*-
# routers/assessment_router.py
# Production Final Version - 20 ‡∏ò‡∏±‡∏ô‡∏ß‡∏≤‡∏Ñ‡∏° 2568 (Fixed parameter order + stable UUID + full assessment flow)

import os
import uuid
import json
import asyncio
import logging
from datetime import datetime
from typing import Optional, Dict, Any, Union, List

from fastapi import APIRouter, BackgroundTasks, HTTPException, Depends
from fastapi.responses import FileResponse
from pydantic import BaseModel

from routers.auth_router import UserMe, get_current_user
from utils.path_utils import _n, get_tenant_year_export_root, load_doc_id_mapping
from core.seam_assessment import SEAMPDCAEngine, AssessmentConfig
from core.vectorstore import load_all_vectorstores
from models.llm import create_llm_instance
from config.global_vars import EVIDENCE_DOC_TYPES, DEFAULT_LLM_MODEL_NAME, DEFAULT_YEAR


logger = logging.getLogger(__name__)
assessment_router = APIRouter(prefix="/api/assess", tags=["Assessment"])

ACTIVE_TASKS: Dict[str, Any] = {}

class StartAssessmentRequest(BaseModel):
    tenant: str
    year: Union[int, str]
    enabler: str
    sub_criteria: Optional[str] = "all"
    sequential_mode: bool = True

# ------------------- Permission Helper -------------------
def check_user_permission(user: UserMe, tenant: str, enabler: str):
    if _n(user.tenant) != _n(tenant):
        raise HTTPException(status_code=403, detail="Tenant mismatch")
    if user.enablers and enabler.upper() not in [e.upper() for e in user.enablers]:
        raise HTTPException(status_code=403, detail=f"Enabler '{enabler}' not allowed")

# ------------------- Helpers -------------------
def parse_safe_date(raw_date_str: Any, file_path: str) -> str:
    if raw_date_str and isinstance(raw_date_str, str):
        try:
            if "_" in raw_date_str:
                dt = datetime.strptime(raw_date_str, "%Y%m%d_%H%M%S")
                return dt.isoformat()
        except:
            pass
    try:
        return datetime.fromtimestamp(os.path.getmtime(file_path)).isoformat()
    except:
        return datetime.now().isoformat()

def clean_suggestion(raw_val: Any) -> str:
    if not raw_val:
        return "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°"
    if isinstance(raw_val, dict):
        return raw_val.get('description', str(raw_val))
    if isinstance(raw_val, str):
        raw_val = raw_val.strip()
        if raw_val.startswith('{'):
            try:
                data = json.loads(raw_val.replace("'", '"'))
                return data.get('description', raw_val)
            except:
                pass
    return raw_val

def _find_assessment_file(search_id: str, current_user: UserMe) -> str:
    export_root = get_tenant_year_export_root(current_user.tenant, current_user.year)
    norm_search = _n(search_id).lower()

    for root, _, files in os.walk(export_root):
        for f in files:
            if f.endswith(".json") and norm_search in _n(f).lower():
                return os.path.join(root, f)
    raise HTTPException(status_code=404, detail="‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô")

def _transform_result_for_ui(raw_data: Dict[str, Any], current_user: Any = None) -> Dict[str, Any]:
    """
    ‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå: ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö PDCA Matrix, Roadmap Stepper ‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏à‡∏∏‡∏î‡πÅ‡∏Ç‡πá‡∏á/‡∏≠‡πà‡∏≠‡∏ô
    ‡πÉ‡∏´‡πâ‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö UI ‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà
    """
    summary = raw_data.get("summary", {})
    sub_results = raw_data.get("sub_criteria_results", [])

    processed_sub_criteria = []
    radar_data = []
    strengths = []
    all_weaknesses = []

    # --- 1. ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤ Metrics ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô ---
    total_expected = int(summary.get("total_subcriteria") or 0)
    passed_count = int(summary.get("total_subcriteria_assessed") or len(sub_results))
    completion_rate = float(summary.get("percentage_achieved_run") or 0.0)
    overall_level = summary.get("Overall Maturity Level (Weighted)") or f"L{summary.get('highest_pass_level', 0)}"
    total_score = round(float(summary.get("Overall Maturity Score (Avg.)") or summary.get("Total Weighted Score Achieved") or 0.0), 2)
    enabler_name = (summary.get("enabler") or "N/A").upper()

    for res in sub_results:
        cid = res.get("sub_criteria_id", "N/A")
        cname = res.get("sub_criteria_name", f"‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏¢‡πà‡∏≠‡∏¢ {cid}")
        highest_pass = int(res.get("highest_full_level") or 0)
        raw_levels_list = res.get("raw_results_ref", [])

        # --- 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á PDCA Matrix (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö UI Badge Grid) ---
        pdca_matrix = []
        raw_levels_map = {item.get("level"): item for item in raw_levels_list}
        
        for lv in range(1, 6):
            lv_info = raw_levels_map.get(lv)
            if lv_info:
                pdca_matrix.append({
                    "level": lv,
                    "is_passed": lv_info.get("is_passed", False),
                    "pdca": lv_info.get("pdca_breakdown", {"P": 0, "D": 0, "C": 0, "A": 0}),
                    "reason": lv_info.get("reason", "‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÅ‡∏•‡πâ‡∏ß")
                })
            else:
                pdca_matrix.append({
                    "level": lv,
                    "is_passed": lv <= highest_pass,
                    "pdca": {"P": 1, "D": 1, "C": 1, "A": 1} if lv <= highest_pass else {"P": 0, "D": 0, "C": 0, "A": 0},
                    "reason": "‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô" if lv <= highest_pass else "‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏ñ‡∏∂‡∏á‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô"
                })

        # --- 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á Roadmap (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö UI Stepper) ---
        ui_roadmap = []
        raw_plans = res.get("action_plan") or []
        for p in raw_plans:
            ui_roadmap.append({
                "phase": p.get("phase", "‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô‡∏û‡∏±‡∏í‡∏ô‡∏≤"),
                "goal": p.get("goal", "‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏¢‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå"),
                "tasks": [
                    {
                        "level": str(act.get("failed_level", lv + 1)),
                        "recommendation": act.get("recommendation", ""),
                        "steps": [
                            {
                                "step": str(s.get("step") or s.get("step_number") or i+1),
                                "description": s.get("description", ""),
                                "responsible": s.get("responsible", "‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á")
                            } for i, s in enumerate(act.get("steps", []))
                        ]
                    } for act in p.get("actions", [])
                ]
            })

        # --- 4. ‡∏î‡∏∂‡∏á Sources/Evidence Link ---
        all_sources = []
        seen_docs = set()
        # ‡∏£‡∏ß‡∏°‡πÅ‡∏´‡∏•‡πà‡∏á‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å level
        for ref in raw_levels_list:
            for source in ref.get("temp_map_for_level", []):
                doc_id = f"{source.get('filename')}-{source.get('page_number')}"
                if doc_id not in seen_docs:
                    all_sources.append({
                        "filename": source.get("filename") or source.get("source"),
                        "page": str(source.get("page_number") or source.get("page", "1")),
                        "snippet": source.get("text", "")[:150]
                    })
                    seen_docs.add(doc_id)

        # --- 5. ‡∏™‡∏£‡∏∏‡∏õ‡∏à‡∏∏‡∏î‡πÅ‡∏Ç‡πá‡∏á/‡∏à‡∏∏‡∏î‡∏≠‡πà‡∏≠‡∏ô ‡∏£‡∏≤‡∏¢‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠ ---
        for lv_item in raw_levels_list:
            if lv_item.get("level", 0) >= 3 and lv_item.get("is_passed"):
                strengths.append(f"‡πÄ‡∏Å‡∏ì‡∏ë‡πå {cid}: ‡∏ö‡∏£‡∏£‡∏•‡∏∏‡∏£‡∏∞‡∏î‡∏±‡∏ö L{lv_item['level']} ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô")
            
        for plan in raw_plans:
            for act in plan.get("actions", []):
                all_weaknesses.append(f"L{act.get('failed_level')}: {act.get('recommendation')}")

        processed_sub_criteria.append({
            "code": cid,
            "name": cname,
            "level": f"L{highest_pass}",
            "score": float(res.get("weighted_score", 0.0)),
            "progress_percent": int((highest_pass / 5) * 100),
            "pdca_matrix": pdca_matrix, # ‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö UI
            "roadmap": ui_roadmap,      # ‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö UI
            "sources": all_sources[:5],
            "evidence": res.get("summary_thai", ""),
            "gap": res.get("gap_analysis", "") # ‡∏´‡∏£‡∏∑‡∏≠‡∏ü‡∏¥‡∏•‡∏î‡πå‡∏ó‡∏µ‡πà‡∏û‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
        })
        
        radar_data.append({"axis": cid, "value": highest_pass})

    return {
        "status": "COMPLETED",
        "record_id": raw_data.get("record_id", "unknown"),
        "tenant": str(summary.get("tenant", "N/A")).upper(),
        "year": str(summary.get("year", "2568")),
        "enabler": enabler_name,
        "level": overall_level,
        "score": total_score,
        "metrics": {
            "total_criteria": total_expected,
            "passed_criteria": passed_count,
            "completion_rate": int(completion_rate)
        },
        "radar_data": radar_data,
        "strengths": list(dict.fromkeys(strengths)) if strengths else ["‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°"],
        "weaknesses": list(dict.fromkeys(all_weaknesses)) if all_weaknesses else ["‡∏ö‡∏£‡∏£‡∏•‡∏∏‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô"],
        "sub_criteria": processed_sub_criteria
    }

# ------------------- API Endpoints -------------------
@assessment_router.get("/status/{record_id}")
async def get_assessment_status(record_id: str, current_user: UserMe = Depends(get_current_user)):
    if record_id in ACTIVE_TASKS:
        return ACTIVE_TASKS[record_id]

    file_path = _find_assessment_file(record_id, current_user)
    with open(file_path, "r", encoding="utf-8") as f:
        raw_data = json.load(f)

    enabler = (raw_data.get("summary", {}).get("enabler") or "KM").upper()
    check_user_permission(current_user, current_user.tenant, enabler)

    return _transform_result_for_ui(raw_data, current_user)

@assessment_router.get("/history")
async def get_assessment_history(tenant: str, year: Union[int, str], current_user: UserMe = Depends(get_current_user)):
    if _n(tenant) != _n(current_user.tenant):
        raise HTTPException(status_code=403, detail="Permission Denied")

    export_root = get_tenant_year_export_root(tenant, str(year))
    history_list = []

    if not os.path.exists(export_root):
        return {"items": []}

    for root, _, files in os.walk(export_root):
        for f in files:
            if f.lower().endswith(".json"):
                try:
                    file_path = os.path.join(root, f)
                    with open(file_path, "r", encoding="utf-8") as jf:
                        data = json.load(jf)
                        summary = data.get("summary", {})
                        enabler = (summary.get("enabler") or "KM").upper()
                        check_user_permission(current_user, tenant, enabler)

                        history_list.append({
                            "record_id": data.get("record_id") or summary.get("record_id") or f.rsplit('.', 1)[0],
                            "date": parse_safe_date(summary.get("export_timestamp"), file_path),
                            "tenant": tenant,
                            "year": str(year),
                            "enabler": enabler,
                            "scope": summary.get("sub_criteria_id", "ALL"),
                            "level": f"L{summary.get('highest_pass_level_overall', summary.get('highest_pass_level', 0))}",
                            "score": round(float(summary.get("Total Weighted Score Achieved", summary.get("achieved_weight", 0.0))), 2),
                            "status": "COMPLETED"
                        })
                except Exception as e:
                    logger.error(f"Error reading history file {f}: {e}")

    return {"items": sorted(history_list, key=lambda x: x['date'], reverse=True)}


@assessment_router.post("/start")
async def start_assessment(request: StartAssessmentRequest, background_tasks: BackgroundTasks, current_user: UserMe = Depends(get_current_user)):
    check_user_permission(current_user, request.tenant, request.enabler)

    # üü¢ ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏ì‡∏µ Year ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏´‡∏£‡∏∑‡∏≠‡∏™‡πà‡∏á‡∏°‡∏≤‡∏ß‡πà‡∏≤‡∏á‡πÜ
    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏™‡πà‡∏á‡∏°‡∏≤ ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏õ‡∏µ‡∏à‡∏≤‡∏Å Profile ‡∏Ç‡∏≠‡∏á User ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Default ‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏ö (‡πÄ‡∏ä‡πà‡∏ô 2568)
    raw_year = request.year
    target_year = str(raw_year).strip() if (raw_year and str(raw_year).strip()) else str(current_user.year or DEFAULT_YEAR)

    # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ sub_criteria (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°)
    target_sub = request.sub_criteria.strip() if (request.sub_criteria and request.sub_criteria.strip()) else "all"

    record_id = uuid.uuid4().hex[:12]
    ACTIVE_TASKS[record_id] = {
        "status": "RUNNING",
        "record_id": record_id,
        "tenant": request.tenant,
        "year": target_year, # ‡πÉ‡∏ä‡πâ‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡πâ‡∏ß
        "enabler": request.enabler.upper(),
        "progress_message": f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏õ‡∏µ {target_year}..."
    }

    background_tasks.add_task(
        run_assessment_engine_task,
        record_id,
        request.tenant,
        int(target_year), # ‡∏™‡πà‡∏á‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏ß‡∏£‡πå‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ
        request.enabler,
        target_sub,
        request.sequential_mode
    )

    return {"record_id": record_id, "status": "RUNNING"}

async def run_assessment_engine_task(record_id: str, tenant: str, year: int, enabler: str, sub_id: str, sequential: bool):
    try:
        # --- 1. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Resource (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°) ---
        vsm = await asyncio.to_thread(
            load_all_vectorstores,
            doc_types=EVIDENCE_DOC_TYPES,
            enabler_filter=enabler,
            tenant=tenant,
            year=str(year)
        )
        
        doc_map_raw = await asyncio.to_thread(
            load_doc_id_mapping, 
            EVIDENCE_DOC_TYPES, 
            tenant, 
            str(year), 
            enabler
        )
        doc_map = {d_id: d.get("file_name", d_id) for d_id, d in doc_map_raw.items()}

        llm = await asyncio.to_thread(create_llm_instance, model_name=DEFAULT_LLM_MODEL_NAME, temperature=0.0)
        
        config = AssessmentConfig(
            enabler=enabler, 
            tenant=tenant, 
            year=str(year),
            force_sequential=sequential
        )

        # üü¢ ‡πÅ‡∏Å‡πâ‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà 2: Initialize Engine (‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô CLI)
        engine = SEAMPDCAEngine(
            config=config,
            llm_instance=llm,
            logger_instance=logger,
            doc_type=EVIDENCE_DOC_TYPES,
            vectorstore_manager=vsm,
            document_map=doc_map
        )

        # üü¢ ‡πÅ‡∏Å‡πâ‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà 3: ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å run_assessment ‡πÅ‡∏ö‡∏ö‡∏£‡∏∞‡∏ö‡∏∏‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£ (Explicit) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏ß‡∏£‡πå
        # ‡πÉ‡∏ä‡πâ sub_id ‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà 1 (‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô 'all' ‡∏´‡∏£‡∏∑‡∏≠ '3.1')
        await asyncio.to_thread(
            engine.run_assessment, 
            target_sub_id=sub_id, 
            export=True, 
            vectorstore_manager=vsm, 
            sequential=sequential, 
            record_id=record_id,
            document_map=doc_map
        )

        if record_id in ACTIVE_TASKS:
            del ACTIVE_TASKS[record_id]
            
    except Exception as e:
        logger.error(f"‚ùå Engine Failed for Record {record_id}: {e}", exc_info=True)
        if record_id in ACTIVE_TASKS:
            ACTIVE_TASKS[record_id]["status"] = "FAILED"
            ACTIVE_TASKS[record_id]["error_message"] = str(e)

@assessment_router.get("/download/{record_id}/{file_type}")
async def download_assessment_file(record_id: str, file_type: str, current_user: UserMe = Depends(get_current_user)):
    file_path = _find_assessment_file(record_id, current_user)

    expected_ext = f".{file_type.lower()}"
    if file_type.lower() == "word":
        expected_ext = ".docx"

    if not file_path.endswith(expected_ext):
        raise HTTPException(status_code=404, detail="‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á")

    with open(file_path, "r", encoding="utf-8") as f:
        raw_data = json.load(f)
        enabler = (raw_data.get("summary", {}).get("enabler") or "KM").upper()
        check_user_permission(current_user, current_user.tenant, enabler)

    return FileResponse(path=file_path, filename=os.path.basename(file_path))