# -*- coding: utf-8 -*-
# routers/assessment_router.py
# Production Final Version - 2026 Optimized for DB Persistence & Professional Reporting

import os
import uuid
import json
import asyncio
import logging
import mimetypes
import tempfile
import pytz
from datetime import datetime
from typing import Optional, Dict, Any, Union, List

from fastapi import APIRouter, BackgroundTasks, HTTPException, Depends, Query
from fastapi.responses import FileResponse
from pydantic import BaseModel

# --- Docx Imports (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á Report) ---
from docx import Document
from docx.shared import Pt, RGBColor, Inches
from docx.enum.text import WD_ALIGN_PARAGRAPH
from docx.oxml.ns import qn

# --- Project Imports ---
from routers.auth_router import get_current_user, check_user_permission, UserMe
from utils.path_utils import (
    _n, 
    get_tenant_year_export_root, 
    load_doc_id_mapping, 
    get_document_file_path,
    get_vectorstore_collection_path,
    get_vectorstore_tenant_root_path
)

from core.seam_assessment import SEAMPDCAEngine, AssessmentConfig
from core.vectorstore import load_all_vectorstores
from models.llm import create_llm_instance
from config.global_vars import (
    EVIDENCE_DOC_TYPES, 
    DEFAULT_LLM_MODEL_NAME, 
    DEFAULT_YEAR, 
    DEFAULT_TENANT,
    DATA_STORE_ROOT
)

# üéØ Database Components (SQLite Persistence)
from database import (
    SessionLocal, 
    AssessmentTaskTable, 
    AssessmentResultTable,
    db_update_task_status,
    db_finish_task
)

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Logger ‡πÅ‡∏•‡∏∞ Router
logger = logging.getLogger(__name__)
assessment_router = APIRouter(prefix="/api/assess", tags=["Assessment"])

# --- Request Models ---
class StartAssessmentRequest(BaseModel):
    tenant: str
    year: Optional[Union[int, str]] = None
    enabler: str = "KM"
    sub_criteria: Optional[str] = "all"
    sequential_mode: bool = False

# ------------------------------------------------------------------
# [Helpers]
# ------------------------------------------------------------------
def parse_safe_date(raw_date_str: Any, file_path: str) -> str:
    """‡πÅ‡∏õ‡∏•‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏≤‡∏Å String ‡∏´‡∏£‡∏∑‡∏≠ File Metadata ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô ISO Format (Bangkok Time)"""
    tz = pytz.timezone('Asia/Bangkok')
    if raw_date_str and isinstance(raw_date_str, str):
        try:
            # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö format yyyymmdd_hhmmss
            if "_" in raw_date_str:
                dt = datetime.strptime(raw_date_str, "%Y%m%d_%H%M%S")
                return tz.localize(dt).isoformat()
        except: pass
    
    # Fallback: ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÑ‡∏ü‡∏•‡πå‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
    try:
        mtime = os.path.getmtime(file_path)
        dt = datetime.fromtimestamp(mtime, tz)
        return dt.isoformat()
    except:
        return datetime.now(tz).isoformat()


def safe_float(value):
    try:
        if isinstance(value, str):
            value = value.replace('%', '') # ‡∏•‡∏ö % ‡∏≠‡∏≠‡∏Å‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
        return float(value)
    except:
        return 0.0
    

@assessment_router.get("/evidence/{doc_type}/{document_uuid}")
async def serve_evidence_file(
    document_uuid: str,
    doc_type: str,
    tenant: str,
    year: str = None,
    enabler: str = None,
    current_user: UserMe = Depends(get_current_user)
):
    check_user_permission(current_user, tenant, enabler or "KM")

    file_info = get_document_file_path(
        document_uuid=document_uuid,
        tenant=tenant,
        year=year,
        enabler=enabler,
        doc_type_name=doc_type
    )

    if not file_info:
        raise HTTPException(status_code=404, detail="File not found")

    file_path = file_info["file_path"]
    
    # ‡∏î‡∏∂‡∏á‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•‡πÑ‡∏ü‡∏•‡πå
    ext = os.path.splitext(file_path)[1].lower()
    
    # üõ°Ô∏è Force MIME Type ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Mac/Safari
    mime_map = {
        ".pdf": "application/pdf",
        ".jpg": "image/jpeg",
        ".jpeg": "image/jpeg",
        ".png": "image/png",
    }
    
    mime_type = mime_map.get(ext) or mimetypes.guess_type(file_path)[0] or "application/octet-stream"

    # ‡∏™‡πà‡∏á FileResponse
    response = FileResponse(
        path=file_path,
        media_type=mime_type,
        content_disposition_type="inline"
    )

    # üí° ‡∏´‡∏±‡∏ß‡πÉ‡∏à‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Mac/Safari:
    # 1. ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ Browser ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å Path ‡∏ã‡∏∂‡πà‡∏á‡∏ö‡∏≤‡∏á‡∏ó‡∏µ‡∏°‡∏µ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÅ‡∏•‡πâ‡∏ß‡∏ó‡∏≥‡πÉ‡∏´‡πâ Header ‡πÄ‡∏û‡∏µ‡πâ‡∏¢‡∏ô
    # 2. ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö Header ‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
    response.headers["Content-Type"] = mime_type
    response.headers["Accept-Ranges"] = "bytes" 
    
    # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô PDF ‡∏ö‡∏ô Mac ‡πÉ‡∏´‡πâ‡πÄ‡∏ï‡∏¥‡∏° Cache-Control ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Viewer ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô
    if ext == ".pdf":
        response.headers["Cache-Control"] = "no-cache, no-store, must-revalidate"

    return response


@assessment_router.get("/view-document")
async def view_document(
    filename: str, 
    document_uuid: Optional[str] = None, 
    current_user: UserMe = Depends(get_current_user)
):
    """ Endpoint ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡∏¥‡∏î‡πÑ‡∏ü‡∏•‡πå PDF ‡πÇ‡∏î‡∏¢‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ó‡∏µ‡πà‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Ç‡∏∂‡πâ‡∏ô """
    
    file_path = None

    # 1. ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏´‡∏≤‡∏à‡∏≤‡∏Å UUID ‡∏Å‡πà‡∏≠‡∏ô (‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ 100% ‡πÅ‡∏°‡πâ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏∞‡∏ã‡πâ‡∏≥)
    if document_uuid:
        file_info = get_document_file_path(
            document_uuid=document_uuid,
            tenant=current_user.tenant,
            year=current_user.year,
            enabler="KM", # ‡∏´‡∏£‡∏∑‡∏≠‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å Query Param
            doc_type_name="evidence"
        )
        if file_info:
            file_path = file_info["file_path"]

    # 2. ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ UUID ‡πÉ‡∏´‡πâ‡∏´‡∏≤‡∏à‡∏≤‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå (‡πÉ‡∏ä‡πâ Logic ‡πÄ‡∏î‡∏¥‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì)
    if not file_path:
        # ‡πÉ‡∏ä‡πâ get_document_source_dir ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡πÉ‡∏ô path_utils ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß
        from utils.path_utils import get_document_source_dir, resolve_filepath_to_absolute
        
        base_path = get_document_source_dir(
            tenant=current_user.tenant,
            year=current_user.year,
            enabler="KM",
            doc_type="evidence"
        )
        file_path = resolve_filepath_to_absolute(os.path.join(base_path, filename))

    if not file_path or not os.path.exists(file_path):
        logger.error(f"‚ùå File not found: {file_path}")
        raise HTTPException(status_code=404, detail=f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ö‡∏ô‡πÄ‡∏ã‡∏¥‡∏£‡πå‡∏ü‡πÄ‡∏ß‡∏≠‡∏£‡πå")

    # ‡∏™‡πà‡∏á‡πÑ‡∏ü‡∏•‡πå PDF ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ
    return FileResponse(file_path, media_type="application/pdf")

import re
from typing import Dict, Any, List

def _transform_result_for_ui(raw_data: Dict[str, Any], current_user: Any = None) -> Dict[str, Any]:
    summary = raw_data.get("summary", {}) or {}
    sub_results = raw_data.get("sub_criteria_results", []) or []
    
    processed_sub_criteria = []
    radar_data = []

    for res in sub_results:
        # --- 1. Identity & Level Root ---
        # ‡∏õ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ã‡πâ‡∏≠‡∏ô nested ‡∏Ç‡∏≠‡∏á SE-AM
        level_root = res.get("level_details", {}).get("0", {})
        inner_level_details = level_root.get("level_details", {})
        highest_pass = int(level_root.get("highest_pass_level") or 0)
        
        level_details_ui = {}
        pdca_matrix_list = []
        all_unique_files = set()
        all_conf_scores = []
        
        # --- 2. Level Details & Evidence Recovery ---
        for lv_idx in range(1, 6):
            lv_key = str(lv_idx)
            lv_info = inner_level_details.get(lv_key) or {}
            reason_text = lv_info.get("reason", "")
            
            # üö© [IMPROVED]: Regex ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡πâ‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Ç‡∏∂‡πâ‡∏ô
            # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö [Source: file_name.pdf, Page: 1]
            found_files = re.findall(r"\[Source:\s*([^,\]]+)", reason_text)
            level_evidences = []
            
            lv_simulated_score = 0
            if found_files:
                for f in found_files:
                    f_name = f.strip()
                    all_unique_files.add(f_name)
                    level_evidences.append({"filename": f_name})
                
                # ‡∏à‡∏≥‡∏•‡∏≠‡∏á Confidence Score ‡∏ï‡∏≤‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏û‡∏ö‡πÉ‡∏ô Level ‡∏ô‡∏±‡πâ‡∏ô‡πÜ
                lv_simulated_score = min(75.0 + (len(found_files) * 5), 98.0) 
                all_conf_scores.append(lv_simulated_score)

            # PDCA Matrix
            pdca_raw = lv_info.get("pdca_breakdown", {}) or {}
            pdca_final = {p: (1 if float(pdca_raw.get(p, 0)) >= 0.5 else 0) for p in ["P", "D", "C", "A"]}
            
            # üö© [ADDED]: ‡∏ö‡∏£‡∏£‡∏à‡∏∏‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Ç‡πâ‡∏≤ Level Details
            level_details_ui[lv_key] = {
                "level": lv_idx,
                "confidence": lv_simulated_score if lv_simulated_score > 0 else 0,
                "is_passed": lv_idx <= highest_pass,
                "pdca_breakdown": pdca_final,
                "context_summary": reason_text,
                "evidences": level_evidences # ‡∏¢‡∏±‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡πÑ‡∏î‡πâ‡∏•‡∏á‡πÑ‡∏õ‡πÉ‡∏´‡πâ UI ‡∏ß‡∏ô Loop ‡πÇ‡∏ä‡∏ß‡πå
            }

            pdca_matrix_list.append({
                "level": lv_idx, 
                "is_passed": lv_idx <= highest_pass, 
                "pdca": pdca_final
            })

        # --- 3. Critical Gaps & Roadmap ---
        first_fail_lv = highest_pass + 1
        gap_info = inner_level_details.get(str(first_fail_lv), {})
        gap_text = f"L{first_fail_lv}: {gap_info.get('coaching_insight') or gap_info.get('reason') or '‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•'}" if first_fail_lv <= 5 else "‡∏ö‡∏£‡∏£‡∏•‡∏∏‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î"

        # --- 4. Final Assembly ---
        source_count = len(all_unique_files)
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Traceability Score ‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢
        trace_score_raw = (sum(all_conf_scores) / len(all_conf_scores)) if all_conf_scores else 0

        processed_sub_criteria.append({
            "code": res.get("sub_id", "1.1"),
            "name": level_root.get("sub_criteria_name", "‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô"),
            "level": f"L{highest_pass}",
            "score": round(float(level_root.get("weighted_score", 0)), 2),
            "summary_thai": f"‡∏ö‡∏£‡∏£‡∏•‡∏∏‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏£‡∏∞‡∏î‡∏±‡∏ö {highest_pass}",
            "gap": gap_text,
            "audit_confidence": {
                "source_count": source_count,
                "traceability_score": round(trace_score_raw / 100, 2),
                "consistency_check": trace_score_raw > 60
            },
            "pdca_matrix": pdca_matrix_list,
            "level_details": level_details_ui,
            "roadmap": level_root.get("action_plan", [])
        })
        radar_data.append({"axis": res.get("sub_id", "1.1"), "value": highest_pass})

    # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ max() Error
    max_lv = max([d['value'] for d in radar_data]) if radar_data else 0

    return {
        "status": "COMPLETED",
        "result_summary": {
            "level": f"L{max_lv}",
            "score": round(float(summary.get("total_weighted_score", 0)), 2),
            "full_score": summary.get("total_possible_weight", 4.0)
        },
        "radar_data": radar_data,
        "sub_criteria": processed_sub_criteria
    }

def set_thai_font(run, size=14, bold=False, color=None):
    run.font.name = 'TH Sarabun New'
    run._element.rPr.rFonts.set(qn('w:eastAsia'), 'TH Sarabun New')
    run.font.size = Pt(size)
    run.bold = bold
    if color:
        run.font.color.rgb = color

def create_docx_report_similar_to_ui(ui_data: dict) -> Document:
    doc = Document()
    
    # Header ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô
    header = doc.add_paragraph()
    header.alignment = WD_ALIGN_PARAGRAPH.CENTER
    run_h = header.add_run(f"‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô Maturity Audit\n")
    set_thai_font(run_h, size=20, bold=True, color=RGBColor(30, 58, 138))

    for item in ui_data.get('sub_criteria', []):
        # ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏¢‡πà‡∏≠‡∏¢
        title_p = doc.add_paragraph()
        run_title = title_p.add_run(f"‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏¢‡πà‡∏≠‡∏¢ {item.get('code', '')}: {item.get('name', '')}")
        set_thai_font(run_title, size=16, bold=True, color=RGBColor(30, 58, 138))

        # 1. Audit Confidence Table
        conf_table = doc.add_table(rows=1, cols=3)
        conf_table.style = 'Table Grid'
        conf = item.get('audit_confidence', {})
        metrics = [
            ("Independence", f"{conf.get('source_count', 0)} Files"),
            ("Traceability", f"{int(conf.get('traceability_score', 0) * 100)}%"),
            ("Consistency", "VERIFIED" if conf.get('consistency_check') else "CONFLICT")
        ]
        for i, (label, val) in enumerate(metrics):
            p = conf_table.rows[0].cells[i].paragraphs[0]
            p.alignment = WD_ALIGN_PARAGRAPH.CENTER
            set_thai_font(p.add_run(label), size=10, bold=True)
            set_thai_font(p.add_run(f"\n{val}"), size=14, bold=True)

        # 2. PDCA Capability Matrix (‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô UI)
        doc.add_paragraph()
        set_thai_font(doc.add_paragraph().add_run("üìä PDCA Capability Matrix:"), size=14, bold=True)
        pdca_table = doc.add_table(rows=2, cols=5)
        pdca_table.style = 'Table Grid'
        for i, lv_data in enumerate(item.get('pdca_matrix', [])):
            # ‡∏´‡∏±‡∏ß‡∏ï‡∏≤‡∏£‡∏≤‡∏á L1-L5
            set_thai_font(pdca_table.cell(0, i).paragraphs[0].add_run(f"L{lv_data['level']}"), bold=True)
            # ‡πÅ‡∏™‡∏î‡∏á P D C A
            p_cells = pdca_table.cell(1, i).paragraphs[0]
            p_cells.alignment = WD_ALIGN_PARAGRAPH.CENTER
            for char, val in lv_data['pdca'].items():
                run_char = p_cells.add_run(f" {char} ")
                # ‡∏™‡∏µ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß‡∏ñ‡πâ‡∏≤‡∏ú‡πà‡∏≤‡∏ô (1), ‡∏™‡∏µ‡πÅ‡∏î‡∏á‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô (0)
                color = RGBColor(22, 101, 52) if val == 1 else RGBColor(185, 28, 28)
                set_thai_font(run_char, size=11, bold=True, color=color)

        # 3. Strength & Gap
        doc.add_paragraph()
        s_title = doc.add_paragraph()
        set_thai_font(s_title.add_run("üí° AI Strength Summary:"), size=14, bold=True, color=RGBColor(22, 101, 52))
        set_thai_font(doc.add_paragraph(item.get('summary_thai', '-')).runs[0], size=13)

        g_title = doc.add_paragraph()
        set_thai_font(g_title.add_run("‚ö†Ô∏è Critical Gaps Found:"), size=14, bold=True, color=RGBColor(185, 28, 28))
        set_thai_font(doc.add_paragraph(item.get('gap', '-')).runs[0], size=13)

        # 4. Roadmap (‡∏î‡∏∂‡∏á‡∏Ñ‡∏£‡∏ö‡∏ó‡∏∏‡∏Å Phase/Action/Step)
        if item.get('roadmap'):
            doc.add_paragraph()
            set_thai_font(doc.add_paragraph().add_run("üõ† Strategic Improvement Roadmap:"), size=14, bold=True, color=RGBColor(30, 58, 138))
            for phase in item['roadmap']:
                p_run = doc.add_paragraph().add_run(f"Phase: {phase.get('phase')} - {phase.get('goal')}")
                set_thai_font(p_run, size=13, bold=True)
                for act in phase.get('actions', []):
                    a_run = doc.add_paragraph(style='List Bullet').add_run(f"‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ L{act.get('failed_level')}: {act.get('recommendation')}")
                    set_thai_font(a_run, size=12, bold=True)
                    for step in act.get('steps', []):
                        step_txt = f"{step.get('description')} (‡∏£‡∏±‡∏ö‡∏ú‡∏¥‡∏î‡∏ä‡∏≠‡∏ö: {step.get('responsible')})"
                        set_thai_font(doc.add_paragraph(style='List Bullet 2').add_run(step_txt), size=11)

        doc.add_page_break()
    return doc

# ==================== API ENDPOINT: GET Status / Get Data ====================
@assessment_router.get("/status/{record_id}")
async def get_assessment_status(
    record_id: str, 
    current_user: UserMe = Depends(get_current_user)
):
    """
    [v2026.6.19 ‚Äî Final Status + Robust Polling & Fallback]
    - Polling PROGRESS ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô (progress %, message, estimated_time)
    - Fallback ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡πÑ‡∏ü‡∏•‡πå ‚Üí ‡∏™‡πà‡∏á "NOT_FOUND" + suggestion
    - ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå check ‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢ + fallback tenant/enabler
    - Error handling ‡πÅ‡∏¢‡∏Å‡∏Å‡∏£‡∏ì‡∏µ + log ‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
    """
    # 1. ‡πÄ‡∏ä‡πá‡∏Ñ‡πÉ‡∏ô Memory ‡∏Å‡πà‡∏≠‡∏ô (Polling ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏£‡∏±‡∏ô)
    active_tasks = globals().get("ACTIVE_TASKS", {})
    if record_id in active_tasks:
        task = active_tasks[record_id]
        progress = task.get("progress", 0)
        message = task.get("message", "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•...")
        estimated_remaining = task.get("estimated_remaining_seconds", None)

        return {
            "status": "PROCESSING",
            "record_id": record_id,
            "progress": progress,
            "message": message,
            "estimated_remaining": estimated_remaining,
            "started_at": task.get("started_at"),
            "updated_at": datetime.now().isoformat()
        }

    # 2. ‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß‡∏ö‡∏ô Disk
    file_path = _find_assessment_file(record_id, current_user)
    
    if not file_path or not os.path.exists(file_path):
        logger.warning(f"[Status] File not found for record_id: {record_id}")
        return {
            "status": "NOT_FOUND",
            "record_id": record_id,
            "message": "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ô‡∏µ‡πâ ‡∏≠‡∏≤‡∏à‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏ñ‡∏π‡∏Å‡∏•‡∏ö ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏≠‡∏™‡∏±‡∏Å‡∏Ñ‡∏£‡∏π‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÉ‡∏´‡∏°‡πà",
            "suggestion": "‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÉ‡∏ô 1-2 ‡∏ô‡∏≤‡∏ó‡∏µ ‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡∏ú‡∏π‡πâ‡∏î‡∏π‡πÅ‡∏•‡∏£‡∏∞‡∏ö‡∏ö"
        }

    try:
        # 3. ‡∏≠‡πà‡∏≤‡∏ô JSON ‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
        with open(file_path, "r", encoding="utf-8") as f:
            raw_data = json.load(f)

        # 4. ‡∏î‡∏∂‡∏á Metadata + fallback tenant/enabler
        summary = raw_data.get("summary", {}) or raw_data.get("metadata", {}) or {}
        file_enabler = (summary.get("enabler") or "KM").upper()
        file_tenant = summary.get("tenant") or current_user.tenant or "unknown"

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå (tenant + enabler)
        try:
            check_user_permission(current_user, file_tenant, file_enabler)
        except Exception as perm_err:
            logger.warning(f"[Status] Permission denied for {record_id}: {perm_err}")
            raise HTTPException(status_code=403, detail="‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ô‡∏µ‡πâ")

        # 5. Transform ‡πÉ‡∏´‡πâ UI ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ
        ui_result = _transform_result_for_ui(raw_data, current_user)
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏° status + metadata ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
        ui_result["status"] = "COMPLETED"
        ui_result["record_id"] = record_id
        ui_result["export_path"] = file_path
        ui_result["exported_at"] = summary.get("export_at") or datetime.now().isoformat()

        logger.info(f"üöÄ [Status] Returning COMPLETED for {record_id} | Enabler: {file_enabler} | Tenant: {file_tenant}")
        return ui_result

    except json.JSONDecodeError:
        logger.error(f"üí• [Status] Invalid JSON for {record_id} at {file_path}")
        raise HTTPException(status_code=500, detail="‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢ ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏î‡πâ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡∏ú‡∏π‡πâ‡∏î‡∏π‡πÅ‡∏•‡∏£‡∏∞‡∏ö‡∏ö")

    except HTTPException as he:
        raise he  # ‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠ permission error

    except Exception as e:
        logger.error(f"üí• [Status] Error processing {record_id}: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail="‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡∏ú‡∏π‡πâ‡∏î‡∏π‡πÅ‡∏•‡∏£‡∏∞‡∏ö‡∏ö"
        )

@assessment_router.get("/history")
async def get_assessment_history(
    tenant: str, 
    year: Optional[str] = Query(None),
    enabler: Optional[str] = Query(None),
    current_user: UserMe = Depends(get_current_user)
):
    """
    [v2026.PDCA.COMPAT] - ‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Hybrid Version
    - ‡∏î‡∏∂‡∏á Record ID ‡∏à‡∏≤‡∏Å Root, Metadata ‡∏´‡∏£‡∏∑‡∏≠ Filename (‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ 404)
    - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Key ‡∏ó‡∏±‡πâ‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡πá‡∏Å (snake_case) ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡πÉ‡∏´‡∏ç‡πà (Title Case)
    - ‡∏£‡∏∞‡∏ö‡∏ö Date Fallback ‡∏ó‡∏µ‡πà‡πÅ‡∏Ç‡πá‡∏á‡πÅ‡∏Å‡∏£‡πà‡∏á
    """
    check_user_permission(current_user, tenant)
    history_list = []
    from config.global_vars import DATA_STORE_ROOT
    from datetime import datetime
    
    norm_tenant = _n(tenant)
    # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô‡∏´‡∏•‡∏≤‡∏¢ Path ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏¢‡πâ‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå
    search_roots = [
        os.path.join(DATA_STORE_ROOT, norm_tenant, "exports"),
        os.path.join("data_store", norm_tenant, "exports")
    ]
    
    tenant_export_root = next((p for p in search_roots if os.path.exists(p)), None)
    if not tenant_export_root:
        return {"items": [], "total_found": 0, "message": "No export data found"}

    user_allowed_enablers = [e.upper() for e in current_user.enablers]
    target_enabler = enabler.upper() if enabler else None

    if not year or str(year).lower() == "all":
        search_years = [d for d in os.listdir(tenant_export_root) if d.isdigit()]
    else:
        search_years = [str(year)]

    for y in search_years:
        year_path = os.path.join(tenant_export_root, y)
        if not os.path.exists(year_path): continue

        for root, _, files in os.walk(year_path):
            for f in files:
                if not f.lower().endswith(".json"): continue
                file_path = os.path.join(root, f)
                
                try:
                    with open(file_path, "r", encoding="utf-8") as jf:
                        data = json.load(jf)

                    summary = data.get("summary") or {}
                    metadata = data.get("metadata") or {}

                    # 1. üõ°Ô∏è EXTRA SAFE ID: ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Search DB Miss / 404
                    # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏´‡∏≤‡∏à‡∏≤‡∏Å Root -> Metadata -> Summary -> ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå
                    record_id = (
                        data.get("record_id") or 
                        metadata.get("record_id") or 
                        summary.get("record_id")
                    )
                    if not record_id:
                        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏£‡∏¥‡∏á‡πÜ ‡πÉ‡∏´‡πâ‡πÅ‡∏Å‡∏∞‡∏à‡∏≤‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå (Pattern: assessment_ENABLER_ID_...)
                        parts = f.replace(".json", "").split("_")
                        record_id = parts[2] if len(parts) >= 3 else f.replace(".json", "")

                    # 2. ENABLER & SCOPE
                    file_enabler = (metadata.get("enabler") or summary.get("enabler") or data.get("enabler") or "KM").upper()
                    scope = str(metadata.get("sub_id") or summary.get("sub_criteria_id") or data.get("sub_criteria_id") or "ALL").strip().upper()

                    if file_enabler not in user_allowed_enablers: continue
                    if target_enabler and file_enabler != target_enabler: continue

                    # 3. LEVEL LOGIC (Fallback ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏ó‡∏∏‡∏Å‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô)
                    display_level = "N/A"
                    raw_lvl = summary.get("highest_pass_level") or summary.get("Overall Maturity Level (Weighted)") or summary.get("overall_level_label")
                    
                    if raw_lvl is not None:
                        l_str = str(raw_lvl).strip().upper()
                        display_level = l_str if l_str.startswith("L") else f"L{l_str}"
                    else:
                        # Fallback ‡∏à‡∏≤‡∏Å‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô
                        score_val = safe_float(summary.get("total_weighted_score") or summary.get("Total Weighted Score Achieved"))
                        if score_val >= 0.8: display_level = "L5"
                        elif score_val >= 0.6: display_level = "L4"
                        elif score_val >= 0.4: display_level = "L3"
                        elif score_val >= 0.2: display_level = "L2"
                        elif score_val > 0: display_level = "L1"
                        else: display_level = "L0"

                    # 4. SCORE LOGIC
                    total_score = round(safe_float(
                        summary.get("total_weighted_score") or 
                        summary.get("Total Weighted Score Achieved") or 
                        summary.get("achieved_weight") or 0.0
                    ), 2)

                    # 5. DATE PARSING (Safe multi-field)
                    date_candidates = [
                        metadata.get("export_at"),
                        summary.get("export_timestamp"),
                        summary.get("assessed_at"),
                        summary.get("timestamp")
                    ]
                    date_str, parsed_dt = "N/A", None
                    for cand in date_candidates:
                        if cand:
                            try:
                                # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á ISO format ‡πÅ‡∏•‡∏∞ Custom format
                                if "_" in str(cand): # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö 20260115_233148
                                    parsed_dt = datetime.strptime(str(cand), "%Y%m%d_%H%M%S")
                                else:
                                    parsed_dt = datetime.fromisoformat(str(cand).replace('Z', '+00:00'))
                                date_str = parsed_dt.isoformat()
                                break
                            except: continue

                    if not parsed_dt: # Last resort
                        mtime = os.path.getmtime(file_path)
                        parsed_dt = datetime.fromtimestamp(mtime)
                        date_str = parsed_dt.isoformat()

                    history_list.append({
                        "record_id": record_id,
                        "date": date_str,
                        "date_dt": parsed_dt,
                        "tenant": tenant,
                        "year": y,
                        "enabler": file_enabler,
                        "scope": scope,
                        "level": display_level,
                        "score": total_score,
                        "status": "COMPLETED"
                    })

                except Exception as e:
                    logger.error(f"‚ùå Skip corrupted file {f}: {e}")
                    continue

    # 6. Sort & Cleanup
    sorted_history = sorted(history_list, key=lambda x: x['date_dt'] or datetime.min, reverse=True)
    for item in sorted_history: item.pop('date_dt', None)

    return {
        "items": sorted_history,
        "total_found": len(history_list),
        "displayed": len(sorted_history)
    }

# ------------------------------------------------------------------
# 1. Start Assessment Endpoint
# ------------------------------------------------------------------
@assessment_router.post("/start")
async def start_assessment(
    request: StartAssessmentRequest, 
    background_tasks: BackgroundTasks, 
    current_user: UserMe = Depends(get_current_user)
):
    """
    [FINAL v2026.6.20 ‚Äî Start Assessment + Friendly Response]
    - Permission + Data Integrity Check
    - Persistent Task Entry (DB)
    - Background Worker Delegation
    - Response ‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏¥‡∏ï‡∏£ + estimated_time
    """
    enabler_uc = request.enabler.upper()
    target_year = str(request.year if request.year else (current_user.year or DEFAULT_YEAR)).strip()
    target_sub = str(request.sub_criteria).strip().lower() if request.sub_criteria else "all"

    # 1. Permission & Data Integrity Check
    check_user_permission(current_user, request.tenant, enabler_uc)

    vs_path = get_vectorstore_collection_path(request.tenant, target_year, "evidence", enabler_uc)
    if not os.path.exists(vs_path):
        raise HTTPException(status_code=400, detail=f"Data Store ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {enabler_uc}/{target_year} ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á")

    # 2. Generate Traceable Record ID
    record_id = uuid.uuid4().hex[:12]
    
    # 3. Persistent Task Entry
    db = SessionLocal()
    try:
        new_task = AssessmentTaskTable(
            record_id=record_id,
            user_id=current_user.id,
            tenant=request.tenant,
            year=target_year,
            enabler=enabler_uc,
            sub_criteria=target_sub,
            status="QUEUED",
            progress_percent=5,
            progress_message="‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡∏¥‡∏ß‡∏á‡∏≤‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô..."
        )
        db.add(new_task)
        db.commit()
        db.refresh(new_task)
    except Exception as e:
        logger.error(f"‚ùå Initial DB Error: {e}")
        db.rollback()
        raise HTTPException(status_code=500, detail="‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏á‡∏≤‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÑ‡∏î‡πâ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà")
    finally:
        db.close()

    # 4. Delegate to Background Worker
    background_tasks.add_task(
        run_assessment_engine_task,
        record_id=record_id,
        tenant=request.tenant,
        year=target_year,
        enabler=enabler_uc,
        sub_id=target_sub,
        sequential=request.sequential_mode  # True ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Mac (sequential)
    )

    return {
        "record_id": record_id,
        "status": "QUEUED",
        "message": f"‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô {enabler_uc} ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß (‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡∏¥‡∏ß‡∏á‡∏≤‡∏ô)",
        "estimated_time": "20-40 ‡∏ô‡∏≤‡∏ó‡∏µ (‡∏Ç‡∏∂‡πâ‡∏ô‡∏Å‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡πÇ‡∏´‡∏°‡∏î sequential)",
        "poll_url": f"/api/assess/status/{record_id}",
        "poll_interval_seconds": 15
    }

# ------------------------------------------------------------------
# 2. Background Task Engine (Robust Implementation)
# ------------------------------------------------------------------
async def run_assessment_engine_task(
    record_id: str, tenant: str, year: str, enabler: str, sub_id: str, sequential: bool
):
    """
    [v2026.6.20 ‚Äî Robust Background Worker + Progress Update]
    - Update progress ‡∏ó‡∏∏‡∏Å‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
    - Use asyncio.to_thread ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö CPU-bound
    - Error handling + DB update ‡πÄ‡∏°‡∏∑‡πà‡∏≠ fail
    """
    try:
        logger.info(f"‚öôÔ∏è [Task {record_id}] Processing Started...")

        # Step 1: Resource Hydration
        db_update_task_status(record_id, 10, "‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Vector Database ‡πÅ‡∏•‡∏∞‡πÇ‡∏´‡∏•‡∏î Mapping...")
        
        vsm = await asyncio.to_thread(
            load_all_vectorstores, tenant, year, None, EVIDENCE_DOC_TYPES, enabler
        )
        
        doc_map_raw = await asyncio.to_thread(
            load_doc_id_mapping, EVIDENCE_DOC_TYPES, tenant, year, enabler
        )
        doc_map = {d_id: d.get("file_name", d_id) for d_id, d in doc_map_raw.items()}

        # Step 2: Engine & Model Setup
        db_update_task_status(record_id, 20, f"‡πÇ‡∏´‡∏•‡∏î AI Model ({DEFAULT_LLM_MODEL_NAME})...")
        
        llm = await asyncio.to_thread(
            create_llm_instance, model_name=DEFAULT_LLM_MODEL_NAME, temperature=0.0
        )
        
        config = AssessmentConfig(
            enabler=enabler, tenant=tenant, year=year, 
            force_sequential=sequential,
            export_path=None
        )
        
        engine = SEAMPDCAEngine(
            config=config, 
            llm_instance=llm, 
            logger_instance=logger, 
            doc_type=EVIDENCE_DOC_TYPES, 
            vectorstore_manager=vsm, 
            document_map=doc_map
        )

        # Step 3: Core Assessment
        db_update_task_status(record_id, 35, "AI ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô (RAG Assessment)...")
        
        result = await asyncio.to_thread(
            engine.run_assessment, 
            target_sub_id=sub_id, 
            export=True, 
            record_id=record_id,
            vectorstore_manager=vsm,
            sequential=sequential
        )

        # Step 4: Finalize
        if isinstance(result, dict) and result.get("status") == "FAILED":
            error_msg = result.get("error_message", "AI Engine Error")
            db_update_task_status(record_id, 0, f"‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {error_msg}", status="FAILED")
        else:
            await asyncio.to_thread(db_finish_task, record_id, result)
            db_update_task_status(record_id, 100, "‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå", status="COMPLETED")
            logger.info(f"‚úÖ [Task {record_id}] Finished Successfully")
            
    except Exception as e:
        logger.error(f"üí• [Task {record_id}] Critical Failure: {str(e)}", exc_info=True)
        db_update_task_status(record_id, 0, f"‡∏£‡∏∞‡∏ö‡∏ö‡∏Ç‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏á: {str(e)}", status="FAILED")


def _find_assessment_file(search_id: str, current_user: UserMe) -> str:
    """
    [HYBRID SEARCH v2026.2 ‚Äî Final Robust]
    - ‡∏ä‡∏±‡πâ‡∏ô 1: DB (fast)
    - ‡∏ä‡∏±‡πâ‡∏ô 2: Disk scan (fallback) + tenant check
    """
    norm_tenant = _n(current_user.tenant)
    norm_search = _n(search_id).lower()

    # ‡∏ä‡∏±‡πâ‡∏ô 1: DB Hit
    db = SessionLocal()
    try:
        res_record = db.query(AssessmentResultTable).filter(
            AssessmentResultTable.record_id == search_id
        ).first()
        
        if res_record and res_record.full_result_json:
            try:
                data = json.loads(res_record.full_result_json)
                db_path = data.get("export_path_used") or data.get("metadata", {}).get("full_path")
                if db_path and os.path.exists(db_path):
                    logger.info(f"‚ö° [Search] DB Hit! Found: {db_path}")
                    return db_path
            except:
                pass
    finally:
        db.close()

    # ‡∏ä‡∏±‡πâ‡∏ô 2: Disk Scan
    search_paths = [
        os.path.join(DATA_STORE_ROOT, norm_tenant, "exports"),
        os.path.join("data_store", norm_tenant, "exports"),
        "/app/data_store/{}/exports".format(norm_tenant)
    ]
    
    logger.info(f"üîç [Search] DB Miss. Scanning Disk for ID: {norm_search}...")

    for s_path in search_paths:
        if not os.path.exists(s_path):
            continue
            
        for root, _, files in os.walk(s_path):
            for f in files:
                norm_filename = _n(f).lower()
                if norm_filename.endswith(".json") and norm_search in norm_filename:
                    if norm_tenant.lower() in _n(root).lower() or "exports" in root:
                        found_path = os.path.join(root, f)
                        logger.info(f"‚úÖ [Search] Disk Scan Success: {found_path}")
                        return found_path
                    
    logger.error(f"‚ùå [Search] Total Failure for ID: {norm_search}")
    raise HTTPException(
        status_code=404, 
        detail=f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô (ID: {search_id}) ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏≠‡∏™‡∏±‡∏Å‡∏Ñ‡∏£‡∏π‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÉ‡∏´‡∏°‡πà"
    )

# ------------------------------------------------------------------
# 3. Task List API (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏´‡∏ô‡πâ‡∏≤ UI ‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏°‡∏≤‡πÇ‡∏ä‡∏ß‡πå)
# ------------------------------------------------------------------
@assessment_router.get("/tasks")
async def get_assessment_tasks(current_user: UserMe = Depends(get_current_user)):
    db = SessionLocal()
    try:
        # ‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡∏≠‡∏á Tenant ‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÉ‡∏´‡∏°‡πà‡∏™‡∏∏‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡∏Å‡πà‡∏≠‡∏ô
        tasks = db.query(AssessmentTaskTable).filter(
            AssessmentTaskTable.tenant == current_user.tenant
        ).order_by(AssessmentTaskTable.created_at.desc()).limit(20).all()
        
        return {"tasks": tasks}
    finally:
        db.close()


# ------------------------------------------------------------------
# 4. Download API (Full Revised)
# ------------------------------------------------------------------
@assessment_router.get("/download/{record_id}/{file_type}")
async def download_assessment_file(
    record_id: str,
    file_type: str,
    background_tasks: BackgroundTasks,
    current_user: UserMe = Depends(get_current_user)
):
    """
    API ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏ú‡∏•‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á (JSON ‡∏´‡∏£‡∏∑‡∏≠ Word)
    """
    logger.info(f"üì• Download request: record_id={record_id}, type={file_type} by {current_user.email}")

    # 1. ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö (JSON)
    json_path = _find_assessment_file(record_id, current_user)

    # 2. ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Permission ‡∏£‡∏∞‡∏î‡∏±‡∏ö Enabler ‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á
    try:
        with open(json_path, "r", encoding="utf-8") as f:
            raw_data = json.load(f)
    except Exception as e:
        logger.error(f"Error reading JSON: {e}")
        raise HTTPException(status_code=500, detail="‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ")

    # 3. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á Enabler (‡πÄ‡∏ä‡πà‡∏ô User PEA-KM ‡∏´‡πâ‡∏≤‡∏°‡πÇ‡∏´‡∏•‡∏î PEA-IT)
    enabler = (raw_data.get("summary", {}).get("enabler") or "KM").upper()
    check_user_permission(current_user, current_user.tenant, enabler)

    file_type = file_type.lower()

    # --- ‡∏Å‡∏£‡∏ì‡∏µ‡∏Ç‡∏≠‡πÑ‡∏ü‡∏•‡πå JSON ---
    if file_type == "json":
        return FileResponse(
            path=json_path,
            filename=f"SEAM_Result_{enabler}_{record_id}.json",
            media_type="application/json"
        )

    # --- ‡∏Å‡∏£‡∏ì‡∏µ‡∏Ç‡∏≠‡πÑ‡∏ü‡∏•‡πå Word (DOCX) ---
    elif file_type in ["word", "docx"]:
        logger.info(f"üìÑ Generating Word report for {record_id}...")

        # ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        ui_data = _transform_result_for_ui(raw_data)
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Document (‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏à‡∏≤‡∏Å‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡πÉ‡∏ô gen_report.py ‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô)
        try:
            doc = create_docx_report_similar_to_ui(ui_data)
        except ImportError:
            # Fallback ‡∏´‡∏≤‡∏Å‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ó‡∏≥‡∏ï‡∏±‡∏ß‡∏™‡∏£‡πâ‡∏≤‡∏á Report
            raise HTTPException(status_code=501, detail="‡∏£‡∏∞‡∏ö‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå Word ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á")

        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß (Temporary File)
        with tempfile.NamedTemporaryFile(delete=False, suffix=".docx") as tmp:
            doc.save(tmp.name)
            temp_path = tmp.name

        logger.info(f"‚úÖ Word report generated at: {temp_path}")

        # ‡πÉ‡∏ä‡πâ Background Task ‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏¥‡πâ‡∏á‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ User ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß
        background_tasks.add_task(os.remove, temp_path)

        return FileResponse(
            path=temp_path,
            filename=f"SEAM_Report_{enabler}_{record_id}.docx",
            media_type="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
        )

    else:
        raise HTTPException(status_code=400, detail="‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö json, word)")