# utils/path_utils.py
# Production Final Version ‚Äì 11 ‡∏ò.‡∏Ñ. 2568 (Path Matching Fixes)

"""
üìå PROJECT FILE STRUCTURE & MAPPING LOGIC (Updated: 2026)
-------------------------------------------------------
‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏ö‡πà‡∏á‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô 2 ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏´‡∏•‡∏±‡∏Å ‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡∏≠‡∏á doc_type:

1. GLOBAL DOCUMENTS (document, faq, seam)
   - ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á: data_store/{tenant}/data/{doc_type}/{filename}
   - ‡∏Å‡∏≤‡∏£ Mapping: ‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà root ‡∏Ç‡∏≠‡∏á mapping folder ‡πÄ‡∏™‡∏°‡∏≠
   - ‡πÑ‡∏ü‡∏•‡πå JSON: {tenant}_{doc_type}_doc_id_mapping.json 
   - ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ year ‡πÅ‡∏•‡∏∞ enabler ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå

2. YEARLY EVIDENCE (evidence)
   - ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á: data_store/{tenant}/data/evidence/{year}/{enabler}/{filename}
   - ‡∏Å‡∏≤‡∏£ Mapping: ‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏°‡∏õ‡∏µ‡πÅ‡∏•‡∏∞‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
   - ‡πÑ‡∏ü‡∏•‡πå JSON: mapping/{year}/{tenant}_{year}_{enabler}_doc_id_mapping.json
   - ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ year ‡πÅ‡∏•‡∏∞ enabler ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏∏‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á

‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ä‡πâ‡∏£‡∏∞‡∏ö‡∏ö Fuzzy Scan (NFKC Normalization) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≤‡∏á‡∏Ç‡∏≠‡∏á Case-sensitive ‡∏ö‡∏ô macOS/Linux
"""

import os
import json
import logging
import hashlib
import uuid
from datetime import datetime, timezone
from typing import Dict, Any, Optional, Tuple, Union, List
import unicodedata # NEW: ‡πÄ‡∏û‡∏¥‡πà‡∏° import ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Path/Filename encoding ‡∏ö‡∏ô macOS

# üìå ASSUME: config.global_vars ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ
from config.global_vars import (
    DATA_STORE_ROOT,
    EVIDENCE_DOC_TYPES,
    DOCUMENT_ID_MAPPING_FILENAME_SUFFIX,
    EVIDENCE_MAPPING_FILENAME_SUFFIX,
    RUBRIC_FILENAME_PATTERN,
    DEFAULT_TENANT,
    DEFAULT_YEAR,
    DEFAULT_ENABLER,
    PROJECT_NAMESPACE_UUID
)

logger = logging.getLogger(__name__)

# ==================== CORE HELPER ====================
def _n(s: Union[str, None]) -> str:
    """Normalize ‡∏ó‡∏∏‡∏Å string ‡∏î‡πâ‡∏ß‡∏¢ NFKC ‚Äì ‡πÅ‡∏Å‡πâ macOS NFD bug ‡∏ñ‡∏≤‡∏ß‡∏£ ‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô clean key"""
    return unicodedata.normalize('NFKC', s.strip().lower().replace(" ", "_")) if isinstance(s, str) and s.strip() else ""

# ==================== INTERNAL BASE PATH ====================
def _build_tenant_base_path(tenant: str) -> str:
    tenant_clean = _n(tenant)
    return os.path.join(DATA_STORE_ROOT, tenant_clean, "data")

# ==================== 1. SOURCE PATHS ====================
def get_document_source_dir(
    tenant: str,
    year: Optional[Union[int, str]] = None,
    enabler: Optional[str] = None,
    doc_type: str = "",
) -> str:
    if not doc_type:
        raise ValueError("doc_type is required")
    base = _build_tenant_base_path(tenant)
    base = os.path.join(base, _n(doc_type))
    if _n(doc_type) == EVIDENCE_DOC_TYPES.lower():
        if year is not None:
            base = os.path.join(base, str(year))
        if enabler:
            base = os.path.join(base, _n(enabler))
    return base

def get_evidence_base_dir(tenant: str, year: Union[int, str], enabler: str) -> str:
    return get_document_source_dir(tenant, year, enabler, EVIDENCE_DOC_TYPES)

# ==================== 2. VECTORSTORE PATHS ====================
def get_doc_type_collection_key(doc_type: str, enabler: Optional[str] = None) -> str:
    dt = _n(doc_type)
    if dt == EVIDENCE_DOC_TYPES.lower():
        return f"{dt}_{_n(enabler or 'default')}"
    return dt

def get_vectorstore_collection_path(
    tenant: str, year: Optional[Union[int, str]], doc_type: str, enabler: Optional[str] = None
) -> str:
    parts = [DATA_STORE_ROOT, _n(tenant), "vectorstore"]
    if _n(doc_type) == EVIDENCE_DOC_TYPES.lower() and year is not None:
        parts.append(str(year))
    parts.append(get_doc_type_collection_key(doc_type, enabler))
    return os.path.join(*parts)

def get_vectorstore_tenant_root_path(tenant: str) -> str:
    return os.path.join(DATA_STORE_ROOT, _n(tenant), "vectorstore")

# ==================== 3. MAPPING FILES ====================

def get_mapping_file_path(
    doc_type: str,
    tenant: str,
    year: Optional[Union[int, str]] = None,
    enabler: Optional[str] = None
) -> str:
    base = get_mapping_tenant_root_path(tenant)
    dt = _n(doc_type)
    
    # ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤ "evidence" ‡∏à‡∏≤‡∏Å config ‡∏°‡∏≤‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö
    from config.global_vars import EVIDENCE_DOC_TYPES
    evidence_type = _n(EVIDENCE_DOC_TYPES)

    # === 1. ‡∏Å‡∏£‡∏ì‡∏µ‡πÄ‡∏õ‡πá‡∏ô Evidence ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÑ‡∏õ‡∏´‡∏≤‡πÉ‡∏ô Folder ‡∏õ‡∏µ (2568/...) ===
    if dt == evidence_type:
        # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô error ‡∏ñ‡πâ‡∏≤‡∏•‡∏∑‡∏°‡∏™‡πà‡∏á‡∏õ‡∏µ‡∏´‡∏£‡∏∑‡∏≠ enabler ‡∏°‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö evidence
        safe_year = year if year else "default_year"
        safe_enabler = _n(enabler) if enabler else "default"
        
        return os.path.join(
            base,
            str(safe_year),
            f"{_n(tenant)}_{safe_year}_{safe_enabler}{DOCUMENT_ID_MAPPING_FILENAME_SUFFIX}"
        )

    # === 2. ‡∏≠‡∏∞‡πÑ‡∏£‡∏Å‡πá‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà evidence ‡πÉ‡∏´‡πâ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô Global Doc-Type ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ===
    # ‡∏ï‡∏±‡∏î year ‡πÅ‡∏•‡∏∞ enabler ‡∏ó‡∏¥‡πâ‡∏á‡πÑ‡∏õ‡πÄ‡∏•‡∏¢ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ path: mapping/pea_document_doc_id_mapping.json
    return os.path.join(
        base,
        f"{_n(tenant)}_{dt}{DOCUMENT_ID_MAPPING_FILENAME_SUFFIX}"
    )


def get_evidence_mapping_file_path(tenant: str, year: Optional[Union[int, str]], enabler: str) -> str:
    # 1. ‡πÑ‡∏î‡πâ Root Path ‡∏Ç‡∏≠‡∏á mapping/
    base = get_mapping_tenant_root_path(tenant)
    
    # 2. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå: ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏µ ‡∏Å‡πá‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏™‡πà‡∏õ‡∏µ‡πÉ‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå
    year_prefix = f"{year}_" if year else ""
    filename = f"{_n(tenant)}_{year_prefix}{_n(enabler)}{EVIDENCE_MAPPING_FILENAME_SUFFIX}"
    
    # 3. ‡∏£‡∏ß‡∏° Path: ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏µ ‡πÉ‡∏´‡πâ‡πÄ‡∏≠‡∏≤‡∏ß‡∏≤‡∏á‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà base ‡πÄ‡∏•‡∏¢ (data_store/pea/mapping/...)
    if year:
        return os.path.join(base, str(year), filename)
    else:
        return os.path.join(base, filename)

def get_mapping_tenant_root_path(tenant: str) -> str:
    return os.path.join(DATA_STORE_ROOT, _n(tenant), "mapping")

# ==================== 4. LOAD / SAVE MAPPING ====================
def load_doc_id_mapping(doc_type: str, tenant: str, year: Optional[Union[int, str]], enabler: Optional[str] = None) -> Dict:
    path = get_mapping_file_path(doc_type, tenant, year, enabler)
    if not os.path.exists(path):
        return {}
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        logger.error(f"Load mapping failed {path}: {e}")
        return {}

def save_doc_id_mapping(
    data: Dict,
    doc_type: str,
    tenant: str,
    year: Optional[Union[int, str]],
    enabler: Optional[str] = None
) -> None:
    path = get_mapping_file_path(doc_type, tenant, year, enabler)
    os.makedirs(os.path.dirname(path), exist_ok=True)

    # Normalize path (macOS-safe)
    path = unicodedata.normalize("NFKC", path)
    tmp_path = f"{path}.tmp"

    try:
        with open(tmp_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        # Atomic replace
        os.replace(tmp_path, path)

    except Exception as e:
        logger.error(f"Failed to save doc_id_mapping: {path} | {e}")

        # Cleanup temp file if exists
        if os.path.exists(tmp_path):
            os.remove(tmp_path)

        # IMPORTANT: propagate error to caller
        raise

# ==================== 6. PARSE COLLECTION NAME ====================
def parse_collection_name(collection_name: str) -> Tuple[str, Optional[str]]:
    name = _n(collection_name)
    if name.startswith(f"{EVIDENCE_DOC_TYPES.lower()}_"):
        parts = name.split("_", 1)
        if len(parts) == 2:
            return parts[0], parts[1]
    return name, None

# ==================== 7. DOCUMENT FILE PATH RESOLVER ====================
def get_document_file_path(
    document_uuid: str,
    tenant: str,
    year: Optional[Union[int, str]],
    enabler: Optional[str],
    doc_type_name: str
) -> Optional[Dict[str, str]]:
    """
    ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Path ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå PDF ‡∏ö‡∏ô Disk ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ UUID
    Logic: 
    1. ‡∏´‡∏≤‡∏à‡∏≤‡∏Å Mapping JSON (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Fallback ‡∏ñ‡πâ‡∏≤‡∏´‡∏≤‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏°‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠)
    2. ‡∏•‡∏≠‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å 'filepath' ‡∏ó‡∏µ‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ß‡πâ‡∏ï‡∏£‡∏á‡πÜ (Direct Access)
    3. ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ ‡πÉ‡∏´‡πâ‡∏ó‡∏≥ 'Fuzzy Scan' (os.walk) ‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
    """
    try:
        tenant_clean = _n(tenant)
        doc_type_clean = _n(doc_type_name).lower()
        
        # --- 1. ‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î Mapping Data ---
        # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÇ‡∏´‡∏•‡∏î‡∏ï‡∏≤‡∏°‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏°‡∏≤‡∏Å‡πà‡∏≠‡∏ô
        mapping_path = get_mapping_file_path(doc_type_name, tenant, year, enabler)
        
        # üí° Fallback: ‡∏ñ‡πâ‡∏≤‡∏´‡∏≤ Mapping ‡∏ï‡∏≤‡∏°‡∏õ‡∏µ‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ (‡πÄ‡∏ä‡πà‡∏ô URL ‡∏™‡πà‡∏á‡∏õ‡∏µ‡∏ú‡∏¥‡∏î) ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå Root ‡∏Ç‡∏≠‡∏á Tenant
        if not os.path.exists(mapping_path):
            logger.debug(f"Yearly mapping not found, trying global mapping: {doc_type_clean}")
            mapping_path = get_mapping_file_path(doc_type_name, tenant, None, None)

        if not os.path.exists(mapping_path):
            logger.warning(f"‚ùå [Path Resolver] Mapping file not found: {mapping_path}")
            return None

        with open(mapping_path, "r", encoding="utf-8") as f:
            mapping_data = json.load(f)

        entry = mapping_data.get(document_uuid)
        if not entry:
            logger.warning(f"‚ùå [Path Resolver] UUID {document_uuid} not found in mapping")
            return None

        # --- 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏î‡πâ‡∏ß‡∏¢ Direct Path (‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á) ---
        stored_path = entry.get("filepath", "")
        filename = entry.get("file_name") or entry.get("filename") or os.path.basename(stored_path)
        
        # ‡πÅ‡∏õ‡∏•‡∏á Relative Path (‡∏à‡∏≤‡∏Å Mapping) ‡πÄ‡∏õ‡πá‡∏ô Absolute Path
        if stored_path:
            # ‡∏ñ‡πâ‡∏≤ stored_path ‡πÄ‡∏õ‡πá‡∏ô relative (‡πÄ‡∏ä‡πà‡∏ô tcg/data/...) ‡πÉ‡∏´‡πâ‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö ROOT
            potential_path = stored_path if os.path.isabs(stored_path) else os.path.join(DATA_STORE_ROOT, stored_path)
            potential_path = resolve_filepath_to_absolute(potential_path)

            if os.path.exists(potential_path):
                logger.info(f"‚úÖ [Path Resolver] Direct hit: {potential_path}")
                return {"file_path": potential_path, "original_filename": filename}

        # --- 3. Fuzzy Scan (‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏ü‡∏•‡πå‡∏ñ‡∏π‡∏Å‡∏¢‡πâ‡∏≤‡∏¢‡∏ó‡∏µ‡πà ‡∏´‡∏£‡∏∑‡∏≠ Path ‡πÉ‡∏ô DB ‡∏Ñ‡∏•‡∏≤‡∏î‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô) ---
        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏à‡∏∏‡∏î‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏™‡πÅ‡∏Å‡∏ô: ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô evidence ‡∏™‡πÅ‡∏Å‡∏ô‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏õ‡∏µ/enabler ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô document ‡∏™‡πÅ‡∏Å‡∏ô‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏Å‡∏•‡∏≤‡∏á
        if doc_type_clean == "evidence":
            year_val = str(year) if year and str(year) != "None" else ""
            # ‡∏™‡πÅ‡∏Å‡∏ô‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏õ‡∏µ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏•‡∏±‡∏ö enabler
            base_search_path = os.path.join(DATA_STORE_ROOT, tenant_clean, "data", "evidence", year_val)
        else:
            base_search_path = os.path.join(DATA_STORE_ROOT, tenant_clean, "data", doc_type_clean)

        # ‡∏ñ‡πâ‡∏≤‡∏à‡∏∏‡∏î‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏™‡πÅ‡∏Å‡∏ô‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏à‡∏£‡∏¥‡∏á ‡πÉ‡∏´‡πâ‡∏ñ‡∏≠‡∏¢‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏ó‡∏µ‡πà data root ‡∏Ç‡∏≠‡∏á tenant
        if not os.path.exists(base_search_path):
            base_search_path = os.path.join(DATA_STORE_ROOT, tenant_clean, "data")

        logger.info(f"üîé [Path Resolver] Scanning: {base_search_path} for: {filename}")
        
        target_fn_norm = _n(filename) # Normalize ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏´‡∏≤
        
        for root, dirs, files in os.walk(base_search_path):
            # Optimization: ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Evidence
            if doc_type_clean == "evidence" and enabler and _n(enabler) not in _n(root):
                # ‡∏ñ‡πâ‡∏≤‡∏≠‡∏¢‡∏≤‡∏Å‡πÉ‡∏´‡πâ‡∏´‡∏≤‡∏Ç‡πâ‡∏≤‡∏° enabler ‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ‡∏Ñ‡∏≠‡∏°‡πÄ‡∏°‡∏ô‡∏ï‡πå 2 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ
                pass 
            
            for f in files:
                if _n(f) == target_fn_norm:
                    final_path = resolve_filepath_to_absolute(os.path.join(root, f))
                    logger.info(f"‚úÖ [Path Resolver] Fuzzy match found: {final_path}")
                    return {"file_path": final_path, "original_filename": f}

        logger.error(f"‚ùå [Path Resolver] File not found on disk: {filename}")
        return None

    except Exception as e:
        logger.error(f"üî¥ [Path Resolver] Critical Error: {str(e)}", exc_info=True)
        return None

# ==================== 8. OTHER PATHS ====================
def get_config_tenant_root_path(tenant: str) -> str:
    """Path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Configuration Files ‡∏ó‡∏µ‡πà‡∏Ñ‡∏á‡∏ó‡∏µ‡πà ‡πÄ‡∏ä‡πà‡∏ô Rubrics, Contextual Rules"""
    return os.path.join(DATA_STORE_ROOT, _n(tenant), "config")

def get_rubric_file_path(tenant: str, enabler: str) -> str:
    return os.path.join(get_config_tenant_root_path(tenant),
                        RUBRIC_FILENAME_PATTERN.format(tenant=_n(tenant), enabler=_n(enabler)))

def get_contextual_rules_file_path(tenant: str, enabler: str) -> str:
    return os.path.join(get_config_tenant_root_path(tenant),
                        f"{_n(tenant)}_{_n(enabler)}_contextual_rules.json")

def get_export_dir(tenant: str, year: Union[int, str], enabler: str) -> str:
    return os.path.join(DATA_STORE_ROOT, _n(tenant), "exports", str(year), _n(enabler))

def get_assessment_export_file_path(tenant: str, year: Union[int, str], enabler: str, suffix: str, ext: str = "json") -> str:
    return os.path.join(get_export_dir(tenant, year, enabler),
                        f"{_n(tenant)}_{year}_{_n(enabler)}_{suffix}.{ext.lower()}")

def get_normalized_metadata(doc_type: str, year_input=None, enabler_input=None, default_enabler=None):
    # Logic: Evidence ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏õ‡∏µ/Enabler, Doc/Global ‡πÉ‡∏ä‡πâ None
    return (None, None) if _n(doc_type) != EVIDENCE_DOC_TYPES.lower() else (year_input, enabler_input or default_enabler)

def resolve_filepath_to_absolute(path: str) -> str:
    """
    ‡πÅ‡∏õ‡∏•‡∏á Path ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô Absolute Path ‡πÅ‡∏•‡∏∞ Normalize (NFKC) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ macOS
    """
    # 1. ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô Absolute Path
    abs_path = os.path.abspath(path)
    # 2. Normalize ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Path ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏û‡∏¥‡πÄ‡∏®‡∏© (‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢) ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏£‡∏´‡∏±‡∏™‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
    return unicodedata.normalize('NFKC', abs_path)

# ==================== 9. EVIDENCE MAPPING ====================
def load_evidence_mapping(
    tenant=DEFAULT_TENANT,
    year=DEFAULT_YEAR,
    enabler=DEFAULT_ENABLER
):
    path = get_evidence_mapping_file_path(tenant, year, enabler)

    if not os.path.exists(path):
        return {}

    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        logger.error(f"Failed to load evidence mapping: {path} | {e}")
        return {}


def save_evidence_mapping(
    data,
    tenant=DEFAULT_TENANT,
    year=DEFAULT_YEAR,
    enabler=DEFAULT_ENABLER
):
    path = get_evidence_mapping_file_path(tenant, year, enabler)
    os.makedirs(os.path.dirname(path), exist_ok=True)

    tmp_path = f"{path}.tmp"

    try:
        with open(tmp_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
        os.replace(tmp_path, path)  # ‚úÖ atomic write
    except Exception as e:
        logger.error(f"Failed to save evidence mapping: {path} | {e}")
        if os.path.exists(tmp_path):
            os.remove(tmp_path)
        raise


# ==================== 10. UPDATE MAPPINGS ====================
def _update_doc_id_mapping(
    new_entries: Dict[str, Any],
    doc_type: str,
    tenant: str,
    year: Optional[Union[str, int]],
    enabler: Optional[str]
) -> None:
    if not new_entries:
        logger.debug("No new entries to update in doc_id_mapping.")
        return

    try:
        existing_map = load_doc_id_mapping(doc_type, tenant, year, enabler) or {}
    except Exception as e:
        logger.error(
            f"Failed to load doc_id_mapping for "
            f"{doc_type} / {enabler or 'None'} / Year {year or 'None'} | {e}"
        )
        existing_map = {}

    before_count = len(existing_map)
    overwrite_keys = set(existing_map) & set(new_entries)

    if overwrite_keys:
        logger.warning(
            f"Overwriting {len(overwrite_keys)} existing doc_id keys "
            f"for {doc_type} / {enabler or 'None'} / Year {year or 'None'}"
        )

    existing_map.update(new_entries)

    save_doc_id_mapping(existing_map, doc_type, tenant, year, enabler)

    logger.info(
        f"Updated doc_id_mapping: +{len(new_entries)} entries "
        f"(overwrite {len(overwrite_keys)}) | "
        f"{doc_type} / {enabler or 'None'} / Year {year or 'None'} | "
        f"Total={before_count}‚Üí{len(existing_map)}"
    )


def _update_evidence_mapping(
    new_entries: Dict[str, Any],
    tenant: str,
    year: Optional[Union[str, int]],
    enabler: Optional[str]
) -> None:
    _update_doc_id_mapping(
        new_entries=new_entries,
        doc_type=EVIDENCE_DOC_TYPES,
        tenant=tenant,
        year=year,
        enabler=enabler
    )

# ==================== 11. PATH KEY RESOLUTION (New Critical Logic) ====================
def get_mapping_key_from_physical_path(physical_path: str) -> str:
    """
    ‡πÅ‡∏õ‡∏•‡∏á Physical Path (Absolute Path ‡∏ó‡∏µ‡πà‡∏™‡πÅ‡∏Å‡∏ô‡πÄ‡∏à‡∏≠) ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô Relative Key (format: tenant/data/doc_type/...) 
    ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô Doc ID Mapping
    
    ‡πÉ‡∏ä‡πâ NFKC normalization ‡πÅ‡∏•‡∏∞ forward slashes ('/').
    """
    if not physical_path:
        return ""
    
    # üìå FIX 3: ‡∏ñ‡πâ‡∏≤ Path ‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô Path ‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ó‡∏ò‡πå‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß (‡πÄ‡∏ä‡πà‡∏ô Path ‡∏à‡∏≤‡∏Å Mapping DB)
    if not os.path.isabs(physical_path):
        # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô Path ‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ó‡∏ò‡πå (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÉ‡∏ô Mapping) ‡πÉ‡∏´‡πâ Normalize ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏•‡∏¢
        relative_key = unicodedata.normalize('NFKC', physical_path).replace('\\', '/')
        return relative_key
        
    # 2. ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô Absolute Path (‡∏°‡∏≤‡∏à‡∏≤‡∏Å os.walk ‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏™‡πÅ‡∏Å‡∏ô)
    
    # üü¢ ‡πÉ‡∏ä‡πâ resolve_filepath_to_absolute ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤‡πÑ‡∏î‡πâ NFKC-normalized Absolute Path
    normalized_abs_path = resolve_filepath_to_absolute(physical_path)
    
    # 3. Normalize DATA_STORE_ROOT
    normalized_abs_data_store_root = resolve_filepath_to_absolute(os.path.abspath(DATA_STORE_ROOT))

    # 4. Get Path relative to DATA_STORE_ROOT
    try:
        relative_path = os.path.relpath(normalized_abs_path, normalized_abs_data_store_root)
    except ValueError as e:
        logger.debug(f"Error getting relative path for {physical_path}: {e}")
        return ""

    # 5. Use forward slashes for the final key format and ensure it doesn't start with '..'
    relative_key = relative_path.replace('\\', '/')
    
    # Safety check: ‡∏ñ‡πâ‡∏≤ Path ‡∏≠‡∏¢‡∏π‡πà‡∏ô‡∏≠‡∏Å Root
    if relative_key.startswith('..'):
         logger.debug(f"File path is outside DATA_STORE_ROOT after relpath: {physical_path}")
         return ""
         
    return relative_key

# OTHER PATHS ‡πÄ
def get_tenant_year_export_root(tenant: str, year: Union[int, str]) -> str:
    """‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ Path ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏õ‡∏µ (Root ‡∏Ç‡∏≠‡∏á exports) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏ß‡∏ô‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡∏ó‡∏∏‡∏Å Enabler"""
    return os.path.join(DATA_STORE_ROOT, _n(tenant), "exports", str(year))
# ==================== ‡∏à‡∏ö utils/path_utils.py ====================

def get_tenant_year_report_root(
    tenant: str,
    year: Union[int, str],
    enabler: Optional[str] = None
) -> str:
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô Path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô (Reports)

    ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á:
        <DATA_STORE_ROOT>/<tenant>/reports/<year>/<enabler?>
    """
    if not tenant:
        raise ValueError("tenant is required")
    if year is None:
        raise ValueError("year is required")

    parts = [
        DATA_STORE_ROOT,
        _n(tenant),
        "reports",
        str(year),
    ]

    if enabler:
        parts.append(_n(enabler))

    base_dir = os.path.join(*parts)

    # Normalize + ensure directory exists (safe & idempotent)
    base_dir = unicodedata.normalize("NFKC", base_dir)
    os.makedirs(base_dir, exist_ok=True)

    return base_dir


# ==================== 12. STABLE UUID V5 GENERATOR (Production Final) ====================

def create_stable_uuid_from_path(
    filepath: str,
    tenant: Optional[str] = None,
    year: Optional[Union[int, str]] = None,
    enabler: Optional[str] = None,
) -> str:
    if not filepath:
        logger.error("Empty filepath provided")
        return str(uuid.uuid4())

    tenant_clean = _n(tenant or "")
    enabler_clean = _n(enabler or "")
    year_str = str(year) if year is not None else ""

    key_seed = None

    # Stat-based (preferred)
    try:
        filepath = resolve_filepath_to_absolute(filepath)
        st = os.stat(filepath)
        filename_norm = _n(os.path.basename(filepath))
        key_seed = f"{filename_norm}:{st.st_size}:{int(st.st_mtime)}:{tenant_clean}:{year_str}:{enabler_clean}"
    except Exception:
        pass

    # Path-based fallback
    if not key_seed:
        try:
            rel_key = get_mapping_key_from_physical_path(filepath)
            if rel_key:
                key_seed = f"{rel_key}:{tenant_clean}:{year_str}:{enabler_clean}"
        except Exception:
            pass

    if not key_seed:
        logger.error("Failed to create stable key, using random UUID4")
        return str(uuid.uuid4())

    # Namespace
    try:
        namespace = uuid.UUID(PROJECT_NAMESPACE_UUID) if isinstance(PROJECT_NAMESPACE_UUID, str) else PROJECT_NAMESPACE_UUID
    except Exception:
        namespace = uuid.NAMESPACE_DNS

    return str(uuid.uuid5(namespace, key_seed))

__all__ = [
    "_n",
    "get_document_source_dir",
    "get_evidence_base_dir",
    "get_doc_type_collection_key",
    "get_vectorstore_collection_path",
    "get_vectorstore_tenant_root_path",
    "get_mapping_file_path",
    "get_evidence_mapping_file_path",
    "get_mapping_tenant_root_path",
    "load_doc_id_mapping",
    "save_doc_id_mapping",
    "load_evidence_mapping",      # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏°
    "save_evidence_mapping",      # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏°
    "_update_doc_id_mapping",     # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏°
    "_update_evidence_mapping",   # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏°
    "parse_collection_name",
    "get_document_file_path",
    "get_config_tenant_root_path",
    "get_rubric_file_path",
    "get_contextual_rules_file_path",
    "get_export_dir",
    "get_assessment_export_file_path",
    "get_tenant_year_export_root",
    "get_tenant_year_report_root",
    "get_mapping_key_from_physical_path",
    "create_stable_uuid_from_path",
    "resolve_filepath_to_absolute" # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏°
]
