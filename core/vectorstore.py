# core/vectorstore.py
import os
import platform
import logging
import threading
from threading import Lock
import multiprocessing
import json
import shutil
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from typing import List, Optional, Union, Sequence, Any, Dict, Set, Tuple
from pathlib import Path
import hashlib
import uuid

# system utils
try:
    import psutil
except ImportError:
    psutil = None

# LangChain-ish imports
from langchain_core.documents import Document as LcDocument
from langchain_core.retrievers import BaseRetriever
from langchain_core.documents import BaseDocumentCompressor
from langchain_core.callbacks.manager import CallbackManagerForRetrieverRun
from langchain_core.runnables import Runnable
from langchain_community.retrievers import BM25Retriever
from langchain_core.documents import Document
# from langchain.retrievers import EnsembleRetriever

try:
    # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Mac (‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏Å‡πà‡∏≤) ‡∏´‡∏£‡∏∑‡∏≠ Server (‡∏ñ‡πâ‡∏≤‡∏•‡∏á‡∏ï‡∏±‡∏ß‡∏´‡∏•‡∏±‡∏Å‡πÑ‡∏ß‡πâ)
    from langchain.retrievers import EnsembleRetriever
except ImportError:
    # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Server (‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà v0.2+)
    from langchain_community.retrievers import EnsembleRetriever

# Thai Tokenizer
from pythainlp.tokenize import word_tokenize

# Pydantic helpers
from pydantic import PrivateAttr, ConfigDict, BaseModel, Field

# Chroma / HF embeddings
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
import chromadb
from chromadb.config import Settings

# CrossEncoder
try:
    from sentence_transformers import CrossEncoder
    _HAS_SENT_TRANS = True
except Exception:
    CrossEncoder = None
    _HAS_SENT_TRANS = False
    logging.warning("‚ö†Ô∏è sentence-transformers CrossEncoder not available. Reranker will be disabled.")

# Path utils
from utils.path_utils import (
    get_doc_type_collection_key,
    get_vectorstore_collection_path,
    get_vectorstore_tenant_root_path,
    get_mapping_file_path,
    _n
)

# Global config
from config.global_vars import (
    FINAL_K_RERANKED,
    INITIAL_TOP_K,
    EVIDENCE_DOC_TYPES,
    MAX_PARALLEL_WORKERS,
    DEFAULT_TENANT,
    DEFAULT_YEAR,
    DEFAULT_ENABLER,
    RERANKER_MODEL_NAME,
    EMBEDDING_MODEL_NAME,
    USE_HYBRID_SEARCH,
    HYBRID_BM25_WEIGHT,
    HYBRID_VECTOR_WEIGHT
)

# Logging
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# Chroma telemetry
try:
    chromadb.configure(anonymized_telemetry=False)
except Exception:
    try:
        chromadb.settings = Settings(anonymized_telemetry=False)
    except Exception:
        pass

# -------------------- Vectorstore Constants --------------------
ENV_FORCE_MODE = os.getenv("VECTOR_MODE", "").lower()
ENV_DISABLE_ACCEL = os.getenv("VECTOR_DISABLE_ACCEL", "").lower() in ("1", "true", "yes")

# Global caches
_CACHED_EMBEDDINGS = None
_EMBED_LOCK = threading.Lock()
_MPS_WARNING_SHOWN = False

# -------------------- Helper: detect environment & device --------------------
@staticmethod
def _detect_system():
    cpu_count = os.cpu_count() or 4
    total_ram_gb = None
    if psutil:
        try:
            total_ram_gb = psutil.virtual_memory().total / (1024 ** 3)
        except Exception:
            total_ram_gb = None
    return {"cpu_count": cpu_count, "total_ram_gb": total_ram_gb, "platform": platform.system().lower()}

@staticmethod
def _detect_torch_device():
    try:
        import torch
        if ENV_DISABLE_ACCEL:
            return "cpu"
        if torch.cuda.is_available():
            return "cuda"
        if platform.system().lower() == "darwin" and hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
            return "mps"
    except Exception:
        pass
    return "cpu"

# -------------------- HuggingFace Embeddings --------------------
def get_hf_embeddings(device_hint: Optional[str] = None):
    global _CACHED_EMBEDDINGS
    device = device_hint or _detect_torch_device()

    if _CACHED_EMBEDDINGS is None:
        with _EMBED_LOCK:
            if _CACHED_EMBEDDINGS is None:
                model_name = EMBEDDING_MODEL_NAME
                logger.info(f"Loading HF Embedding: {model_name} on {device}")
                try:
                    _CACHED_EMBEDDINGS = HuggingFaceEmbeddings(
                        model_name=model_name,
                        model_kwargs={"device": device},
                        encode_kwargs={"normalize_embeddings": True}
                    )
                except Exception as e:
                    logger.error(f"Failed to load {model_name}: {e}")
                    logger.warning("Falling back to paraphrase-multilingual-MiniLM-L12-v2")
                    _CACHED_EMBEDDINGS = HuggingFaceEmbeddings(
                        model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
                        model_kwargs={"device": "cpu"}
                    )
    return _CACHED_EMBEDDINGS

# -------------------- Thai Tokenizer --------------------
def thai_tokenizer_for_bm25(text: str) -> List[str]:
    return word_tokenize(text.lower().strip())

# -------------------- HuggingFace CrossEncoder Reranker --------------------
class HuggingFaceCrossEncoderCompressor(BaseDocumentCompressor):
    model_config = ConfigDict(arbitrary_types_allowed=True)
    
    rerank_model: str = RERANKER_MODEL_NAME
    rerank_device: str = Field(default_factory=lambda: _detect_torch_device())
    rerank_max_length: int = 512
    top_n: int = FINAL_K_RERANKED
    
    _cross_encoder: Optional[Any] = PrivateAttr(default=None)

    def __init__(self, **data):
        super().__init__(**data)
        detected_device = _detect_torch_device()
        object.__setattr__(self, 'rerank_device', detected_device)

        try:
            if not _HAS_SENT_TRANS:
                raise ImportError("sentence-transformers not installed")
            encoder = CrossEncoder(
                model_name_or_path=self.rerank_model,
                device=self.rerank_device,
                max_length=self.rerank_max_length
            )
            object.__setattr__(self, '_cross_encoder', encoder)
        except Exception as e:
            logger.error(f"‚ùå Error loading Reranker: {e}", exc_info=True)
            object.__setattr__(self, '_cross_encoder', None)

    def compress_documents(
        self,
        documents: Sequence[LcDocument],
        query: str,
        callbacks: Optional[Any] = None,
        top_n: Optional[int] = None  # <- ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ
    ) -> Sequence[LcDocument]:
        if not self._cross_encoder or not documents:
            return documents

        # ‡πÉ‡∏ä‡πâ top_n ‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤ ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏´‡πâ fallback ‡πÄ‡∏õ‡πá‡∏ô self.top_n
        current_top_n = min(len(documents), top_n or self.top_n)

        pairs = [[query, doc.page_content] for doc in documents]
        scores = self._cross_encoder.predict(pairs)

        ranked_docs = []
        for doc, score in zip(documents, scores):
            doc.metadata["rerank_score"] = float(score)
            ranked_docs.append(doc)

        ranked_docs.sort(key=lambda x: x.metadata["rerank_score"], reverse=True)
        final_docs = ranked_docs[:current_top_n]

        if final_docs:
            logger.info(f"üìä Reranking Stats | Top Score: {final_docs[0].metadata['rerank_score']:.4f} | Selected: {len(final_docs)} docs")

        return final_docs


# -------------------- Global Reranker Singleton --------------------
_global_reranker_instance = None
_global_reranker_lock = threading.Lock()

def get_global_reranker() -> Optional[HuggingFaceCrossEncoderCompressor]:
    global _global_reranker_instance
    with _global_reranker_lock:
        if _global_reranker_instance is None:
            try:
                _global_reranker_instance = HuggingFaceCrossEncoderCompressor(
                    rerank_model=RERANKER_MODEL_NAME,
                    top_n=FINAL_K_RERANKED
                )
            except Exception as e:
                logger.error(f"Failed to create global reranker: {e}")
                _global_reranker_instance = None
        return _global_reranker_instance

# -------------------- Path Helper Function (REVISED to use Path Utility) --------------------

def get_vectorstore_path(
    tenant: str, 
    year: Optional[int], 
    doc_type: Optional[str] = None, 
    enabler: Optional[str] = None
) -> str:
    """
    Calculates the full persist directory path for the vector store instance
    based on the Centralized KM Logic by calling path_utils.
    """
    # üéØ FIX: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏õ‡πÉ‡∏ä‡πâ get_vectorstore_tenant_root_path
    if not doc_type:
        return get_vectorstore_tenant_root_path(tenant=tenant)
        
    # üéØ FIX: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏õ‡πÉ‡∏ä‡πâ get_vectorstore_collection_path
    return get_vectorstore_collection_path(
        tenant=tenant, 
        year=year, 
        doc_type=doc_type, 
        enabler=enabler, 
        # EVIDENCE_DOC_TYPES ‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏™‡πà‡∏á‡πÅ‡∏•‡πâ‡∏ß ‡∏ñ‡∏π‡∏Å hardcode ‡πÉ‡∏ô path_utils
    )

def vectorstore_exists(
    doc_id: str = "N/A", # ‡∏£‡∏±‡∏Å‡∏©‡∏≤‡πÑ‡∏ß‡πâ‡∏ï‡∏≤‡∏° Signature ‡πÄ‡∏î‡∏¥‡∏°
    tenant: str = DEFAULT_TENANT,
    year: Optional[int] = DEFAULT_YEAR, # <--- ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö None ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö General Docs
    doc_type: Optional[str] = None, 
    enabler: Optional[str] = None, 
    base_path: str = "" # base_path ‡∏ñ‡∏π‡∏Å‡∏•‡∏∞‡πÄ‡∏•‡∏¢ ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÉ‡∏ä‡πâ global VECTORSTORE_DIR ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô path_utils
) -> bool:
    """
    Checks if the Vector Store directory exists for the given context.
    """
    if not doc_type:
        return False
        
    # 1. Get the full path using the updated logic (‡πÄ‡∏£‡∏µ‡∏¢‡∏Å get_vectorstore_path ‡πÉ‡∏´‡∏°‡πà)
    # NOTE: get_vectorstore_path ‡∏à‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Path ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÇ‡∏î‡∏¢‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å doc_type (‡∏°‡∏µ‡∏õ‡∏µ/‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏µ)
    path = get_vectorstore_path(tenant, year, doc_type, enabler) 
    
    # 2. Check for the actual data file created by Chroma
    file_path = os.path.join(path, "chroma.sqlite3")
    
    if not os.path.isdir(path):
        logger.warning(f"‚ùå V-Exists Check: Directory not found for doc_type '{doc_type}' at {path}")
        return False
    if os.path.isfile(file_path):
        return True
    logger.error(f"‚ùå V-Exists Check: FAILED to find file chroma.sqlite3 at {file_path} in {path}")
    return False


def list_vectorstore_folders(
    tenant: str, 
    year: int, 
    doc_type: Optional[str] = None, 
    enabler: Optional[str] = None, 
    base_path: str = "" 
) -> List[str]:
    """
    Lists the actual collection names that exist under the specified tenant and year context.
    Fixed: Checks for chroma.sqlite3 at the DB Root level instead of inside collection folders.
    """
    tenant_root = get_vectorstore_tenant_root_path(tenant) 
    
    # Scenario 1: Specific doc_type/enabler requested
    if doc_type:
        doc_type_norm = doc_type.lower().strip()
        collection_name = get_doc_type_collection_key(doc_type_norm, enabler)
        
        target_year = year if doc_type_norm == EVIDENCE_DOC_TYPES.lower() else None
        
        # Path ‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö Collection (e.g., .../2568/evidence_km)
        full_collection_path = get_vectorstore_collection_path(tenant, target_year, doc_type_norm, enabler)
        # Path ‡πÑ‡∏õ‡∏¢‡∏±‡∏á DB Root (e.g., .../2568/)
        db_root_path = os.path.dirname(full_collection_path.rstrip('/'))
        
        # ‚úÖ FIX: ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏°‡∏µ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå collection ‡πÅ‡∏•‡∏∞‡∏°‡∏µ chroma.sqlite3 ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô DB Root
        if os.path.isdir(full_collection_path) and os.path.isfile(os.path.join(db_root_path, "chroma.sqlite3")):
            return [collection_name] 
        return []

    # Scenario 2: List ALL collections
    collections: Set[str] = set()
    
    # 1. Scan the Year Root (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö evidence) - Path: V_ROOT/tenant/year
    root_year = os.path.join(tenant_root, str(year)) 
    if os.path.isdir(root_year):
        # ‚úÖ FIX: ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏õ‡∏µ‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå DB ‡∏´‡∏•‡∏±‡∏Å‡πÑ‡∏´‡∏°
        has_db_file = os.path.isfile(os.path.join(root_year, "chroma.sqlite3"))
        
        if has_db_file:
            for sub_dir in os.listdir(root_year):
                 sub_dir_lower = sub_dir.lower()
                 # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå ‡πÅ‡∏•‡∏∞‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ evidence_ (‡πÄ‡∏ä‡πà‡∏ô evidence_km)
                 if sub_dir_lower.startswith(f"{EVIDENCE_DOC_TYPES.lower()}_"): 
                     if os.path.isdir(os.path.join(root_year, sub_dir)):
                        collections.add(sub_dir_lower) 

    # 2. Scan the Common Root (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö document, faq) - Path: V_ROOT/tenant
    if os.path.isdir(tenant_root):
        has_common_db = os.path.isfile(os.path.join(tenant_root, "chroma.sqlite3"))
        
        for sub_dir in os.listdir(tenant_root):
            if sub_dir.isdigit(): continue # ‡∏Ç‡πâ‡∏≤‡∏°‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏õ‡∏µ
            
            sub_dir_lower = sub_dir.lower()
            full_path = os.path.join(tenant_root, sub_dir)
            
            # ‚úÖ FIX: ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå ‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå DB ‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏î‡∏±‡∏ö tenant root
            if os.path.isdir(full_path) and has_common_db:
                 collections.add(sub_dir_lower) 
    
    return sorted(list(collections))


# -------------------- VECTORSTORE MANAGER (SINGLETON) --------------------
class VectorStoreManager:
    _instance = None
    _is_initialized = False
    _lock = threading.Lock()

    # ‡πÉ‡∏ä‡πâ default_factory ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏™‡πà {} ‡∏´‡∏£‡∏∑‡∏≠ None ‡∏ï‡∏£‡∏á‡πÜ
    _chroma_cache: Dict[str, Chroma] = PrivateAttr(default_factory=dict)
    _multi_doc_retriever: Optional['MultiDocRetriever'] = PrivateAttr(default=None)
    
    tenant: str = PrivateAttr(default=DEFAULT_TENANT)
    year: int = PrivateAttr(default=DEFAULT_YEAR)

    _doc_id_mapping: Dict[str, Dict[str, Any]] = PrivateAttr(default_factory=dict)
    _uuid_to_doc_id: Dict[str, str] = PrivateAttr(default_factory=dict)

    _embeddings: Any = PrivateAttr(default=None)

    # ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î: ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ default_factory=dict ‡∏´‡∏£‡∏∑‡∏≠ default=None ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô!
    _client: Optional[chromadb.PersistentClient] = PrivateAttr(default=None)


    def __new__(cls, *args, **kwargs):
        with cls._lock:
            if cls._instance is None:
                cls._instance = super(VectorStoreManager, cls).__new__(cls)
        return cls._instance

    def __init__(self, base_path: str = "", tenant: str = DEFAULT_TENANT, 
                 year: Optional[int] = None, enabler: Optional[str] = None, 
                 doc_type: str = EVIDENCE_DOC_TYPES):
        if not self._is_initialized:
            with self._lock:
                if not self._is_initialized:
                    # --- Basic Setup ---
                    self._base_path = base_path
                    self.tenant = tenant.lower()
                    self.year = year if year is not None else DEFAULT_YEAR    
                    self.doc_type = doc_type
                    self.enabler = enabler.upper() if enabler else DEFAULT_ENABLER 

                    # --- Caches ---
                    self._chroma_cache: Dict[str, Any] = {}
                    self._multi_doc_retriever: Optional[Any] = None
                    self._doc_id_mapping: Dict[str, Dict[str, Any]] = {}
                    self._uuid_to_doc_id: Dict[str, str] = {}
                    self._hybrid_retriever_cache: Dict[str, Any] = {}
                    self._bm25_docs_cache: Dict[str, List[Document]] = {}

                    # --- Core Components ---
                    self._embeddings = get_hf_embeddings()
                    self._client: Optional[chromadb.PersistentClient] = None

                    # --- Logger (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç!) ---
                    self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
                    self.logger.info(f"VectorStoreManager initialized for tenant={self.tenant}, year={self.year}")

                    # --- Initialize Client ---
                    try:
                        client_base_path = self._get_chroma_client_base_path(tenant, year)
                        self._client = chromadb.PersistentClient(path=client_base_path)
                        self.logger.info(f"ChromaDB Client initialized at: {client_base_path}")
                    except Exception as e:
                        self.logger.error(f"Failed to initialize ChromaDB client: {e}")
                        self._client = None

                    # --- Load Mapping ---
                    try:
                        self._load_doc_id_mapping()
                        self.logger.info(f"Loaded doc_id_mapping: {len(self._doc_id_mapping)} documents")
                    except Exception as e:
                        self.logger.error(f"Failed to load doc_id_mapping: {e}")

                    self._is_initialized = True  # ‚Üê ‡πÉ‡∏ä‡πâ instance variable
                    self.logger.info(f"VectorStoreManager fully initialized (Tenant: {self.tenant})")
    
    def _get_chroma_client_base_path(self, tenant: str, year: Optional[int]) -> str:
        """
        Determines the base path for the Chroma PersistentClient.
        - Global Docs (document, seam): ‡∏ä‡∏µ‡πâ‡πÑ‡∏õ‡∏ó‡∏µ‡πà root ‡∏Ç‡∏≠‡∏á vectorstore
        - Evidence Docs (KM): ‡∏ä‡∏µ‡πâ‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏õ‡∏µ (‡πÄ‡∏ä‡πà‡∏ô vectorstore/2568)
        """
        # ‡∏î‡∏∂‡∏á root path ‡∏Ç‡∏≠‡∏á tenant (‡πÄ‡∏ä‡πà‡∏ô .../data_store/pea/vectorstore)
        root_path = get_vectorstore_tenant_root_path(tenant) 
        
        # ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤ doc_type ‡∏°‡∏≤ normalize ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö
        current_dt = _n(getattr(self, 'doc_type', EVIDENCE_DOC_TYPES))
        evidence_type = _n(EVIDENCE_DOC_TYPES)

        # üéØ FIX LOGIC:
        # ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô Evidence ‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏∏‡∏õ‡∏µ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡∏ñ‡∏∂‡∏á‡∏à‡∏∞‡∏ä‡∏µ‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏õ‡∏µ
        if current_dt == evidence_type and year is not None:
            target_path = os.path.join(root_path, str(year))
            self.logger.info(f"üìÇ VSM Path Mode: YEARLY -> {target_path}")
            return target_path
        
        # ‡∏ô‡∏≠‡∏Å‡πÄ‡∏´‡∏ô‡∏∑‡∏≠‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô (‡πÄ‡∏ä‡πà‡∏ô document, seam) ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ Root Path ‡πÄ‡∏™‡∏°‡∏≠
        self.logger.info(f"üìÇ VSM Path Mode: GLOBAL -> {root_path}")
        return root_path
    
    # -------------------- START FIXES (3 Functions) --------------------
    
    def set_multi_doc_retriever(self, mdr: 'MultiDocRetriever'):
        """
        Sets the active MultiDocRetriever instance.
        NOTE: This is the setter for the PrivateAttr _multi_doc_retriever.
        (FIX: ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç AttributeError ‡πÉ‡∏ô load_all_vectorstores)
        """
        # üéØ FIX: ‡πÉ‡∏ä‡πâ object.__setattr__ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ PrivateAttr
        object.__setattr__(self, '_multi_doc_retriever', mdr)
        logger.info("‚úÖ MultiDocRetriever has been set in VectorStoreManager.")

    def get_multi_doc_retriever(self) -> Optional['MultiDocRetriever']:
        """Gets the active MultiDocRetriever instance."""
        return self._multi_doc_retriever
    
    def close(self):
        with self._lock:
            if self._multi_doc_retriever and hasattr(self._multi_doc_retriever, "shutdown"):
                logger.info("Closing MultiDocRetriever executor via VSM.")
                self._multi_doc_retriever.shutdown()
                self._multi_doc_retriever = None
            self._chroma_cache = {}
            VectorStoreManager._is_initialized = False

    def __del__(self):
        try:
            self.close()
        except Exception:
            pass

    def _load_doc_id_mapping(self):
        """
        ‡πÇ‡∏´‡∏•‡∏î Document ID Mapping ‡πÇ‡∏î‡∏¢‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Path ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏´‡∏ô‡∏∂‡πà‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (Simplified Version)
        - Evidence: ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏ä‡πâ Path ‡∏£‡∏≤‡∏¢‡∏õ‡∏µ/‡∏£‡∏≤‡∏¢ Enabler
        - ‡∏≠‡∏∑‡πà‡∏ô‡πÜ: ‡πÉ‡∏ä‡πâ Path ‡∏Å‡∏•‡∏≤‡∏á‡∏£‡∏∞‡∏î‡∏±‡∏ö Tenant Root
        """
        from threading import Lock

        # 1. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Lock ‡πÅ‡∏•‡∏∞‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏†‡∏≤‡∏¢‡πÉ‡∏ô (Internal State)
        if not hasattr(self, "_mapping_lock") or self._mapping_lock is None:
            self._mapping_lock = Lock()

        self._doc_id_mapping = {}
        self._uuid_to_doc_id = {}

        # 2. ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤ Attributes ‡∏à‡∏≤‡∏Å Instance
        current_tenant = getattr(self, 'tenant', 'default_tenant')
        current_year = getattr(self, 'year', None)
        current_enabler = getattr(self, 'enabler', None)
        current_doc_type = getattr(self, 'doc_type', EVIDENCE_DOC_TYPES) 

        # 3. ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Path ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (Single Path Decision)
        target_path = None
        
        # ‡πÉ‡∏ä‡πâ _n() ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤ NFD/NFC ‡∏ö‡∏ô macOS ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡∏û‡∏¥‡∏°‡∏û‡πå
        if _n(current_doc_type) == EVIDENCE_DOC_TYPES.lower():
            # ‡∏™‡∏≤‡∏¢ Evidence: ‡∏Å‡∏é‡πÉ‡∏ô path_utils ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏ß‡πà‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ year ‡πÅ‡∏•‡∏∞ enabler
            try:
                target_path = get_mapping_file_path(
                    doc_type=current_doc_type,
                    tenant=current_tenant, 
                    year=current_year, 
                    enabler=current_enabler
                )
            except ValueError:
                # ‡∏Å‡∏£‡∏ì‡∏µ year/enabler ‡πÄ‡∏õ‡πá‡∏ô None ‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏û‡πà‡∏ô Warning ‡πÅ‡∏ï‡πà‡∏à‡∏∞‡∏õ‡∏•‡πà‡∏≠‡∏¢‡πÉ‡∏´‡πâ target_path ‡πÄ‡∏õ‡πá‡∏ô None
                target_path = None
        else:
            # ‡∏™‡∏≤‡∏¢ Global (seam, faq, policy, etc.): ‡πÉ‡∏ä‡πâ Path ‡∏Å‡∏•‡∏≤‡∏á ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏∏‡∏õ‡∏µ
            try:
                target_path = get_mapping_file_path(
                    doc_type=current_doc_type,
                    tenant=current_tenant,
                    year=None,
                    enabler=None 
                )
            except ValueError:
                target_path = None

        # 4. Validation: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏Å‡πà‡∏≠‡∏ô‡∏≠‡πà‡∏≤‡∏ô
        if not target_path or not os.path.exists(target_path):
            logger.warning(f"‚ö†Ô∏è No mapping file found for type '{current_doc_type}' at: {target_path}")
            return

        # 5. ‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏î‡∏±‡∏ä‡∏ô‡∏µ (Indexing)
        logger.info(f"üìÇ Loading mapping from: {target_path}")

        try:
            with open(target_path, "r", encoding="utf-8") as f:
                mapping_data = json.load(f)
                
            with self._mapping_lock:
                for doc_id, doc_entry in mapping_data.items():
                    doc_id_clean = doc_id.strip()
                    self._doc_id_mapping[doc_id_clean] = doc_entry
                    
                    # ‡∏™‡∏£‡πâ‡∏≤‡∏á UUID Lookup Table ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ RAG ‡∏ó‡∏£‡∏≤‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ï‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡πâ‡∏≠‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ (Chunk)
                    if isinstance(doc_entry, dict) and "chunk_uuids" in doc_entry:
                        for uid in doc_entry["chunk_uuids"]:
                            uid_clean = uid.replace("-", "")
                            # ‡πÄ‡∏Å‡πá‡∏ö‡∏ó‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö‡∏°‡∏µ‡∏Ç‡∏µ‡∏î‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡∏µ‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
                            self._uuid_to_doc_id[uid] = doc_id_clean
                            self._uuid_to_doc_id[uid_clean] = doc_id_clean
            
            logger.info(f"‚úÖ Success: Loaded {len(self._doc_id_mapping)} documents into Memory.")
                
        except Exception as e:
            logger.error(f"‚ùå Failed to load mapping: {e}")

    def _re_parse_collection_name(self, collection_name: str) -> Tuple[str, Optional[str]]:
        collection_name_lower = collection_name.strip().lower()
        if collection_name_lower.startswith(f"{EVIDENCE_DOC_TYPES}_"):
            parts = collection_name_lower.split("_", 1)
            return EVIDENCE_DOC_TYPES, parts[1].upper() if len(parts) == 2 else None
        return collection_name_lower, None

    def _load_chroma_instance(self, collection_name: str) -> Optional[Chroma]:
        # 1. Fast cache hit
        if collection_name in self._chroma_cache:
            return self._chroma_cache[collection_name]

        # 2. Thread-safe double-check
        with self._lock:
            if collection_name in self._chroma_cache:
                return self._chroma_cache[collection_name]

            # ------------------------------------------------------------------
            # 3. ‡πÅ‡∏¢‡∏Å doc_type ‡∏Å‡∏±‡∏ö enabler ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤
            # ------------------------------------------------------------------
            doc_type, enabler = self._re_parse_collection_name(collection_name)

            # ------------------------------------------------------------------
            # 4. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î target_year
            # ------------------------------------------------------------------
            if doc_type.lower() == EVIDENCE_DOC_TYPES.lower():
                target_year: Optional[int] = self.year
            else:
                target_year = None

            # ------------------------------------------------------------------
            # 5. ‡∏™‡∏£‡πâ‡∏≤‡∏á persist_directory ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á 100% (Full Path ‡∏Ç‡∏≠‡∏á Collection)
            # ------------------------------------------------------------------
            persist_directory = get_vectorstore_collection_path(
                tenant=self.tenant,
                year=target_year,
                doc_type=doc_type,
                enabler=enabler,
            )

            # ------------------------------------------------------------------
            # 6. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ folder ‡∏à‡∏£‡∏¥‡∏á‡πÑ‡∏´‡∏°
            # ------------------------------------------------------------------
            if not os.path.exists(persist_directory):
                logger.warning(
                    f"Vectorstore directory NOT FOUND!\n"
                    f"   Collection      : {collection_name}\n"
                    f"   Expected path: {persist_directory}\n"
                    f"   tenant={self.tenant} | year={self.year} | doc_type={str(doc_type)} | enabler={enabler or 'None'}" 
                )
                return None

            # ------------------------------------------------------------------
            # 7. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ client ‡∏ñ‡∏π‡∏Å init ‡πÅ‡∏•‡πâ‡∏ß (‡πÉ‡∏ä‡πâ self.client ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏•‡πâ‡∏ß)
            # ------------------------------------------------------------------
            if self.client is None: # ‡πÉ‡∏ä‡πâ property client ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏Å _ensure_chroma_client_is_valid
                logger.error("Chroma PersistentClient is None! ‡∏ï‡πâ‡∏≠‡∏á init ‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")
            
            # üéØ ‡∏î‡∏∂‡∏á Global Embedding Model (768-dim) ‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
            try:
                correct_embeddings = get_hf_embeddings() 
            except Exception as e:
                logger.error(f"FATAL: Failed to get correct embeddings for Chroma init: {e}")
                return None

            try:
                # ------------------------------------------------------------------
                # 8. ‡∏™‡∏£‡πâ‡∏≤‡∏á Chroma instance (‡πÉ‡∏ä‡πâ Path ‡πÅ‡∏ó‡∏ô Client ‡∏ï‡∏±‡∏ß‡πÅ‡∏°‡πà)
                # ------------------------------------------------------------------
                
                vectordb = Chroma(
                    # client=self._client,       # ‚¨ÖÔ∏è ‡∏•‡∏ö Client ‡∏ï‡∏±‡∏ß‡πÅ‡∏°‡πà‡∏ó‡∏µ‡πà Root ‡∏≠‡∏≠‡∏Å
                    embedding_function=correct_embeddings,
                    collection_name=collection_name,
                    persist_directory=persist_directory  # ‚¨ÖÔ∏è ‡πÉ‡∏ä‡πâ Full Path ‡∏Ç‡∏≠‡∏á Collection
                )

                # üéØ FIX 7: ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÇ‡∏´‡∏•‡∏î Collection Object ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ (‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Lazy Loading ‡πÉ‡∏ô Worker)
                collection_test = vectordb._collection 
                # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏Å method ‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤ collection ‡πÑ‡∏°‡πà‡∏ï‡∏≤‡∏¢
                collection_test.count()

                # Cache ‡πÑ‡∏ß‡πâ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ï‡πà‡∏≠‡πÑ‡∏õ
                self._chroma_cache[collection_name] = vectordb

                logger.info(
                    f"Loaded Chroma collection '{collection_name}' ‚Üí Path: {persist_directory} (Retrieval Test Pending)"
                )
                return vectordb

            except Exception as e:
                logger.error(
                    f"FAILED to load collection '{collection_name}' from {persist_directory}\n"
                    f"Error: {e}",
                    exc_info=True,
                )
                return None


    def get_documents_by_id(self, stable_doc_ids: Union[str, List[str]], doc_type: str = "default_collection", enabler: Optional[str] = None) -> List[LcDocument]:
        """
        ‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á: ‡πÄ‡∏ô‡πâ‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á Metadata
        ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡∏ú‡πà‡∏≤‡∏ô Primary Keys (Chunk IDs) ‡πÅ‡∏•‡∏∞ Metadata (Stable IDs)
        """
        from langchain_core.documents import Document as LcDocument
        
        # 1. ‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Input
        if isinstance(stable_doc_ids, str):
            stable_doc_ids = [stable_doc_ids]
        
        stable_doc_ids_cleaned = list(set([uid.strip() for uid in stable_doc_ids if uid.strip()]))
        if not stable_doc_ids_cleaned:
            return []

        # 2. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Collection ‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Client (Worker Safety)
        collection_name = get_doc_type_collection_key(doc_type=doc_type, enabler=enabler)
        self._ensure_chroma_client_is_valid() # üõ°Ô∏è ‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤ Client ‡πÑ‡∏°‡πà‡∏´‡∏•‡∏∏‡∏î‡πÉ‡∏ô Worker
        chroma_instance = self._load_chroma_instance(collection_name)
        
        if not chroma_instance:
            logger.error(f"‚ùå VSM: Collection '{collection_name}' load failed.")
            return []

        collection = chroma_instance._collection
        documents: List[LcDocument] = []

        try:
            # 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ Chunk IDs ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å Doc ID Map
            search_ids: Set[str] = set()
            for s_id in stable_doc_ids_cleaned:
                map_entry = self.doc_id_map.get(s_id)
                if map_entry and map_entry.get("chunk_uuids"):
                    search_ids.update(map_entry["chunk_uuids"])
                # ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏Å‡∏£‡∏ì‡∏µ s_id ‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô Chunk ID ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
                search_ids.add(s_id)
            
            # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î ID (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö‡∏°‡∏µ dash ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏µ dash)
            final_ids = list(search_ids)
            for cid in list(search_ids):
                if "-" in cid: final_ids.append(cid.replace("-", ""))
            
            # --- Attempt 1: ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏î‡πâ‡∏ß‡∏¢ Primary Key (IDs) ---
            logger.info(f"üîÑ Attempt 1: Fetching {len(final_ids)} IDs from {collection_name}")
            result = collection.get(ids=final_ids, include=["documents", "metadatas"])

            # --- Attempt 2: Fallback ‡∏î‡πâ‡∏ß‡∏¢ Metadata Search (‡∏Å‡∏£‡∏ì‡∏µ ID Map ‡πÑ‡∏°‡πà‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï) ---
            if not result.get("documents"):
                logger.warning("‚ö†Ô∏è Primary key search empty. Falling back to Metadata filter...")
                result = collection.get(
                    where={"$or": [
                        {"stable_doc_uuid": {"$in": stable_doc_ids_cleaned}},
                        {"doc_id": {"$in": stable_doc_ids_cleaned}}
                    ]},
                    include=["documents", "metadatas"]
                )

            # 4. ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á LcDocument
            docs_raw = result.get("documents", [])
            metas_raw = result.get("metadatas", [])
            ids_raw = result.get("ids", [])

            for i, text in enumerate(docs_raw):
                meta = metas_raw[i].copy() if metas_raw and metas_raw[i] else {}
                current_id = ids_raw[i]
                
                p_val = meta.get("page_label") or meta.get("page_number") or meta.get("page") or "N/A"
                meta["page"] = str(p_val)
                meta["page_label"] = str(p_val) # UI ‡∏°‡∏±‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡∏ô‡∏µ‡πâ
                
                            # ‡∏ù‡∏±‡∏á ID ‡∏ó‡∏µ‡πà‡πÅ‡∏ó‡πâ‡∏à‡∏£‡∏¥‡∏á‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ
                meta["chunk_uuid"] = current_id
                
                # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏° Map ‡∏Å‡∏•‡∏±‡∏ö‡∏´‡∏≤ Stable ID ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
                stable_ref = (
                    self.uuid_to_doc_id_map.get(current_id) or 
                    self.uuid_to_doc_id_map.get(current_id.replace("-", "")) or
                    meta.get("stable_doc_uuid") or 
                    meta.get("doc_id")
                )
                if stable_ref:
                    meta["stable_doc_uuid"] = stable_ref

                documents.append(LcDocument(page_content=text, metadata=meta))

            logger.info(f"‚úÖ Success: Retrieved {len(documents)} chunks from '{collection_name}'")
            return documents

        except Exception as e:
            logger.error(f"‚ùå Error in get_documents_by_id: {str(e)}", exc_info=True)
            return []

    def get_chunks_by_page(self, collection_name: str, stable_doc_uuid: str, page_label: str) -> List[LcDocument]:
        """
        [NEW] ‡∏î‡∏∂‡∏á Chunks ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Ç‡∏≠‡∏á‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏ (Exact Metadata Match)
        ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏∂‡∏á‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Ç‡πâ‡∏≤‡∏á‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á (Neighbor Context) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Act (A) ‡∏Ç‡∏≤‡∏î‡∏´‡∏≤‡∏¢
        """
        try:
            # 1. ‡πÇ‡∏´‡∏•‡∏î Chroma Instance ‡∏ú‡πà‡∏≤‡∏ô Cache/Logic ‡πÄ‡∏î‡∏¥‡∏°‡∏Ç‡∏≠‡∏á VSM
            self._ensure_chroma_client_is_valid()
            chroma_instance = self._load_chroma_instance(collection_name)
            
            if not chroma_instance:
                self.logger.error(f"‚ùå Neighbor Fetch: ‡πÑ‡∏°‡πà‡∏û‡∏ö Collection {collection_name}")
                return []

            # 2. ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á Collection ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ï‡πà‡∏≥ (Chroma native collection) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ filter
            collection = chroma_instance._collection

            # üéØ ‡∏™‡∏£‡πâ‡∏≤‡∏á Filter ‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡∏∞‡∏´‡∏ô‡πâ‡∏≤
            # ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: page_label ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô String ‡∏ï‡∏≤‡∏°‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏Å‡∏≤‡∏£ Ingest ‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤
            where_filter = {
                "$and": [
                    {"stable_doc_uuid": {"$eq": str(stable_doc_uuid)}},
                    {"page_label": {"$eq": str(page_label)}}
                ]
            }

            # 3. ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡∏ï‡∏±‡πâ‡∏á limit=10 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏Å‡∏£‡∏ì‡∏µ 1 ‡∏´‡∏ô‡πâ‡∏≤‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢ chunks)
            results = collection.get(
                where=where_filter,
                limit=10, 
                include=["documents", "metadatas", "ids"]
            )

            extra_docs = []
            if results and results['documents']:
                for idx, text in enumerate(results['documents']):
                    meta = results['metadatas'][idx].copy() if results['metadatas'] else {}
                    
                    # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î Metadata ‡πÉ‡∏´‡πâ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏≠‡∏∑‡πà‡∏ô‡πÜ
                    p_val = meta.get("page_label") or meta.get("page_number") or "N/A"
                    meta["page_label"] = str(p_val)
                    meta["chunk_uuid"] = results['ids'][idx].replace("-", "")
                    
                    extra_docs.append(LcDocument(page_content=text, metadata=meta))
            
            if extra_docs:
                self.logger.info(f"‚ûï Neighbor Fetch: ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏ô‡πâ‡∏≤ {page_label} ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå {stable_doc_uuid} ({len(extra_docs)} chunks)")
            
            return extra_docs

        except Exception as e:
            self.logger.error(f"‚ùå Error ‡πÉ‡∏ô get_chunks_by_page: {str(e)}", exc_info=True)
            return []
        
    def _ensure_chroma_client_is_valid(self):
        """
        Re-initializes the Chroma client if it is None or lost during serialization (Worker Process).
        """
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ _client attribute ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô None ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if not hasattr(self, '_client') or self._client is None:
            logger.warning(f"Chroma client lost in worker process for tenant '{self.tenant}', re-initializing...")
            
            # ‡πÉ‡∏ä‡πâ VSM attributes (tenant) ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÉ‡∏´‡πâ‡∏Ñ‡∏á‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Path
            tenant_root_path = get_vectorstore_tenant_root_path(self.tenant)
            
            # Re-initialize the Persistent Client
            try:
                # üéØ FIX: ‡πÉ‡∏ä‡πâ logger ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å import ‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤
                self._client = chromadb.PersistentClient(path=tenant_root_path, settings=Settings(anonymized_telemetry=True))
                
                # ‡πÄ‡∏°‡∏∑‡πà‡∏≠ Client ‡∏ñ‡∏π‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà, Collection Handles ‡πÄ‡∏Å‡πà‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ï‡πâ‡∏≠‡∏á‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÉ‡∏ä‡πâ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ
                # ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ _chroma_cache ‡πÅ‡∏ó‡∏ô _collections ‡πÄ‡∏£‡∏≤‡∏à‡∏∂‡∏á‡∏ï‡πâ‡∏≠‡∏á‡∏•‡πâ‡∏≤‡∏á cache
                self._chroma_cache = {} 
                
                logger.info(f"‚úÖ ChromaDB Client re-initialized at TENANT ROOT PATH: {tenant_root_path}. Collections cache cleared.")
            except Exception as e:
                logger.error(f"FATAL: Failed to re-initialize Chroma Client in worker: {e}", exc_info=False)
                # ‡πÑ‡∏°‡πà raise ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÇ‡∏Ñ‡πâ‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏±‡∏ö error ‡πÑ‡∏î‡πâ
            
    def retrieve_by_chunk_uuids(self, chunk_uuids: List[str], collection_name: Optional[str] = None) -> List[LcDocument]:
        """
        Hydrate documents by chunk UUIDs.
        - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö UUID ‡∏ó‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö‡∏°‡∏µ dash ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏µ dash
        - Retry mechanism + cache clear ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠ retrieval fail
        """
        # (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£ import ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß)

        self._ensure_chroma_client_is_valid()

        if not chunk_uuids:
            logger.info("VSM: No chunk_uuids provided for hydration.")
            return []

        if collection_name is None:
            collection_name = get_doc_type_collection_key(
                doc_type=EVIDENCE_DOC_TYPES, 
                enabler=getattr(self, 'enabler', 'km')
            )

        # Prepare UUIDs: no-dash + attempt 64-char dash formatting
        no_dash = [u.replace("-", "") for u in chunk_uuids if u]
        with_dash = []
        for u in no_dash:
            if len(u) == 32: # UUID4 is 32 chars, not 64
                # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤ UUID ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÉ‡∏ä‡πâ‡∏°‡∏µ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô 8-4-4-4-12 = 32
                try:
                    uuid_obj = uuid.UUID(u, version=4) # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô UUID
                    with_dash.append(str(uuid_obj))
                except ValueError:
                    # ‡∏ñ‡πâ‡∏≤‡πÅ‡∏õ‡∏•‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≤‡∏°
                    pass 
            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô 64-char hash (‡∏ã‡∏∂‡πà‡∏á‡πÑ‡∏°‡πà‡∏ô‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô UUID) ‡πÉ‡∏´‡πâ‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡∏µ‡∏î‡∏ï‡πà‡∏≠‡πÑ‡∏õ
            # ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏ó‡∏≥ 64-char hash ‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ö‡∏ö‡∏Ç‡∏µ‡∏î‡∏Å‡∏•‡∏≤‡∏á ‡∏ã‡∏∂‡πà‡∏á‡∏≠‡∏≤‡∏à‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô UUID
            # ‡πÅ‡∏ï‡πà‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏Å‡∏©‡∏≤‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏î‡∏¥‡∏°:
            elif len(u) == 64:
                 part1, part2, part3, part4, part5 = u[:8], u[8:12], u[12:16], u[16:20], u[20:]
                 with_dash.append(f"{part1}-{part2}-{part3}-{part4}-{part5}")
        
        # All formats
        all_formats = list(set(chunk_uuids + no_dash + with_dash))  # Remove duplicates

        result = {"documents": [], "metadatas": [], "ids": []}
        max_retries = 3

        for attempt in range(1, max_retries + 1):
            chroma = self._load_chroma_instance(collection_name)
            if not chroma:
                logger.warning(f"Cannot load '{collection_name}' (attempt {attempt})")
                if attempt < max_retries and collection_name in self._chroma_cache:
                    logger.warning(f"VSM: Clearing Chroma cache for '{collection_name}' due to load failure.")
                    del self._chroma_cache[collection_name]
                    self._ensure_chroma_client_is_valid()
                continue

            try:
                logger.info(f"Hydration attempt {attempt}/{max_retries} ‚Üí {len(all_formats)} UUIDs from '{collection_name}'")
                result = chroma.get(ids=all_formats, include=["documents", "metadatas", "ids"])

                if result.get("documents"):
                    logger.info(f"Success: Retrieved {len(result['documents'])} chunks on attempt {attempt}")
                    break

                logger.warning(f"Got 0 chunks on attempt {attempt}, retrying...")

            except Exception as e:
                logger.error(f"Hydration failed (attempt {attempt}): {e}")

            # Clear cache only on failure
            if attempt < max_retries and collection_name in self._chroma_cache:
                logger.warning(f"VSM: Clearing Chroma cache for '{collection_name}' for retry.")
                del self._chroma_cache[collection_name]
                self._ensure_chroma_client_is_valid()

        # Build LcDocument objects
        docs = []
        documents_raw = result.get("documents", [])
        metas = result.get("metadatas", [{}] * len(documents_raw))
        ids = result.get("ids", [])

        for i, text in enumerate(documents_raw):
            if not text or not text.strip():
                continue
            meta = metas[i].copy()
            id_clean = ids[i].replace("-", "") if ids[i] else ""
            meta["chunk_uuid"] = id_clean

            # Map to stable_doc_uuid
            stable = self._uuid_to_doc_id.get(id_clean) or meta.get("stable_doc_uuid")
            if stable:
                meta["stable_doc_uuid"] = stable

            p_val = meta.get("page_label") or meta.get("page_number") or meta.get("page") or "N/A"
            meta["page"] = str(p_val)
            meta["page_label"] = str(p_val)

            docs.append(LcDocument(page_content=text.strip(), metadata=meta))

        logger.info(f"Hydration complete ‚Üí Retrieved {len(docs)} full-text chunks (requested {len(chunk_uuids)})")
        return docs

    def retrieve(
        self,
        query: str,
        collection_name: str,
        top_k: int = 10,
        filter_doc_ids: Optional[Set[str]] = None,
        metadata_filter: Optional[Dict[str, Any]] = None  # üëà ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏°‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏ö Rubric Filter
    ) -> List[LcDocument]:
        """
        ‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏≠‡∏±‡∏õ‡πÄ‡∏Å‡∏£‡∏î: ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Hybrid + Flexible Post-filtering
        """
        self.logger.info(f"üîç VSM: Retrieving from {collection_name} | Query: {query[:50]}...")

        # 1. ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ get_retriever
        retriever = self.get_retriever(
            collection_name=collection_name, 
            top_k=top_k, 
            use_hybrid=True
        )

        if not retriever:
            return []

        # 2. ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        docs = retriever.invoke(query)

        # 3. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Filter (‡πÅ‡∏ö‡∏ö Flexible)
        if filter_doc_ids:
            # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î ID ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô lowercase string ‡πÅ‡∏•‡∏∞‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á
            clean_targets = {str(tid).lower().strip() for tid in filter_doc_ids}
            
            filtered_docs = []
            for d in docs:
                m = d.metadata or {}
                # ‡∏î‡∏∂‡∏á ID ‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å key ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ
                m_stable = str(m.get("stable_doc_uuid", "")).lower().strip()
                m_doc = str(m.get("doc_id", "")).lower().strip()
                
                if m_stable in clean_targets or m_doc in clean_targets:
                    filtered_docs.append(d)
                # Fallback: ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏Å‡∏£‡∏ì‡∏µ ID ‡∏´‡∏•‡∏∏‡∏î
                elif any(tid in str(m.get("source", "")).lower() for tid in clean_targets):
                    filtered_docs.append(d)
            
            docs = filtered_docs

        # 4. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Metadata Filter (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Rubric/Enabler)
        if metadata_filter:
            for key, value in metadata_filter.items():
                docs = [d for d in docs if d.metadata.get(key) == value]

        return docs[:top_k]


    def create_hybrid_retriever(self, collection_name: str, top_k: int = 20) -> EnsembleRetriever:
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞ Cache Hybrid Retriever (Vector + BM25)
        ‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á: ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÅ‡∏•‡∏∞‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Metadata ‡πÄ‡∏õ‡πá‡∏ô None
        """
        # 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Cache ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£
        if collection_name in self._hybrid_retriever_cache:
            logger.info(f"‚ôªÔ∏è Using cached Hybrid Retriever for: {collection_name}")
            return self._hybrid_retriever_cache[collection_name]
            
        logger.info(f"üèóÔ∏è Creating NEW Hybrid Retriever for: {collection_name}...")

        try:
            # 2. ‡πÇ‡∏´‡∏•‡∏î Chroma Instance
            chroma_instance = self._load_chroma_instance(collection_name) 
            if not chroma_instance:
                raise ValueError(f"Chroma instance for '{collection_name}' failed to load.")
            
            # 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á Vector Retriever (Dense)
            # ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ k ‡πÉ‡∏´‡πâ‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤ top_k ‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Ensemble ‡∏°‡∏µ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô
            vector_retriever = chroma_instance.as_retriever(
                search_kwargs={"k": top_k}
            )

            # 4. ‡∏î‡∏∂‡∏á Documents ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏°‡∏≤‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏ó‡∏≥ BM25 Index (Sparse)
            if collection_name in self._bm25_docs_cache:
                langchain_docs = self._bm25_docs_cache[collection_name]
                logger.info(f"üì¶ Loaded {len(langchain_docs)} docs for BM25 from cache.")
            else:
                logger.info(f"üîç Fetching docs from Chroma collection '{collection_name}' for BM25 indexing...")
                
                # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö‡∏à‡∏≤‡∏Å Chroma (‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô)
                raw_data = chroma_instance._collection.get(
                    include=["documents", "metadatas"]
                )
                
                texts = raw_data.get("documents", [])
                metas = raw_data.get("metadatas", [])
                
                # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏£‡∏ì‡∏µ metas ‡πÄ‡∏õ‡πá‡∏ô None ‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏≤‡∏ß‡πÑ‡∏°‡πà‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö texts
                if not metas:
                    metas = [{} for _ in texts]
                
                # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô LangChain Document Objects
                langchain_docs = [
                    Document(page_content=text, metadata=meta if meta else {})
                    for text, meta in zip(texts, metas)
                ]
                
                # ‡πÄ‡∏Å‡πá‡∏ö‡∏•‡∏á Cache ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏ï‡πâ‡∏≠‡∏á‡∏î‡∏∂‡∏á‡πÉ‡∏´‡∏°‡πà‡∏ö‡πà‡∏≠‡∏¢‡πÜ
                self._bm25_docs_cache[collection_name] = langchain_docs
                logger.info(f"‚úÖ Indexed {len(langchain_docs)} documents for BM25.")

            # 5. ‡∏™‡∏£‡πâ‡∏≤‡∏á BM25 Retriever ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ï‡∏±‡∏ß‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
            if not langchain_docs:
                logger.warning(f"‚ö†Ô∏è Collection '{collection_name}' is empty. Returning vector retriever only.")
                return vector_retriever

            bm25_retriever = BM25Retriever.from_documents(
                langchain_docs, 
                preprocess_func=word_tokenize # üéØ FIX: ‡πÉ‡∏ä‡πâ pythainlp ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Search ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥
            )
            bm25_retriever.k = top_k

            # 6. ‡∏£‡∏ß‡∏°‡∏£‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô Ensemble Retriever (Hybrid)
            # ‡πÇ‡∏î‡∏¢‡∏õ‡∏Å‡∏ï‡∏¥ Vector 0.7 ‡πÅ‡∏•‡∏∞ BM25 0.3 ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô RAG
            ensemble_retriever = EnsembleRetriever(
                retrievers=[vector_retriever, bm25_retriever],
                weights=[0.7, 0.3] # ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å global_vars
            )
            
            # 7. ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤ Cache ‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å
            self._hybrid_retriever_cache[collection_name] = ensemble_retriever
            logger.info(f"üöÄ Hybrid Retriever for '{collection_name}' is ready (Vector + BM25).")
            return ensemble_retriever
        
        except Exception as e:
            logger.error(f"‚ùå Failed to create Hybrid Retriever for '{collection_name}': {str(e)}", exc_info=True)
            # ‡∏Å‡∏£‡∏ì‡∏µ‡∏û‡∏•‡∏≤‡∏î ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô Vector Retriever ‡∏õ‡∏Å‡∏ï‡∏¥‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡∏•‡πà‡∏°
            try:
                return chroma_instance.as_retriever(search_kwargs={"k": top_k})
            except:
                return None
        
    def get_limited_chunks_from_doc_ids(self, stable_doc_ids: Union[str, List[str]], query: Union[str, List[str]], doc_type: str, enabler: Optional[str] = None, limit_per_doc: int = 5) -> List[LcDocument]:
        if isinstance(stable_doc_ids, str):
            stable_doc_ids = [stable_doc_ids]
        stable_doc_ids = [uid for uid in stable_doc_ids if uid]
        if not stable_doc_ids:
            return []
        vector_search_query = query[0] if isinstance(query, list) and query else (query if isinstance(query, str) else "")
        if not vector_search_query:
            logger.warning("Limited chunk search skipped: Query is empty.")
            return []
        # üéØ FIX: ‡πÉ‡∏ä‡πâ get_doc_type_collection_key ‡πÅ‡∏ó‡∏ô _get_collection_name
        collection_name = get_doc_type_collection_key(doc_type, enabler)
        chroma_instance = self._load_chroma_instance(collection_name)
        if not chroma_instance:
            logger.error(f"Collection '{collection_name}' is not loaded.")
            return []
        all_limited_documents: List[LcDocument] = []
        total_chunks_retrieved = 0
        for stable_id in stable_doc_ids:
            stable_id_clean = stable_id.strip()
            # üéØ FIX: ‡πÉ‡∏ä‡πâ 'stable_doc_uuid' ‡πÄ‡∏õ‡πá‡∏ô filter key ‡πÅ‡∏ó‡∏ô 'doc_id' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô
            doc_filter = {"stable_doc_uuid": stable_id_clean}
            try:
                custom_retriever = ChromaRetriever(vectorstore=chroma_instance, k=limit_per_doc, filter=doc_filter)
                limited_docs = custom_retriever.get_relevant_documents(query=vector_search_query)
                for doc in limited_docs:
                    doc.metadata["priority_search_type"] = "limited_vector_search"
                    doc.metadata["priority_limit"] = limit_per_doc
                    all_limited_documents.append(doc)
                total_chunks_retrieved += len(limited_docs)
            except Exception as e:
                logger.error(f"‚ùå Error performing limited vector search for Stable ID '{stable_id_clean}': {e}")
                continue
        logger.info(f"‚úÖ Retrieved {total_chunks_retrieved} limited chunks (max {limit_per_doc}/doc) for {len(stable_doc_ids)} Stable IDs from '{collection_name}'.")
        return all_limited_documents

    def get_retriever(self, collection_name: str, top_k: int = INITIAL_TOP_K, final_k: int = FINAL_K_RERANKED, use_rerank: bool = USE_HYBRID_SEARCH, use_hybrid: bool = True) -> Any:
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á Retriever ‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Hybrid Search (Vector + BM25) ‡πÅ‡∏•‡∏∞ Reranking 
        ‡πÇ‡∏î‡∏¢‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Scope ‡∏Ç‡∏≠‡∏á‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
        """
        
        # ‡πÇ‡∏´‡∏•‡∏î Chroma Instance
        chroma_instance = self._load_chroma_instance(collection_name)
        if not chroma_instance:
            logger.warning(f"Retriever creation failed: Collection '{collection_name}' not loaded.")
            return None

        # --- [INTERNAL HELPER 1]: Reranker Wrapper ---
        # ‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡πÑ‡∏ß‡πâ‡∏ö‡∏ô‡∏™‡∏∏‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ó‡∏±‡πâ‡∏á Hybrid ‡πÅ‡∏•‡∏∞ Fallback ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ
        def retrieve_with_rerank(docs: List[LcDocument], query: str) -> List[LcDocument]:
            reranker = get_global_reranker()
            if not (use_rerank and reranker and hasattr(reranker, "compress_documents")):
                return docs[:final_k]

            try:
                reranked = reranker.compress_documents(documents=docs, query=query, top_n=final_k)
                # Inject score ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô metadata ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏´‡∏£‡∏∑‡∏≠ debug
                scores = getattr(reranker, "scores", None)
                if scores and len(scores) >= len(reranked):
                    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ï‡∏≤‡∏° score
                    doc_scores = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)
                    for i, (doc, score) in enumerate(doc_scores[:len(reranked)]):
                        for r_doc in reranked:
                            if r_doc.page_content == doc.page_content:
                                score_val = float(score) if score is not None else 0.0
                                r_doc.metadata["_rerank_score_force"] = score_val
                                orig = r_doc.metadata.get("source_filename", "UNKNOWN")
                                r_doc.metadata["source_filename"] = f"{orig}|SCORE:{score_val:.4f}"
                                break
                logger.info(f"Reranking success ‚Üí kept {len(reranked)} docs")
                return reranked
            except Exception as e:
                logger.warning(f"Rerank failed: {e}, fallback to raw")
                return docs[:final_k]

        # --- [INTERNAL HELPER 2]: Raw Vector Retrieve ---
        def raw_vector_retrieve(query: str, filter_dict: Optional[dict] = None, k: int = top_k) -> List[LcDocument]:
            try:
                # ‡πÄ‡∏û‡∏¥‡πà‡∏° Prefix ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö BGE-M3 (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
                bge_prefix = "‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå: "
                query_with_prefix = f"{bge_prefix}{query.strip()}"
                
                docs = chroma_instance.similarity_search(
                    query=query_with_prefix,
                    k=k,
                    filter=filter_dict
                )
                return docs
            except Exception as e:
                logger.error(f"Vector retrieval failed: {e}")
                return []

        # 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á Vector Retriever ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        vector_retriever = chroma_instance.as_retriever(search_kwargs={"k": top_k})

        # 2. ‡∏Å‡∏£‡∏ì‡∏µ‡πÉ‡∏ä‡πâ Hybrid (BM25 + Vector)
        if use_hybrid:
            try:
                # üü¢ FIX CRITICAL: ‡πÉ‡∏ä‡πâ _collection ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
                if not hasattr(chroma_instance, "_collection"):
                    raise ValueError("chroma_instance has no _collection attribute.")
                
                collection = chroma_instance._collection
                
                # üü¢ FIX: ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥ BM25 Index (‡∏•‡∏ö "ids" ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å include)
                result = collection.get(include=["documents", "metadatas"])
                texts = result.get("documents", [])
                metadatas = result.get("metadatas", [])

                if texts:
                    langchain_docs = [
                        LcDocument(page_content=text, metadata=meta or {})
                        for text, meta in zip(texts, metadatas)
                    ]

                    # üü¢ KEY FIX: ‡πÉ‡∏™‡πà Tokenizer ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ (pythainlp)
                    from pythainlp.tokenize import word_tokenize as thai_tokenizer
                    bm25_retriever = BM25Retriever.from_documents(
                        langchain_docs,
                        preprocess_func=thai_tokenizer # ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠ tokenizer ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏ï‡∏±‡πâ‡∏á‡πÑ‡∏ß‡πâ
                    )
                    bm25_retriever.k = top_k

                    # ‡∏£‡∏ß‡∏°‡∏£‡πà‡∏≤‡∏á Ensemble
                    ensemble_retriever = EnsembleRetriever(
                        retrievers=[vector_retriever, bm25_retriever],
                        weights=[HYBRID_VECTOR_WEIGHT, HYBRID_BM25_WEIGHT]
                    )

                    # ‡∏Ñ‡∏•‡∏≤‡∏™‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Hybrid + Rerank
                    class UltimateHybridRetriever(BaseRetriever):
                        def _get_relevant_documents(self, query: str, *, run_manager=None) -> List[LcDocument]:
                            # ‡∏î‡∏∂‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ú‡πà‡∏≤‡∏ô Ensemble
                            docs = ensemble_retriever.invoke(query)
                            # ‡∏™‡πà‡∏á‡πÑ‡∏õ Rerank ‡∏ú‡πà‡∏≤‡∏ô‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô Helper ‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡πÑ‡∏ß‡πâ‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô
                            return retrieve_with_rerank(docs, query)

                        def invoke(self, query: str, config: Optional[dict] = None, **kwargs) -> List[LcDocument]:
                            return self._get_relevant_documents(query)
                    
                    return UltimateHybridRetriever()

            except Exception as e:
                logger.error(f"Hybrid setup failed for '{collection_name}': {e}", exc_info=False)
                # ‡∏´‡∏≤‡∏Å Hybrid ‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ ‡πÉ‡∏´‡πâ‡πÑ‡∏´‡∏•‡∏•‡∏á‡πÑ‡∏õ‡πÉ‡∏ä‡πâ Fallback ‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á
                pass

        # 3. Fallback: ‡∏Å‡∏£‡∏ì‡∏µ Rerank ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡∏´‡∏£‡∏∑‡∏≠ Hybrid ‡∏û‡∏±‡∏á
        if use_rerank and get_global_reranker():
            class SimpleVectorRerankRetriever(BaseRetriever):
                def _get_relevant_documents(self, query: str, *, run_manager=None) -> List[LcDocument]:
                    # ‡∏î‡∏∂‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ú‡πà‡∏≤‡∏ô Vector Search
                    docs = raw_vector_retrieve(query, filter_dict=None, k=top_k)
                    # ‡∏™‡πà‡∏á‡πÑ‡∏õ Rerank
                    return retrieve_with_rerank(docs, query)
            
                def invoke(self, query: str, config: Optional[dict] = None, **kwargs) -> List[LcDocument]:
                    return self._get_relevant_documents(query)
            
            return SimpleVectorRerankRetriever()
        
        # 4. ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢: ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ Vector Retriever ‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤
        return vector_retriever

    def get_all_collection_names(self) -> List[str]:
        # üéØ FIX: ‡∏•‡∏ö base_path ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å list_vectorstore_folders
        return list_vectorstore_folders(tenant=self.tenant, year=self.year)

    def get_chunks_from_doc_ids(self, stable_doc_ids: Union[str, List[str]], doc_type: str, enabler: Optional[str] = None) -> List[LcDocument]:
        """
        ‡∏î‡∏∂‡∏á Chunk ‡∏Ç‡∏≠‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏à‡∏≤‡∏Å ChromaDB ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Stable Document IDs
        ‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£ Mapping ‡∏à‡∏≤‡∏Å doc_id_mapping.json
        """
        import chromadb
        from langchain_core.documents import Document as LcDocument

        # 1. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° input ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô List ‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á
        if isinstance(stable_doc_ids, str):
            stable_doc_ids = [stable_doc_ids]
        stable_doc_ids = [uid.strip() for uid in stable_doc_ids if uid and isinstance(uid, str)]
        
        if not stable_doc_ids:
            logger.warning("No valid Stable Document IDs provided.")
            return []
        
        # 2. ‡∏£‡∏∞‡∏ö‡∏∏‡∏ä‡∏∑‡πà‡∏≠ Collection
        # üéØ FIX: ‡πÉ‡∏ä‡πâ get_doc_type_collection_key ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏≤‡∏°‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏∞‡∏ö‡∏ö
        collection_name = get_doc_type_collection_key(doc_type, enabler)
        
        all_chunk_uuids = []
        skipped_docs = []
        found_stable_ids = []
        
        # 3. ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Chunk UUIDs ‡∏à‡∏≤‡∏Å Mapping
        for stable_id in stable_doc_ids:
            if stable_id in self._doc_id_mapping:
                doc_entry = self._doc_id_mapping[stable_id]
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô Mapping
                if isinstance(doc_entry, dict) and "chunk_uuids" in doc_entry:
                    chunk_uuids = doc_entry["chunk_uuids"]
                    if isinstance(chunk_uuids, list) and chunk_uuids:
                        all_chunk_uuids.extend(chunk_uuids)
                        found_stable_ids.append(stable_id)
                    else:
                        logger.warning(f"Stable ID '{stable_id}' has an empty or invalid chunk_uuids list.")
                else:
                    logger.warning(f"Mapping for Stable ID '{stable_id}' is malformed or missing 'chunk_uuids'.")
            else:
                skipped_docs.append(stable_id)
                
        if skipped_docs:
            logger.warning(f"Skipping Stable IDs not found in mapping: {skipped_docs}")
            
        if not all_chunk_uuids:
            logger.warning(f"No valid chunk UUIDs found in collection '{collection_name}' for provided IDs.")
            return []
            
        # 4. ‡πÇ‡∏´‡∏•‡∏î Chroma Instance
        chroma_instance = self._load_chroma_instance(collection_name)
        if not chroma_instance:
            logger.error(f"Collection '{collection_name}' could not be loaded.")
            return []
            
        try:
            # üü¢ FIX CRITICAL: ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á _collection ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á ‡πÅ‡∏•‡∏∞‡∏•‡∏ö "ids" ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å include 
            # ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô TypeError ‡πÉ‡∏ô ChromaDB version ‡πÉ‡∏´‡∏°‡πà
            collection = chroma_instance._collection
            result = collection.get(
                ids=all_chunk_uuids, 
                include=["documents", "metadatas"]
            ) 
            
            documents: List[LcDocument] = []
            retrieved_texts = result.get("documents", [])
            retrieved_metas = result.get("metadatas", [])
            retrieved_ids = result.get("ids", []) # IDs ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏Ñ‡∏∑‡∏ô‡∏°‡∏≤‡πÉ‡∏´‡πâ‡πÇ‡∏î‡∏¢‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
            
            if not retrieved_texts:
                logger.warning(f"Chroma DB returned 0 documents for {len(all_chunk_uuids)} chunk UUIDs in '{collection_name}'.")
                return []
                
            # 5. ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏£‡πà‡∏≤‡∏á LangChain Documents
            for i, text in enumerate(retrieved_texts):
                if text:
                    metadata = retrieved_metas[i] if i < len(retrieved_metas) else {}
                    chunk_uuid = retrieved_ids[i]
                    
                    # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ doc_id ‡∏î‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏¥‡∏°‡∏à‡∏≤‡∏Å uuid_to_doc_id mapping
                    doc_id = self._uuid_to_doc_id.get(chunk_uuid, "UNKNOWN") 
                    
                    # ‡∏â‡∏µ‡∏î Metadata ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡πÅ‡∏•‡∏∞ Traceability
                    metadata["chunk_uuid"] = chunk_uuid
                    metadata["doc_id"] = doc_id
                    metadata["doc_type"] = doc_type
                    
                    documents.append(LcDocument(page_content=text, metadata=metadata))
            
            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ï‡∏≤‡∏° Chunk Order (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• index ‡πÉ‡∏ô metadata) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á
            try:
                documents.sort(key=lambda x: (x.metadata.get("doc_id", ""), x.metadata.get("chunk_index", 0)))
            except:
                pass

            logger.info(f"‚úÖ Successfully retrieved {len(documents)} chunks from '{collection_name}'.")
            return documents

        except Exception as e:
            logger.error(f"‚ùå Error retrieving chunks from collection '{collection_name}': {e}", exc_info=True)
            return []
    
    @property
    def client(self) -> Optional[chromadb.PersistentClient]:
        """
        Provides access to the underlying Chroma Persistent Client (Re-validate in worker).
        (FIX: ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç AttributeError ‡πÉ‡∏ô get_retriever)
        """
        # üéØ FIX: ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å ensure ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤ client ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏´‡∏≤‡∏¢‡πÑ‡∏õ‡πÉ‡∏ô worker context
        self._ensure_chroma_client_is_valid()
        return self._client
    
    @property
    def doc_id_map(self) -> Dict[str, Dict[str, Any]]:
        """Provides access to the Stable Doc ID -> Chunk UUIDs mapping."""
        return self._doc_id_mapping

    @property
    def uuid_to_doc_id_map(self) -> Dict[str, str]:
        """Provides access to the Chunk UUID -> Stable Doc ID mapping."""
        return self._uuid_to_doc_id
    

# Helper function
def get_vectorstore_manager(
    doc_type: str = "all",           # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡πà‡∏≤ default
    tenant: str = DEFAULT_TENANT,
    year: Optional[int] = None,
    enabler: Optional[str] = None,
) -> VectorStoreManager:
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏∑‡∏ô VectorStoreManager (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏ó‡∏∏‡∏Å doc_type)
    """
    return VectorStoreManager(
        # doc_type=doc_type,
        tenant=tenant,
        # year=year or DEFAULT_YEAR,
        # enabler=enabler
    )

def load_vectorstore(doc_type: str, enabler: Optional[str] = None) -> Optional[Chroma]:
    collection_name = get_doc_type_collection_key(doc_type, enabler)
    vsm = get_vectorstore_manager(
        doc_type=doc_type,           # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ!
        enabler=enabler
    )
    return vsm._load_chroma_instance(collection_name)

class VectorStoreExecutorSingleton:
    _instance = None
    _is_initialized = False

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            cls._instance = super(VectorStoreExecutorSingleton, cls).__new__(cls)
        return cls._instance

    def __init__(self):
        if not VectorStoreExecutorSingleton._is_initialized:
            self.max_workers = MAX_PARALLEL_WORKERS
            self._executor = ThreadPoolExecutor(max_workers=self.max_workers)
            logger.info(f"Initialized VectorStoreExecutorSingleton (ThreadPoolExecutor with {self.max_workers} workers) for batch tasks.")
            VectorStoreExecutorSingleton._is_initialized = True

    @property
    def executor(self) -> ThreadPoolExecutor:
        return self._executor

    def close(self):
        if self._is_initialized:
            logger.info("Shutting down VectorStoreExecutorSingleton ThreadPoolExecutor...")
            self._executor.shutdown(wait=True)
            VectorStoreExecutorSingleton._is_initialized = False

def get_vectorstore(
    collection_name: str, 
    tenant: str, 
    year: Optional[int],
    # üí° ‡πÄ‡∏û‡∏¥‡πà‡∏° Argument ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á Chroma Client ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
    # ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡πÉ‡∏ä‡πâ Embedding Model ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô VectorStoreExecutorSingleton
    # ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏° embedding_function ‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏î‡πâ‡∏ß‡∏¢
) -> VectorStoreExecutorSingleton:
    """
    Wrapper function ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ VectorStoreExecutorSingleton 
    ‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á‡∏ú‡πà‡∏≤‡∏ô Argument ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏∏ Path ‡πÅ‡∏•‡∏∞ Collection Name.
    """
    
    # üéØ FIX: ‡∏™‡πà‡∏á‡∏ú‡πà‡∏≤‡∏ô Argument ‡πÑ‡∏õ‡∏¢‡∏±‡∏á Constructor ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏•‡∏≤‡∏™‡∏´‡∏•‡∏±‡∏Å
    return VectorStoreExecutorSingleton(
        collection_name=collection_name, 
        tenant=tenant, 
        year=year
    )

# -------------------- Custom Retriever for Chroma --------------------
class ChromaRetriever(BaseRetriever):
    vectorstore: Any
    k: int
    filter: Optional[Dict] = None
    model_config = ConfigDict(arbitrary_types_allowed=True)

    def _get_relevant_documents(self, query: str, *, run_manager=None) -> List[LcDocument]:
        
        # üéØ DEBUG: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Collection Count ‡πÅ‡∏•‡∏∞ Embedding (‡πÇ‡∏Ñ‡πâ‡∏î‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏°)
        try:
            raw_collection = self.vectorstore._collection
            count = raw_collection.count() 
            logger.critical(f"üéØ [DEBUG CHROMA] Collection Count: {count}")
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Query Embedding
            if hasattr(self.vectorstore, '_embedding_function') and self.vectorstore._embedding_function:
                embedding_function = self.vectorstore._embedding_function
                # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö embed query
                query_embedding = embedding_function.embed_query(query)
                logger.critical(f"üéØ [DEBUG CHROMA] Query Embedding Success. Vector size: {len(query_embedding)}")
            else:
                logger.critical("üéØ [DEBUG CHROMA] Cannot access _embedding_function.")

        except Exception as debug_e:
            logger.critical(f"‚ùå [DEBUG CHROMA] Debug check failed (Skip search): {debug_e}")
            return []
        # END DEBUG

        try:
            # ‡∏£‡∏±‡∏ô similarity search
            return self.vectorstore.similarity_search(query=query, k=self.k, filter=self.filter)
        except Exception as e:
            logger.error(f"‚ùå Chroma similarity_search failed in custom retriever: {e}")
            return []

    def get_relevant_documents(self, query: str, **kwargs) -> List[LcDocument]:
        return self._get_relevant_documents(query, **kwargs)

    async def _aget_relevant_documents(self, query: str, *, run_manager=None) -> List[LcDocument]:
        return self._get_relevant_documents(query, run_manager=run_manager)

# -------------------- MultiDoc / Parallel Retriever --------------------
class NamedRetriever(BaseModel):
    """
    Defines a single retriever configuration, mapping a document type/enabler
    to a specific VectorStore collection context (tenant/year).
    """
    doc_id: str
    doc_type: str
    top_k: int = INITIAL_TOP_K
    final_k: int = FINAL_K_RERANKED
    # üéØ FIX: ‡∏•‡∏ö base_path ‡∏≠‡∏≠‡∏Å
    enabler: Optional[str] = None
    tenant: str = DEFAULT_TENANT
    year: Optional[int] = DEFAULT_YEAR # <--- üéØ FIX: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô Optional[int] ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö None
    
    # load_instance ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏Å VSM ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ self.tenant ‡πÅ‡∏•‡∏∞ self.year
    def load_instance(self) -> Any:
        """
        Loads the actual VectorStore Retriever instance using VectorStoreManager 
        with the correct tenant and year context.
        """
        # ‚ö†Ô∏è VSM Singleton ‡∏ï‡πâ‡∏≠‡∏á‡∏ñ‡∏π‡∏Å init ‡∏î‡πâ‡∏ß‡∏¢‡∏õ‡∏µ‡πÄ‡∏™‡∏°‡∏≠ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÇ‡∏´‡∏•‡∏î Doc ID Mapping
        # ‡∏ñ‡πâ‡∏≤ self.year ‡πÄ‡∏õ‡πá‡∏ô None (‡πÄ‡∏ä‡πà‡∏ô ‡πÄ‡∏õ‡πá‡∏ô General Docs ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô‡∏ó‡∏∏‡∏Å‡∏õ‡∏µ) ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ DEFAULT_YEAR
        manager = VectorStoreManager(
            # üéØ FIX: ‡∏•‡∏ö base_path ‡∏≠‡∏≠‡∏Å
            tenant=self.tenant, 
            year=self.year if self.year is not None else DEFAULT_YEAR # <--- üéØ FIX: ‡πÉ‡∏ä‡πâ DEFAULT_YEAR ‡∏ñ‡πâ‡∏≤ self.year ‡πÄ‡∏õ‡πá‡∏ô None
        ) 
        # üéØ FIX: ‡πÉ‡∏ä‡πâ get_doc_type_collection_key ‡πÅ‡∏ó‡∏ô _get_collection_name
        collection_name = get_doc_type_collection_key(self.doc_type, self.enabler)
        
        retriever = manager.get_retriever(collection_name=collection_name, top_k=self.top_k, final_k=self.final_k) 
        
        if not retriever:
            raise ValueError(f"Retriever not found for collection '{collection_name}' at path based on tenant={self.tenant}, year={self.year}")
        
        return retriever

class MultiDocRetriever(BaseRetriever): # FIX: ‡πÑ‡∏°‡πà‡∏°‡∏µ BaseModel ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á Metaclass Conflict
    # üéØ FIX: Pydantic Fields (‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ _ ‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ö Input)
    retrievers_list: List[NamedRetriever] = Field(default_factory=list)
    k_per_doc: int = Field(default=INITIAL_TOP_K)
    doc_ids_filter: Optional[Set[str]] = Field(default=None) 
    
    # Reranking fields 
    compressor: Optional[BaseDocumentCompressor] = Field(default=None)
    final_k: int = Field(default=FINAL_K_RERANKED)
    
    # üéØ Internal Fields (‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏ó‡∏∏‡∏Å‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡πÉ‡∏ô __init__ ‡πÅ‡∏•‡∏∞ exclude=True)
    _executor: Optional[Union[ThreadPoolExecutor, ProcessPoolExecutor]] = Field(default=None, exclude=True)
    _executor_type: Optional[str] = Field(default=None, exclude=True) 
    _executor_mode: Optional[str] = Field(default=None, exclude=True)
    _all_retrievers: Dict[str, Any] = Field(default_factory=dict, exclude=True)
    _doc_ids_filter_list: Optional[List[str]] = Field(default=None, exclude=True) 
    _chroma_filter: Optional[Dict[str, Any]] = Field(default=None, exclude=True)
    _manager: Optional['VectorStoreManager'] = Field(default=None, exclude=True) 
    _is_running: bool = Field(default=False, exclude=True) # ‡πÄ‡∏û‡∏¥‡πà‡∏° is_running ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö cleanup
    _lock: threading.Lock = Field(default_factory=threading.Lock, exclude=True) # ‡πÄ‡∏û‡∏¥‡πà‡∏° lock
    
    # NOTE: _retrievers_list ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô _get_relevant_documents
    # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏°‡∏±‡∏ô‡∏ñ‡∏π‡∏Å‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å self.retrievers_list
    _retrievers_list: List[NamedRetriever] = Field(default_factory=list, exclude=True) 

    class Config:
        arbitrary_types_allowed = True
    
    # -------------------- Property: num_workers (FIXED) --------------------
    @property
    def num_workers(self) -> int:
        """Calculates the optimal number of workers for the current executor type."""
        # ‡∏î‡∏∂‡∏á MAX_PARALLEL_WORKERS ‡∏à‡∏≤‡∏Å globals()
        max_workers_from_config = globals().get('MAX_PARALLEL_WORKERS', 4) 
        
        # ‡∏î‡∏∂‡∏á _executor_type ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡πÅ‡∏•‡πâ‡∏ß
        # ‡πÉ‡∏ä‡πâ getattr ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏ñ‡πâ‡∏≤ __init__ ‡∏¢‡∏±‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏°‡πà‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå
        executor_type = getattr(self, '_executor_type', 'thread') 
        
        if executor_type == "process":
            # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Process Pool ‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏≥‡∏Å‡∏±‡∏î
            return max(1, min(max_workers_from_config, os.cpu_count() - 1 if os.cpu_count() else 4))
        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Thread Pool ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤‡πÑ‡∏î‡πâ
        return max_workers_from_config

    # -------------------- Initializer --------------------
    def __init__(self, **data: Any) -> None:
        """Initializes the MultiDocRetriever and its internal state."""
        
        # 1. ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å Pydantic init ‡∏Å‡πà‡∏≠‡∏ô
        super().__init__(**data)

        # 2. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡πâ‡∏Å‡∏±‡∏ö Internal Fields ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ object.__setattr__
        #    ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏Å‡∏≤‡∏£‡∏ñ‡∏π‡∏Å‡∏î‡∏±‡∏Å‡πÇ‡∏î‡∏¢ Pydantic V1 __setattr__ (FIXED)
        
        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
        object.__setattr__(self, '_retrievers_list', self.retrievers_list)
        
        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Executor Type
        executor_type_val = self._choose_executor()
        object.__setattr__(self, '_executor_type', executor_type_val)
        
        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Executor instance
        object.__setattr__(self, '_executor', self._initialize_executor())
        object.__setattr__(self, '_is_running', True)
        
        # 3. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Retriever
        object.__setattr__(self, '_all_retrievers', {
            r.doc_id: r for r in self.retrievers_list
        })
        
        # 4. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Doc ID Filter
        if self.doc_ids_filter:
            doc_ids_list = list(self.doc_ids_filter)
            object.__setattr__(self, '_doc_ids_filter_list', doc_ids_list)
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á Chroma Filter
            chroma_filter = {"$or": [{"chunk_uuid": {"$in": doc_ids_list}}]}
            object.__setattr__(self, '_chroma_filter', chroma_filter)
        
        # 5. Logging (‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ num_workers ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡πÅ‡∏•‡πâ‡∏ß)
        if self._executor_type == "process":
            logger.info(f"Initialized MultiDocRetriever using ProcessPoolExecutor ({self.num_workers} workers).")
        else:
            logger.info(f"Initialized MultiDocRetriever using ThreadPoolExecutor ({self.num_workers} threads).")
            
    # -------------------- Executor Management --------------------
    
    def _initialize_executor(self) -> Union[ThreadPoolExecutor, ProcessPoolExecutor]:
        """Initializes the appropriate executor."""
        # NOTE: ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å _get_executor ‡∏°‡∏µ logic ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏Å‡πá‡∏ö instance ‡∏≠‡∏¢‡∏π‡πà
        # ‡πÄ‡∏£‡∏≤‡∏à‡∏∂‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏°‡∏±‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏á‡πÜ ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢
        return self._get_executor() 
        
    def _choose_executor(self) -> str:
        # NOTE: ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤ ENV_FORCE_MODE, _detect_system(), _detect_torch_device() ‡∏ñ‡∏π‡∏Å import
        # ‡∏î‡∏∂‡∏á ENV_FORCE_MODE ‡∏à‡∏≤‡∏Å globals()
        ENV_FORCE_MODE = globals().get('ENV_FORCE_MODE', None) 
        
        if ENV_FORCE_MODE == "process":
            return "process"
        if ENV_FORCE_MODE == "thread":
            return "thread"
            
        # NOTE: ‡∏ñ‡πâ‡∏≤ _detect_system, _detect_torch_device ‡πÄ‡∏õ‡πá‡∏ô Global functions
        # ‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏°‡∏±‡∏ô‡∏ú‡πà‡∏≤‡∏ô globals() ‡∏´‡∏£‡∏∑‡∏≠ import ‡∏°‡∏±‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏á‡πÜ
        system = globals().get('_detect_system', lambda: {'platform': 'unknown', 'cpu_count': 1, 'total_ram_gb': 1})()
        _detect_torch_device_func = globals().get('_detect_torch_device', lambda: 'cpu')
        
        if _detect_torch_device_func() == "mps" or system['platform'] == 'darwin':
            return "thread"
        
        if system['cpu_count'] >= 4 and (system['total_ram_gb'] is None or system['total_ram_gb'] > 8):
            return "process"
            
        return "thread" 

    def _get_executor(self) -> Union[ThreadPoolExecutor, ProcessPoolExecutor]:
        if self._executor is None:
            # ‡πÉ‡∏ä‡πâ self.num_workers ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô Property ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÅ‡∏•‡πâ‡∏ß
            workers = self.num_workers 
            
            # üìå FIX: ‡πÉ‡∏ä‡πâ object.__setattr__ ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤ self._executor ‡πÉ‡∏ô lazy init
            if self._executor_type == "process":
                new_executor = ProcessPoolExecutor(max_workers=workers)
                logger.info(f"üõ†Ô∏è Using ProcessPoolExecutor with {workers} workers.")
            else:
                new_executor = ThreadPoolExecutor(max_workers=workers)
                logger.info(f"üõ†Ô∏è Using ThreadPoolExecutor with {workers} workers.")
            
            object.__setattr__(self, '_executor', new_executor)
            
        return self._executor
    
    # (‡πÄ‡∏°‡∏ò‡∏≠‡∏î get_relevant_documents, _choose_executor, shutdown, __del__, 
    # _static_retrieve_task, _thread_retrieve_task, _get_relevant_documents 
    # ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡πÉ‡∏ä‡πâ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÉ‡∏´‡πâ‡∏°‡∏≤‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î)

    # ... (‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏•‡∏≤‡∏™) ...
    
    def shutdown(self):
        with self._lock: # ‡πÉ‡∏ä‡πâ lock ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô race condition
            if self._executor and self._is_running:
                executor_type_name = "ProcessPoolExecutor" if self._executor_type == "process" else "ThreadPoolExecutor"
                # ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ self.num_workers ‡πÑ‡∏î‡πâ‡πÅ‡∏•‡πâ‡∏ß
                workers = self.num_workers
                
                logger.info(f"Shutting down MultiDocRetriever's {executor_type_name} executor ({workers} workers).")
                self._executor.shutdown(wait=True)
                object.__setattr__(self, '_executor', None)
                object.__setattr__(self, '_is_running', False)

    def __del__(self):
        try:
            self.shutdown()
        except Exception:
            pass


    @staticmethod
    def _static_retrieve_task(named_r: NamedRetriever, query: str, chroma_filter: Optional[Dict]):
        """Static task method for ProcessPoolExecutor."""
        # NOTE: ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ñ‡∏π‡∏Å‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å‡∏°‡∏≤‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°
        try:
            # load_instance ensures the correct VSM context is used
            retriever_instance = named_r.load_instance()
            if not retriever_instance:
                return []
                
            # Prepare config for the invoke method of the RerankRetriever
            search_kwargs = {"k": named_r.top_k}
            if chroma_filter:
                # The Chroma filter is applied as 'where' in Chroma's implementation
                # üéØ FIX: ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ key 'where' ‡πÉ‡∏ô config ‡∏Ç‡∏≠‡∏á invoke()
                search_kwargs["where"] = chroma_filter 
                
            # üéØ FIX: ‡∏ï‡πâ‡∏≠‡∏á‡∏™‡πà‡∏á filter ‡πÄ‡∏õ‡πá‡∏ô 'where' ‡πÉ‡∏ô config ‡∏î‡πâ‡∏ß‡∏¢
            config = {"configurable": {"search_kwargs": {"where": chroma_filter}}}
            if not chroma_filter:
                config = {"configurable": {"search_kwargs": {}}} # ‡∏™‡πà‡∏á config ‡πÄ‡∏õ‡∏•‡πà‡∏≤‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ filter
            
            # retriever_instance is a RerankRetriever (which implements Runnable.invoke)
            docs = retriever_instance.invoke(query, config=config)
            
            for doc in docs:
                doc.metadata["retrieval_source"] = named_r.doc_type
                # üéØ FIX: ‡πÉ‡∏ä‡πâ get_doc_type_collection_key ‡πÅ‡∏ó‡∏ô _get_collection_name
                doc.metadata["collection_name"] = get_doc_type_collection_key(named_r.doc_type, named_r.enabler)
                
                # üéØ FIX 1A: Ensure chunk_uuid is present for final filtering.
                chunk_uuid = doc.metadata.get("chunk_uuid") 
                
                if not chunk_uuid:
                    # Try to find the ID from common Langchain/Chroma internal keys
                    potential_uuid = doc.metadata.get("id") or doc.metadata.get("_id") 
                    
                    if potential_uuid:
                        doc.metadata["chunk_uuid"] = str(potential_uuid)
                    else:
                        # Final Fallback: Use a stable hash of the content/metadata for deduplication/ID
                        key_content = f"{doc.page_content}{doc.metadata.get('doc_id')}"
                        hashed_uuid = hashlib.sha256(key_content.encode('utf-8')).hexdigest()
                        doc.metadata["chunk_uuid"] = hashed_uuid[:32] # Use first 32 chars for uniqueness
                        
            return docs
        except Exception as e:
            # Use print here as logger might not be configured correctly in child process
            print(f"‚ùå Child retrieval error for {named_r.doc_id} ({named_r.doc_type}): {e}")
            return []

    def _thread_retrieve_task(self, named_r: NamedRetriever, query: str, chroma_filter: Optional[Dict]):
        """Instance method for ThreadPoolExecutor."""
        # NOTE: ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ñ‡∏π‡∏Å‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å‡∏°‡∏≤‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°
        try:
            # load_instance ensures the correct VSM context is used
            retriever_instance = named_r.load_instance()
            if not retriever_instance:
                return []
                
            # Prepare config for the invoke method of the RerankRetriever
            search_kwargs = {"k": named_r.top_k}
            if chroma_filter:
                # The Chroma filter is applied as 'where' in Chroma's implementation
                # üéØ FIX: ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ key 'where' ‡πÉ‡∏ô config ‡∏Ç‡∏≠‡∏á invoke()
                search_kwargs["where"] = chroma_filter 
                
            # üéØ FIX: ‡∏ï‡πâ‡∏≠‡∏á‡∏™‡πà‡∏á filter ‡πÄ‡∏õ‡πá‡∏ô 'where' ‡πÉ‡∏ô config ‡∏î‡πâ‡∏ß‡∏¢
            config = {"configurable": {"search_kwargs": {"where": chroma_filter}}}
            if not chroma_filter:
                config = {"configurable": {"search_kwargs": {}}} # ‡∏™‡πà‡∏á config ‡πÄ‡∏õ‡∏•‡πà‡∏≤‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ filter

            # retriever_instance is a RerankRetriever (which implements Runnable.invoke)
            docs = retriever_instance.invoke(query, config=config)
            
            for doc in docs:
                doc.metadata["retrieval_source"] = named_r.doc_type
                # üéØ FIX: ‡πÉ‡∏ä‡πâ get_doc_type_collection_key ‡πÅ‡∏ó‡∏ô _get_collection_name
                doc.metadata["collection_name"] = get_doc_type_collection_key(named_r.doc_type, named_r.enabler)

                # üéØ FIX 1A: Ensure chunk_uuid is present for final filtering.
                chunk_uuid = doc.metadata.get("chunk_uuid") 
                
                if not chunk_uuid:
                    # Try to find the ID from common Langchain/Chroma internal keys
                    potential_uuid = doc.metadata.get("id") or doc.metadata.get("_id") 
                    
                    if potential_uuid:
                        doc.metadata["chunk_uuid"] = str(potential_uuid)
                    else:
                        # Final Fallback: Use a stable hash of the content/metadata for deduplication/ID
                        key_content = f"{doc.page_content}{doc.metadata.get('doc_id')}"
                        hashed_uuid = hashlib.sha256(key_content.encode('utf-8')).hexdigest()
                        doc.metadata["chunk_uuid"] = hashed_uuid[:32] # Use first 32 chars for uniqueness
                    
            return docs
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Thread retrieval error for {named_r.doc_id}: {e}")
            return []

    def _get_relevant_documents(self, query: str, *, run_manager: Any = None) -> List[LcDocument]:
        # ‡πÉ‡∏ä‡πâ self.num_workers ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì max_workers ‡πÉ‡∏ô‡πÄ‡∏°‡∏ò‡∏≠‡∏î‡∏ô‡∏µ‡πâ
        max_workers = self.num_workers
        # NOTE: self._retrievers_list ‡∏ñ‡∏π‡∏Å‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡πÉ‡∏ô __init__ ‡πÅ‡∏•‡πâ‡∏ß
        num_retrievers = len(self._retrievers_list) 
        
        # ‡∏õ‡∏£‡∏±‡∏ö max_workers ‡πÉ‡∏´‡πâ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡∏à‡∏≥‡∏ô‡∏ß‡∏ô retriever
        max_workers = min(num_retrievers, self.num_workers)
        if max_workers <= 0:
            max_workers = 1
            
        chosen = self._executor_type
        logger.info(f"‚öôÔ∏è Running MultiDocRetriever with {chosen} executor ({max_workers} workers) [Filter: {bool(self._chroma_filter)}]")
        all_docs: List[LcDocument] = []
        
        executor = self._get_executor()
        futures = []
        for named_r in self._retrievers_list:
            if chosen == "process":
                # Use static method for ProcessPoolExecutor
                future = executor.submit(MultiDocRetriever._static_retrieve_task, named_r, query, self._chroma_filter)
            else:
                # Use instance method for ThreadPoolExecutor
                future = executor.submit(self._thread_retrieve_task, named_r, query, self._chroma_filter)
            futures.append(future)
            
        for f in futures:
            try:
                docs = f.result()
                if docs:
                    all_docs.extend(docs)
            except Exception as e:
                logger.warning(f"Future failed: {e}")
                
        # Deduplication using chunk metadata
        seen = set()
        unique_docs = []
        for d in all_docs:
            src = d.metadata.get("retrieval_source") or ""
            # Use 'chunk_uuid' or 'ids' (which is the UUID from Chroma) for unique identification
            # NOTE: chunk_uuid should now be present due to the fix in the task methods
            chunk_uuid = d.metadata.get("chunk_uuid") or d.metadata.get("ids") or "" 
            
            # Fallback to content if UUIDs are missing (less reliable)
            if not chunk_uuid:
                 # Use a hash or truncated content as a fallback unique key
                 key = f"{src}_{d.page_content[:120]}_{d.metadata.get('doc_id', 'no_doc_id')}"
            else:
                 key = f"{src}_{chunk_uuid}"
                 
            if key not in seen:
                seen.add(key)
                unique_docs.append(d)
                
        logger.info(f"üìù Query='{query[:80]}...' found {len(unique_docs)} unique docs across sources (Executor={chosen})")
        
        for d in unique_docs:
            score = d.metadata.get("relevance_score")
            if score is not None:
                logger.debug(f" - [Reranked] Source={d.metadata.get('doc_type')}, Score={score:.4f}, Content='{d.page_content[:80]}...'")
        
        return unique_docs

    def get_relevant_documents(self, query: str, **kwargs) -> List[LcDocument]:
        return self._get_relevant_documents(query, **kwargs)

# -------------------- END OF MultiDocRetriever --------------------
# -------------------- load_all_vectorstores --------------------
def load_all_vectorstores(
    tenant: str = DEFAULT_TENANT, 
    year: int = DEFAULT_YEAR, 
    doc_ids: Optional[Set[str]] = None,
    doc_types: Optional[Union[str, List[str]]] = None,
    enabler_filter: Optional[str] = None,
    top_k: int = INITIAL_TOP_K,
    final_k: int = FINAL_K_RERANKED
) -> 'VectorStoreManager':
    """
    Initializes the VSM and the main MultiDocRetriever for the current assessment context.
    Improved with Case-Insensitive matching for Mac/Linux environments.
    """
    # 1. Initialize VSM (Singleton)
    manager = VectorStoreManager(
        tenant=tenant, 
        year=year, 
    )
    
    # 2. Prepare the list of target collection keys
    target_collection_keys: Set[str] = set()
    
    # ‡∏™‡πÅ‡∏Å‡∏ô‡∏´‡∏≤ collections ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô Tenant/Year
    existing_collections = list_vectorstore_folders(tenant, year, doc_type=None, enabler=None) 
    
    # üéØ [FIX] ‡∏™‡∏£‡πâ‡∏≤‡∏á Mapping ‡πÅ‡∏ö‡∏ö Case-Insensitive (key=‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡πá‡∏Å, value=‡∏ä‡∏∑‡πà‡∏≠‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á)
    # ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏£‡∏ì‡∏µ‡πÉ‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô 'evidence_km' ‡πÅ‡∏ï‡πà‡∏£‡∏∞‡∏ö‡∏ö‡∏™‡πà‡∏á 'evidence_KM'
    existing_map = {c.lower(): c for c in existing_collections}
    
    # Filtering Logic
    if doc_types:
        if isinstance(doc_types, str):
            doc_types = [dt.strip() for dt in doc_types.split(',')]
        
        for dt in doc_types:
            dt_norm = dt.lower().strip()
            
            if dt_norm == EVIDENCE_DOC_TYPES.lower():
                # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô doc_type 'evidence' ‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á enabler
                if enabler_filter:
                    # ‡πÅ‡∏õ‡∏•‡∏á enabler ‡πÄ‡∏õ‡πá‡∏ô list ‡πÅ‡∏•‡∏∞‡∏•‡πâ‡∏≤‡∏á‡∏Ñ‡πà‡∏≤‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á
                    enabler_list = [e.strip() for e in enabler_filter.split(',')]
                    for enabler in enabler_list:
                        # ‡∏™‡∏£‡πâ‡∏≤‡∏á key ‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á (‡∏°‡∏±‡∏Å‡∏à‡∏∞‡πÑ‡∏î‡πâ evidence_KM ‡∏´‡∏£‡∏∑‡∏≠ evidence_km)
                        key_expected = get_doc_type_collection_key(dt_norm, enabler).lower()
                        
                        if key_expected in existing_map:
                            # ‚úÖ ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡∏à‡∏£‡∏¥‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠‡πÉ‡∏ô Folder (‡πÄ‡∏ä‡πà‡∏ô 'evidence_km')
                            target_collection_keys.add(existing_map[key_expected])
                        else:
                            logger.warning(
                                f"üîç DEBUG: Skipping collection '{key_expected}' "
                                f"(Not found in existing: {list(existing_map.keys())})."
                            )
                else:
                    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏ enabler ‡πÉ‡∏´‡πâ‡πÄ‡∏≠‡∏≤ evidence ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ evidence_
                    for c_low, c_orig in existing_map.items():
                        if c_low.startswith(f"{EVIDENCE_DOC_TYPES.lower()}_"):
                            target_collection_keys.add(c_orig)
            else: 
                # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö doc_type ‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏°‡∏õ‡∏µ (‡πÄ‡∏ä‡πà‡∏ô document, faq)
                key_gen = get_doc_type_collection_key(dt_norm, None).lower()
                if key_gen in existing_map:
                     target_collection_keys.add(existing_map[key_gen])
                
                # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏£‡∏ì‡∏µ collection ‡∏ä‡∏∑‡πà‡∏≠ doc_type_all
                key_all = get_doc_type_collection_key(dt_norm, "ALL").lower()
                if key_all in existing_map:
                     target_collection_keys.add(existing_map[key_all])
    
    else:
        # ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏ doc_types ‡πÄ‡∏•‡∏¢ ‡πÉ‡∏´‡πâ‡πÇ‡∏´‡∏•‡∏î‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏™‡πÅ‡∏Å‡∏ô‡πÄ‡∏à‡∏≠
        target_collection_keys.update(existing_collections)

    logger.info(f"üîç DEBUG: Attempting to load {len(target_collection_keys)} total target collections: {target_collection_keys}")

    # 3. Build NamedRetriever objects
    all_retrievers: List[NamedRetriever] = []
    
    for collection_name in target_collection_keys:
        # ‡πÅ‡∏¢‡∏Å‡∏ä‡∏∑‡πà‡∏≠ collection ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤ doc_type ‡πÅ‡∏•‡∏∞ enabler (e.g., 'evidence_km' -> ['evidence', 'km'])
        parts = collection_name.split('_')
        doc_type_for_check = parts[0]
        # ‡∏£‡∏±‡∏Å‡∏©‡∏≤ Case ‡∏Ç‡∏≠‡∏á enabler ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ (‡∏°‡∏±‡∏Å‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÉ‡∏´‡∏ç‡πà)
        enabler_for_check = parts[1].upper() if len(parts) > 1 else None
        
        # üéØ ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á: evidence ‡πÉ‡∏ä‡πâ‡∏õ‡∏µ‡∏à‡∏≤‡∏Å config, ‡∏≠‡∏∑‡πà‡∏ô‡πÜ (Global) ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ‡∏õ‡∏µ
        target_year = year
        if doc_type_for_check.lower() != EVIDENCE_DOC_TYPES.lower():
            target_year = None
            
        nr = NamedRetriever(
            doc_id=collection_name, 
            doc_type=doc_type_for_check, 
            enabler=enabler_for_check, 
            top_k=top_k, 
            final_k=final_k, 
            tenant=tenant, 
            year=target_year
        )
        all_retrievers.append(nr)
        logger.info(f"üîç DEBUG: Added retriever for '{collection_name}' (Year={target_year}, Enabler={enabler_for_check}).")

    if not all_retrievers:
        # ‡∏û‡πà‡∏ô Error ‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏ Path ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Debug ‡∏´‡∏ô‡πâ‡∏≤‡∏á‡∏≤‡∏ô
        debug_vstore_path = f"data_store/{tenant}/vectorstore/{year}"
        raise ValueError(
            f"No vectorstore collections found matching:\n"
            f" - Path: {debug_vstore_path}\n"
            f" - DocTypes: {doc_types}\n"
            f" - Enabler: {enabler_filter}\n"
            f"Please check if ChromaDB folders exist in the path above."
        )
        
    # 4. Initialize MultiDocRetriever (MDR)
    
    # 4.1 Prepare Reranker (Compressor)
    reranker = None
    if final_k > 0:
        reranker = get_global_reranker()
        if reranker is None:
             logger.warning("‚ùå Reranker failed to initialize. Reranking disabled.")
             final_k = top_k 
        else:
             logger.info(f"‚úÖ Reranker initialized ({reranker.rerank_model}).")
             

    # 4.2 Create MDR instance
    mdr = MultiDocRetriever( 
        retrievers_list=all_retrievers, 
        k_per_doc=top_k, 
        doc_ids_filter=doc_ids,
        compressor=reranker, 
        final_k=final_k
    )
    
    # 5. Set MDR in VSM (Singleton)
    manager._multi_doc_retriever = mdr
    
    return manager


def get_multi_doc_retriever(
    tenant: str = DEFAULT_TENANT,
    year: int = DEFAULT_YEAR,
    doc_types: List[str] = [],
    doc_ids: Optional[List[str]] = None,
    evidence_enabler: Optional[str] = None,
    base_path: str = "", # üéØ FIX: base_path ‡∏ñ‡∏π‡∏Å‡∏•‡∏∞‡πÄ‡∏•‡∏¢
    top_k: int = INITIAL_TOP_K,
    final_k: int = FINAL_K_RERANKED
) -> MultiDocRetriever:
    """
    Factory function to create a MultiDocRetriever based on configuration.
    It determines which NamedRetrievers to initialize based on the tenant, year, and doc_types.
    """
    all_retrievers: List[NamedRetriever] = []

    # 1. Dynamic Check for Year-Specific Collections
    # Loop through requested doc_types and check against the target year
    target_year = year
    for doc_type_for_check in doc_types:
        # üéØ FIX: ‡πÉ‡∏ä‡πâ get_doc_type_collection_key ‡πÅ‡∏ó‡∏ô _get_collection_name
        collection_name = get_doc_type_collection_key(doc_type_for_check, evidence_enabler)
        
        enabler_for_check = evidence_enabler
        
        # Check if collection exists for the specific year
        if not vectorstore_exists(tenant=tenant, year=target_year, doc_type=doc_type_for_check, enabler=enabler_for_check):
            logger.warning(f"üîç DEBUG: Skipping collection '{collection_name}' (vectorstore_exists failed at tenant={tenant}, year={target_year}).")
            continue
            
        # üéØ FIX 2C: ‡∏™‡πà‡∏á target_year ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô NamedRetriever
        nr = NamedRetriever(
            doc_id=collection_name, 
            doc_type=doc_type_for_check, 
            enabler=enabler_for_check, 
            top_k=top_k, 
            final_k=final_k, 
            # üéØ FIX: ‡∏•‡∏ö base_path ‡∏≠‡∏≠‡∏Å
            tenant=tenant, 
            year=target_year # <--- ‡∏™‡πà‡∏á‡∏Ñ‡πà‡∏≤‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
        )
        all_retrievers.append(nr)
        logger.info(f"üîç DEBUG: Successfully added retriever for collection '{collection_name}' (Year={target_year}).")

    final_filter_ids = doc_ids
    if doc_ids:
        logger.info(f"‚úÖ Hard Filter Enabled: Using {len(doc_ids)} original 64-char UUIDs for filtering.")
    logger.info(f"üîç DEBUG: Final count of all_retrievers = {len(all_retrievers)}")

    if not all_retrievers:
        raise ValueError(f"No vectorstore collections found matching tenant={tenant}, year={year}, doc_types={doc_types} and evidence_enabler={evidence_enabler}")
        
    mdr = MultiDocRetriever(retrievers_list=all_retrievers, k_per_doc=top_k, doc_ids_filter=final_filter_ids)
    return mdr