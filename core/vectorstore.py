#core/vectorstore.py
import os
import platform
import logging
import threading
import multiprocessing
import json
import shutil
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from typing import List, Optional, Union, Sequence, Any, Dict, Set, Tuple


# system utils
try:
    import psutil
except ImportError:
    psutil = None

# LangChain imports (‡∏£‡∏∏‡πà‡∏ô‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö V1.x)
from langchain_core.documents import Document as LcDocument
from langchain_core.retrievers import BaseRetriever
from langchain_core.documents import BaseDocumentCompressor 


# External libraries
# ‚úÖ FIXED: ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤ PrivateAttr, ConfigDict, BaseModel ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡∏à‡∏≤‡∏Å pydantic
from pydantic import PrivateAttr, ConfigDict, BaseModel
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
import chromadb
from chromadb.config import Settings

# Logging
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)


# üü¢ FIX (Option C): ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà Flashrank ‡∏î‡πâ‡∏ß‡∏¢ Sentence Transformers CrossEncoder
try:
    from sentence_transformers import CrossEncoder 
except ImportError:
    # Placeholder class to avoid crash if not installed
    logger.warning("‚ö†Ô∏è CrossEncoder not found. Install 'sentence-transformers' to use this reranker.")
    class CrossEncoder:
        def __init__(self, model_name, device, max_length): pass
        def predict(self, sentences, show_progress_bar): return [0] * len(sentences)

# Configure chromadb telemetry if available
try:
    chromadb.configure(anonymized_telemetry=False)
except Exception:
    try:
        chromadb.settings = Settings(anonymized_telemetry=False)
    except Exception:
        pass

# -------------------- Global Config --------------------
# NOTE: ‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤ import ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
# ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ‡∏ñ‡∏π‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡πÅ‡∏•‡∏∞ config.global_vars ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ñ‡∏π‡∏Å‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡πÑ‡∏î‡πâ
from config.global_vars import (
    VECTORSTORE_DIR,
    MAPPING_FILE_PATH,
    FINAL_K_RERANKED,
    INITIAL_TOP_K,
    EVIDENCE_DOC_TYPES,
    MAX_PARALLEL_WORKERS,
)

# -------------------- Vectorstore Constants --------------------
ENV_FORCE_MODE = os.getenv("VECTOR_MODE", "").lower()  # "thread", "process", or ""

# Global caches (per process)
_CACHED_EMBEDDINGS = None
_EMBED_LOCK = threading.Lock()
_MPS_WARNING_SHOWN = False

# -------------------- Helper: detect environment & device --------------------
def _detect_system():
    """Return dict with cpu_count and total_ram_gb (may be None if psutil missing)."""
    cpu_count = os.cpu_count() or 4
    total_ram_gb = None
    if psutil:
        try:
            total_ram_gb = psutil.virtual_memory().total / (1024 ** 3)
        except Exception:
            total_ram_gb = None
    return {"cpu_count": cpu_count, "total_ram_gb": total_ram_gb, "platform": platform.system().lower()}


def _detect_torch_device():
    """
    Return best device string for HuggingFaceEmbeddings: 'cuda'|'mps'|'cpu' when available.
    """
    # avoid importing torch at top-level if not installed; check safely
    try:
        import torch
        if torch.cuda.is_available():
            return "cuda"
        # mac mps support: torch.backends.mps.is_available() may exist
        # üü¢ CLEANUP: ‡πÉ‡∏ä‡πâ platform.system() ‡πÅ‡∏ó‡∏ô sys.platform ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö platform
        if platform.system().lower() == "darwin" and hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
            return "mps"
    except Exception:
        pass
    # If no accelerator is found (or torch not installed), default to 'cpu'.
    return "cpu"


def get_hf_embeddings(device_hint: Optional[str] = None):
    """
    Return a HuggingFaceEmbeddings instance (cached per process).
    """
    global _CACHED_EMBEDDINGS, _MPS_WARNING_SHOWN
    device = device_hint or _detect_torch_device()

    # Safety: MPS + multiprocessing is fragile on macOS.
    sys_info = _detect_system()
    force_mode = ENV_FORCE_MODE
    
    # ‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢ (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ Thread pool ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô Process-parallel)
    using_process = (force_mode == "process") or (sys_info["cpu_count"] >= 8 and (sys_info["total_ram_gb"] or 0) >= 16)

    if device == "mps" and using_process and not _MPS_WARNING_SHOWN:
        logger.warning("‚ö†Ô∏è Detected MPS but running in process-parallel mode: forcing device -> cpu to avoid MPS multi-process failures")
        _MPS_WARNING_SHOWN = True
        device = "cpu"

    # allow env override to disable GPU/MPS: VECTOR_DISABLE_ACCEL=1
    if os.getenv("VECTOR_DISABLE_ACCEL", "").lower() in ("1", "true", "yes"):
        device = "cpu"

    if _CACHED_EMBEDDINGS is None:
        with _EMBED_LOCK:
            if _CACHED_EMBEDDINGS is None:
                try:
                    model_name = "intfloat/multilingual-e5-large"
                    logger.info(f"üì¶ Creating HuggingFaceEmbeddings (model={model_name}, device={device})")
                    
                    _CACHED_EMBEDDINGS = HuggingFaceEmbeddings(
                        model_name=model_name, 
                        model_kwargs={"device": device},
                        # query_instruction="query: ", 
                        # encode_kwargs={'normalize_embeddings': True} 
                    )

                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Failed to create embeddings on device={device}: {e}. Falling back to CPU.")
                    _CACHED_EMBEDDINGS = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2", model_kwargs={"device": "cpu"})
    return _CACHED_EMBEDDINGS
    

# -------------------- Vectorstore helpers (REVISED/CLEANED) --------------------

def _get_collection_name(doc_type: str, enabler: Optional[str] = None) -> str:
    """
    Calculates the Chroma collection name and directory name based on doc_type and enabler.
    """
    doc_type_norm = doc_type.strip().lower()

    if doc_type_norm == EVIDENCE_DOC_TYPES:
        enabler_norm = (enabler or "km").strip().lower() 
        collection_name = f"{doc_type_norm}_{enabler_norm}"
    else:
        collection_name = doc_type_norm
        
    logger.critical(f"üß≠ DEBUG: _get_collection_name(doc_type={doc_type}, enabler={enabler}) => {collection_name}")
    
    return collection_name

def get_vectorstore_path(doc_type: Optional[str] = None, enabler: Optional[str] = None) -> str:
    """
    Returns the full path to the base dir or the specific collection directory.
    Uses _get_collection_name logic.
    """
    if not doc_type:
        return VECTORSTORE_DIR
    
    collection_name = _get_collection_name(doc_type, enabler)
    return os.path.join(VECTORSTORE_DIR, collection_name)

def list_vectorstore_folders(base_path: str = VECTORSTORE_DIR, doc_type: Optional[str] = None, enabler: Optional[str] = None) -> List[str]:
    """
    Lists available Chroma collections (which are folders inside VECTORSTORE_DIR).
    Returns collection names (e.g., 'document', 'evidence_km').
    """
    if not os.path.exists(base_path):
        return []
    
    folders = [f for f in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, f))]
    
    if doc_type:
        doc_type_norm = doc_type.lower().strip()
        
        if doc_type_norm == EVIDENCE_DOC_TYPES and not enabler:
            # Special case: 'evidence' without enabler means list ALL evidence_*
            return [f for f in folders if f.startswith(f"{EVIDENCE_DOC_TYPES}_")]
            
        # Specific collection requested (e.g., 'document' or 'evidence_km')
        collection_name = _get_collection_name(doc_type_norm, enabler)
        return [collection_name] if collection_name in folders else []
        
    return folders


def vectorstore_exists(doc_id: str, base_path: str = VECTORSTORE_DIR, doc_type: Optional[str] = None, enabler: Optional[str] = None) -> bool:
    """
    Checks if a Chroma collection exists on disk.
    doc_type and enabler define the collection name.
    """
    if not doc_type:
        return False
    
    collection_name = _get_collection_name(doc_type, enabler)
    path = os.path.join(base_path, collection_name)
    file_path = os.path.join(path, "chroma.sqlite3")
    
    if not os.path.isdir(path):
        logger.warning(f"‚ùå V-Exists Check 1: Directory not found for collection '{collection_name}' at {path}")
        return False
        
    if os.path.isfile(file_path):
        return True
        
    logger.error(f"‚ùå V-Exists Check 3: FAILED to find file chroma.sqlite3 at {file_path} for collection '{collection_name}'")
    return False

# =================================================================
# Custom HuggingFace Cross-Encoder Compressor
# =================================================================

class HuggingFaceCrossEncoderCompressor(BaseDocumentCompressor, BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)
    
    # üü¢ FIX Pydantic NameError: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠ fields ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏Ç‡πâ‡∏≠‡∏Ç‡∏±‡∏î‡πÅ‡∏¢‡πâ‡∏á‡∏Å‡∏±‡∏ö Pydantic/LangChain
    # rerank_model: str = "cross-encoder/ms-marco-MiniLM-L-6-v2"
    rerank_model: str = "intfloat/multilingual-e5-large" # <<-- ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ
    rerank_device: str = _detect_torch_device() 
    rerank_max_length: int = 512
    _cross_encoder: Any = PrivateAttr(None)
    
    def __init__(self, **data):
        super().__init__(**data)
        # Initialization logic is now fully handled in get_global_reranker
        pass 
        
    def set_encoder_instance(self, encoder: Any):
        """Method to manually set the globally created CrossEncoder instance."""
        self._cross_encoder = encoder

    def compress_documents(
        self,
        documents: Sequence[LcDocument],
        query: str,
        top_n: int, 
        callbacks: Optional[Any] = None,
    ) -> List[LcDocument]:
        
        if not documents:
            return []
        # Check for predict method
        if self._cross_encoder is None or not hasattr(self._cross_encoder, 'predict'):
            logger.error("HuggingFace Cross-Encoder is not initialized. Returning truncated documents.")
            return list(documents)[:top_n]

        # 1. Prepare inputs: (query, document_text) pairs
        # Sentence Transformers CrossEncoder expects a list of [query, document] pairs
        sentence_pairs = [[query, doc.page_content] for doc in documents]

        # 2. Perform Reranking (Prediction)
        try:
            # Predict returns a list of scores (logits)
            scores = self._cross_encoder.predict(sentence_pairs, show_progress_bar=False)
        except Exception as e:
            logger.error(f"‚ùå Cross-Encoder prediction failed: {e}. Returning truncated documents.")
            return list(documents)[:top_n]

        # 3. Combine documents and scores, then sort
        doc_scores = sorted(
            zip(documents, scores), key=lambda x: x[1], reverse=True
        )

        # 4. Map back to LcDocuments and apply top_n
        final_docs = []
        for doc, score in doc_scores[:top_n]:
            # Add the relevance score to the metadata
            doc.metadata["relevance_score"] = float(score)
            final_docs.append(doc)
            
        return final_docs


# B. ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô get_global_reranker: ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Encoder ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß

_CACHED_RERANKER_INSTANCE: Optional[HuggingFaceCrossEncoderCompressor] = None
_CACHED_CROSS_ENCODER: Any = None # Instance ‡∏Ç‡∏≠‡∏á sentence_transformers.CrossEncoder ‡∏à‡∏£‡∏¥‡∏á‡πÜ

def get_global_reranker(final_k: int) -> Optional[HuggingFaceCrossEncoderCompressor]:
    """
    Return a global (cached) HuggingFaceCrossEncoderCompressor instance.
    The actual CrossEncoder is initialized only once.
    """
    global _CACHED_RERANKER_INSTANCE, _CACHED_CROSS_ENCODER
    
    if _CACHED_RERANKER_INSTANCE is None:
        try:
            # 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á Compressor Wrapper Instance
            instance = HuggingFaceCrossEncoderCompressor()
            
            if _CACHED_CROSS_ENCODER is None:
                # üü¢ FIX: ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠ fields ‡πÉ‡∏´‡∏°‡πà
                model_name = instance.rerank_model 
                device = instance.rerank_device     
                
                logger.info(f"üì¶ Initializing global CrossEncoder (model={model_name}, device={device})")
                
                # 2. Try to create the actual CrossEncoder object
                try: 
                    from sentence_transformers import CrossEncoder # Import again inside the function for robustness
                    
                    # Call the CrossEncoder constructor
                    _CACHED_CROSS_ENCODER = CrossEncoder(
                        model_name_or_path=model_name, # <--- **‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ**
                        device=device,
                        # üü¢ FIX: ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠ fields ‡πÉ‡∏´‡∏°‡πà
                        max_length=instance.rerank_max_length 
                    )
                    logger.info("‚úÖ CrossEncoder initialized successfully.")

                except ImportError:
                    logger.error("‚ùå FATAL: sentence-transformers library not found. Cannot initialize CrossEncoder.")
                    _CACHED_CROSS_ENCODER = None
                    
                except Exception as encoder_e:
                    # Catch PyTorch/device/download errors
                    logger.error(f"‚ùå FATAL: CrossEncoder constructor failed: {encoder_e}. Try running in CPU mode.")
                    _CACHED_CROSS_ENCODER = None 
                    
            # 3. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏Å‡∏≥‡∏´‡∏ô‡∏î Encoder ‡πÉ‡∏´‡πâ‡∏Å‡∏±‡∏ö Singleton
            if _CACHED_CROSS_ENCODER and hasattr(_CACHED_CROSS_ENCODER, 'predict'):
                 instance.set_encoder_instance(_CACHED_CROSS_ENCODER)
                 _CACHED_RERANKER_INSTANCE = instance
                 logger.critical("‚úÖ Reranker set to HuggingFace Cross-Encoder.")
            else:
                 logger.error("‚ùå HuggingFace Cross-Encoder failed to initialize or missing 'predict' method. Reranking disabled.")
                 return None
                 
        except Exception as e:
            logger.error(f"‚ùå Failed to create global HuggingFaceCrossEncoderCompressor: {e}")
            _CACHED_RERANKER_INSTANCE = None 
            return None
    
    return _CACHED_RERANKER_INSTANCE
    
    
# -------------------- VECTORSTORE MANAGER (SINGLETON) --------------------
class VectorStoreManager:
    """
    Singleton class to manage and cache Chroma vectorstore instances (collections).
    Handles initialization of Embeddings, Reranker, and Doc ID Mapping.
    """
    _instance = None
    _is_initialized = False
    
    _chroma_cache: Dict[str, Chroma] = PrivateAttr({}) 
    _multi_doc_retriever: Optional['MultiDocRetriever'] = PrivateAttr(None) 
    _lock = threading.Lock()
    
    _doc_id_mapping: Dict[str, Dict[str, Any]] = PrivateAttr({}) 
    _uuid_to_doc_id: Dict[str, str] = PrivateAttr({})
    
    _embeddings: Any = PrivateAttr(None)


    def __new__(cls, *args, **kwargs):
        with cls._lock:
            if cls._instance is None:
                cls._instance = super(VectorStoreManager, cls).__new__(cls)
        return cls._instance

    def __init__(self, base_path: str = VECTORSTORE_DIR):
        if not self._is_initialized:
            self._base_path = base_path
            self._chroma_cache = {}
            self._embeddings = get_hf_embeddings()
            
            self._load_doc_id_mapping()
            
            logger.info(f"Initialized VectorStoreManager. Loaded {len(self._doc_id_mapping)} stable doc IDs.")
            VectorStoreManager._is_initialized = True

    # ‡πÄ‡∏û‡∏¥‡πà‡∏° close/del method ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Executor ‡∏Ç‡∏≠‡∏á MultiDocRetriever (‡∏´‡∏≤‡∏Å‡∏°‡∏µ)
    def close(self):
        """Cleanly shuts down all managed resources, including MultiDocRetriever's executor."""
        with self._lock:
            # 1. Shutdown MultiDocRetriever's Executor
            if self._multi_doc_retriever and hasattr(self._multi_doc_retriever, 'shutdown'):
                logger.info("Closing MultiDocRetriever executor via VSM.")
                self._multi_doc_retriever.shutdown()
                self._multi_doc_retriever = None
                
            # 2. Clear caches
            self._chroma_cache = {}
            # 3. ‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ Singleton
            VectorStoreManager._is_initialized = False

    def __del__(self):
        """Fallback cleanup for the VSM Singleton."""
        self.close()
    
    def _load_doc_id_mapping(self):
            """Loads doc_id_mapping.json into memory."""
            self._doc_id_mapping = {}
            self._uuid_to_doc_id = {}
            try:
                with open(MAPPING_FILE_PATH, 'r', encoding='utf-8') as f:
                    mapping_data: Dict[str, Dict[str, Any]] = json.load(f)
                    
                    # FIX: ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î (strip) ‡∏Ñ‡∏µ‡∏¢‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Ç‡∏≠‡∏á Dictionary ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î
                    cleaned_mapping = {k.strip(): v for k, v in mapping_data.items()}
                    
                    self._doc_id_mapping = cleaned_mapping
                    
                    for doc_id, doc_entry in cleaned_mapping.items(): 
                        if isinstance(doc_entry, dict) and 'chunk_uuids' in doc_entry and isinstance(doc_entry.get('chunk_uuids'), list):
                            for uid in doc_entry['chunk_uuids']:
                                self._uuid_to_doc_id[uid] = doc_id
                            
                logger.info(f"‚úÖ Loaded Doc ID Mapping: {len(self._doc_id_mapping)} original documents, {len(self._uuid_to_doc_id)} total chunks.")
            except FileNotFoundError:
                logger.warning(f"‚ö†Ô∏è Doc ID Mapping file not found at {MAPPING_FILE_PATH}. This is expected if no documents have been ingested yet.")
            except Exception as e:
                logger.error(f"‚ùå Failed to load Doc ID Mapping: {e}")

    def _re_parse_collection_name(self, collection_name: str) -> Tuple[str, Optional[str]]:
        """Helper to safely re-parse collection name back to doc_type and enabler."""
        collection_name_lower = collection_name.strip().lower()
        if collection_name_lower.startswith(f"{EVIDENCE_DOC_TYPES}_"):
            parts = collection_name_lower.split("_", 1)
            # Return 'evidence' as doc_type, and the enabler part (uppercase)
            return EVIDENCE_DOC_TYPES, parts[1].upper() if len(parts) == 2 else None
            
        return collection_name_lower, None 

    def _load_chroma_instance(self, collection_name: str) -> Optional[Chroma]:
        """Loads a Chroma instance from disk or returns from cache."""
        if collection_name in self._chroma_cache:
            return self._chroma_cache[collection_name]

        with self._lock:
            if collection_name in self._chroma_cache:
                return self._chroma_cache[collection_name]
            
            persist_directory = os.path.join(self._base_path, collection_name)
            
            doc_type, enabler = self._re_parse_collection_name(collection_name)
            
            if not vectorstore_exists(doc_id="N/A", base_path=self._base_path, doc_type=doc_type, enabler=enabler):
                logger.warning(f"‚ö†Ô∏è Chroma collection '{collection_name}' folder not found at {persist_directory}")
                return None

            try:
                vectordb = Chroma(
                    persist_directory=persist_directory, 
                    embedding_function=self._embeddings,
                    collection_name=collection_name
                )
                self._chroma_cache[collection_name] = vectordb
                logger.info(f"‚úÖ Loaded Chroma instance for collection: {collection_name}")
                return vectordb
            except Exception as e:
                logger.error(f"‚ùå Failed to load Chroma collection '{collection_name}': {e}")
                return None

    def get_documents_by_id(self, stable_doc_ids: Union[str, List[str]], doc_type: str = "default_collection", enabler: Optional[str] = None) -> List[LcDocument]:
                """
                Retrieves chunks (Documents) from a specific Chroma collection 
                using their **Stable Document UUIDs** (64-char IDs).
                """
                if isinstance(stable_doc_ids, str):
                    stable_doc_ids = [stable_doc_ids]
                    
                stable_doc_ids = [uid.strip() for uid in stable_doc_ids if uid]
                if not stable_doc_ids:
                    return []
                    
                collection_name = _get_collection_name(doc_type, enabler)
                chroma_instance = self._load_chroma_instance(collection_name)

                if not chroma_instance:
                    logger.warning(f"Cannot retrieve documents: Collection '{collection_name}' is not loaded.")
                    return []
                
                try:
                    collection = chroma_instance._collection
                    
                    # FIX: ‡πÉ‡∏ä‡πâ "doc_id" ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏µ‡∏¢‡πå‡πÉ‡∏ô where clause ‡πÅ‡∏•‡∏∞‡∏£‡∏ß‡∏° 'ids'
                    result = collection.get(
                        where={"stable_doc_uuid": {"$in": stable_doc_ids}}, 
                        include=['documents', 'metadatas']
                    )
                    
                    documents: List[LcDocument] = []
                    num_docs = len(result.get('documents', []))
                    
                    for i in range(num_docs):
                        text = result['documents'][i]
                        if text:
                            metadata = result.get('metadatas', [{}])[i]
                            chunk_uuid_from_result = result.get('ids', [''])[i]
                            
                            doc_id = metadata.get("doc_id", "UNKNOWN") 
                            
                            metadata["chunk_uuid"] = chunk_uuid_from_result
                            metadata["doc_id"] = doc_id
                            metadata["doc_type"] = doc_type 
                            
                            documents.append(LcDocument(page_content=text, metadata=metadata))
                    
                    logger.info(f"‚úÖ Retrieved {len(documents)} documents for {len(stable_doc_ids)} Stable IDs from '{collection_name}'.")
                    return documents
                    
                except Exception as e:
                    logger.error(f"‚ùå Error retrieving documents by Stable IDs from collection '{collection_name}': {e}")
                    return []

    def retrieve_by_chunk_ids(self, chunk_ids: List[str], collection_name: str) -> List[LcDocument]:
            """
            [NEW] Retrieves a list of LangChain Document objects based on their internal chunk UUIDs 
            (‡∏ã‡∏∂‡πà‡∏á‡∏ñ‡∏π‡∏Å‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô internal ID ‡πÉ‡∏ô ChromaDB ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Persistent Mapping).

            Args:
                chunk_ids: List ‡∏Ç‡∏≠‡∏á chunk_uuid strings (Chroma IDs).
                collection_name: ‡∏ä‡∏∑‡πà‡∏≠‡∏Ç‡∏≠‡∏á Collection ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤.
                
            Returns:
                List of LcDocument objects (‡∏´‡∏£‡∏∑‡∏≠‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ß‡πà‡∏≤‡∏á‡∏ñ‡πâ‡∏≤‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à).
            """
            if not chunk_ids:
                return []
            
            try:
                # 1. ‡πÇ‡∏´‡∏•‡∏î Chroma instance
                chroma_instance = self._load_chroma_instance(collection_name)

                if not chroma_instance:
                    logger.error(f"VSM: Collection '{collection_name}' ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏∂‡∏á chunk IDs.")
                    return []
                    
                # 2. ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á internal Chroma collection
                collection = chroma_instance._collection 
                    
                # 3. ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Vector Store ‡∏î‡πâ‡∏ß‡∏¢ ID (‡∏ã‡∏∂‡πà‡∏á‡∏Ñ‡∏∑‡∏≠ chunk_id)
                retrieval_result = collection.get(
                    ids=chunk_ids,
                    # ‡∏î‡∏∂‡∏á content, metadata ‡πÅ‡∏•‡∏∞ IDs ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤
                    include=['documents', 'metadatas'] 
                )
                
                # 4. ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö LcDocument
                retrieved_docs: List[LcDocument] = []
                
                documents = retrieval_result.get('documents', [])
                metadatas = retrieval_result.get('metadatas', [])
                ids = retrieval_result.get('ids', []) # Internal IDs (chunk_uuid)
                
                num_results = len(documents)
                if num_results != len(chunk_ids):
                    logger.warning(f"VSM: ‡∏î‡∏∂‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÑ‡∏î‡πâ {num_results} ‡∏ä‡∏¥‡πâ‡∏ô, ‡∏£‡πâ‡∏≠‡∏á‡∏Ç‡∏≠ {len(chunk_ids)} ‡∏ä‡∏¥‡πâ‡∏ô.")
                    
                # ‡∏ß‡∏ô‡∏ã‡πâ‡∏≥‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á LcDocument
                for content, metadata, chunk_id in zip(documents, metadatas, ids):
                    if content and isinstance(metadata, dict):
                        
                        # üìå ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ chunk_uuid (Chroma ID)
                        metadata['chunk_uuid'] = chunk_id
                        
                        # üìå ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Stable Doc ID ‡∏à‡∏≤‡∏Å Mapping ‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ß‡πâ‡πÉ‡∏ô __init__
                        stable_doc_id = self._uuid_to_doc_id.get(chunk_id, metadata.get('stable_doc_uuid', "UNKNOWN"))
                        metadata["doc_id"] = stable_doc_id 
                        
                        # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ doc_type ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ
                        metadata["doc_type"] = metadata.get("doc_type", self._re_parse_collection_name(collection_name)[0])
                            
                        # ‡∏™‡∏£‡πâ‡∏≤‡∏á LcDocument
                        retrieved_docs.append(LcDocument(
                            page_content=content,
                            metadata=metadata
                        ))
                    
                logger.info(f"VSM: ‡∏î‡∏∂‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ Priority ‡πÑ‡∏î‡πâ {len(retrieved_docs)} ‡∏ä‡∏¥‡πâ‡∏ô‡∏à‡∏≤‡∏Å Persistent Map ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö '{collection_name}'.")
                return retrieved_docs

            except Exception as e:
                logger.error(f"VSM: Error ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏° chunk ID ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö collection '{collection_name}': {e}")
                return []
        
    def get_limited_chunks_from_doc_ids(
            self, 
            stable_doc_ids: Union[str, List[str]], 
            query: Union[str, List[str]], # üìå ‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ‡∏ó‡∏±‡πâ‡∏á str ‡πÅ‡∏•‡∏∞ List[str]
            doc_type: str, 
            enabler: Optional[str] = None, 
            limit_per_doc: int = 5 
        ) -> List[LcDocument]:
            """
            Retrieves a limited number of chunks (Documents) for a list of Stable Document IDs 
            by performing a similarity search on the documents' chunks.
            """
            if isinstance(stable_doc_ids, str):
                stable_doc_ids = [stable_doc_ids]
                
            stable_doc_ids = [uid for uid in stable_doc_ids if uid]
            if not stable_doc_ids:
                return []
                
            # üìå FIX 1: ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ä‡πâ Query ‡∏ï‡∏±‡∏ß‡πÅ‡∏ó‡∏ô (Query ‡πÅ‡∏£‡∏Å) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Vector Search
            if isinstance(query, list):
                # ‡πÉ‡∏ä‡πâ query ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÅ‡∏ó‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Similarity Search
                vector_search_query = query[0] if query else ""
            else:
                vector_search_query = query
                
            if not vector_search_query:
                logger.warning("Limited chunk search skipped: Query is empty.")
                return []
                
            collection_name = _get_collection_name(doc_type, enabler)
            
            # 1. ‡πÇ‡∏´‡∏•‡∏î Chroma Instance
            chroma_instance = self._load_chroma_instance(collection_name) 

            if not chroma_instance:
                logger.error(f"Collection '{collection_name}' is not loaded.")
                return []

            all_limited_documents: List[LcDocument] = []
            total_chunks_retrieved = 0
            
            # 2. ‡∏ß‡∏ô‡∏ã‡πâ‡∏≥‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥ Similarity Search ‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏° Stable ID ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡∏±‡∏ß
            for stable_id in stable_doc_ids:
                stable_id_clean = stable_id.strip()

                # 2.1 ‡∏™‡∏£‡πâ‡∏≤‡∏á Filter ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Chunks ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô Stable ID ‡∏ô‡∏µ‡πâ
                doc_filter = {
                    "stable_doc_uuid": stable_id_clean
                }
                
                # 2.2 ‡πÉ‡∏ä‡πâ ChromaRetriever ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î K ‡∏ä‡∏¥‡πâ‡∏ô
                try:
                    # ‡πÉ‡∏ä‡πâ ChromaRetriever ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á similarity_search ‡∏û‡∏£‡πâ‡∏≠‡∏° filter
                    custom_retriever = ChromaRetriever(
                        vectorstore=chroma_instance,
                        k=limit_per_doc, # K ‡∏ñ‡∏π‡∏Å‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏ï‡∏≤‡∏° limit_per_doc
                        filter=doc_filter
                    )
                    
                    # ‡∏ó‡∏≥ Similarity Search ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Query ‡∏ï‡∏±‡∏ß‡πÅ‡∏ó‡∏ô
                    limited_docs = custom_retriever.get_relevant_documents(query=vector_search_query) # üìå FIX 2: ‡πÉ‡∏ä‡πâ vector_search_query

                    # 2.3 ‡πÄ‡∏û‡∏¥‡πà‡∏° metadata ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö tracking
                    for doc in limited_docs:
                        doc.metadata['priority_search_type'] = 'limited_vector_search'
                        doc.metadata['priority_limit'] = limit_per_doc
                        all_limited_documents.append(doc)
                        
                    total_chunks_retrieved += len(limited_docs)

                except Exception as e:
                    logger.error(f"‚ùå Error performing limited vector search for Stable ID '{stable_id_clean}': {e}")
                    continue 
            
            logger.info(f"‚úÖ Retrieved {total_chunks_retrieved} limited chunks (max {limit_per_doc}/doc) for {len(stable_doc_ids)} Stable IDs from '{collection_name}' using vector search.")
            return all_limited_documents

# -------------------- Retriever Creation --------------------
    def get_retriever(self, collection_name: str, top_k: int = INITIAL_TOP_K, final_k: int = FINAL_K_RERANKED, use_rerank: bool = True) -> Any:
        """
        Loads the base Chroma retriever for the given collection, and returns a wrapper
        that applies reranking and final truncation (final_k).
        """
        # 1. Load Chroma instance
        chroma_instance = self._load_chroma_instance(collection_name)
        if not chroma_instance:
            logger.warning(f"Retriever creation failed: Collection '{collection_name}' not loaded.")
            return None 
            
        # 2. Create the base retriever (Chroma as_retriever)
        search_kwargs = {"k": top_k}
        try:
            base_retriever = chroma_instance.as_retriever(search_type="similarity", search_kwargs=search_kwargs)
        except Exception as e:
            logger.error(f"Failed to create base retriever for '{collection_name}': {e}")
            return None

        # 3. Apply Rerank/Truncation Wrapper
        
        if use_rerank:
            # Wrapper Function: ‡πÉ‡∏ä‡πâ invoke_with_rerank ‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ö config (search_kwargs) ‡∏à‡∏≤‡∏Å MultiDocRetriever
            def invoke_with_rerank(query: str, config: Optional[Dict] = None):
                
                # Initialize docs to prevent NameError
                docs = [] 
                
                # 1. ‡∏î‡∏∂‡∏á filter ‡∏à‡∏≤‡∏Å config ‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏°‡∏≤‡∏à‡∏≤‡∏Å MultiDocRetriever
                chroma_filter = config.get('configurable', {}).get('search_kwargs', {}).get('filter') if config else None
                
                try: # ‡∏´‡πà‡∏≠‡∏´‡∏∏‡πâ‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ base_retriever.invoke() ‡∏î‡πâ‡∏ß‡∏¢ try-except
                    # 2. ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ filter ‡πÉ‡∏´‡πâ override search_kwargs ‡∏Ç‡∏≠‡∏á base_retriever
                    if chroma_filter:
                        # ‡πÉ‡∏ä‡πâ top_k ‡πÄ‡∏î‡∏¥‡∏°‡∏Ç‡∏≠‡∏á collection ‡∏ô‡∏µ‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Å‡πà‡∏≠‡∏ô Rerank
                        new_config = {'configurable': {"search_kwargs": {"k": top_k, "filter": chroma_filter}}}
                        docs = base_retriever.invoke(query, config=new_config)
                    else:
                        docs = base_retriever.invoke(query, config=config)
                except Exception as e:
                    logger.error(f"‚ùå Retrieval failed before rerank: {e}")
                    return [] # ‡∏´‡∏≤‡∏Å‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏•‡∏¥‡∏™‡∏ï‡πå‡∏ß‡πà‡∏≤‡∏á‡∏ó‡∏±‡∏ô‡∏ó‡∏µ (docs ‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡πÄ‡∏õ‡πá‡∏ô [])
                
                # 3. Perform Reranking
                try:
                    # 4. ‡πÉ‡∏ä‡πâ HuggingFaceCrossEncoderCompressor (‡∏ú‡πà‡∏≤‡∏ô get_global_reranker)
                    # NOTE: final_k ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏™‡πà‡∏á‡πÑ‡∏õ‡πÉ‡∏ô get_global_reranker (‡πÉ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î 380) ‡∏ñ‡∏π‡∏Å‡πÉ‡∏ä‡πâ‡πÅ‡∏Ñ‡πà‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Init ‡∏ï‡∏±‡∏ß Compressor
                    reranker = get_global_reranker(final_k) 

                    if reranker and hasattr(reranker, 'compress_documents'):
                        # // üü¢ FIX: Rerank ‡πÅ‡∏•‡∏∞ TRUNCATE ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏ó‡∏µ‡πà top_k (30/50) ‡πÅ‡∏ó‡∏ô final_k (5)
                        # // ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ llm_data_utils.py ‡πÑ‡∏î‡πâ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÄ‡∏ï‡πá‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡πÅ‡∏•‡πâ‡∏ß‡πÑ‡∏õ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ï‡πà‡∏≠
                        return reranker.compress_documents(docs, query, top_n=top_k) 
                    
                    logger.warning("‚ö†Ô∏è Reranker not available. Returning base docs truncated.")
                    return docs[:top_k] 
                    
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Rerank failed: {e}. Returning base docs truncated to {top_k}")
                    return docs[:top_k] 

            # Wrapper class: ‡∏´‡πà‡∏≠‡∏´‡∏∏‡πâ‡∏°‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô invoke
            class SimpleRetrieverWrapper(BaseRetriever):
                model_config = ConfigDict(arbitrary_types_allowed=True)
                def invoke(self, query: str, config: Optional[Dict] = None):
                    return invoke_with_rerank(query, config=config)
                # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö BaseRetriever ‡πÉ‡∏´‡∏°‡πà, ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ _get_relevant_documents
                def _get_relevant_documents(self, query: str, *, run_manager: Any = None) -> List[LcDocument]:
                    config = None # LangChain config is typically passed via the 'invoke' method.
                    return invoke_with_rerank(query, config=config)


            return SimpleRetrieverWrapper()
        else:
            # No Rerank, just Truncate
            class TruncatedRetrieverWrapper(BaseRetriever):
                model_config = ConfigDict(arbitrary_types_allowed=True)
                def invoke(self, query: str, config: Optional[Dict] = None):
                    # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á filter ‡∏°‡∏≤ ‡πÉ‡∏´‡πâ override search_kwargs ‡∏Ç‡∏≠‡∏á base_retriever
                    chroma_filter = config.get('configurable', {}).get('search_kwargs', {}).get('filter') if config else None
                    if chroma_filter:
                        new_config = {'configurable': {"search_kwargs": {"k": top_k, "filter": chroma_filter}}}
                        docs = base_retriever.invoke(query, config=new_config)
                    else:
                        docs = base_retriever.invoke(query, config=config)
                        
                    return docs[:top_k] 
                
                def _get_relevant_documents(self, query: str, *, run_manager: Any = None) -> List[LcDocument]:
                    config = run_manager.get_session_info() if run_manager else None
                    return self.invoke(query, config=config)

            return TruncatedRetrieverWrapper()

    def get_all_collection_names(self) -> List[str]:
        """Returns a list of all available collection names (folders in VECTORSTORE_DIR)."""
        return list_vectorstore_folders(base_path=self._base_path)
    
    def get_chunks_from_doc_ids(self, stable_doc_ids: Union[str, List[str]], doc_type: str, enabler: Optional[str] = None) -> List[LcDocument]:
            """
            Retrieves chunks (Documents) for a list of Stable Document IDs from a specific collection.
            """
            if isinstance(stable_doc_ids, str):
                stable_doc_ids = [stable_doc_ids]
                
            stable_doc_ids = [uid for uid in stable_doc_ids if uid]
            if not stable_doc_ids:
                return []
                
            collection_name = _get_collection_name(doc_type, enabler)

            all_chunk_uuids = []
            skipped_docs = []
            found_stable_ids = []

            # 1. ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Chunk UUIDs ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏à‡∏≤‡∏Å Mapping
            for stable_id in stable_doc_ids:
                stable_id_clean = stable_id.strip()

                if stable_id_clean in self._doc_id_mapping:
                    doc_entry = self._doc_id_mapping[stable_id_clean] 
                    
                    if isinstance(doc_entry, dict) and 'chunk_uuids' in doc_entry and isinstance(doc_entry.get('chunk_uuids'), list):
                        chunk_uuids = doc_entry['chunk_uuids']
                        if chunk_uuids:
                            all_chunk_uuids.extend(chunk_uuids)
                            found_stable_ids.append(stable_id_clean) 
                        else:
                            logger.warning(f"Mapping found for Stable ID '{stable_id_clean}' but 'chunk_uuids' list is empty.")
                    else:
                        logger.warning(f"Mapping entry for Stable ID '{stable_id_clean}' is malformed or missing 'chunk_uuids'.")
                else:
                    skipped_docs.append(stable_id_clean)
                    
            if skipped_docs:
                logger.warning(f"Skipping Stable IDs not found in mapping: {skipped_docs}")

            if not all_chunk_uuids:
                logger.warning(f"No valid chunk UUIDs found for provided Stable Document IDs: {skipped_docs}. Check doc_id_mapping.json.")
                return []
                
            # 2. ‡πÇ‡∏´‡∏•‡∏î Chroma Instance
            chroma_instance = self._load_chroma_instance(collection_name) 

            if not chroma_instance:
                logger.error(f"Collection '{collection_name}' is not loaded.")
                return []

            # 3. Fetch data by Chunk IDs (Chroma UUIDs)
            try:
                collection = chroma_instance._collection
                result = collection.get(
                    ids=all_chunk_uuids,
                    include=['documents', 'metadatas'] 
                )
                
                # 4. Process results into LangChain Documents
                documents: List[LcDocument] = []
                
                if not result.get('documents'):
                    logger.warning(f"Chroma DB returned 0 documents for {len(all_chunk_uuids)} chunk UUIDs in collection '{collection_name}'.")
                    return []
                    
                for i, text in enumerate(result.get('documents', [])):
                    if text:
                        metadata = result.get('metadatas', [{}])[i]
                        chunk_uuid_from_result = result.get('ids', [''])[i]
                        doc_id = self._uuid_to_doc_id.get(chunk_uuid_from_result, "UNKNOWN")
                        
                        metadata["chunk_uuid"] = chunk_uuid_from_result
                        metadata["doc_id"] = doc_id
                        metadata["doc_type"] = doc_type
                        
                        documents.append(LcDocument(page_content=text, metadata=metadata))
                
                logger.info(f"‚úÖ Retrieved {len(documents)} chunks for {len(found_stable_ids)} Stable IDs from '{collection_name}'.")
                return documents
                
            except Exception as e:
                logger.error(f"‚ùå Error retrieving documents by Chunk UUIDs from collection '{collection_name}': {e}")
                return []

# Helper function to get the manager instance
def get_vectorstore_manager() -> VectorStoreManager:
    """Returns the singleton instance of VectorStoreManager."""
    return VectorStoreManager()
    
# Backward compatibility function 
def load_vectorstore(doc_type: str, enabler: Optional[str] = None) -> Optional[Chroma]:
    """Helper for other modules to load a Chroma instance directly."""
    collection_name = _get_collection_name(doc_type, enabler)
    return get_vectorstore_manager()._load_chroma_instance(collection_name)


class VectorStoreExecutorSingleton:
    _instance = None
    _is_initialized = False

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            cls._instance = super(VectorStoreExecutorSingleton, cls).__new__(cls)
        return cls._instance

    def __init__(self):
        if not VectorStoreExecutorSingleton._is_initialized:
            self.max_workers = MAX_PARALLEL_WORKERS
            self._executor = ThreadPoolExecutor(max_workers=self.max_workers)
            logger.info(f"Initialized VectorStoreExecutorSingleton (ThreadPoolExecutor with {self.max_workers} workers) for batch tasks.")
            VectorStoreExecutorSingleton._is_initialized = True

    @property
    def executor(self) -> ThreadPoolExecutor:
        """Returns the shared ThreadPoolExecutor."""
        return self._executor

    def close(self):
        """Shutdown the executor when the application is done."""
        if self._is_initialized:
            logger.info("Shutting down VectorStoreExecutorSingleton ThreadPoolExecutor...")
            self._executor.shutdown(wait=True)
            VectorStoreExecutorSingleton._is_initialized = False 

def get_vectorstore() -> VectorStoreExecutorSingleton:
    """
    REQUIRED by ingest_batch.py. Returns the singleton instance managing the executor.
    """
    return VectorStoreExecutorSingleton()
    
def load_all_vectorstores_dummy():
    """Placeholder for loading all vectorstores if needed globally."""
    pass


# -------------------- Custom Retriever for Chroma --------------------
class ChromaRetriever(BaseRetriever):
    """
    A simple custom retriever wrapper that uses the underlying Chroma vectorstore
    instance to retrieve documents with specific k and filter parameters.
    """
    vectorstore: Any
    k: int
    filter: Optional[Dict] = None
    
    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÉ‡∏´‡πâ pydantic ‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà Base Model
    model_config = ConfigDict(arbitrary_types_allowed=True) 

    def _get_relevant_documents(self, query: str, *, run_manager=None) -> List[LcDocument]:
        """
        Synchronous retrieval method for the Chroma instance.
        """
        try:
            # self.vectorstore ‡∏Ñ‡∏∑‡∏≠ Chroma instance
            # Chroma.similarity_search ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö query, k, ‡πÅ‡∏•‡∏∞ filter ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
            return self.vectorstore.similarity_search(
                query=query, 
                k=self.k, 
                filter=self.filter 
            )
        except Exception as e:
            logger.error(f"‚ùå Chroma similarity_search failed in custom retriever: {e}")
            return []
    
    # Method ‡∏™‡∏≤‡∏ò‡∏≤‡∏£‡∏ì‡∏∞‡∏ó‡∏µ‡πà‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ (get_relevant_documents)
    def get_relevant_documents(self, query: str, **kwargs) -> List[LcDocument]:
        """Public synchronous method for retrieval."""
        return self._get_relevant_documents(query, **kwargs)

    async def _aget_relevant_documents(self, query: str, *, run_manager=None) -> List[LcDocument]:
        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Chroma, ‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ sync method ‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
        return self._get_relevant_documents(query, run_manager=run_manager)

# -------------------- END Custom Retriever --------------------

# -------------------- MultiDoc / Parallel Retriever --------------------
class NamedRetriever(BaseModel):
    # BaseRetriever requires model_config
    model_config = ConfigDict(arbitrary_types_allowed=True) 
    
    doc_id: str
    doc_type: str
    top_k: int
    final_k: int
    base_path: str = VECTORSTORE_DIR
    enabler: Optional[str] = None

    def load_instance(self) -> Any:
        manager = VectorStoreManager(base_path=self.base_path)
        # set use_rerank=True 
        retriever = manager.get_retriever(
            collection_name=_get_collection_name(self.doc_type, self.enabler),
            top_k=self.top_k,
            final_k=self.final_k,
            use_rerank=True
        )
        if not retriever:
            raise ValueError(f"Retriever not found for collection '{_get_collection_name(self.doc_type, self.enabler)}' at path '{self.base_path}'")
        return retriever


class MultiDocRetriever(BaseRetriever):
    """
    Combine multiple NamedRetriever sources. Choose executor automatically (thread vs process).
    """
    model_config = ConfigDict(arbitrary_types_allowed=True) 

    _all_retrievers: Dict[str, Any] = PrivateAttr(default_factory=dict)
    
    _retrievers_list: list[NamedRetriever] = PrivateAttr()
    _k_per_doc: int = PrivateAttr()
    _manager: VectorStoreManager = PrivateAttr() 
    _doc_ids_filter: Optional[List[str]] = PrivateAttr()
    _chroma_filter: Optional[Dict[str, Any]] = PrivateAttr()
    _executor_type: str = PrivateAttr()
    _executor: Union[ThreadPoolExecutor, ProcessPoolExecutor, None] = PrivateAttr(None)

    def __init__(self, retrievers_list: list[NamedRetriever], k_per_doc: int = INITIAL_TOP_K, doc_ids_filter: Optional[List[str]] = None):
        super().__init__()
        self._retrievers_list = retrievers_list
        self._k_per_doc = k_per_doc
        self._manager = VectorStoreManager() 
        
        # Initialize _all_retrievers ‡πÅ‡∏•‡∏∞‡πÇ‡∏´‡∏•‡∏î instance
        self._all_retrievers = {} 
        for named_r in retrievers_list:
            collection_name = _get_collection_name(named_r.doc_type, named_r.enabler)
            try:
                # ‡πÇ‡∏´‡∏•‡∏î Retriever Instance ‡∏î‡πâ‡∏ß‡∏¢ Key ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á (collection_name)
                retriever_instance = named_r.load_instance()
                if retriever_instance:
                    self._all_retrievers[collection_name] = retriever_instance # üëà ‡πÉ‡∏ä‡πâ _all_retrievers
                    logger.info(f"‚úÖ MultiDocRetriever cached collection: {collection_name}")
                else:
                    logger.warning(f"‚ö†Ô∏è Failed to load instance for {collection_name} during MDR init.")
            except Exception as e:
                logger.error(f"‚ùå Error loading instance {collection_name} into MDR cache: {e}")
        # -------------------- END NEW LOGIC --------------------

        # NEW LOGIC: ‡∏™‡∏£‡πâ‡∏≤‡∏á Chroma Filter ‡∏à‡∏≤‡∏Å Doc IDs
        self._doc_ids_filter = doc_ids_filter
        self._chroma_filter = None
        if doc_ids_filter:
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á Chroma DB Metadata Filter: ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ 'doc_id' ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏•‡∏¥‡∏™‡∏ï‡πå
            self._chroma_filter = {
                "doc_id": {"$in": doc_ids_filter} 
            }

            logger.info(f"‚úÖ MultiDocRetriever initialized with doc_ids filter for {len(doc_ids_filter)} Stable IDs.")
            
        self._executor_type = self._choose_executor()
        logger.info(f"MultiDocRetriever will use executor type: {self._executor_type} (workers={MAX_PARALLEL_WORKERS})")
        
    
    # ‡πÄ‡∏°‡∏ò‡∏≠‡∏î _choose_executor ‡∏ó‡∏µ‡πà‡∏´‡∏≤‡∏¢‡πÑ‡∏õ
    def _choose_executor(self) -> str:
        """
        Decide whether to use ProcessPoolExecutor or ThreadPoolExecutor.
        """
        sys_info = _detect_system()
        device = _detect_torch_device()
        force = ENV_FORCE_MODE

        # 1. Force mode if user set
        if force in ("thread", "process"):
            mode = force
            logger.info(f"VECTOR_MODE override: forcing '{mode}' executor")
            return mode
        
        # 2. MPS + Multiprocessing Safety on macOS
        if sys_info["platform"] == "darwin" and device == "mps":
            logger.warning("‚ö†Ô∏è Detected MPS on macOS: forcing executor -> thread to avoid multi-process failures.")
            return "thread"
            
        # 3. Low RAM check 
        if sys_info["total_ram_gb"] and sys_info["total_ram_gb"] < 12:
            logger.warning(f"‚ö†Ô∏è Detected low RAM ({sys_info['total_ram_gb']:.1f}GB): forcing executor -> thread.")
            return "thread"
            
        # 4. High Resource check: Prefer ProcessPoolExecutor
        if sys_info["cpu_count"] >= 8 and (sys_info["total_ram_gb"] or 0) >= 16:
            logger.info("High-resources machine detected -> choosing 'process' executor")
            return "process"

        # 5. Default
        logger.info("Defaulting to 'thread' executor")
        return "thread"

    def shutdown(self):
        """Cleanly shuts down the internal executor if it was created."""
        if self._executor:
            executor_type_name = "ProcessPoolExecutor" if self._executor_type == "process" else "ThreadPoolExecutor"
            workers = self._executor._max_workers if hasattr(self._executor, '_max_workers') else "N/A"
            logger.info(f"Shutting down MultiDocRetriever's {executor_type_name} executor ({workers} workers).")
            
            self._executor.shutdown(wait=True)
            self._executor = None
            
    def __del__(self):
        """Fallback cleanup. Attempts to shutdown the executor when the object is garbage collected."""
        self.shutdown() 
        
    def _get_executor(self) -> Union[ThreadPoolExecutor, ProcessPoolExecutor]:
        """Returns the cached or newly created executor based on the chosen type."""
        if self._executor is None:
            workers = MAX_PARALLEL_WORKERS
            if self._executor_type == "process":
                self._executor = ProcessPoolExecutor(max_workers=workers)
                logger.info(f"üõ†Ô∏è Using ProcessPoolExecutor with {workers} workers.")
            else:
                self._executor = ThreadPoolExecutor(max_workers=workers)
                logger.info(f"üõ†Ô∏è Using ThreadPoolExecutor with {workers} workers.")
        return self._executor

    # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç _static_retrieve_task ‡πÉ‡∏´‡πâ‡∏£‡∏±‡∏ö filter
    @staticmethod
    def _static_retrieve_task(named_r: NamedRetriever, query: str, chroma_filter: Optional[Dict]):
        """
        Static helper used in ProcessPoolExecutor. Executes inside child process.
        """
        try:
            retriever_instance = named_r.load_instance()
            
            # NEW: ‡∏™‡πà‡∏á filter ‡∏ú‡πà‡∏≤‡∏ô search_kwargs
            search_kwargs = {"k": named_r.top_k, "filter": chroma_filter} if chroma_filter else {"k": named_r.top_k}
            config = {'configurable': {'search_kwargs': search_kwargs}}

            docs = retriever_instance.invoke(query, config=config)
            
            # 3. Add source info
            for doc in docs:
                doc.metadata["retrieval_source"] = named_r.doc_type
                doc.metadata["collection_name"] = _get_collection_name(named_r.doc_type, named_r.enabler)
            
            return docs
        except Exception as e:
            # ‡πÉ‡∏ä‡πâ print ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö exceptions ‡πÉ‡∏ô child processes
            print(f"‚ùå Child retrieval error for {named_r.doc_id} ({named_r.doc_type}): {e}")
            return []

    # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç _thread_retrieve_task ‡πÉ‡∏´‡πâ‡∏£‡∏±‡∏ö filter
    def _thread_retrieve_task(self, named_r: NamedRetriever, query: str, chroma_filter: Optional[Dict]):
        """
        Retrieval performed in a thread inside the same process.
        """
        try:
            retriever_instance = named_r.load_instance()
            
            # NEW: ‡∏™‡πà‡∏á filter ‡∏ú‡πà‡∏≤‡∏ô search_kwargs
            search_kwargs = {"k": named_r.top_k, "filter": chroma_filter} if chroma_filter else {"k": named_r.top_k}
            config = {'configurable': {'search_kwargs': search_kwargs}}
            
            docs = retriever_instance.invoke(query, config=config)

            # 3. Add source info
            for doc in docs:
                doc.metadata["retrieval_source"] = named_r.doc_type
                doc.metadata["collection_name"] = _get_collection_name(named_r.doc_type, named_r.enabler)
            
            return docs
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Thread retrieval error for {named_r.doc_id}: {e}")
            return []

    # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç _get_relevant_documents ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á filter ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ
    def _get_relevant_documents(self, query: str, *, run_manager: Any = None) -> List[LcDocument]:
        """
        Runs multiple retrievers in parallel using the chosen executor, and aggregates results.
        This method is required by BaseRetriever.
        """
        max_workers = min(len(self._retrievers_list), MAX_PARALLEL_WORKERS)
        if max_workers <= 0:
            max_workers = 1

        chosen = self._executor_type # ‡πÉ‡∏ä‡πâ _executor_type ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å set ‡πÉ‡∏ô __init__

        logger.info(f"‚öôÔ∏è Running MultiDocRetriever with {chosen} executor ({max_workers} workers) [Filter: {bool(self._chroma_filter)}]")

        all_docs: List[LcDocument] = []
        
        # ‡πÉ‡∏ä‡πâ _get_executor() ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏£‡∏µ‡∏¢‡∏Å Executor ‡∏ó‡∏µ‡πà‡πÅ‡∏Ñ‡∏ä‡πÑ‡∏ß‡πâ
        executor = self._get_executor() 
        
        futures = []
        for named_r in self._retrievers_list:
            if chosen == "process":
                # For process, use the static method
                # NEW: ‡∏™‡πà‡∏á self._chroma_filter ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô task
                future = executor.submit(MultiDocRetriever._static_retrieve_task, named_r, query, self._chroma_filter)
            else:
                # For thread, use the instance method
                # NEW: ‡∏™‡πà‡∏á self._chroma_filter ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô task
                future = executor.submit(self._thread_retrieve_task, named_r, query, self._chroma_filter)
            futures.append(future)

        # Wait for all tasks to complete and collect results
        for f in futures:
            try:
                docs = f.result()
                if docs:
                    all_docs.extend(docs)
            except Exception as e:
                logger.warning(f"Future failed: {e}")

        # Combine results and deduplicate
        seen = set()
        unique_docs = []
        for d in all_docs:
            # dedupe key: source + chunk + doc_id + a snippet
            # ‡πÉ‡∏ä‡πâ metadata ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÇ‡∏î‡∏¢ worker (retrieval_source ‡πÅ‡∏•‡∏∞ collection_name)
            src = d.metadata.get("retrieval_source") or ""
            chunk_uuid = d.metadata.get("chunk_uuid") or d.metadata.get("ids") or ""
            key = f"{src}_{chunk_uuid}_{d.page_content[:120]}" 
            
            if key not in seen:
                seen.add(key)
                unique_docs.append(d)

        logger.info(f"üìù Query='{query[:80]}...' found {len(unique_docs)} unique docs across sources (Executor={chosen})")
        
        # Final log of top documents
        for d in unique_docs:
            score = d.metadata.get("relevance_score")
            if score is not None:
                logger.debug(f" - [Reranked] Source={d.metadata.get('doc_type')}, Score={score:.4f}, Content='{d.page_content[:80]}...'")
        
        return unique_docs
     
    # Required method for BaseRetriever
    def get_relevant_documents(self, query: str, **kwargs) -> List[LcDocument]:
        """Synchronous public method for retrieval."""
        return self._get_relevant_documents(query, **kwargs)


# -------------------- Load single vectorstore retriever (REVISED) --------------------
def load_vectorstore_retriever(doc_id: str, top_k: int = INITIAL_TOP_K, final_k: int = FINAL_K_RERANKED, doc_types: Union[list, str] = "default_collection", base_path: str = VECTORSTORE_DIR, enabler: Optional[str] = None):
    """
    Loads a retriever instance for a specific collection name (doc_type) and optional enabler. 
    """
    if isinstance(doc_types, str):
        target_doc_type = doc_types
    elif isinstance(doc_types, list) and doc_types:
         target_doc_type = doc_types[0] 
    else:
        raise ValueError("doc_types must be a single string or a non-empty list containing the target doc_type.")

    collection_name = _get_collection_name(target_doc_type, enabler)

    manager = VectorStoreManager(base_path=base_path)
    retriever = None

    if vectorstore_exists(doc_id="N/A", base_path=base_path, doc_type=target_doc_type, enabler=enabler):
        retriever = manager.get_retriever(collection_name, top_k, final_k)
    
    if retriever is None:
        raise ValueError(f"‚ùå Vectorstore for collection '{collection_name}' (derived from doc_type='{target_doc_type}' and enabler='{enabler}') not found.")
    return retriever


# -------------------- load_all_vectorstores (FINAL REVISED WITH CONDITIONAL ENABLER FIX) --------------------
def load_all_vectorstores(doc_types: Optional[Union[str, List[str]]] = None,
                          top_k: int = INITIAL_TOP_K,
                          final_k: int = FINAL_K_RERANKED,
                          base_path: str = VECTORSTORE_DIR,
                          evidence_enabler: Optional[str] = None,
                          doc_ids: Optional[List[str]] = None) -> VectorStoreManager: 
    
    """
    Load multiple vectorstore collections as MultiDocRetriever.
    """
    doc_types = [doc_types] if isinstance(doc_types, str) else doc_types or []
    doc_type_filter = {dt.strip().lower() for dt in doc_types}
    
    manager = VectorStoreManager(base_path=base_path)
    all_retrievers: List[NamedRetriever] = []
    
    # 1. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ Collection ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡∏à‡∏£‡∏¥‡∏á ‡πÜ (Collection Name)
    target_collection_names: Set[str] = set()
    
    if not doc_type_filter:
        target_collection_names.update(manager.get_all_collection_names())
    else:
        for dt_norm in doc_type_filter:
            if dt_norm == EVIDENCE_DOC_TYPES:
                if evidence_enabler:
                    collection_name = _get_collection_name(EVIDENCE_DOC_TYPES, evidence_enabler)
                    target_collection_names.add(collection_name)
                    logger.info(f"üîç Added specific evidence collection: {collection_name}")
                else:
                    # ‡πÇ‡∏´‡∏•‡∏î‡∏ó‡∏∏‡∏Å Evidence Collection (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ Enabler ‡∏£‡∏∞‡∏ö‡∏∏)
                    evidence_collections = list_vectorstore_folders(base_path=base_path, doc_type=EVIDENCE_DOC_TYPES)
                    target_collection_names.update(evidence_collections)
                    logger.info(f"üîç Added all evidence collections found: {evidence_collections}")
            else:
                # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö document, faq (‡πÉ‡∏ä‡πâ None ‡πÄ‡∏õ‡πá‡∏ô Enabler)
                collection_name = _get_collection_name(dt_norm, None)
                target_collection_names.add(collection_name)
                logger.info(f"üîç Added standard collection: {collection_name}")
                
    # 2. ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡∏ó‡∏µ‡∏•‡∏∞ Collection
    logger.info(f"üîç DEBUG: Attempting to load {len(target_collection_names)} total target collections: {target_collection_names}")
    
    for collection_name in target_collection_names:
        doc_type_for_check, enabler_for_check = manager._re_parse_collection_name(collection_name)
        
        if not vectorstore_exists(doc_id="N/A", base_path=base_path, doc_type=doc_type_for_check, enabler=enabler_for_check):
            logger.warning(f"üîç DEBUG: Skipping collection '{collection_name}' (vectorstore_exists failed).")
            continue
            
        nr = NamedRetriever(
            doc_id=collection_name, 
            doc_type=doc_type_for_check, # ‡πÉ‡∏ä‡πâ doc_type ‡∏ó‡∏µ‡πà‡πÅ‡∏¢‡∏Å‡∏≠‡∏≠‡∏Å‡∏°‡∏≤ (e.g., 'evidence')
            enabler=enabler_for_check,   # ‡πÉ‡∏ä‡πâ enabler ‡∏ó‡∏µ‡πà‡πÅ‡∏¢‡∏Å‡∏≠‡∏≠‡∏Å‡∏°‡∏≤ (e.g., 'KM')
            top_k=top_k,
            final_k=final_k,
            base_path=base_path
        )
        all_retrievers.append(nr)
        logger.info(f"üîç DEBUG: Successfully added retriever for collection '{collection_name}'.")

    # 3. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Hard Filter ID (‡πÉ‡∏ä‡πâ 64-char ID ‡∏î‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏¥‡∏°‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á)
    final_filter_ids = doc_ids 
    
    if doc_ids:
        logger.info(f"‚úÖ Hard Filter Enabled: Using {len(doc_ids)} original 64-char UUIDs for filtering.")
        
    # 4. ‡∏™‡∏£‡πâ‡∏≤‡∏á MultiDocRetriever
    logger.info(f"üîç DEBUG: Final count of all_retrievers = {len(all_retrievers)}")
    if not all_retrievers:
        raise ValueError(f"No vectorstore collections found matching doc_types={doc_types} and evidence_enabler={evidence_enabler}")

    mdr = MultiDocRetriever(
            retrievers_list=all_retrievers, 
            k_per_doc=top_k,
            doc_ids_filter=final_filter_ids 
        )
        
    # ‡∏ú‡∏π‡∏Å MultiDocRetriever ‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ö VectorStoreManager
    manager._multi_doc_retriever = mdr
    
    # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏à‡∏≤‡∏Å .all_retrievers ‡πÄ‡∏õ‡πá‡∏ô ._all_retrievers
    logger.info(f"‚úÖ MultiDocRetriever loaded with {len(mdr._all_retrievers)} collections and cached in VSM.")

    # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ VectorStoreManager ‡πÅ‡∏ó‡∏ô
    return manager