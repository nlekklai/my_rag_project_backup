# core/vectorstore.py
import os
import platform
import logging
import threading
import multiprocessing
import json
import shutil
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from typing import List, Optional, Union, Sequence, Any, Dict, Set, Tuple
from pathlib import Path
import hashlib
from threading import Lock

# system utils
try:
    import psutil
except ImportError:
    psutil = None

# LangChain-ish imports (adjust to your project's versions)
from langchain_core.documents import Document as LcDocument
from langchain_core.retrievers import BaseRetriever
from langchain_core.documents import BaseDocumentCompressor
from langchain_core.callbacks.manager import CallbackManagerForRetrieverRun
from langchain_core.runnables import Runnable 

# Pydantic helpers
from pydantic import PrivateAttr, ConfigDict, BaseModel

# Chroma / HF embeddings
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
import chromadb
from chromadb.config import Settings

# üí° NEW: Import Path Utilities (‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏ä‡∏∑‡πà‡∏≠‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö utils/path_utils.py ‡πÉ‡∏´‡∏°‡πà)
from utils.path_utils import (
    get_doc_type_collection_key, # ‡πÉ‡∏ä‡πâ‡πÅ‡∏ó‡∏ô _get_collection_name
    get_vectorstore_collection_path, 
    get_vectorstore_tenant_root_path,
    get_mapping_file_path, # ‡πÉ‡∏ä‡πâ‡πÅ‡∏ó‡∏ô‡∏ó‡∏±‡πâ‡∏á year_specific ‡πÅ‡∏•‡∏∞ tenant_root
    # ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ get_vectorstore_collection_parent_dir ‡πÅ‡∏•‡πâ‡∏ß
)


# Logging
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)


# Try import CrossEncoder (sentence-transformers)
try:
    from sentence_transformers import CrossEncoder
    _HAS_SENT_TRANS = True
except Exception:
    CrossEncoder = None
    _HAS_SENT_TRANS = False
    logger.warning("‚ö†Ô∏è sentence-transformers CrossEncoder not available. Reranker will be disabled.")

# Configure chromadb telemetry if available
try:
    chromadb.configure(anonymized_telemetry=False)
except Exception:
    try:
        chromadb.settings = Settings(anonymized_telemetry=False)
    except Exception:
        pass

# -------------------- Global Config (Path Vars Removed) --------------------
from config.global_vars import (
    # üí• ‡∏•‡∏ö VECTORSTORE_DIR, MAPPING_BASE_DIR
    FINAL_K_RERANKED,
    INITIAL_TOP_K,
    EVIDENCE_DOC_TYPES,
    MAX_PARALLEL_WORKERS,
    DEFAULT_TENANT,
    DEFAULT_YEAR,
    DEFAULT_ENABLER,
    RERANKER_MODEL_NAME,
    EMBEDDING_MODEL_NAME
)
# -------------------- Vectorstore Constants --------------------
ENV_FORCE_MODE = os.getenv("VECTOR_MODE", "").lower()  # "thread", "process", or ""
ENV_DISABLE_ACCEL = os.getenv("VECTOR_DISABLE_ACCEL", "").lower() in ("1", "true", "yes")

# Global caches (per process)
_CACHED_EMBEDDINGS = None
_EMBED_LOCK = threading.Lock()
_MPS_WARNING_SHOWN = False

# -------------------- Helper: detect environment & device --------------------
@staticmethod
def _detect_system():
    cpu_count = os.cpu_count() or 4
    total_ram_gb = None
    if psutil:
        try:
            total_ram_gb = psutil.virtual_memory().total / (1024 ** 3)
        except Exception:
            total_ram_gb = None
    return {"cpu_count": cpu_count, "total_ram_gb": total_ram_gb, "platform": platform.system().lower()}

@staticmethod
def _detect_torch_device():
    try:
        import torch
        if ENV_DISABLE_ACCEL:
            return "cpu"
        if torch.cuda.is_available():
            return "cuda"
        if platform.system().lower() == "darwin" and hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
            # MPS can be fragile for multiprocessing; we'll warn and possibly force cpu later
            return "mps"
    except Exception:
        pass
    return "cpu"

def get_hf_embeddings(device_hint: Optional[str] = None):
    global _CACHED_EMBEDDINGS, _MPS_WARNING_SHOWN
    device = device_hint or _detect_torch_device()

    if _CACHED_EMBEDDINGS is None:
        # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ _EMBED_LOCK ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ thread safe
        # with _EMBED_LOCK: 
        if _CACHED_EMBEDDINGS is None:
            
            # üü¢ ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏°‡∏≤‡πÉ‡∏ä‡πâ Global Variable ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÑ‡∏ß‡πâ‡πÉ‡∏ô global_vars.py
            model_name = EMBEDDING_MODEL_NAME 

            logger.info(f"Loading BEST Thai RAG embedding 2025: {model_name} on {device}")
            logger.info("This model will be used to build ALL PEA 2568 vectorstores (evidence_km, document, etc.)")
            
            try:
                # 
                _CACHED_EMBEDDINGS = HuggingFaceEmbeddings(
                    model_name=model_name,
                    model_kwargs={
                        "device": device,
                        # BGE-M3 ‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ prefix!
                    },
                    encode_kwargs={
                        "normalize_embeddings": True,
                        # ‡∏Å‡∏≤‡∏£‡∏•‡∏ö 'prompt': 'query:' ‡∏≠‡∏≠‡∏Å ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö BGE-M3 ‡∏ô‡∏±‡πâ‡∏ô ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡∏£‡∏±‡∏ö!
                    }
                )
            except Exception as e:
                logger.error(f"Failed to load {model_name}: {e}")
                logger.warning("Falling back to paraphrase-multilingual-MiniLM-L12-v2")
                # ‡πÉ‡∏ä‡πâ Fallback model ‡∏ï‡∏±‡∏ß‡πÄ‡∏î‡∏¥‡∏°
                _CACHED_EMBEDDINGS = HuggingFaceEmbeddings(
                    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
                    model_kwargs={"device": "cpu"}
                )
    return _CACHED_EMBEDDINGS

# =================================================================
# HuggingFace Cross-Encoder Reranker wrapper (singleton)
# =================================================================
class HuggingFaceCrossEncoderCompressor(BaseDocumentCompressor, BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)
    rerank_model: str = RERANKER_MODEL_NAME
    rerank_device: str = "cpu"
    rerank_max_length: int = 512
    _cross_encoder: Any = PrivateAttr(None)

    def __init__(self, **data):
        super().__init__(**data)
        # determine device safely (force cpu for CrossEncoder on mac mps)
        try:
            device = _detect_torch_device()
            if device == "mps":
                # CrossEncoder on MPS can be unstable; force CPU
                device = "cpu"
            self.rerank_device = device
        except Exception:
            self.rerank_device = "cpu"

    def set_encoder_instance(self, encoder: Any):
        self._cross_encoder = encoder

    def compress_documents(self, documents: Sequence[LcDocument], query: str, top_n: int, callbacks: Optional[Any] = None) -> List[LcDocument]:
        if not documents:
            return []
        if self._cross_encoder is None or not hasattr(self._cross_encoder, "predict"):
            logger.error("HuggingFace Cross-Encoder is not initialized. Returning truncated documents.")
            return list(documents)[:top_n]

        # Prepare input pairs
        sentence_pairs = [[query, doc.page_content] for doc in documents]

        try:
            scores = self._cross_encoder.predict(sentence_pairs, show_progress_bar=False)
        except TypeError:
            # Some CrossEncoder versions accept different args
            try:
                scores = self._cross_encoder.predict(sentence_pairs)
            except Exception as e:
                logger.error(f"‚ùå Cross-Encoder prediction failed: {e}. Returning truncated documents.")
                return list(documents)[:top_n]
        except Exception as e:
            logger.error(f"‚ùå Cross-Encoder prediction failed: {e}. Returning truncated documents.")
            return list(documents)[:top_n]

        # sort and return top_n
        doc_scores = sorted(zip(documents, scores), key=lambda x: x[1], reverse=True)
        final_docs = []
        for doc, score in doc_scores[:top_n]:
            # üü¢ FIX: Check and initialize metadata if it is None (prevents TypeError)
            if doc.metadata is None:
                doc.metadata = {}
                logger.warning(f"‚ö†Ô∏è Reranker: Found Document with None metadata. Initializing metadata to {{}}.")
                
            doc.metadata["relevance_score"] = float(score)
            final_docs.append(doc)
        return final_docs

_CACHED_RERANKER_INSTANCE: Optional[HuggingFaceCrossEncoderCompressor] = None
_CACHED_CROSS_ENCODER: Any = None

def get_global_reranker() -> Optional[HuggingFaceCrossEncoderCompressor]:
    """
    Returns a cached HuggingFaceCrossEncoderCompressor instance (singleton).
    """
    global _CACHED_RERANKER_INSTANCE, _CACHED_CROSS_ENCODER

    if _CACHED_RERANKER_INSTANCE is None:
        try:
            if not _HAS_SENT_TRANS:
                logging.warning("sentence-transformers not installed. Cross-Encoder reranker disabled.")
                return None

            instance = HuggingFaceCrossEncoderCompressor(
                # rerank_model="mixedbread-ai/mxbai-rerank-xsmall-v1"
                rerank_model=RERANKER_MODEL_NAME
            )

            from sentence_transformers import CrossEncoder
            cross_encoder_model = CrossEncoder(
                instance.rerank_model,
                device=instance.rerank_device
            )

            instance.set_encoder_instance(cross_encoder_model)

            _CACHED_RERANKER_INSTANCE = instance
            _CACHED_CROSS_ENCODER = cross_encoder_model

            logging.info(f"Initialized global Cross-Encoder reranker: {instance.rerank_model} on {instance.rerank_device}")

        except Exception as e:
            logging.warning(f"Failed to initialize global reranker: {e}")
            return None

    return _CACHED_RERANKER_INSTANCE

# -------------------- Path Helper Function (REVISED to use Path Utility) --------------------
# ‚ö†Ô∏è ‡∏•‡∏ö‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô _get_collection_name (‡πÉ‡∏ä‡πâ get_doc_type_collection_key ‡πÅ‡∏ó‡∏ô)

def get_vectorstore_path(
    tenant: str, 
    year: Optional[int], 
    doc_type: Optional[str] = None, 
    enabler: Optional[str] = None
) -> str:
    """
    Calculates the full persist directory path for the vector store instance
    based on the Centralized KM Logic by calling path_utils.
    """
    # üéØ FIX: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏õ‡πÉ‡∏ä‡πâ get_vectorstore_tenant_root_path
    if not doc_type:
        return get_vectorstore_tenant_root_path(tenant=tenant)
        
    # üéØ FIX: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏õ‡πÉ‡∏ä‡πâ get_vectorstore_collection_path
    return get_vectorstore_collection_path(
        tenant=tenant, 
        year=year, 
        doc_type=doc_type, 
        enabler=enabler, 
        # EVIDENCE_DOC_TYPES ‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏™‡πà‡∏á‡πÅ‡∏•‡πâ‡∏ß ‡∏ñ‡∏π‡∏Å hardcode ‡πÉ‡∏ô path_utils
    )

def vectorstore_exists(
    doc_id: str = "N/A", # ‡∏£‡∏±‡∏Å‡∏©‡∏≤‡πÑ‡∏ß‡πâ‡∏ï‡∏≤‡∏° Signature ‡πÄ‡∏î‡∏¥‡∏°
    tenant: str = DEFAULT_TENANT,
    year: Optional[int] = DEFAULT_YEAR, # <--- ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö None ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö General Docs
    doc_type: Optional[str] = None, 
    enabler: Optional[str] = None, 
    base_path: str = "" # base_path ‡∏ñ‡∏π‡∏Å‡∏•‡∏∞‡πÄ‡∏•‡∏¢ ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÉ‡∏ä‡πâ global VECTORSTORE_DIR ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô path_utils
) -> bool:
    """
    Checks if the Vector Store directory exists for the given context.
    """
    if not doc_type:
        return False
        
    # 1. Get the full path using the updated logic (‡πÄ‡∏£‡∏µ‡∏¢‡∏Å get_vectorstore_path ‡πÉ‡∏´‡∏°‡πà)
    # NOTE: get_vectorstore_path ‡∏à‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Path ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÇ‡∏î‡∏¢‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å doc_type (‡∏°‡∏µ‡∏õ‡∏µ/‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏µ)
    path = get_vectorstore_path(tenant, year, doc_type, enabler) 
    
    # 2. Check for the actual data file created by Chroma
    file_path = os.path.join(path, "chroma.sqlite3")
    
    if not os.path.isdir(path):
        logger.warning(f"‚ùå V-Exists Check: Directory not found for doc_type '{doc_type}' at {path}")
        return False
    if os.path.isfile(file_path):
        return True
    logger.error(f"‚ùå V-Exists Check: FAILED to find file chroma.sqlite3 at {file_path} in {path}")
    return False

# ‚ö†Ô∏è ‡∏•‡∏ö‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô _get_collection_parent_dir ‡∏≠‡∏≠‡∏Å (Logic ‡∏ñ‡∏π‡∏Å‡∏¢‡∏∏‡∏ö‡∏£‡∏ß‡∏°‡πÅ‡∏•‡∏∞‡∏ñ‡∏π‡∏Å‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡πÉ‡∏ô list_vectorstore_folders)

def list_vectorstore_folders(
    tenant: str, 
    year: int, # NOTE: ‡πÉ‡∏ä‡πâ‡∏õ‡∏µ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ô (int)
    doc_type: Optional[str] = None, 
    enabler: Optional[str] = None, 
    base_path: str = "" # base_path ‡∏ñ‡∏π‡∏Å‡∏•‡∏∞‡πÄ‡∏•‡∏¢
) -> List[str]:
    """
    Lists the actual collection folder names (e.g., 'evidence_km', 'document')
    that exist under the specified tenant and year context.
    """
    # üéØ FIX: ‡πÉ‡∏ä‡πâ get_vectorstore_tenant_root_path ‡πÅ‡∏ó‡∏ô
    tenant_root = get_vectorstore_tenant_root_path(tenant) # VECTORSTORE_DIR / tenant
    
    # Scenario 1: Specific doc_type/enabler is requested
    if doc_type:
        doc_type_norm = doc_type.lower().strip()
        # üéØ FIX: ‡πÉ‡∏ä‡πâ get_doc_type_collection_key ‡πÅ‡∏ó‡∏ô _get_collection_name
        collection_name = get_doc_type_collection_key(doc_type_norm, enabler)
        
        # ‡∏ï‡πâ‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì target_year ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡πà‡∏≠‡∏ô (None ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö General Docs)
        target_year = year
        if doc_type_norm != EVIDENCE_DOC_TYPES.lower():
            target_year = None
            
        # üéØ FIX: ‡πÉ‡∏ä‡πâ get_vectorstore_collection_path
        full_collection_path = get_vectorstore_collection_path(tenant, target_year, doc_type_norm, enabler)
        
        if os.path.isdir(full_collection_path) and os.path.isfile(os.path.join(full_collection_path, "chroma.sqlite3")):
            return [collection_name] 
        return []

    # Scenario 2: List ALL collections for the given tenant/year context (List All)
    
    collections: Set[str] = set()
    
    # 1. Scan the Year Root (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Doc Type: evidence) - Path: V_ROOT/tenant/year
    root_year_evidence = os.path.join(tenant_root, str(year)) 
    if os.path.isdir(root_year_evidence):
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ evidence_... collections ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏õ‡∏µ
        for sub_dir in os.listdir(root_year_evidence):
             sub_dir_lower = sub_dir.lower()
             if sub_dir_lower.startswith(f"{EVIDENCE_DOC_TYPES.lower()}_"): 
                 full_collection_path = os.path.join(root_year_evidence, sub_dir)
                 if os.path.isfile(os.path.join(full_collection_path, "chroma.sqlite3")):
                    collections.add(sub_dir_lower) 

    # 2. Scan the Common Root (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Doc Type: document, faq, ‡∏Ø‡∏•‡∏Ø) - Path: V_ROOT/tenant
    root_common = tenant_root 
    if os.path.isdir(root_common):
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå Doc Type
        for sub_dir in os.listdir(root_common):
            sub_dir_lower = sub_dir.lower()
            
            # ‡∏Ç‡πâ‡∏≤‡∏°‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç (‡∏õ‡∏µ) ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ñ‡∏π‡∏Å‡∏™‡πÅ‡∏Å‡∏ô‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1 ‡πÅ‡∏•‡πâ‡∏ß
            if sub_dir.isdigit():
                 continue 
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏õ‡πá‡∏ô Collection ‡∏à‡∏£‡∏¥‡∏á (‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå Chroma)
            full_collection_path = os.path.join(root_common, sub_dir)
            
            if os.path.isfile(os.path.join(full_collection_path, "chroma.sqlite3")):
                 collections.add(sub_dir_lower) 
    
    return sorted(list(collections))


# -------------------- VECTORSTORE MANAGER (SINGLETON) --------------------
class VectorStoreManager:
    _instance = None
    _is_initialized = False
    _lock = threading.Lock()

    # ‡πÉ‡∏ä‡πâ default_factory ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏™‡πà {} ‡∏´‡∏£‡∏∑‡∏≠ None ‡∏ï‡∏£‡∏á‡πÜ
    _chroma_cache: Dict[str, Chroma] = PrivateAttr(default_factory=dict)
    _multi_doc_retriever: Optional['MultiDocRetriever'] = PrivateAttr(default=None)
    
    tenant: str = PrivateAttr(default=DEFAULT_TENANT)
    year: int = PrivateAttr(default=DEFAULT_YEAR)

    _doc_id_mapping: Dict[str, Dict[str, Any]] = PrivateAttr(default_factory=dict)
    _uuid_to_doc_id: Dict[str, str] = PrivateAttr(default_factory=dict)

    _embeddings: Any = PrivateAttr(default=None)

    # ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î: ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ default_factory=dict ‡∏´‡∏£‡∏∑‡∏≠ default=None ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô!
    _client: Optional[chromadb.PersistentClient] = PrivateAttr(default=None)

    def __new__(cls, *args, **kwargs):
        with cls._lock:
            if cls._instance is None:
                cls._instance = super(VectorStoreManager, cls).__new__(cls)
        return cls._instance

    def __init__(self, base_path: str = "", tenant: str = DEFAULT_TENANT,  year: Optional[int] = None, enabler: Optional[str] = None, doc_type: str = EVIDENCE_DOC_TYPES,): # ‚¨ÖÔ∏è FIX: ‡∏£‡∏±‡∏ö enabler ‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡∏î‡πâ‡∏ß‡∏¢
        # üìå FIX: ‡∏ó‡∏≥‡πÉ‡∏´‡πâ init ‡∏£‡∏±‡∏ö‡πÅ‡∏Ñ‡πà base_path ‡πÅ‡∏•‡∏∞ tenant ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ñ‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô Singleton
        if not self._is_initialized:
            self._base_path = base_path
            self.tenant = tenant.lower()
            
            # üí° FIX: ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÉ‡∏´‡πâ Attributes ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡πÄ‡∏°‡∏ò‡∏≠‡∏î‡∏≠‡∏∑‡πà‡∏ô ‡πÜ ‡∏Ç‡∏≠‡∏á Class
            #        ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Default ‡∏à‡∏≤‡∏Å config
            self.year = year if year is not None else DEFAULT_YEAR    
            self.doc_type = doc_type
            self.enabler = enabler.upper() if enabler else DEFAULT_ENABLER # ‚¨ÖÔ∏è FIX: ‡∏Å‡∏≥‡∏´‡∏ô‡∏î enabler ‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
            
            self._chroma_cache = {}
            self._embeddings = get_hf_embeddings()
            
            chroma_client_root = get_vectorstore_tenant_root_path(tenant=self.tenant)
            self._client = chromadb.PersistentClient(path=chroma_client_root)

            logger.info(f"ChromaDB Client initialized at TENANT ROOT PATH: {chroma_client_root}")
            
            self._load_doc_id_mapping()
            
            logger.info(f"Initialized VectorStoreManager (Tenant: {self.tenant})") 
            
            VectorStoreManager._is_initialized = True
    
    @property
    def doc_id_map(self) -> Dict[str, Dict[str, Any]]:
        """Provides access to the Stable Doc ID -> Chunk UUIDs mapping."""
        return self._doc_id_mapping

    @property
    def uuid_to_doc_id_map(self) -> Dict[str, str]:
        """Provides access to the Chunk UUID -> Stable Doc ID mapping."""
        return self._uuid_to_doc_id
    
    def close(self):
        with self._lock:
            if self._multi_doc_retriever and hasattr(self._multi_doc_retriever, "shutdown"):
                logger.info("Closing MultiDocRetriever executor via VSM.")
                self._multi_doc_retriever.shutdown()
                self._multi_doc_retriever = None
            self._chroma_cache = {}
            VectorStoreManager._is_initialized = False

    def __del__(self):
        try:
            self.close()
        except Exception:
            pass

    def _load_doc_id_mapping(self):
        """
        ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏£‡∏ß‡∏° Document ID Mapping ‡∏à‡∏≤‡∏Å 2 Path (Global + Year/Enabler Specific) ‡πÅ‡∏ö‡∏ö thread-safe
        ‡πÄ‡∏ô‡πâ‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏ô‡∏ó‡∏≤‡∏ô‡∏ï‡πà‡∏≠ Worker Context (Handle ValueError ‡πÅ‡∏•‡∏∞ Attribute Missing)
        """

        # Lock ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö thread-safe update
        if not hasattr(self, "_mapping_lock") or self._mapping_lock is None:
            self._mapping_lock = Lock()

        self._doc_id_mapping = {}
        self._uuid_to_doc_id = {}

        # üéØ CRITICAL: ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤ attributes ‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡πÉ‡∏ô Worker Context
        current_tenant = getattr(self, 'tenant', 'default_tenant')
        current_year = getattr(self, 'year', None)
        current_enabler = getattr(self, 'enabler', None)
        # üéØ FIX: ‡πÉ‡∏ä‡πâ self.doc_type ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÉ‡∏ô __init__ 
        # (‡∏ã‡∏∂‡πà‡∏á‡∏ñ‡∏π‡∏Å‡∏™‡πà‡∏á‡∏°‡∏≤‡∏à‡∏≤‡∏Å load_all_vectorstores ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß)
        current_doc_type = getattr(self, 'doc_type', EVIDENCE_DOC_TYPES) 

        logger.info(f"üîç VSM MAP LOAD PARAMS: Tenant={current_tenant}, Year={current_year}, "
                    f"Enabler={current_enabler}, DocType={current_doc_type}")
        
        path_A = None # Specific Map
        path_B = None # Global Map

        # 1. PATH A: Year-Specific/Enabler Mapping
        try:
            path_A = get_mapping_file_path(
                doc_type=current_doc_type,
                tenant=current_tenant, 
                year=current_year, 
                enabler=current_enabler
            )
        except ValueError as e:
            logger.warning(f"‚ö†Ô∏è VSM MAP PATH A (Specific) failed generation: {e}. Skipping specific map.")
            path_A = None
        
        # 2. PATH B: Global/Tenant Root Mapping
        try:
             path_B = get_mapping_file_path(
                doc_type=current_doc_type,
                tenant=current_tenant,
                year=None, # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô None ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Path Logic ‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡πÅ‡∏ö‡∏ö Global
                enabler=None 
            )
        except ValueError as e:
            logger.warning(f"‚ö†Ô∏è VSM MAP PATH B (Global) failed generation: {e}. Skipping global map.")
            path_B = None
            
        # üéØ FIX: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î (Specific ‡∏Å‡πà‡∏≠‡∏ô Global, ‡∏ñ‡πâ‡∏≤ Path ‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô)
        paths_to_load = []
        # A ‡∏Å‡πà‡∏≠‡∏ô B ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Specific ‡∏ó‡∏±‡∏ö Global
        if path_A and os.path.exists(path_A):
            paths_to_load.append(path_A)
        # B ‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ö A
        if path_B and path_B != path_A and os.path.exists(path_B):
            paths_to_load.append(path_B)


        # Log Path details
        logger.info(f"üîç VSM MAP PATH A (Specific): {path_A} (Exists: {os.path.exists(path_A) if path_A else 'N/A'})")
        logger.info(f"üîç VSM MAP PATH B (Global): {path_B} (Exists: {os.path.exists(path_B) if path_B else 'N/A'})")
        logger.info(f"üîç VSM MAP Loading from {len(paths_to_load)} path(s): {paths_to_load}")
        
        total_loaded_docs = 0
        total_loaded_uuids = 0

        # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î
        for path in paths_to_load:
            
            try:
                with open(path, "r", encoding="utf-8") as f:
                    mapping_data: Dict[str, Dict[str, Any]] = json.load(f)
                    
                # Thread-safe update
                with self._mapping_lock:
                    for doc_id, doc_entry in mapping_data.items():
                        doc_id_clean = doc_id.strip()
                        
                        self._doc_id_mapping[doc_id_clean] = doc_entry
                        
                        # ‡∏™‡∏£‡πâ‡∏≤‡∏á uuid to doc_id mapping
                        if isinstance(doc_entry, dict) and isinstance(doc_entry.get("chunk_uuids"), list):
                            for uid in doc_entry["chunk_uuids"]:
                                uid_clean = uid.replace("-", "")
                                
                                # üéØ FIX: ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ UUID ‡∏ã‡πâ‡∏≥ (‡∏ï‡∏≤‡∏°‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥)
                                if uid in self._uuid_to_doc_id and self._uuid_to_doc_id[uid] != doc_id_clean:
                                    logger.warning(f"‚ö†Ô∏è Duplicate UUID {uid} detected. Existing: {self._uuid_to_doc_id[uid]}, New: {doc_id_clean}")
                                    
                                self._uuid_to_doc_id[uid] = doc_id_clean
                                self._uuid_to_doc_id[uid_clean] = doc_id_clean
                                
                    current_total_docs = len(self._doc_id_mapping)
                    current_total_uuids = len(self._uuid_to_doc_id)
                
                logger.info(f"‚úÖ Loaded {len(mapping_data)} documents from MAPPING: {path} (Current Total Docs: {current_total_docs}, Chunks: {current_total_uuids})")
                total_loaded_docs = current_total_docs
                total_loaded_uuids = current_total_uuids
                    
            except Exception as e:
                logger.error(f"‚ùå Failed to load Doc ID Mapping from {path}: {e}", exc_info=True)


        logger.info(f"Initialized Doc ID Mapping. Total documents loaded: {total_loaded_docs}, Total chunks mapped: {total_loaded_uuids}.")

    def _re_parse_collection_name(self, collection_name: str) -> Tuple[str, Optional[str]]:
        collection_name_lower = collection_name.strip().lower()
        if collection_name_lower.startswith(f"{EVIDENCE_DOC_TYPES}_"):
            parts = collection_name_lower.split("_", 1)
            return EVIDENCE_DOC_TYPES, parts[1].upper() if len(parts) == 2 else None
        return collection_name_lower, None

    def _load_chroma_instance(self, collection_name: str) -> Optional[Chroma]:
        # 1. Fast cache hit
        if collection_name in self._chroma_cache:
            return self._chroma_cache[collection_name]

        # 2. Thread-safe double-check
        with self._lock:
            if collection_name in self._chroma_cache:
                return self._chroma_cache[collection_name]

            # ------------------------------------------------------------------
            # 3. ‡πÅ‡∏¢‡∏Å doc_type ‡∏Å‡∏±‡∏ö enabler ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤
            # ------------------------------------------------------------------
            doc_type, enabler = self._re_parse_collection_name(collection_name)

            # ------------------------------------------------------------------
            # 4. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î target_year
            # ------------------------------------------------------------------
            if doc_type.lower() == EVIDENCE_DOC_TYPES.lower():
                target_year: Optional[int] = self.year
            else:
                target_year = None

            # ------------------------------------------------------------------
            # 5. ‡∏™‡∏£‡πâ‡∏≤‡∏á persist_directory ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á 100% (Full Path ‡∏Ç‡∏≠‡∏á Collection)
            # üéØ FIX: ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ get_vectorstore_collection_path
            # ------------------------------------------------------------------
            persist_directory = get_vectorstore_collection_path(
                tenant=self.tenant,
                year=target_year,
                doc_type=doc_type,
                enabler=enabler,
            )

            # ------------------------------------------------------------------
            # 6. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ folder ‡∏à‡∏£‡∏¥‡∏á‡πÑ‡∏´‡∏°
            # ------------------------------------------------------------------
            if not os.path.exists(persist_directory):
                logger.warning(
                    f"Vectorstore directory NOT FOUND!\n"
                    f"   Collection   : {collection_name}\n"
                    f"   Expected path: {persist_directory}\n"
                    f"   tenant={self.tenant} | year={self.year} | doc_type={str(doc_type)} | enabler={enabler or 'None'}" # üí° FIX: ‡∏Ñ‡∏£‡∏≠‡∏ö doc_type ‡∏î‡πâ‡∏ß‡∏¢ str()
                )
                # ‡∏•‡∏ö alt_path ‡∏≠‡∏≠‡∏Å‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å Logic ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Path ‡πÑ‡∏î‡πâ‡∏£‡∏ß‡∏°‡∏®‡∏π‡∏ô‡∏¢‡πå‡πÅ‡∏•‡πâ‡∏ß
                return None

            # ------------------------------------------------------------------
            # 7. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ client ‡∏ñ‡∏π‡∏Å init ‡πÅ‡∏•‡πâ‡∏ß (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡πÑ‡∏ß‡πâ)
            # ------------------------------------------------------------------
            if self._client is None:
                logger.error("Chroma PersistentClient is None! ‡∏ï‡πâ‡∏≠‡∏á init ‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")
            
            # üéØ ‡∏î‡∏∂‡∏á Global Embedding Model (768-dim) ‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
            try:
                correct_embeddings = get_hf_embeddings() 
            except Exception as e:
                logger.error(f"FATAL: Failed to get correct embeddings for Chroma init: {e}")
                return None

            try:
                # ------------------------------------------------------------------
                # 8. ‡∏™‡∏£‡πâ‡∏≤‡∏á Chroma instance (‡πÉ‡∏ä‡πâ Path ‡πÅ‡∏ó‡∏ô Client ‡∏ï‡∏±‡∏ß‡πÅ‡∏°‡πà)
                # ------------------------------------------------------------------
                
                # üéØ FIX 6: ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏´‡πâ LangChain Chroma ‡∏™‡∏£‡πâ‡∏≤‡∏á PersistentClient ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô
                # ‡∏ó‡∏µ‡πà‡∏ä‡∏µ‡πâ‡πÑ‡∏õ‡∏ó‡∏µ‡πà Collection Path ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Raw Retrieval: 0 Docs
                
                vectordb = Chroma(
                    # client=self._client,               # ‚¨ÖÔ∏è ‡∏•‡∏ö Client ‡∏ï‡∏±‡∏ß‡πÅ‡∏°‡πà‡∏ó‡∏µ‡πà Root ‡∏≠‡∏≠‡∏Å
                    embedding_function=correct_embeddings,
                    collection_name=collection_name,
                    persist_directory=persist_directory  # ‚¨ÖÔ∏è ‡πÉ‡∏ä‡πâ Full Path ‡∏Ç‡∏≠‡∏á Collection
                )

                # üéØ FIX 7: ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÇ‡∏´‡∏•‡∏î Collection Object ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ (‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Lazy Loading ‡πÉ‡∏ô Worker)
                # ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å vectordb._collection ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å Collection.get_collection() ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô
                collection_test = vectordb._collection 
                # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏Å method ‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤ collection ‡πÑ‡∏°‡πà‡∏ï‡∏≤‡∏¢
                collection_test.count()

                # Cache ‡πÑ‡∏ß‡πâ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ï‡πà‡∏≠‡πÑ‡∏õ
                self._chroma_cache[collection_name] = vectordb

                logger.info(
                    f"Loaded Chroma collection '{collection_name}' ‚Üí Path: {persist_directory} (Retrieval Test Pending)"
                )
                return vectordb

            except Exception as e:
                logger.error(
                    f"FAILED to load collection '{collection_name}' from {persist_directory}\n"
                    f"Error: {e}",
                    exc_info=True,
                )
                return None


    def get_documents_by_id(self, stable_doc_ids: Union[str, List[str]], doc_type: str = "default_collection", enabler: Optional[str] = None) -> List[LcDocument]:
        """
        Retrieve documents from Chroma collection by stable_doc_ids (64-char hash).

        üéØ FIX 21.0: ‡∏•‡∏ö 'ids' ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å 'include' parameter ‡πÉ‡∏ô collection.get() ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç ValueError
        """
        import chromadb 
        from langchain_core.documents import Document as LcDocument
        from typing import Set, Dict, Any 

        if isinstance(stable_doc_ids, str):
            stable_doc_ids = [stable_doc_ids]
            
        if not stable_doc_ids:
            return []

        # 1. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ä‡∏∑‡πà‡∏≠ Collection ‡πÅ‡∏•‡∏∞‡πÇ‡∏´‡∏•‡∏î Instance
        collection_name = get_doc_type_collection_key(doc_type=doc_type, enabler=enabler)
        chroma_instance = self._load_chroma_instance(collection_name)
        
        if not chroma_instance:
            logger.warning(f"VSM: Cannot load collection '{collection_name}' for document retrieval.")
            return []

        # 2. ‡πÅ‡∏õ‡∏•‡∏á Stable Doc IDs ‡πÄ‡∏õ‡πá‡∏ô Chunk UUIDs (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Primary Key)
        chunk_uuids_for_search: List[str] = []
        
        for stable_id in stable_doc_ids:
            stable_id_clean = stable_id.strip() 
            map_entry = self.doc_id_map.get(stable_id_clean)
            if map_entry and map_entry.get("chunk_uuids"):
                chunk_uuids_for_search.extend(map_entry["chunk_uuids"])
            else:
                chunk_uuids_for_search.append(stable_id_clean) 
                
        # 3. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ ID ‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Query ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Primary Key Search (Chunk UUIDs)
        search_ids_raw = list(set([
            str(i).strip()
            for i in chunk_uuids_for_search if str(i).strip()
        ]))
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏° Flexible UUID Search: ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ó‡∏±‡πâ‡∏á ID ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡∏µ‡∏î‡∏Å‡∏•‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡∏µ‡∏î‡∏Å‡∏•‡∏≤‡∏á
        final_chunk_uuids_to_try: Set[str] = set()
        for chunk_id in search_ids_raw:
            final_chunk_uuids_to_try.add(chunk_id) 
            if "-" in chunk_id:
                final_chunk_uuids_to_try.add(chunk_id.replace("-", "")) 
                
        final_chunk_uuids_list = list(final_chunk_uuids_to_try)

        if not final_chunk_uuids_list:
             logger.warning(f"Hydration failed: No valid Chunk UUIDs derived from {len(stable_doc_ids)} Stable IDs.")
             return []


        try:
            collection = chroma_instance._collection
            documents: List[LcDocument] = []
            result: Dict[str, Any] = {}
            
            # --- Attempt 1: Primary Key Search (Chunk UUIDs) ---
            logger.info(f"Attempt 1/2: Primary Key Search ({len(final_chunk_uuids_list)} Chunk UUIDs)")
            result = collection.get(
                ids=final_chunk_uuids_list,
                include=["documents", "metadatas"] # <-- üéØ FIX 21.0: ‡∏•‡∏ö "ids"
            )

            docs_result = result.get("documents", [])
            
            # üéØ FINAL FIX 19.0: ‡∏ñ‡πâ‡∏≤‡πÑ‡∏î‡πâ 0 chunks ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á Fallback Search ‡∏î‡πâ‡∏ß‡∏¢ $or
            if not docs_result:
                
                # --- Attempt 2: Fallback Search (Metadata: stable_doc_uuid OR doc_id) ---
                logger.warning("Attempt 1 returned 0 chunks. Falling back to Robust Metadata Search (stable_doc_uuid / doc_id).")
                
                # ‡πÉ‡∏ä‡πâ Stable Doc IDs ‡∏ó‡∏µ‡πà Cleaned ‡πÅ‡∏•‡πâ‡∏ß ‡πÄ‡∏õ‡πá‡∏ô Query ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Metadata Search
                stable_doc_ids_cleaned = list(set([uid.strip() for uid in stable_doc_ids if uid.strip()]))

                if stable_doc_ids_cleaned:
                    # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏î‡πâ‡∏ß‡∏¢ $or: stable_doc_uuid ‡∏´‡∏£‡∏∑‡∏≠ doc_id
                    result = collection.get(
                        where={"$or": [
                            {"stable_doc_uuid": {"$in": stable_doc_ids_cleaned}},
                            {"doc_id": {"$in": stable_doc_ids_cleaned}}
                        ]},
                        include=["documents", "metadatas"] # <-- üéØ FIX 21.0: ‡∏•‡∏ö "ids"
                    )
                    docs_result = result.get("documents", [])
                else:
                    logger.warning("Fallback Search failed: No valid Stable Doc IDs for metadata query.")
            
            # --- ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå ---
            docs = docs_result
            metadatas = result.get("metadatas", [{}] * len(docs))
            # ids ‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡∏ñ‡∏π‡∏Å‡∏Ñ‡∏∑‡∏ô‡∏°‡∏≤‡πÄ‡∏™‡∏°‡∏≠
            ids = result.get("ids", [""] * len(docs)) 

            for i, text in enumerate(docs):
                meta = metadatas[i].copy() if metadatas and metadatas[i] else {}
                chunk_uuid = ids[i] if ids else (meta.get("chunk_uuid") or "")
                
                if chunk_uuid:
                    meta["chunk_uuid"] = chunk_uuid

                # ‡πÉ‡∏ä‡πâ map (uuid_to_doc_id_map) ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏´‡∏≤ Stable ID ‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô
                stable_doc_id = self.uuid_to_doc_id_map.get(chunk_uuid) or meta.get("stable_doc_uuid") or meta.get("doc_id")
                
                # Fallback: ‡∏´‡∏≤‡∏Å‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡∏µ‡∏î‡∏î‡πâ‡∏ß‡∏¢
                if not stable_doc_id and "-" in chunk_uuid:
                    stable_doc_id = self.uuid_to_doc_id_map.get(chunk_uuid.replace("-", ""))
                
                if stable_doc_id:
                     meta["stable_doc_uuid"] = stable_doc_id

                doc = LcDocument(page_content=text, metadata=meta)
                documents.append(doc)
                
            logger.info(f"‚úÖ Retrieved {len(documents)} documents for {len(stable_doc_ids)} Stable IDs from '{collection_name}' (Search Mode: {'Primary/Fallback'}).")
            return documents

        except Exception as e:
            logger.error(f"‚ùå Error retrieving documents by Stable/Chunk IDs from collection '{collection_name}': {e}", exc_info=True)
            return []

    def _ensure_chroma_client_is_valid(self):
        """
        üéØ FIX 4A: Re-initializes the Chroma client if it is None or lost during serialization (Worker Process).
        """
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ _client attribute ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô None ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if not hasattr(self, '_client') or self._client is None:
            self.logger.warning(f"Chroma client lost in worker process for tenant '{self.tenant}', re-initializing...")
            
            # ‡πÉ‡∏ä‡πâ VSM attributes (tenant) ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÉ‡∏´‡πâ‡∏Ñ‡∏á‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Path
            tenant_root_path = get_vectorstore_tenant_root_path(self.tenant)
            
            # Re-initialize the Persistent Client
            try:
                self._client = chromadb.PersistentClient(path=tenant_root_path, settings=Settings(anonymized_telemetry=True))
                
                # ‡πÄ‡∏°‡∏∑‡πà‡∏≠ Client ‡∏ñ‡∏π‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà, Collection Handles ‡πÄ‡∏Å‡πà‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ï‡πâ‡∏≠‡∏á‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÉ‡∏ä‡πâ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ
                self._collections = {} 
                
                self.logger.info(f"‚úÖ ChromaDB Client re-initialized at TENANT ROOT PATH: {tenant_root_path}. Collections cleared.")
            except Exception as e:
                self.logger.error(f"FATAL: Failed to re-initialize Chroma Client in worker: {e}", exc_info=False)
                # ‡πÑ‡∏°‡πà raise ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÇ‡∏Ñ‡πâ‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏±‡∏ö error ‡πÑ‡∏î‡πâ
            
    def retrieve_by_chunk_uuids(self, chunk_uuids: List[str], collection_name: Optional[str] = None) -> List[LcDocument]:
        """
        Hydrate documents by chunk UUIDs.
        - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö UUID ‡∏ó‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö‡∏°‡∏µ dash ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏µ dash
        - Retry mechanism + cache clear ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠ retrieval fail
        """
        from core.vectorstore import get_doc_type_collection_key, EVIDENCE_DOC_TYPES
        import logging
        logger = logging.getLogger(__name__)

        self._ensure_chroma_client_is_valid()

        if not chunk_uuids:
            logger.info("VSM: No chunk_uuids provided for hydration.")
            return []

        if collection_name is None:
            collection_name = get_doc_type_collection_key(
                doc_type=EVIDENCE_DOC_TYPES, 
                enabler=getattr(self, 'enabler', 'km')
            )

        # Prepare UUIDs: no-dash + attempt 64-char dash formatting
        no_dash = [u.replace("-", "") for u in chunk_uuids if u]
        with_dash = []
        for u in no_dash:
            if len(u) == 64:
                part1, part2, part3, part4, part5 = u[:8], u[8:12], u[12:16], u[16:20], u[20:]
                with_dash.append(f"{part1}-{part2}-{part3}-{part4}-{part5}")

        all_formats = list(set(chunk_uuids + no_dash + with_dash))  # Remove duplicates

        result = {"documents": [], "metadatas": [], "ids": []}
        max_retries = 3

        for attempt in range(1, max_retries + 1):
            chroma = self._load_chroma_instance(collection_name)
            if not chroma:
                logger.warning(f"Cannot load '{collection_name}' (attempt {attempt})")
                if attempt < max_retries and collection_name in self._chroma_cache:
                    logger.warning(f"VSM: Clearing Chroma cache for '{collection_name}' due to load failure.")
                    del self._chroma_cache[collection_name]
                    self._ensure_chroma_client_is_valid()
                continue

            try:
                logger.info(f"Hydration attempt {attempt}/{max_retries} ‚Üí {len(all_formats)} UUIDs from '{collection_name}'")
                result = chroma.get(ids=all_formats, include=["documents", "metadatas", "ids"])

                if result.get("documents"):
                    logger.info(f"Success: Retrieved {len(result['documents'])} chunks on attempt {attempt}")
                    break

                logger.warning(f"Got 0 chunks on attempt {attempt}, retrying...")

            except Exception as e:
                logger.error(f"Hydration failed (attempt {attempt}): {e}")

            # Clear cache only on failure
            if attempt < max_retries and collection_name in self._chroma_cache:
                logger.warning(f"VSM: Clearing Chroma cache for '{collection_name}' for retry.")
                del self._chroma_cache[collection_name]
                self._ensure_chroma_client_is_valid()

        # Build LcDocument objects
        docs = []
        documents_raw = result.get("documents", [])
        metas = result.get("metadatas", [{}] * len(documents_raw))
        ids = result.get("ids", [])

        for i, text in enumerate(documents_raw):
            if not text or not text.strip():
                continue
            meta = metas[i].copy()
            id_clean = ids[i].replace("-", "") if ids[i] else ""
            meta["chunk_uuid"] = id_clean

            # Map to stable_doc_uuid
            stable = self._uuid_to_doc_id.get(id_clean) or meta.get("stable_doc_uuid")
            if stable:
                meta["stable_doc_uuid"] = stable

            docs.append(LcDocument(page_content=text.strip(), metadata=meta))

        logger.info(f"Hydration complete ‚Üí Retrieved {len(docs)} full-text chunks (requested {len(chunk_uuids)})")
        return docs

            
    def get_limited_chunks_from_doc_ids(self, stable_doc_ids: Union[str, List[str]], query: Union[str, List[str]], doc_type: str, enabler: Optional[str] = None, limit_per_doc: int = 5) -> List[LcDocument]:
        if isinstance(stable_doc_ids, str):
            stable_doc_ids = [stable_doc_ids]
        stable_doc_ids = [uid for uid in stable_doc_ids if uid]
        if not stable_doc_ids:
            return []
        vector_search_query = query[0] if isinstance(query, list) and query else (query if isinstance(query, str) else "")
        if not vector_search_query:
            logger.warning("Limited chunk search skipped: Query is empty.")
            return []
        # üéØ FIX: ‡πÉ‡∏ä‡πâ get_doc_type_collection_key ‡πÅ‡∏ó‡∏ô _get_collection_name
        collection_name = get_doc_type_collection_key(doc_type, enabler)
        chroma_instance = self._load_chroma_instance(collection_name)
        if not chroma_instance:
            logger.error(f"Collection '{collection_name}' is not loaded.")
            return []
        all_limited_documents: List[LcDocument] = []
        total_chunks_retrieved = 0
        for stable_id in stable_doc_ids:
            stable_id_clean = stable_id.strip()
            # üéØ FIX: ‡πÉ‡∏ä‡πâ 'stable_doc_uuid' ‡πÄ‡∏õ‡πá‡∏ô filter key ‡πÅ‡∏ó‡∏ô 'doc_id' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô
            doc_filter = {"stable_doc_uuid": stable_id_clean}
            try:
                custom_retriever = ChromaRetriever(vectorstore=chroma_instance, k=limit_per_doc, filter=doc_filter)
                limited_docs = custom_retriever.get_relevant_documents(query=vector_search_query)
                for doc in limited_docs:
                    doc.metadata["priority_search_type"] = "limited_vector_search"
                    doc.metadata["priority_limit"] = limit_per_doc
                    all_limited_documents.append(doc)
                total_chunks_retrieved += len(limited_docs)
            except Exception as e:
                logger.error(f"‚ùå Error performing limited vector search for Stable ID '{stable_id_clean}': {e}")
                continue
        logger.info(f"‚úÖ Retrieved {total_chunks_retrieved} limited chunks (max {limit_per_doc}/doc) for {len(stable_doc_ids)} Stable IDs from '{collection_name}'.")
        return all_limited_documents

    # -------------------- Retriever Creation --------------------
    def get_retriever(self, collection_name: str, top_k: int = INITIAL_TOP_K, final_k: int = FINAL_K_RERANKED, use_rerank: bool = True) -> Any:
        chroma_instance = self._load_chroma_instance(collection_name)
        if not chroma_instance:
            logger.warning(f"Retriever creation failed: Collection '{collection_name}' not loaded.")
            return None

        # 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á retriever ‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏° k ‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á
        def raw_retrieve(query: str, filter_dict: Optional[dict] = None, k: int = top_k) -> List[LcDocument]:
            try:
                original_query = query
                
                # BGE-M3 ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡πÉ‡∏™‡πà Query Instruction/Prefix
                bge_prefix = "‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå: "
                query_with_prefix = f"{bge_prefix}{query.strip()}"
                
                logger.info(f"[BGE-M3 PREFIX ADDED] Using prefixed query: '{query_with_prefix[:100]}...'")
                # -------------------------------------------------------------

                search_kwargs = {"k": k}
                if filter_dict:
                    search_kwargs["filter"] = filter_dict
                
                docs = chroma_instance.similarity_search(
                    query=query_with_prefix, # üü¢ ‡πÉ‡∏ä‡πâ Query ‡∏ó‡∏µ‡πà‡∏°‡∏µ Prefix ‡πÅ‡∏•‡πâ‡∏ß
                    k=k,
                    filter=filter_dict
                )
                logger.info(f"Raw retrieval: {len(docs)} docs (k={k}, filter={bool(filter_dict)})")
                return docs
            except Exception as e:
                logger.error(f"Raw retrieval failed: {e}", exc_info=True)
                return []

        # 2. Reranker wrapper (‡∏â‡∏ö‡∏±‡∏ö‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡πÅ‡∏ö‡∏ö)
        def retrieve_with_rerank(query: str, config: Optional[dict] = None):
            filter_dict = None
            if config and isinstance(config, dict):
                # üéØ FIX: ‡∏ï‡πâ‡∏≠‡∏á‡∏î‡∏∂‡∏á filter ‡∏à‡∏≤‡∏Å 'where' key ‡∏´‡∏≤‡∏Å‡∏°‡∏µ
                search_kwargs = config.get("configurable", {}).get("search_kwargs", {})
                filter_dict = search_kwargs.get("filter") or search_kwargs.get("where")
            
            # ‡∏î‡∏∂‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏î‡πâ‡∏ß‡∏¢ k ‡πÄ‡∏ï‡πá‡∏°
            docs = raw_retrieve(query, filter_dict, k=top_k)

            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ reranker ‚Üí ‡∏Ñ‡∏∑‡∏ô‡∏ï‡∏≤‡∏° top_k
            reranker = get_global_reranker()
            if not (use_rerank and reranker and hasattr(reranker, "compress_documents")):
                return docs[:final_k]

            try:
                reranked = reranker.compress_documents(
                    documents=docs,
                    query=query,
                    top_n=final_k
                )
                # ‡∏î‡∏∂‡∏á score ‡∏à‡∏≤‡∏Å reranker (‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î)
                scores = getattr(reranker, "scores", None)
                # üéØ FIX: scores ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å predict() ‡∏ï‡πâ‡∏≠‡∏á‡∏ñ‡∏π‡∏Å‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏Å‡πà‡∏≠‡∏ô
                if scores and len(scores) >= len(reranked):
                    # scores ‡∏ó‡∏µ‡πà CrossEncoder.predict ‡∏Ñ‡∏∑‡∏ô‡∏°‡∏≤ ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á
                    # ‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ doc_scores ‡∏à‡∏≤‡∏Å compress_documents ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á score ‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö doc
                    doc_scores = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)
                    
                    for i, (doc, score) in enumerate(doc_scores[:len(reranked)]):
                        # ‡∏´‡∏≤ doc ‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô‡πÉ‡∏ô reranked list ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï metadata
                        for reranked_doc in reranked:
                            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ page_content ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô (‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå 100% ‡πÅ‡∏ï‡πà‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ)
                            if reranked_doc.page_content == doc.page_content:
                                score = float(score) if score is not None else 0.0
                                # ‡∏â‡∏µ‡∏î‡πÅ‡∏Ñ‡πà key ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô ‚Üí _rerank_score_force
                                reranked_doc.metadata["_rerank_score_force"] = score
                                # ‡πÅ‡∏•‡∏∞ source_filename (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö extraction)
                                orig = reranked_doc.metadata.get("source_filename", "UNKNOWN")
                                reranked_doc.metadata["source_filename"] = f"{orig}|SCORE:{score:.4f}"
                                break
                else:
                    # Fallback: ‡πÉ‡∏ä‡πâ score ‡∏à‡∏≤‡∏Å reranked doc.metadata ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ (‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á compress_documents)
                    for doc in reranked:
                        score = doc.metadata.get("relevance_score")
                        if score is not None:
                             orig = doc.metadata.get("source_filename", "UNKNOWN")
                             doc.metadata["source_filename"] = f"{orig}|SCORE:{score:.4f}"
                             doc.metadata["_rerank_score_force"] = score # ‡πÉ‡∏™‡πà key ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô
                             
                logger.info(f"Reranking success ‚Üí kept {len(reranked)} docs")
                return reranked

            except Exception as e:
                logger.warning(f"Rerank failed ({e}), fallback to raw")
                return docs[:final_k]

        # 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á LangChain-compatible Retriever
        class UltimateRetriever(BaseRetriever):
            def _get_relevant_documents(self, query: str, *, run_manager=None) -> List[LcDocument]:
                return retrieve_with_rerank(query, config=None)

            def invoke(self, query: str, config: Optional[dict] = None) -> List[LcDocument]:
                return retrieve_with_rerank(query, config=config)

        logger.info(f"Ultimate Retriever ready ‚Üí {collection_name} | top_k={top_k} ‚Üí final_k={final_k} | rerank={use_rerank}")
        return UltimateRetriever()

    def get_all_collection_names(self) -> List[str]:
        # üéØ FIX: ‡∏•‡∏ö base_path ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å list_vectorstore_folders
        return list_vectorstore_folders(tenant=self.tenant, year=self.year)


    def get_chunks_from_doc_ids(self, stable_doc_ids: Union[str, List[str]], doc_type: str, enabler: Optional[str] = None) -> List[LcDocument]:
        import chromadb # Import locally if not already imported
        from langchain_core.documents import Document as LcDocument

        if isinstance(stable_doc_ids, str):
            stable_doc_ids = [stable_doc_ids]
        stable_doc_ids = [uid for uid in stable_doc_ids if uid]
        if not stable_doc_ids:
            return []
        
        # üéØ FIX: ‡πÉ‡∏ä‡πâ get_doc_type_collection_key ‡πÅ‡∏ó‡∏ô _get_collection_name
        collection_name = get_doc_type_collection_key(doc_type, enabler)
        all_chunk_uuids = []
        skipped_docs = []
        found_stable_ids = []
        
        for stable_id in stable_doc_ids:
            stable_id_clean = stable_id.strip()
            if stable_id_clean in self._doc_id_mapping:
                doc_entry = self._doc_id_mapping[stable_id_clean]
                if isinstance(doc_entry, dict) and "chunk_uuids" in doc_entry and isinstance(doc_entry.get("chunk_uuids"), list):
                    chunk_uuids = doc_entry["chunk_uuids"]
                    if chunk_uuids:
                        all_chunk_uuids.extend(chunk_uuids)
                        found_stable_ids.append(stable_id_clean)
                    else:
                        logger.warning(f"Mapping found for Stable ID '{stable_id_clean}' but 'chunk_uuids' list is empty.")
                else:
                    # [Minor Fix] ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏à‡∏≤‡∏Å stable_doc_clean ‡πÄ‡∏õ‡πá‡∏ô stable_id_clean
                    logger.warning(f"Mapping entry for Stable ID '{stable_id_clean}' is malformed or missing 'chunk_uuids'.") 
            else:
                skipped_docs.append(stable_id_clean)
                
        if skipped_docs:
            logger.warning(f"Skipping Stable IDs not found in mapping: {skipped_docs}")
        if not all_chunk_uuids:
            logger.warning(f"No valid chunk UUIDs found for provided Stable Document IDs: {skipped_docs}. Check doc_id_mapping.json.")
            return []
            
        chroma_instance = self._load_chroma_instance(collection_name)
        if not chroma_instance:
            logger.error(f"Collection '{collection_name}' is not loaded.")
            return []
            
        try:
            collection = chroma_instance._collection
            
            # üéØ FINAL FIX 16.0: ‡∏•‡∏ö "ids" ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å include ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ ChromaDB API Error
            result = collection.get(ids=all_chunk_uuids, include=["documents", "metadatas"]) 
            
            documents: List[LcDocument] = []
            if not result.get("documents"):
                logger.warning(f"Chroma DB returned 0 documents for {len(all_chunk_uuids)} chunk UUIDs in collection '{collection_name}'.")
                return []
                
            for i, text in enumerate(result.get("documents", [])):
                if text:
                    metadata = result.get("metadatas", [{}])[i]
                    # IDs ‡∏ñ‡∏π‡∏Å‡∏Ñ‡∏∑‡∏ô‡∏°‡∏≤‡πÇ‡∏î‡∏¢‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á include
                    chunk_uuid_from_result = result.get("ids", [""])[i] 
                    
                    # NOTE: ‡πÉ‡∏ä‡πâ self._uuid_to_doc_id
                    doc_id = self._uuid_to_doc_id.get(chunk_uuid_from_result, "UNKNOWN") 
                    
                    metadata["chunk_uuid"] = chunk_uuid_from_result
                    metadata["doc_id"] = doc_id
                    metadata["doc_type"] = doc_type
                    documents.append(LcDocument(page_content=text, metadata=metadata))
                    
            logger.info(f"‚úÖ Retrieved {len(documents)} chunks for {len(found_stable_ids)} Stable IDs from '{collection_name}'.")
            return documents
        except Exception as e:
            logger.error(f"‚ùå Error retrieving documents by Chunk UUIDs from collection '{collection_name}': {e}")
            return []

# Helper function
def get_vectorstore_manager(
    doc_type: str = "all",           # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡πà‡∏≤ default
    tenant: str = DEFAULT_TENANT,
    year: Optional[int] = None,
    enabler: Optional[str] = None,
) -> VectorStoreManager:
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏∑‡∏ô VectorStoreManager (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏ó‡∏∏‡∏Å doc_type)
    """
    return VectorStoreManager(
        # doc_type=doc_type,
        tenant=tenant,
        # year=year or DEFAULT_YEAR,
        # enabler=enabler
    )

def load_vectorstore(doc_type: str, enabler: Optional[str] = None) -> Optional[Chroma]:
    collection_name = get_doc_type_collection_key(doc_type, enabler)
    vsm = get_vectorstore_manager(
        doc_type=doc_type,           # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ!
        enabler=enabler
    )
    return vsm._load_chroma_instance(collection_name)

class VectorStoreExecutorSingleton:
    _instance = None
    _is_initialized = False

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            cls._instance = super(VectorStoreExecutorSingleton, cls).__new__(cls)
        return cls._instance

    def __init__(self):
        if not VectorStoreExecutorSingleton._is_initialized:
            self.max_workers = MAX_PARALLEL_WORKERS
            self._executor = ThreadPoolExecutor(max_workers=self.max_workers)
            logger.info(f"Initialized VectorStoreExecutorSingleton (ThreadPoolExecutor with {self.max_workers} workers) for batch tasks.")
            VectorStoreExecutorSingleton._is_initialized = True

    @property
    def executor(self) -> ThreadPoolExecutor:
        return self._executor

    def close(self):
        if self._is_initialized:
            logger.info("Shutting down VectorStoreExecutorSingleton ThreadPoolExecutor...")
            self._executor.shutdown(wait=True)
            VectorStoreExecutorSingleton._is_initialized = False

def get_vectorstore() -> VectorStoreExecutorSingleton:
    return VectorStoreExecutorSingleton()

# -------------------- Custom Retriever for Chroma --------------------
class ChromaRetriever(BaseRetriever):
    vectorstore: Any
    k: int
    filter: Optional[Dict] = None
    model_config = ConfigDict(arbitrary_types_allowed=True)

    def _get_relevant_documents(self, query: str, *, run_manager=None) -> List[LcDocument]:
        
        # üéØ DEBUG: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Collection Count ‡πÅ‡∏•‡∏∞ Embedding (‡πÇ‡∏Ñ‡πâ‡∏î‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏°)
        try:
            raw_collection = self.vectorstore._collection
            count = raw_collection.count() 
            logger.critical(f"üéØ [DEBUG CHROMA] Collection Count: {count}")
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Query Embedding
            if hasattr(self.vectorstore, '_embedding_function') and self.vectorstore._embedding_function:
                embedding_function = self.vectorstore._embedding_function
                # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö embed query
                query_embedding = embedding_function.embed_query(query)
                logger.critical(f"üéØ [DEBUG CHROMA] Query Embedding Success. Vector size: {len(query_embedding)}")
            else:
                logger.critical("üéØ [DEBUG CHROMA] Cannot access _embedding_function.")

        except Exception as debug_e:
            logger.critical(f"‚ùå [DEBUG CHROMA] Debug check failed (Skip search): {debug_e}")
            return []
        # END DEBUG

        try:
            # ‡∏£‡∏±‡∏ô similarity search
            return self.vectorstore.similarity_search(query=query, k=self.k, filter=self.filter)
        except Exception as e:
            logger.error(f"‚ùå Chroma similarity_search failed in custom retriever: {e}")
            return []

    def get_relevant_documents(self, query: str, **kwargs) -> List[LcDocument]:
        return self._get_relevant_documents(query, **kwargs)

    async def _aget_relevant_documents(self, query: str, *, run_manager=None) -> List[LcDocument]:
        return self._get_relevant_documents(query, run_manager=run_manager)

# -------------------- MultiDoc / Parallel Retriever --------------------
class NamedRetriever(BaseModel):
    """
    Defines a single retriever configuration, mapping a document type/enabler
    to a specific VectorStore collection context (tenant/year).
    """
    doc_id: str
    doc_type: str
    top_k: int = INITIAL_TOP_K
    final_k: int = FINAL_K_RERANKED
    # üéØ FIX: ‡∏•‡∏ö base_path ‡∏≠‡∏≠‡∏Å
    enabler: Optional[str] = None
    tenant: str = DEFAULT_TENANT
    year: Optional[int] = DEFAULT_YEAR # <--- üéØ FIX: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô Optional[int] ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö None
    
    # load_instance ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏Å VSM ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ self.tenant ‡πÅ‡∏•‡∏∞ self.year
    def load_instance(self) -> Any:
        """
        Loads the actual VectorStore Retriever instance using VectorStoreManager 
        with the correct tenant and year context.
        """
        # ‚ö†Ô∏è VSM Singleton ‡∏ï‡πâ‡∏≠‡∏á‡∏ñ‡∏π‡∏Å init ‡∏î‡πâ‡∏ß‡∏¢‡∏õ‡∏µ‡πÄ‡∏™‡∏°‡∏≠ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÇ‡∏´‡∏•‡∏î Doc ID Mapping
        # ‡∏ñ‡πâ‡∏≤ self.year ‡πÄ‡∏õ‡πá‡∏ô None (‡πÄ‡∏ä‡πà‡∏ô ‡πÄ‡∏õ‡πá‡∏ô General Docs ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô‡∏ó‡∏∏‡∏Å‡∏õ‡∏µ) ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ DEFAULT_YEAR
        manager = VectorStoreManager(
            # üéØ FIX: ‡∏•‡∏ö base_path ‡∏≠‡∏≠‡∏Å
            tenant=self.tenant, 
            year=self.year if self.year is not None else DEFAULT_YEAR # <--- üéØ FIX: ‡πÉ‡∏ä‡πâ DEFAULT_YEAR ‡∏ñ‡πâ‡∏≤ self.year ‡πÄ‡∏õ‡πá‡∏ô None
        ) 
        # üéØ FIX: ‡πÉ‡∏ä‡πâ get_doc_type_collection_key ‡πÅ‡∏ó‡∏ô _get_collection_name
        collection_name = get_doc_type_collection_key(self.doc_type, self.enabler)
        
        retriever = manager.get_retriever(collection_name=collection_name, top_k=self.top_k, final_k=self.final_k) 
        
        if not retriever:
            raise ValueError(f"Retriever not found for collection '{collection_name}' at path based on tenant={self.tenant}, year={self.year}")
        
        return retriever


class MultiDocRetriever(BaseRetriever):
    model_config = ConfigDict(arbitrary_types_allowed=True)
    _all_retrievers: Dict[str, Any] = PrivateAttr(default_factory=dict)
    _retrievers_list: list[NamedRetriever] = PrivateAttr()
    _k_per_doc: int = PrivateAttr()
    _manager: VectorStoreManager = PrivateAttr()
    _doc_ids_filter: Optional[List[str]] = PrivateAttr()
    _chroma_filter: Optional[Dict[str, Any]] = PrivateAttr()
    _executor_type: str = PrivateAttr()
    _executor: Union[ThreadPoolExecutor, ProcessPoolExecutor, None] = PrivateAttr(None)

    def __init__(self, retrievers_list: list[NamedRetriever], k_per_doc: int = INITIAL_TOP_K, doc_ids_filter: Optional[List[str]] = None):
        super().__init__()
        self._retrievers_list = retrievers_list
        self._k_per_doc = k_per_doc

        # üéØ FIX 2: ‡∏î‡∏∂‡∏á Context ‡∏à‡∏≤‡∏Å NamedRetriever ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å
        tenant_context = retrievers_list[0].tenant if retrievers_list else DEFAULT_TENANT
        year_context = retrievers_list[0].year if retrievers_list else DEFAULT_YEAR

        # üéØ FIX 2A: ‡∏™‡∏£‡πâ‡∏≤‡∏á VSM ‡∏î‡πâ‡∏ß‡∏¢ Context ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Doc ID Mapping)
        # ‡∏ñ‡πâ‡∏≤ year_context ‡πÄ‡∏õ‡πá‡∏ô None (‡∏à‡∏≤‡∏Å NamedRetriever ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô General doc) ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ DEFAULT_YEAR
        if year_context is None:
             year_context = DEFAULT_YEAR 
             
        self._manager = VectorStoreManager(tenant=tenant_context, year=year_context)
        
        self._all_retrievers = {}
        for named_r in retrievers_list:
            # üéØ FIX: ‡πÉ‡∏ä‡πâ get_doc_type_collection_key ‡πÅ‡∏ó‡∏ô _get_collection_name
            collection_name = get_doc_type_collection_key(named_r.doc_type, named_r.enabler)
            try:
                # Load the RerankRetriever instance
                retriever_instance = named_r.load_instance()
                if retriever_instance:
                    self._all_retrievers[collection_name] = retriever_instance
                    logger.info(f"‚úÖ MultiDocRetriever cached collection: {collection_name}")
                else:
                    logger.warning(f"‚ö†Ô∏è Failed to load instance for {collection_name} during MDR init.")
            except Exception as e:
                logger.error(f"‚ùå Error loading instance {collection_name} into MDR cache: {e}")

        self._doc_ids_filter = doc_ids_filter
        self._chroma_filter = None
        if doc_ids_filter:
            # Chroma filter applied during retrieval
            self._chroma_filter = {"doc_id": {"$in": doc_ids_filter}}
            logger.info(f"‚úÖ MultiDocRetriever initialized with doc_ids filter for {len(doc_ids_filter)} Stable IDs.")

        # Using a simpler executor choice that doesn't rely on undefined imports
        self._executor_type = self._choose_executor() 
        logger.info(f"MultiDocRetriever will use executor type: {self._executor_type} (workers={MAX_PARALLEL_WORKERS})")
    
    def _choose_executor(self) -> str:
        """Selects the executor type based on basic platform info."""
        # Simplify executor choice to avoid dependency on undefined imports
        if platform.system() == "Windows":
             return "process"
        # Defaulting to thread pool for efficiency on other platforms unless specified otherwise
        return "thread" 

    def shutdown(self):
        if self._executor:
            executor_type_name = "ProcessPoolExecutor" if self._executor_type == "process" else "ThreadPoolExecutor"
            workers = self._executor._max_workers if hasattr(self._executor, "_max_workers") else "N/A"
            logger.info(f"Shutting down MultiDocRetriever's {executor_type_name} executor ({workers} workers).")
            self._executor.shutdown(wait=True)
            self._executor = None

    def __del__(self):
        try:
            self.shutdown()
        except Exception:
            pass

    def _get_executor(self) -> Union[ThreadPoolExecutor, ProcessPoolExecutor]:
        if self._executor is None:
            workers = MAX_PARALLEL_WORKERS
            if self._executor_type == "process":
                self._executor = ProcessPoolExecutor(max_workers=workers)
                logger.info(f"üõ†Ô∏è Using ProcessPoolExecutor with {workers} workers.")
            else:
                self._executor = ThreadPoolExecutor(max_workers=workers)
                logger.info(f"üõ†Ô∏è Using ThreadPoolExecutor with {workers} workers.")
        return self._executor

    @staticmethod
    def _static_retrieve_task(named_r: NamedRetriever, query: str, chroma_filter: Optional[Dict]):
        """Static task method for ProcessPoolExecutor."""
        try:
            # load_instance ensures the correct VSM context is used
            retriever_instance = named_r.load_instance()
            if not retriever_instance:
                return []
                
            # Prepare config for the invoke method of the RerankRetriever
            search_kwargs = {"k": named_r.top_k}
            if chroma_filter:
                # The Chroma filter is applied as 'where' in Chroma's implementation
                # üéØ FIX: ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ key 'where' ‡πÉ‡∏ô config ‡∏Ç‡∏≠‡∏á invoke()
                search_kwargs["where"] = chroma_filter 
                
            # üéØ FIX: ‡∏ï‡πâ‡∏≠‡∏á‡∏™‡πà‡∏á filter ‡πÄ‡∏õ‡πá‡∏ô 'where' ‡πÉ‡∏ô config ‡∏î‡πâ‡∏ß‡∏¢
            config = {"configurable": {"search_kwargs": {"where": chroma_filter}}}
            if not chroma_filter:
                config = {"configurable": {"search_kwargs": {}}} # ‡∏™‡πà‡∏á config ‡πÄ‡∏õ‡∏•‡πà‡∏≤‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ filter
            
            # retriever_instance is a RerankRetriever (which implements Runnable.invoke)
            docs = retriever_instance.invoke(query, config=config)
            
            for doc in docs:
                doc.metadata["retrieval_source"] = named_r.doc_type
                # üéØ FIX: ‡πÉ‡∏ä‡πâ get_doc_type_collection_key ‡πÅ‡∏ó‡∏ô _get_collection_name
                doc.metadata["collection_name"] = get_doc_type_collection_key(named_r.doc_type, named_r.enabler)
                
                # üéØ FIX 1A: Ensure chunk_uuid is present for final filtering.
                chunk_uuid = doc.metadata.get("chunk_uuid") 
                
                if not chunk_uuid:
                    # Try to find the ID from common Langchain/Chroma internal keys
                    potential_uuid = doc.metadata.get("id") or doc.metadata.get("_id") 
                    
                    if potential_uuid:
                        doc.metadata["chunk_uuid"] = str(potential_uuid)
                    else:
                        # Final Fallback: Use a stable hash of the content/metadata for deduplication/ID
                        key_content = f"{doc.page_content}{doc.metadata.get('doc_id')}"
                        hashed_uuid = hashlib.sha256(key_content.encode('utf-8')).hexdigest()
                        doc.metadata["chunk_uuid"] = hashed_uuid[:32] # Use first 32 chars for uniqueness
                        
            return docs
        except Exception as e:
            # Use print here as logger might not be configured correctly in child process
            print(f"‚ùå Child retrieval error for {named_r.doc_id} ({named_r.doc_type}): {e}")
            return []

    def _thread_retrieve_task(self, named_r: NamedRetriever, query: str, chroma_filter: Optional[Dict]):
        """Instance method for ThreadPoolExecutor."""
        try:
            # load_instance ensures the correct VSM context is used
            retriever_instance = named_r.load_instance()
            if not retriever_instance:
                return []
                
            # Prepare config for the invoke method of the RerankRetriever
            search_kwargs = {"k": named_r.top_k}
            if chroma_filter:
                # The Chroma filter is applied as 'where' in Chroma's implementation
                # üéØ FIX: ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ key 'where' ‡πÉ‡∏ô config ‡∏Ç‡∏≠‡∏á invoke()
                search_kwargs["where"] = chroma_filter 
                
            # üéØ FIX: ‡∏ï‡πâ‡∏≠‡∏á‡∏™‡πà‡∏á filter ‡πÄ‡∏õ‡πá‡∏ô 'where' ‡πÉ‡∏ô config ‡∏î‡πâ‡∏ß‡∏¢
            config = {"configurable": {"search_kwargs": {"where": chroma_filter}}}
            if not chroma_filter:
                config = {"configurable": {"search_kwargs": {}}} # ‡∏™‡πà‡∏á config ‡πÄ‡∏õ‡∏•‡πà‡∏≤‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ filter

            # retriever_instance is a RerankRetriever (which implements Runnable.invoke)
            docs = retriever_instance.invoke(query, config=config)
            
            for doc in docs:
                doc.metadata["retrieval_source"] = named_r.doc_type
                # üéØ FIX: ‡πÉ‡∏ä‡πâ get_doc_type_collection_key ‡πÅ‡∏ó‡∏ô _get_collection_name
                doc.metadata["collection_name"] = get_doc_type_collection_key(named_r.doc_type, named_r.enabler)

                # üéØ FIX 1A: Ensure chunk_uuid is present for final filtering.
                chunk_uuid = doc.metadata.get("chunk_uuid") 
                
                if not chunk_uuid:
                    # Try to find the ID from common Langchain/Chroma internal keys
                    potential_uuid = doc.metadata.get("id") or doc.metadata.get("_id") 
                    
                    if potential_uuid:
                        doc.metadata["chunk_uuid"] = str(potential_uuid)
                    else:
                        # Final Fallback: Use a stable hash of the content/metadata for deduplication/ID
                        key_content = f"{doc.page_content}{doc.metadata.get('doc_id')}"
                        hashed_uuid = hashlib.sha256(key_content.encode('utf-8')).hexdigest()
                        doc.metadata["chunk_uuid"] = hashed_uuid[:32] # Use first 32 chars for uniqueness
                    
            return docs
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Thread retrieval error for {named_r.doc_id}: {e}")
            return []

    def _get_relevant_documents(self, query: str, *, run_manager: Any = None) -> List[LcDocument]:
        max_workers = min(len(self._retrievers_list), MAX_PARALLEL_WORKERS)
        if max_workers <= 0:
            max_workers = 1
        chosen = self._executor_type
        logger.info(f"‚öôÔ∏è Running MultiDocRetriever with {chosen} executor ({max_workers} workers) [Filter: {bool(self._chroma_filter)}]")
        all_docs: List[LcDocument] = []
        
        executor = self._get_executor()
        futures = []
        for named_r in self._retrievers_list:
            if chosen == "process":
                # Use static method for ProcessPoolExecutor
                future = executor.submit(MultiDocRetriever._static_retrieve_task, named_r, query, self._chroma_filter)
            else:
                # Use instance method for ThreadPoolExecutor
                future = executor.submit(self._thread_retrieve_task, named_r, query, self._chroma_filter)
            futures.append(future)
            
        for f in futures:
            try:
                docs = f.result()
                if docs:
                    all_docs.extend(docs)
            except Exception as e:
                logger.warning(f"Future failed: {e}")
                
        # Deduplication using chunk metadata
        seen = set()
        unique_docs = []
        for d in all_docs:
            src = d.metadata.get("retrieval_source") or ""
            # Use 'chunk_uuid' or 'ids' (which is the UUID from Chroma) for unique identification
            # NOTE: chunk_uuid should now be present due to the fix in the task methods
            chunk_uuid = d.metadata.get("chunk_uuid") or d.metadata.get("ids") or "" 
            
            # Fallback to content if UUIDs are missing (less reliable)
            if not chunk_uuid:
                 # Use a hash or truncated content as a fallback unique key
                 key = f"{src}_{d.page_content[:120]}_{d.metadata.get('doc_id', 'no_doc_id')}"
            else:
                 key = f"{src}_{chunk_uuid}"
                 
            if key not in seen:
                seen.add(key)
                unique_docs.append(d)
                
        logger.info(f"üìù Query='{query[:80]}...' found {len(unique_docs)} unique docs across sources (Executor={chosen})")
        
        for d in unique_docs:
            score = d.metadata.get("relevance_score")
            if score is not None:
                logger.debug(f" - [Reranked] Source={d.metadata.get('doc_type')}, Score={score:.4f}, Content='{d.page_content[:80]}...'")
        
        return unique_docs

    def get_relevant_documents(self, query: str, **kwargs) -> List[LcDocument]:
        return self._get_relevant_documents(query, **kwargs)

# -------------------- load_all_vectorstores --------------------
def load_all_vectorstores( 
    tenant: str, 
    year: int,    
    doc_types: Optional[Union[str, List[str]]] = None, 
    top_k: int = INITIAL_TOP_K, 
    final_k: int = FINAL_K_RERANKED, 
    base_path: Path = "", # üéØ FIX: base_path ‡∏ñ‡∏π‡∏Å‡∏•‡∏∞‡πÄ‡∏•‡∏¢
    evidence_enabler: Optional[str] = None, 
    doc_ids: Optional[List[str]] = None
) -> 'VectorStoreManager':
    """
    Loads all relevant vectorstore collections based on tenant, year, and document types.
    Handles segregation logic for year-specific (evidence) and general (standard) documents.
    """
    
    doc_types = [doc_types] if isinstance(doc_types, str) else doc_types or []
    doc_type_filter = {dt.strip().lower() for dt in doc_types}
    
    # üìå 1. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î doc_type ‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö VSM
    # ‡πÄ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á‡∏ß‡πà‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Engine ‡∏à‡∏∞‡∏™‡πà‡∏á EVIDENCE_DOC_TYPES ‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å‡πÉ‡∏ô List
    # ‡πÉ‡∏ä‡πâ "all" ‡πÄ‡∏õ‡πá‡∏ô fallback (‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤ default ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏Ç‡∏≠‡∏á VSM ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î)
    primary_doc_type = doc_types[0] if doc_types else "all" 

    # üéØ FIX 1 & 1.1: ‡∏™‡∏£‡πâ‡∏≤‡∏á VSM ‡πÇ‡∏î‡∏¢‡πÉ‡∏™‡πà doc_type ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
    manager = VectorStoreManager(
        tenant=tenant, 
        year=year,
        enabler=evidence_enabler, # ‡πÉ‡∏™‡πà enabler ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå
        # üü¢ CRITICAL FIX: ‡∏™‡πà‡∏á doc_type ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á VSM
        doc_type=primary_doc_type 
    ) 
    
    all_retrievers: List['NamedRetriever'] = []
    target_collection_names: Set[str] = set()

    # --- 1. Collection Discovery ---
    if not doc_type_filter:
        logger.error("Must specify doc_types for multi-year compatibility.")
        raise ValueError("Must specify doc_types when using multi-tenant setup.")
    
    # NEW: ‡πÄ‡∏£‡∏≤‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ list collections ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÑ‡∏´‡∏ô‡πÉ‡∏ä‡πâ‡∏õ‡∏µ/‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ‡∏õ‡∏µ 
    # ‡πÄ‡∏£‡∏≤‡∏à‡∏∂‡∏á‡πÉ‡∏ä‡πâ doc_type_filter ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á collection_name ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£ list ‡∏à‡∏≤‡∏Å folder
    for dt_norm in doc_type_filter:
        if dt_norm == EVIDENCE_DOC_TYPES.lower(): 
            if evidence_enabler:
                # ‚úÖ FIX: Specific evidence collection: ‡πÉ‡∏ä‡πâ year ‡πÅ‡∏•‡∏∞ enabler
                collection_name = get_doc_type_collection_key(EVIDENCE_DOC_TYPES, evidence_enabler)
                target_collection_names.add(collection_name)
                logger.info(f"üîç Added specific evidence collection: {collection_name} (Year-Specific)")
            else:
                # All evidence collections: ‡∏ï‡πâ‡∏≠‡∏á list ‡∏à‡∏≤‡∏Å folder ‡∏†‡∏≤‡∏¢‡πÉ‡∏ï‡πâ tenant/year
                evidence_collections = list_vectorstore_folders(tenant=tenant, year=year, doc_type=EVIDENCE_DOC_TYPES)
                target_collection_names.update(evidence_collections)
                logger.info(f"üîç Added all evidence collections found: {evidence_collections} (Year-Specific)")
        else:
            # Standard Collections: ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ year
            collection_name = get_doc_type_collection_key(dt_norm, None)
            target_collection_names.add(collection_name)
            logger.info(f"üîç Added standard collection: {collection_name} (Shared/General)")
    
    logger.info(f"üîç DEBUG: Attempting to load {len(target_collection_names)} total target collections: {target_collection_names}")
    
    # --- 2. Retriever List Creation & Existence Check ---
    for collection_name in target_collection_names:
        doc_type_for_check, enabler_for_check = manager._re_parse_collection_name(collection_name)
        
        # üéØ FIX 2A: Logic ‡∏Å‡∏≥‡∏´‡∏ô‡∏î target_year (None ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö General Docs)
        target_year: Optional[int] = year
        if doc_type_for_check.lower() != EVIDENCE_DOC_TYPES.lower() and enabler_for_check is None:
            # ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£‡∏ñ‡∏π‡∏Å‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏°‡∏õ‡∏µ
            target_year = None # <--- ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏õ‡πá‡∏ô None
            
        # üéØ FIX 2B: ‡∏™‡πà‡∏á target_year ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏Ç‡∏≠‡∏á Vectorstore
        if not vectorstore_exists(doc_id="N/A", tenant=tenant, year=target_year, doc_type=doc_type_for_check, enabler=enabler_for_check):
            logger.warning(f"üîç DEBUG: Skipping collection '{collection_name}' (vectorstore_exists failed at tenant={tenant}, year={target_year}).")
            continue
            
        # üéØ FIX 2C: ‡∏™‡πà‡∏á target_year ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô NamedRetriever
        nr = NamedRetriever(
            doc_id=collection_name, 
            doc_type=doc_type_for_check, 
            enabler=enabler_for_check, 
            top_k=top_k, 
            final_k=final_k, 
            tenant=tenant, 
            year=target_year # <--- ‡∏™‡πà‡∏á None ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö General Docs
        )
        all_retrievers.append(nr)
        logger.info(f"üîç DEBUG: Successfully added retriever for collection '{collection_name}' (Year={target_year}).")

    final_filter_ids = doc_ids
    if doc_ids:
        logger.info(f"‚úÖ Hard Filter Enabled: Using {len(doc_ids)} original 64-char UUIDs for filtering.")
    logger.info(f"üîç DEBUG: Final count of all_retrievers = {len(all_retrievers)}")

    if not all_retrievers:
        raise ValueError(f"No vectorstore collections found matching tenant={tenant}, year={year}, doc_types={doc_types} and evidence_enabler={evidence_enabler}")
        
    mdr = MultiDocRetriever(retrievers_list=all_retrievers, k_per_doc=top_k, doc_ids_filter=final_filter_ids)
    manager._multi_doc_retriever = mdr
    logger.info(f"‚úÖ MultiDocRetriever loaded with {len(mdr._all_retrievers)} collections and cached in VSM.")
    return manager


def get_multi_doc_retriever(
    tenant: str = DEFAULT_TENANT,
    year: int = DEFAULT_YEAR,
    doc_types: List[str] = [],
    doc_ids: Optional[List[str]] = None,
    evidence_enabler: Optional[str] = None,
    base_path: str = "", # üéØ FIX: base_path ‡∏ñ‡∏π‡∏Å‡∏•‡∏∞‡πÄ‡∏•‡∏¢
    top_k: int = INITIAL_TOP_K,
    final_k: int = FINAL_K_RERANKED
) -> MultiDocRetriever:
    """
    Factory function to create a MultiDocRetriever based on configuration.
    It determines which NamedRetrievers to initialize based on the tenant, year, and doc_types.
    """
    all_retrievers: List[NamedRetriever] = []

    # 1. Dynamic Check for Year-Specific Collections
    # Loop through requested doc_types and check against the target year
    target_year = year
    for doc_type_for_check in doc_types:
        # üéØ FIX: ‡πÉ‡∏ä‡πâ get_doc_type_collection_key ‡πÅ‡∏ó‡∏ô _get_collection_name
        collection_name = get_doc_type_collection_key(doc_type_for_check, evidence_enabler)
        
        enabler_for_check = evidence_enabler
        
        # Check if collection exists for the specific year
        if not vectorstore_exists(tenant=tenant, year=target_year, doc_type=doc_type_for_check, enabler=enabler_for_check):
            logger.warning(f"üîç DEBUG: Skipping collection '{collection_name}' (vectorstore_exists failed at tenant={tenant}, year={target_year}).")
            continue
            
        # üéØ FIX 2C: ‡∏™‡πà‡∏á target_year ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô NamedRetriever
        nr = NamedRetriever(
            doc_id=collection_name, 
            doc_type=doc_type_for_check, 
            enabler=enabler_for_check, 
            top_k=top_k, 
            final_k=final_k, 
            # üéØ FIX: ‡∏•‡∏ö base_path ‡∏≠‡∏≠‡∏Å
            tenant=tenant, 
            year=target_year # <--- ‡∏™‡πà‡∏á‡∏Ñ‡πà‡∏≤‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
        )
        all_retrievers.append(nr)
        logger.info(f"üîç DEBUG: Successfully added retriever for collection '{collection_name}' (Year={target_year}).")

    final_filter_ids = doc_ids
    if doc_ids:
        logger.info(f"‚úÖ Hard Filter Enabled: Using {len(doc_ids)} original 64-char UUIDs for filtering.")
    logger.info(f"üîç DEBUG: Final count of all_retrievers = {len(all_retrievers)}")

    if not all_retrievers:
        raise ValueError(f"No vectorstore collections found matching tenant={tenant}, year={year}, doc_types={doc_types} and evidence_enabler={evidence_enabler}")
        
    mdr = MultiDocRetriever(retrievers_list=all_retrievers, k_per_doc=top_k, doc_ids_filter=final_filter_ids)
    return mdr