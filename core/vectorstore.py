# core/vectorstore.py
import os
import platform
import logging
import threading
import multiprocessing
import json
import shutil
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from typing import List, Optional, Union, Sequence, Any, Dict, Set, Tuple
from pathlib import Path
import hashlib
from threading import Lock
import threading # <-- ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ
import uuid

# system utils
try:
    import psutil
except ImportError:
    psutil = None

# LangChain-ish imports (adjust to your project's versions)
from langchain_core.documents import Document as LcDocument # Document (LangChain Core)
from langchain_core.retrievers import BaseRetriever # BaseRetriever
from langchain_core.documents import BaseDocumentCompressor
from langchain_core.callbacks.manager import CallbackManagerForRetrieverRun
from langchain_core.runnables import Runnable 
# üí° NEW/FIX: Imports ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Hybrid Search
from langchain_community.retrievers import BM25Retriever # FIX: Import BM25 ‡∏à‡∏≤‡∏Å community
from langchain.retrievers import EnsembleRetriever
from langchain_core.documents import Document

# ...
# üí° NEW/FIX: Import ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Thai Tokenizer
from pythainlp.tokenize import word_tokenize # ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á: pip install pythainlp

# Pydantic helpers
from pydantic import PrivateAttr, ConfigDict, BaseModel, Field

# Chroma / HF embeddings
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
import chromadb
from chromadb.config import Settings
from sentence_transformers import CrossEncoder

# üí° NEW: Import Path Utilities (‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏ä‡∏∑‡πà‡∏≠‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö utils/path_utils.py ‡πÉ‡∏´‡∏°‡πà)
from utils.path_utils import (
    get_doc_type_collection_key, # ‡πÉ‡∏ä‡πâ‡πÅ‡∏ó‡∏ô _get_collection_name
    get_vectorstore_collection_path, 
    get_vectorstore_tenant_root_path,
    get_mapping_file_path, # ‡πÉ‡∏ä‡πâ‡πÅ‡∏ó‡∏ô‡∏ó‡∏±‡πâ‡∏á year_specific ‡πÅ‡∏•‡∏∞ tenant_root
    # ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ get_vectorstore_collection_parent_dir ‡πÅ‡∏•‡πâ‡∏ß
)


# Logging
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)


# Try import CrossEncoder (sentence-transformers)
try:
    from sentence_transformers import CrossEncoder
    _HAS_SENT_TRANS = True
except Exception:
    CrossEncoder = None
    _HAS_SENT_TRANS = False
    logger.warning("‚ö†Ô∏è sentence-transformers CrossEncoder not available. Reranker will be disabled.")

# Configure chromadb telemetry if available
try:
    chromadb.configure(anonymized_telemetry=False)
except Exception:
    try:
        chromadb.settings = Settings(anonymized_telemetry=False)
    except Exception:
        pass


# *****************************************************************
# [NEW FUNCTION] Thai Tokenizer (‡∏ß‡∏≤‡∏á‡πÑ‡∏ß‡πâ‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ, ‡∏î‡πâ‡∏≤‡∏ô‡∏ô‡∏≠‡∏Å‡∏Ñ‡∏•‡∏≤‡∏™)
# *****************************************************************
def thai_tokenizer_for_bm25(text: str) -> List[str]:
    """‡πÉ‡∏ä‡πâ PyThaiNLP ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏ö‡πà‡∏á‡∏Ñ‡∏≥‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö BM25Retriever"""
    return word_tokenize(text.lower().strip())
# *****************************************************************


# -------------------- Global Config (Path Vars Removed) --------------------
from config.global_vars import (
    # üí• ‡∏•‡∏ö VECTORSTORE_DIR, MAPPING_BASE_DIR
    FINAL_K_RERANKED,
    INITIAL_TOP_K,
    EVIDENCE_DOC_TYPES,
    MAX_PARALLEL_WORKERS,
    DEFAULT_TENANT,
    DEFAULT_YEAR,
    DEFAULT_ENABLER,
    RERANKER_MODEL_NAME,
    EMBEDDING_MODEL_NAME,
    USE_HYBRID_SEARCH,
    HYBRID_BM25_WEIGHT,
    HYBRID_VECTOR_WEIGHT

)

# -------------------- Vectorstore Constants --------------------
ENV_FORCE_MODE = os.getenv("VECTOR_MODE", "").lower()  # "thread", "process", or ""
ENV_DISABLE_ACCEL = os.getenv("VECTOR_DISABLE_ACCEL", "").lower() in ("1", "true", "yes")

# Global caches (per process)
_CACHED_EMBEDDINGS = None
_EMBED_LOCK = threading.Lock()
_MPS_WARNING_SHOWN = False

# -------------------- Helper: detect environment & device --------------------
@staticmethod
def _detect_system():
    cpu_count = os.cpu_count() or 4
    total_ram_gb = None
    if psutil:
        try:
            total_ram_gb = psutil.virtual_memory().total / (1024 ** 3)
        except Exception:
            total_ram_gb = None
    return {"cpu_count": cpu_count, "total_ram_gb": total_ram_gb, "platform": platform.system().lower()}

@staticmethod
def _detect_torch_device():
    try:
        import torch
        if ENV_DISABLE_ACCEL:
            return "cpu"
        if torch.cuda.is_available():
            return "cuda"
        if platform.system().lower() == "darwin" and hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
            # MPS can be fragile for multiprocessing; we'll warn and possibly force cpu later
            return "mps"
    except Exception:
        pass
    return "cpu"

def get_hf_embeddings(device_hint: Optional[str] = None):
    global _CACHED_EMBEDDINGS, _MPS_WARNING_SHOWN
    device = device_hint or _detect_torch_device()

    if _CACHED_EMBEDDINGS is None:
        # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ _EMBED_LOCK ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ thread safe
        # with _EMBED_LOCK: 
        if _CACHED_EMBEDDINGS is None:
            
            # üü¢ ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏°‡∏≤‡πÉ‡∏ä‡πâ Global Variable ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÑ‡∏ß‡πâ‡πÉ‡∏ô global_vars.py
            model_name = EMBEDDING_MODEL_NAME 

            logger.info(f"Loading BEST Thai RAG embedding 2025: {model_name} on {device}")
            logger.info("This model will be used to build ALL PEA 2568 vectorstores (evidence_km, document, etc.)")
            
            try:
                # 
                _CACHED_EMBEDDINGS = HuggingFaceEmbeddings(
                    model_name=model_name,
                    model_kwargs={
                        "device": device,
                        # BGE-M3 ‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ prefix!
                    },
                    encode_kwargs={
                        "normalize_embeddings": True,
                        # ‡∏Å‡∏≤‡∏£‡∏•‡∏ö 'prompt': 'query:' ‡∏≠‡∏≠‡∏Å ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö BGE-M3 ‡∏ô‡∏±‡πâ‡∏ô ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡∏£‡∏±‡∏ö!
                    }
                )
            except Exception as e:
                logger.error(f"Failed to load {model_name}: {e}")
                logger.warning("Falling back to paraphrase-multilingual-MiniLM-L12-v2")
                # ‡πÉ‡∏ä‡πâ Fallback model ‡∏ï‡∏±‡∏ß‡πÄ‡∏î‡∏¥‡∏°
                _CACHED_EMBEDDINGS = HuggingFaceEmbeddings(
                    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
                    model_kwargs={"device": "cpu"}
                )
    return _CACHED_EMBEDDINGS

# =================================================================
# HuggingFace Cross-Encoder Reranker wrapper (singleton)
# =================================================================
class HuggingFaceCrossEncoderCompressor(BaseDocumentCompressor):
    model_config = ConfigDict(arbitrary_types_allowed=True)
    
    rerank_model: str = RERANKER_MODEL_NAME
    rerank_device: str = "cpu"
    rerank_max_length: int = 512
    top_n: int = FINAL_K_RERANKED
    
    _cross_encoder: Optional[Any] = PrivateAttr(default=None)

    def __init__(self, **data):
        super().__init__(**data)
        self.rerank_device = "cpu"  # Force CPU

        try:
            logger.info(f"Loading CrossEncoder: {self.rerank_model} on CPU")
            encoder = CrossEncoder(
                model_name_or_path=self.rerank_model,  # ‡πÅ‡∏Å‡πâ deprecated warning
                device=self.rerank_device,
                max_length=self.rerank_max_length,
            )
            # üéØ ‡πÉ‡∏ä‡πâ object.__setattr__ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ö‡∏≤‡∏¢‡∏û‡∏≤‡∏™ Pydantic validation
            object.__setattr__(self, '_cross_encoder', encoder)
            logger.info(f"‚úÖ CrossEncoder loaded successfully: {self.rerank_model}")
        except Exception as e:
            logger.error(f"‚ùå Failed to load CrossEncoder {self.rerank_model}: {e}", exc_info=True)
            object.__setattr__(self, '_cross_encoder', None)

    def compress_documents(
        self,
        documents: Sequence[LcDocument],
        query: str,
        top_n: int = FINAL_K_RERANKED,  # <--- ‡πÄ‡∏û‡∏¥‡πà‡∏° parameter ‡∏ô‡∏µ‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏ö‡∏à‡∏≤‡∏Å caller (LangChain ‡∏™‡πà‡∏á‡∏°‡∏≤)
        callbacks: Optional[Any] = None,
    ) -> List[LcDocument]:
        if not documents:
            return []

        # ‡πÉ‡∏ä‡πâ top_n ‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏°‡∏≤‡∏à‡∏≤‡∏Å caller (LangChain) ‡∏´‡∏£‡∏∑‡∏≠ fallback ‡πÑ‡∏õ FINAL_K_RERANKED
        effective_top_n = top_n if top_n is not None else FINAL_K_RERANKED

        # ‡∏î‡∏∂‡∏á global instance ‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î model ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡πÉ‡∏ô main thread
        global_reranker_instance = get_global_reranker()

        # ‡∏î‡∏∂‡∏á CrossEncoder model ‡∏à‡∏£‡∏¥‡∏á
        reranker_to_use = None
        if global_reranker_instance is not None:
            reranker_to_use = global_reranker_instance._cross_encoder

        # Fallback ‡∏ñ‡πâ‡∏≤ model ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°
        if reranker_to_use is None:
            logger.error("HuggingFace Cross-Encoder is not available in this thread. Returning truncated documents.")
            return list(documents)[:effective_top_n]

        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ñ‡∏π‡πà query-document
        pairs = [[query, doc.page_content] for doc in documents]

        # Reranking ‡∏à‡∏£‡∏¥‡∏á
        try:
            scores = reranker_to_use.predict(
                pairs,
                batch_size=32,
                show_progress_bar=False,
            )
        except Exception as e:
            logger.error(f"Reranking failed: {e}", exc_info=True)
            return list(documents)[:effective_top_n]

        # ‡πÄ‡∏û‡∏¥‡πà‡∏° relevance_score ‡πÄ‡∏Ç‡πâ‡∏≤ metadata ‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á
        scored_docs = []
        for doc, score in zip(documents, scores):
            if doc.metadata is None:
                doc.metadata = {}
            doc.metadata["relevance_score"] = float(score)
            scored_docs.append((score, doc))

        scored_docs.sort(key=lambda x: x[0], reverse=True)
        final_docs = [doc for _, doc in scored_docs[:effective_top_n]]

        # Log ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå reranking
        if final_docs:
            top_score = scored_docs[0][0]
            logger.info(f"‚úÖ Reranking completed | Top score: {top_score:.4f} | Model: {RERANKER_MODEL_NAME} | Returned: {len(final_docs)} docs")

        return final_docs

# -------------------- Reranker Cache (GLOBAL SINGLETON - FIXED) --------------------
_global_reranker_instance = None
_global_reranker_lock = threading.Lock()

def get_global_reranker() -> Optional[HuggingFaceCrossEncoderCompressor]:
    global _global_reranker_instance
    with _global_reranker_lock:
        if _global_reranker_instance is None:
            try:
                # 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á instance ‡∏Å‡πà‡∏≠‡∏ô (Pydantic ‡∏à‡∏∞ validate fields ‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤)
                _global_reranker_instance = HuggingFaceCrossEncoderCompressor(
                    rerank_model=RERANKER_MODEL_NAME,
                    top_n=FINAL_K_RERANKED
                )
                logger.info("Created HuggingFaceCrossEncoderCompressor instance for global reranker")
            except Exception as e:
                logger.error(f"Failed to create HuggingFaceCrossEncoderCompressor instance: {e}")
                _global_reranker_instance = None
                return None

            # 2. ‡πÇ‡∏´‡∏•‡∏î CrossEncoder ‡πÅ‡∏•‡∏∞ set ‡∏î‡πâ‡∏ß‡∏¢ object.__setattr__
            try:
                encoder_instance = CrossEncoder(
                    model_name_or_path=RERANKER_MODEL_NAME,  # ‡πÅ‡∏Å‡πâ deprecated warning
                    device="cpu",  # Force CPU ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£
                    max_length=512
                )
                
                # ‡∏ö‡∏≤‡∏¢‡∏û‡∏≤‡∏™ Pydantic
                object.__setattr__(_global_reranker_instance, '_cross_encoder', encoder_instance)
                
                logger.info(f"‚úÖ Global CrossEncoder loaded successfully: {RERANKER_MODEL_NAME} on CPU")
            except Exception as e:
                logger.error(f"‚ùå Failed to load global CrossEncoder: {e}", exc_info=True)
                object.__setattr__(_global_reranker_instance, '_cross_encoder', None)
                # ‡πÑ‡∏°‡πà return None ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ instance ‡∏¢‡∏±‡∏á‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà (compress_documents ‡∏à‡∏∞ fallback)

        return _global_reranker_instance

# -------------------- Path Helper Function (REVISED to use Path Utility) --------------------

def get_vectorstore_path(
    tenant: str, 
    year: Optional[int], 
    doc_type: Optional[str] = None, 
    enabler: Optional[str] = None
) -> str:
    """
    Calculates the full persist directory path for the vector store instance
    based on the Centralized KM Logic by calling path_utils.
    """
    # üéØ FIX: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏õ‡πÉ‡∏ä‡πâ get_vectorstore_tenant_root_path
    if not doc_type:
        return get_vectorstore_tenant_root_path(tenant=tenant)
        
    # üéØ FIX: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏õ‡πÉ‡∏ä‡πâ get_vectorstore_collection_path
    return get_vectorstore_collection_path(
        tenant=tenant, 
        year=year, 
        doc_type=doc_type, 
        enabler=enabler, 
        # EVIDENCE_DOC_TYPES ‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏™‡πà‡∏á‡πÅ‡∏•‡πâ‡∏ß ‡∏ñ‡∏π‡∏Å hardcode ‡πÉ‡∏ô path_utils
    )

def vectorstore_exists(
    doc_id: str = "N/A", # ‡∏£‡∏±‡∏Å‡∏©‡∏≤‡πÑ‡∏ß‡πâ‡∏ï‡∏≤‡∏° Signature ‡πÄ‡∏î‡∏¥‡∏°
    tenant: str = DEFAULT_TENANT,
    year: Optional[int] = DEFAULT_YEAR, # <--- ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö None ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö General Docs
    doc_type: Optional[str] = None, 
    enabler: Optional[str] = None, 
    base_path: str = "" # base_path ‡∏ñ‡∏π‡∏Å‡∏•‡∏∞‡πÄ‡∏•‡∏¢ ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÉ‡∏ä‡πâ global VECTORSTORE_DIR ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô path_utils
) -> bool:
    """
    Checks if the Vector Store directory exists for the given context.
    """
    if not doc_type:
        return False
        
    # 1. Get the full path using the updated logic (‡πÄ‡∏£‡∏µ‡∏¢‡∏Å get_vectorstore_path ‡πÉ‡∏´‡∏°‡πà)
    # NOTE: get_vectorstore_path ‡∏à‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Path ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÇ‡∏î‡∏¢‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å doc_type (‡∏°‡∏µ‡∏õ‡∏µ/‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏µ)
    path = get_vectorstore_path(tenant, year, doc_type, enabler) 
    
    # 2. Check for the actual data file created by Chroma
    file_path = os.path.join(path, "chroma.sqlite3")
    
    if not os.path.isdir(path):
        logger.warning(f"‚ùå V-Exists Check: Directory not found for doc_type '{doc_type}' at {path}")
        return False
    if os.path.isfile(file_path):
        return True
    logger.error(f"‚ùå V-Exists Check: FAILED to find file chroma.sqlite3 at {file_path} in {path}")
    return False

# ‚ö†Ô∏è ‡∏•‡∏ö‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô _get_collection_parent_dir ‡∏≠‡∏≠‡∏Å (Logic ‡∏ñ‡∏π‡∏Å‡∏¢‡∏∏‡∏ö‡∏£‡∏ß‡∏°‡πÅ‡∏•‡∏∞‡∏ñ‡∏π‡∏Å‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡πÉ‡∏ô list_vectorstore_folders)

def list_vectorstore_folders(
    tenant: str, 
    year: int, # NOTE: ‡πÉ‡∏ä‡πâ‡∏õ‡∏µ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ô (int)
    doc_type: Optional[str] = None, 
    enabler: Optional[str] = None, 
    base_path: str = "" # base_path ‡∏ñ‡∏π‡∏Å‡∏•‡∏∞‡πÄ‡∏•‡∏¢
) -> List[str]:
    """
    Lists the actual collection folder names (e.g., 'evidence_km', 'document')
    that exist under the specified tenant and year context.
    """
    # üéØ FIX: ‡πÉ‡∏ä‡πâ get_vectorstore_tenant_root_path ‡πÅ‡∏ó‡∏ô
    tenant_root = get_vectorstore_tenant_root_path(tenant) # VECTORSTORE_DIR / tenant
    
    # Scenario 1: Specific doc_type/enabler is requested
    if doc_type:
        doc_type_norm = doc_type.lower().strip()
        # üéØ FIX: ‡πÉ‡∏ä‡πâ get_doc_type_collection_key ‡πÅ‡∏ó‡∏ô _get_collection_name
        collection_name = get_doc_type_collection_key(doc_type_norm, enabler)
        
        # ‡∏ï‡πâ‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì target_year ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡πà‡∏≠‡∏ô (None ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö General Docs)
        target_year = year
        if doc_type_norm != EVIDENCE_DOC_TYPES.lower():
            target_year = None
            
        # üéØ FIX: ‡πÉ‡∏ä‡πâ get_vectorstore_collection_path
        full_collection_path = get_vectorstore_collection_path(tenant, target_year, doc_type_norm, enabler)
        
        if os.path.isdir(full_collection_path) and os.path.isfile(os.path.join(full_collection_path, "chroma.sqlite3")):
            return [collection_name] 
        return []

    # Scenario 2: List ALL collections for the given tenant/year context (List All)
    
    collections: Set[str] = set()
    
    # 1. Scan the Year Root (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Doc Type: evidence) - Path: V_ROOT/tenant/year
    root_year_evidence = os.path.join(tenant_root, str(year)) 
    if os.path.isdir(root_year_evidence):
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ evidence_... collections ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏õ‡∏µ
        for sub_dir in os.listdir(root_year_evidence):
             sub_dir_lower = sub_dir.lower()
             if sub_dir_lower.startswith(f"{EVIDENCE_DOC_TYPES.lower()}_"): 
                 full_collection_path = os.path.join(root_year_evidence, sub_dir)
                 if os.path.isfile(os.path.join(full_collection_path, "chroma.sqlite3")):
                    collections.add(sub_dir_lower) 

    # 2. Scan the Common Root (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Doc Type: document, faq, ‡∏Ø‡∏•‡∏Ø) - Path: V_ROOT/tenant
    root_common = tenant_root 
    if os.path.isdir(root_common):
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå Doc Type
        for sub_dir in os.listdir(root_common):
            sub_dir_lower = sub_dir.lower()
            
            # ‡∏Ç‡πâ‡∏≤‡∏°‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç (‡∏õ‡∏µ) ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ñ‡∏π‡∏Å‡∏™‡πÅ‡∏Å‡∏ô‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1 ‡πÅ‡∏•‡πâ‡∏ß
            if sub_dir.isdigit():
                 continue 
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏õ‡πá‡∏ô Collection ‡∏à‡∏£‡∏¥‡∏á (‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå Chroma)
            full_collection_path = os.path.join(root_common, sub_dir)
            
            if os.path.isfile(os.path.join(full_collection_path, "chroma.sqlite3")):
                 collections.add(sub_dir_lower) 
    
    return sorted(list(collections))


# -------------------- VECTORSTORE MANAGER (SINGLETON) --------------------
class VectorStoreManager:
    _instance = None
    _is_initialized = False
    _lock = threading.Lock()

    # ‡πÉ‡∏ä‡πâ default_factory ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏™‡πà {} ‡∏´‡∏£‡∏∑‡∏≠ None ‡∏ï‡∏£‡∏á‡πÜ
    _chroma_cache: Dict[str, Chroma] = PrivateAttr(default_factory=dict)
    _multi_doc_retriever: Optional['MultiDocRetriever'] = PrivateAttr(default=None)
    
    tenant: str = PrivateAttr(default=DEFAULT_TENANT)
    year: int = PrivateAttr(default=DEFAULT_YEAR)

    _doc_id_mapping: Dict[str, Dict[str, Any]] = PrivateAttr(default_factory=dict)
    _uuid_to_doc_id: Dict[str, str] = PrivateAttr(default_factory=dict)

    _embeddings: Any = PrivateAttr(default=None)

    # ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î: ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ default_factory=dict ‡∏´‡∏£‡∏∑‡∏≠ default=None ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô!
    _client: Optional[chromadb.PersistentClient] = PrivateAttr(default=None)

    def __new__(cls, *args, **kwargs):
        with cls._lock:
            if cls._instance is None:
                cls._instance = super(VectorStoreManager, cls).__new__(cls)
        return cls._instance

    def __init__(self, base_path: str = "", tenant: str = DEFAULT_TENANT,  year: Optional[int] = None, enabler: Optional[str] = None, doc_type: str = EVIDENCE_DOC_TYPES,): 
        # üìå FIX: ‡∏ó‡∏≥‡πÉ‡∏´‡πâ init ‡∏£‡∏±‡∏ö‡πÅ‡∏Ñ‡πà base_path ‡πÅ‡∏•‡∏∞ tenant ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ñ‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô Singleton
        if not self._is_initialized:
            self._base_path = base_path
            self.tenant = tenant.lower()
            
            # üí° FIX: ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÉ‡∏´‡πâ Attributes ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡πÄ‡∏°‡∏ò‡∏≠‡∏î‡∏≠‡∏∑‡πà‡∏ô ‡πÜ ‡∏Ç‡∏≠‡∏á Class
            self.year = year if year is not None else DEFAULT_YEAR    
            self.doc_type = doc_type
            self.enabler = enabler.upper() if enabler else DEFAULT_ENABLER 
            
            self._chroma_cache = {}
            self._embeddings = get_hf_embeddings()
            

            client_base_path = self._get_chroma_client_base_path(tenant, year)

            chroma_client_root = get_vectorstore_tenant_root_path(tenant=self.tenant)
            self._client = chromadb.PersistentClient(path=client_base_path)

            self._hybrid_retriever_cache: Dict[str, EnsembleRetriever] = {}
            self._bm25_docs_cache: Dict[str, List[Document]] = {}

            logger.info(f"ChromaDB Client initialized at CLIENT BASE PATH: {client_base_path}")
            
            self._load_doc_id_mapping()
            
            logger.info(f"Initialized VectorStoreManager (Tenant: {self.tenant})") 
            
            VectorStoreManager._is_initialized = True
    
    def _get_chroma_client_base_path(self, tenant: str, year: Optional[int]) -> str:
        """
        Determines the base path for the Chroma PersistentClient.
        
        For year-specific document types (like 'evidence'), the base path
        must point to the YEAR folder, not just the root 'vectorstore'.
        """
        # ‡πÉ‡∏ä‡πâ Path ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡∏ç‡πà‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Ñ‡∏∑‡∏≠ Root Path
        root_path = get_vectorstore_tenant_root_path(tenant) 
        
        # NOTE: Logic ‡∏ô‡∏µ‡πâ‡∏≠‡∏≤‡∏à‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö‡∏ß‡πà‡∏≤ VSM ‡∏ñ‡∏π‡∏Å‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á
        # ‡πÅ‡∏ï‡πà‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤ KM/2568: ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏∏‡∏õ‡∏µ ‡πÉ‡∏´‡πâ‡∏ä‡∏µ‡πâ‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏õ‡∏µ‡∏ô‡∏±‡πâ‡∏ô‡πÜ
        
        if year is not None:
             # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Evidence (‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô doc_type ‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏õ‡∏µ)
             # ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏ä‡∏µ‡πâ Path Client ‡πÑ‡∏õ‡∏ó‡∏µ‡πà .../vectorstore/2568
             return os.path.join(root_path, str(year))
        
        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Collection ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ (Global Docs) ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏Ç‡∏∂‡πâ‡∏ô‡∏Å‡∏±‡∏ö‡∏õ‡∏µ
        return root_path
    
    # -------------------- START FIXES (3 Functions) --------------------
    
    def set_multi_doc_retriever(self, mdr: 'MultiDocRetriever'):
        """
        Sets the active MultiDocRetriever instance.
        NOTE: This is the setter for the PrivateAttr _multi_doc_retriever.
        (FIX: ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç AttributeError ‡πÉ‡∏ô load_all_vectorstores)
        """
        # üéØ FIX: ‡πÉ‡∏ä‡πâ object.__setattr__ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ PrivateAttr
        object.__setattr__(self, '_multi_doc_retriever', mdr)
        logger.info("‚úÖ MultiDocRetriever has been set in VectorStoreManager.")

    def get_multi_doc_retriever(self) -> Optional['MultiDocRetriever']:
        """Gets the active MultiDocRetriever instance."""
        return self._multi_doc_retriever

    @property
    def client(self) -> Optional[chromadb.PersistentClient]:
        """
        Provides access to the underlying Chroma Persistent Client (Re-validate in worker).
        (FIX: ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç AttributeError ‡πÉ‡∏ô get_retriever)
        """
        # üéØ FIX: ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å ensure ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤ client ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏´‡∏≤‡∏¢‡πÑ‡∏õ‡πÉ‡∏ô worker context
        self._ensure_chroma_client_is_valid()
        return self._client
    
    # -------------------- END FIXES --------------------

    @property
    def doc_id_map(self) -> Dict[str, Dict[str, Any]]:
        """Provides access to the Stable Doc ID -> Chunk UUIDs mapping."""
        return self._doc_id_mapping

    @property
    def uuid_to_doc_id_map(self) -> Dict[str, str]:
        """Provides access to the Chunk UUID -> Stable Doc ID mapping."""
        return self._uuid_to_doc_id
    
    def close(self):
        with self._lock:
            if self._multi_doc_retriever and hasattr(self._multi_doc_retriever, "shutdown"):
                logger.info("Closing MultiDocRetriever executor via VSM.")
                self._multi_doc_retriever.shutdown()
                self._multi_doc_retriever = None
            self._chroma_cache = {}
            VectorStoreManager._is_initialized = False

    def __del__(self):
        try:
            self.close()
        except Exception:
            pass

    def _load_doc_id_mapping(self):
        """
        ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏£‡∏ß‡∏° Document ID Mapping ‡∏à‡∏≤‡∏Å 2 Path (Global + Year/Enabler Specific) ‡πÅ‡∏ö‡∏ö thread-safe
        """

        # Lock ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö thread-safe update
        if not hasattr(self, "_mapping_lock") or self._mapping_lock is None:
            self._mapping_lock = Lock()

        self._doc_id_mapping = {}
        self._uuid_to_doc_id = {}

        # üéØ CRITICAL: ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤ attributes ‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡πÉ‡∏ô Worker Context
        current_tenant = getattr(self, 'tenant', 'default_tenant')
        current_year = getattr(self, 'year', None)
        current_enabler = getattr(self, 'enabler', None)
        # üéØ FIX: ‡πÉ‡∏ä‡πâ self.doc_type ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÉ‡∏ô __init__ 
        current_doc_type = getattr(self, 'doc_type', EVIDENCE_DOC_TYPES) 

        logger.info(f"üîç VSM MAP LOAD PARAMS: Tenant={current_tenant}, Year={current_year}, "
                     f"Enabler={current_enabler}, DocType={current_doc_type}")
        
        path_A = None # Specific Map
        path_B = None # Global Map

        # 1. PATH A: Year-Specific/Enabler Mapping
        try:
            path_A = get_mapping_file_path(
                doc_type=current_doc_type,
                tenant=current_tenant, 
                year=current_year, 
                enabler=current_enabler
            )
        except ValueError as e:
            logger.warning(f"‚ö†Ô∏è VSM MAP PATH A (Specific) failed generation: {e}. Skipping specific map.")
            path_A = None
        
        # 2. PATH B: Global/Tenant Root Mapping
        try:
             path_B = get_mapping_file_path(
                doc_type=current_doc_type,
                tenant=current_tenant,
                year=None, # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô None ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Path Logic ‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡πÅ‡∏ö‡∏ö Global
                enabler=None 
            )
        except ValueError as e:
            logger.warning(f"‚ö†Ô∏è VSM MAP PATH B (Global) failed generation: {e}. Skipping global map.")
            path_B = None
            
        # üéØ FIX: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î (Specific ‡∏Å‡πà‡∏≠‡∏ô Global, ‡∏ñ‡πâ‡∏≤ Path ‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô)
        paths_to_load = []
        # A ‡∏Å‡πà‡∏≠‡∏ô B ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Specific ‡∏ó‡∏±‡∏ö Global
        if path_A and os.path.exists(path_A):
            paths_to_load.append(path_A)
        # B ‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ö A
        if path_B and path_B != path_A and os.path.exists(path_B):
            paths_to_load.append(path_B)


        # Log Path details
        logger.info(f"üîç VSM MAP PATH A (Specific): {path_A} (Exists: {os.path.exists(path_A) if path_A else 'N/A'})")
        logger.info(f"üîç VSM MAP PATH B (Global): {path_B} (Exists: {os.path.exists(path_B) if path_B else 'N/A'})")
        logger.info(f"üîç VSM MAP Loading from {len(paths_to_load)} path(s): {paths_to_load}")
        
        total_loaded_docs = 0
        total_loaded_uuids = 0

        # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î
        for path in paths_to_load:
            
            try:
                with open(path, "r", encoding="utf-8") as f:
                    mapping_data: Dict[str, Dict[str, Any]] = json.load(f)
                    
                # Thread-safe update
                with self._mapping_lock:
                    for doc_id, doc_entry in mapping_data.items():
                        doc_id_clean = doc_id.strip()
                        
                        self._doc_id_mapping[doc_id_clean] = doc_entry
                        
                        # ‡∏™‡∏£‡πâ‡∏≤‡∏á uuid to doc_id mapping
                        if isinstance(doc_entry, dict) and isinstance(doc_entry.get("chunk_uuids"), list):
                            for uid in doc_entry["chunk_uuids"]:
                                uid_clean = uid.replace("-", "")
                                
                                # üéØ FIX: ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ UUID ‡∏ã‡πâ‡∏≥ (‡∏ï‡∏≤‡∏°‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥)
                                if uid in self._uuid_to_doc_id and self._uuid_to_doc_id[uid] != doc_id_clean:
                                    logger.warning(f"‚ö†Ô∏è Duplicate UUID {uid} detected. Existing: {self._uuid_to_doc_id[uid]}, New: {doc_id_clean}")
                                    
                                self._uuid_to_doc_id[uid] = doc_id_clean
                                self._uuid_to_doc_id[uid_clean] = doc_id_clean
                                
                    current_total_docs = len(self._doc_id_mapping)
                    current_total_uuids = len(self._uuid_to_doc_id)
                
                logger.info(f"‚úÖ Loaded {len(mapping_data)} documents from MAPPING: {path} (Current Total Docs: {current_total_docs}, Chunks: {current_total_uuids})")
                total_loaded_docs = current_total_docs
                total_loaded_uuids = current_total_uuids
                    
            except Exception as e:
                logger.error(f"‚ùå Failed to load Doc ID Mapping from {path}: {e}", exc_info=True)


        logger.info(f"Initialized Doc ID Mapping. Total documents loaded: {total_loaded_docs}, Total chunks mapped: {total_loaded_uuids}.")

    def _re_parse_collection_name(self, collection_name: str) -> Tuple[str, Optional[str]]:
        collection_name_lower = collection_name.strip().lower()
        if collection_name_lower.startswith(f"{EVIDENCE_DOC_TYPES}_"):
            parts = collection_name_lower.split("_", 1)
            return EVIDENCE_DOC_TYPES, parts[1].upper() if len(parts) == 2 else None
        return collection_name_lower, None

    def _load_chroma_instance(self, collection_name: str) -> Optional[Chroma]:
        # 1. Fast cache hit
        if collection_name in self._chroma_cache:
            return self._chroma_cache[collection_name]

        # 2. Thread-safe double-check
        with self._lock:
            if collection_name in self._chroma_cache:
                return self._chroma_cache[collection_name]

            # ------------------------------------------------------------------
            # 3. ‡πÅ‡∏¢‡∏Å doc_type ‡∏Å‡∏±‡∏ö enabler ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤
            # ------------------------------------------------------------------
            doc_type, enabler = self._re_parse_collection_name(collection_name)

            # ------------------------------------------------------------------
            # 4. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î target_year
            # ------------------------------------------------------------------
            if doc_type.lower() == EVIDENCE_DOC_TYPES.lower():
                target_year: Optional[int] = self.year
            else:
                target_year = None

            # ------------------------------------------------------------------
            # 5. ‡∏™‡∏£‡πâ‡∏≤‡∏á persist_directory ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á 100% (Full Path ‡∏Ç‡∏≠‡∏á Collection)
            # ------------------------------------------------------------------
            persist_directory = get_vectorstore_collection_path(
                tenant=self.tenant,
                year=target_year,
                doc_type=doc_type,
                enabler=enabler,
            )

            # ------------------------------------------------------------------
            # 6. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ folder ‡∏à‡∏£‡∏¥‡∏á‡πÑ‡∏´‡∏°
            # ------------------------------------------------------------------
            if not os.path.exists(persist_directory):
                logger.warning(
                    f"Vectorstore directory NOT FOUND!\n"
                    f"   Collection      : {collection_name}\n"
                    f"   Expected path: {persist_directory}\n"
                    f"   tenant={self.tenant} | year={self.year} | doc_type={str(doc_type)} | enabler={enabler or 'None'}" 
                )
                return None

            # ------------------------------------------------------------------
            # 7. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ client ‡∏ñ‡∏π‡∏Å init ‡πÅ‡∏•‡πâ‡∏ß (‡πÉ‡∏ä‡πâ self.client ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏•‡πâ‡∏ß)
            # ------------------------------------------------------------------
            if self.client is None: # ‡πÉ‡∏ä‡πâ property client ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏Å _ensure_chroma_client_is_valid
                logger.error("Chroma PersistentClient is None! ‡∏ï‡πâ‡∏≠‡∏á init ‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")
            
            # üéØ ‡∏î‡∏∂‡∏á Global Embedding Model (768-dim) ‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
            try:
                correct_embeddings = get_hf_embeddings() 
            except Exception as e:
                logger.error(f"FATAL: Failed to get correct embeddings for Chroma init: {e}")
                return None

            try:
                # ------------------------------------------------------------------
                # 8. ‡∏™‡∏£‡πâ‡∏≤‡∏á Chroma instance (‡πÉ‡∏ä‡πâ Path ‡πÅ‡∏ó‡∏ô Client ‡∏ï‡∏±‡∏ß‡πÅ‡∏°‡πà)
                # ------------------------------------------------------------------
                
                vectordb = Chroma(
                    # client=self._client,       # ‚¨ÖÔ∏è ‡∏•‡∏ö Client ‡∏ï‡∏±‡∏ß‡πÅ‡∏°‡πà‡∏ó‡∏µ‡πà Root ‡∏≠‡∏≠‡∏Å
                    embedding_function=correct_embeddings,
                    collection_name=collection_name,
                    persist_directory=persist_directory  # ‚¨ÖÔ∏è ‡πÉ‡∏ä‡πâ Full Path ‡∏Ç‡∏≠‡∏á Collection
                )

                # üéØ FIX 7: ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÇ‡∏´‡∏•‡∏î Collection Object ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ (‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Lazy Loading ‡πÉ‡∏ô Worker)
                collection_test = vectordb._collection 
                # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏Å method ‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤ collection ‡πÑ‡∏°‡πà‡∏ï‡∏≤‡∏¢
                collection_test.count()

                # Cache ‡πÑ‡∏ß‡πâ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ï‡πà‡∏≠‡πÑ‡∏õ
                self._chroma_cache[collection_name] = vectordb

                logger.info(
                    f"Loaded Chroma collection '{collection_name}' ‚Üí Path: {persist_directory} (Retrieval Test Pending)"
                )
                return vectordb

            except Exception as e:
                logger.error(
                    f"FAILED to load collection '{collection_name}' from {persist_directory}\n"
                    f"Error: {e}",
                    exc_info=True,
                )
                return None


    def get_documents_by_id(self, stable_doc_ids: Union[str, List[str]], doc_type: str = "default_collection", enabler: Optional[str] = None) -> List[LcDocument]:
        """
        Retrieve documents from Chroma collection by stable_doc_ids (64-char hash).
        """
        import chromadb 
        from langchain_core.documents import Document as LcDocument
        from typing import Set, Dict, Any 

        if isinstance(stable_doc_ids, str):
            stable_doc_ids = [stable_doc_ids]
            
        if not stable_doc_ids:
            return []

        # 1. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ä‡∏∑‡πà‡∏≠ Collection ‡πÅ‡∏•‡∏∞‡πÇ‡∏´‡∏•‡∏î Instance
        collection_name = get_doc_type_collection_key(doc_type=doc_type, enabler=enabler)
        chroma_instance = self._load_chroma_instance(collection_name)
        
        if not chroma_instance:
            logger.warning(f"VSM: Cannot load collection '{collection_name}' for document retrieval.")
            return []

        # 2. ‡πÅ‡∏õ‡∏•‡∏á Stable Doc IDs ‡πÄ‡∏õ‡πá‡∏ô Chunk UUIDs (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Primary Key)
        chunk_uuids_for_search: List[str] = []
        
        for stable_id in stable_doc_ids:
            stable_id_clean = stable_id.strip() 
            map_entry = self.doc_id_map.get(stable_id_clean)
            if map_entry and map_entry.get("chunk_uuids"):
                chunk_uuids_for_search.extend(map_entry["chunk_uuids"])
            else:
                chunk_uuids_for_search.append(stable_id_clean) 
                
        # 3. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ ID ‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Query ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Primary Key Search (Chunk UUIDs)
        search_ids_raw = list(set([
            str(i).strip()
            for i in chunk_uuids_for_search if str(i).strip()
        ]))
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏° Flexible UUID Search: ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ó‡∏±‡πâ‡∏á ID ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡∏µ‡∏î‡∏Å‡∏•‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡∏µ‡∏î‡∏Å‡∏•‡∏≤‡∏á
        final_chunk_uuids_to_try: Set[str] = set()
        for chunk_id in search_ids_raw:
            final_chunk_uuids_to_try.add(chunk_id) 
            if "-" in chunk_id:
                final_chunk_uuids_to_try.add(chunk_id.replace("-", "")) 
                
        final_chunk_uuids_list = list(final_chunk_uuids_to_try)

        if not final_chunk_uuids_list:
              logger.warning(f"Hydration failed: No valid Chunk UUIDs derived from {len(stable_doc_ids)} Stable IDs.")
              return []


        try:
            collection = chroma_instance._collection
            documents: List[LcDocument] = []
            result: Dict[str, Any] = {}
            
            # --- Attempt 1: Primary Key Search (Chunk UUIDs) ---
            logger.info(f"Attempt 1/2: Primary Key Search ({len(final_chunk_uuids_list)} Chunk UUIDs)")
            result = collection.get(
                ids=final_chunk_uuids_list,
                include=["documents", "metadatas"] # <-- üéØ FIX 21.0: ‡∏•‡∏ö "ids"
            )

            docs_result = result.get("documents", [])
            
            # üéØ FINAL FIX 19.0: ‡∏ñ‡πâ‡∏≤‡πÑ‡∏î‡πâ 0 chunks ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á Fallback Search ‡∏î‡πâ‡∏ß‡∏¢ $or
            if not docs_result:
                
                # --- Attempt 2: Fallback Search (Metadata: stable_doc_uuid OR doc_id) ---
                logger.warning("Attempt 1 returned 0 chunks. Falling back to Robust Metadata Search (stable_doc_uuid / doc_id).")
                
                # ‡πÉ‡∏ä‡πâ Stable Doc IDs ‡∏ó‡∏µ‡πà Cleaned ‡πÅ‡∏•‡πâ‡∏ß ‡πÄ‡∏õ‡πá‡∏ô Query ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Metadata Search
                stable_doc_ids_cleaned = list(set([uid.strip() for uid in stable_doc_ids if uid.strip()]))

                if stable_doc_ids_cleaned:
                    # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏î‡πâ‡∏ß‡∏¢ $or: stable_doc_uuid ‡∏´‡∏£‡∏∑‡∏≠ doc_id
                    result = collection.get(
                        where={"$or": [
                            {"stable_doc_uuid": {"$in": stable_doc_ids_cleaned}},
                            {"doc_id": {"$in": stable_doc_ids_cleaned}}
                        ]},
                        include=["documents", "metadatas"] # <-- üéØ FIX 21.0: ‡∏•‡∏ö "ids"
                    )
                    docs_result = result.get("documents", [])
                else:
                    logger.warning("Fallback Search failed: No valid Stable Doc IDs for metadata query.")
            
            # --- ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå ---
            docs = docs_result
            metadatas = result.get("metadatas", [{}] * len(docs))
            # ids ‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡∏ñ‡∏π‡∏Å‡∏Ñ‡∏∑‡∏ô‡∏°‡∏≤‡πÄ‡∏™‡∏°‡∏≠
            ids = result.get("ids", [""] * len(docs)) 

            for i, text in enumerate(docs):
                meta = metadatas[i].copy() if metadatas and metadatas[i] else {}
                chunk_uuid = ids[i] if ids else (meta.get("chunk_uuid") or "")
                
                if chunk_uuid:
                    meta["chunk_uuid"] = chunk_uuid

                # ‡πÉ‡∏ä‡πâ map (uuid_to_doc_id_map) ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏´‡∏≤ Stable ID ‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô
                stable_doc_id = self.uuid_to_doc_id_map.get(chunk_uuid) or meta.get("stable_doc_uuid") or meta.get("doc_id")
                
                # Fallback: ‡∏´‡∏≤‡∏Å‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡∏µ‡∏î‡∏î‡πâ‡∏ß‡∏¢
                if not stable_doc_id and "-" in chunk_uuid:
                    stable_doc_id = self.uuid_to_doc_id_map.get(chunk_uuid.replace("-", ""))
                
                if stable_doc_id:
                      meta["stable_doc_uuid"] = stable_doc_id

                doc = LcDocument(page_content=text, metadata=meta)
                documents.append(doc)
                
            logger.info(f"‚úÖ Retrieved {len(documents)} documents for {len(stable_doc_ids)} Stable IDs from '{collection_name}' (Search Mode: {'Primary/Fallback'}).")
            return documents

        except Exception as e:
            logger.error(f"‚ùå Error retrieving documents by Stable/Chunk IDs from collection '{collection_name}': {e}", exc_info=True)
            return []

    def _ensure_chroma_client_is_valid(self):
        """
        Re-initializes the Chroma client if it is None or lost during serialization (Worker Process).
        """
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ _client attribute ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô None ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if not hasattr(self, '_client') or self._client is None:
            logger.warning(f"Chroma client lost in worker process for tenant '{self.tenant}', re-initializing...")
            
            # ‡πÉ‡∏ä‡πâ VSM attributes (tenant) ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÉ‡∏´‡πâ‡∏Ñ‡∏á‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Path
            tenant_root_path = get_vectorstore_tenant_root_path(self.tenant)
            
            # Re-initialize the Persistent Client
            try:
                # üéØ FIX: ‡πÉ‡∏ä‡πâ logger ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å import ‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤
                self._client = chromadb.PersistentClient(path=tenant_root_path, settings=Settings(anonymized_telemetry=True))
                
                # ‡πÄ‡∏°‡∏∑‡πà‡∏≠ Client ‡∏ñ‡∏π‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà, Collection Handles ‡πÄ‡∏Å‡πà‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ï‡πâ‡∏≠‡∏á‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÉ‡∏ä‡πâ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ
                # ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ _chroma_cache ‡πÅ‡∏ó‡∏ô _collections ‡πÄ‡∏£‡∏≤‡∏à‡∏∂‡∏á‡∏ï‡πâ‡∏≠‡∏á‡∏•‡πâ‡∏≤‡∏á cache
                self._chroma_cache = {} 
                
                logger.info(f"‚úÖ ChromaDB Client re-initialized at TENANT ROOT PATH: {tenant_root_path}. Collections cache cleared.")
            except Exception as e:
                logger.error(f"FATAL: Failed to re-initialize Chroma Client in worker: {e}", exc_info=False)
                # ‡πÑ‡∏°‡πà raise ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÇ‡∏Ñ‡πâ‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏±‡∏ö error ‡πÑ‡∏î‡πâ
            
    def retrieve_by_chunk_uuids(self, chunk_uuids: List[str], collection_name: Optional[str] = None) -> List[LcDocument]:
        """
        Hydrate documents by chunk UUIDs.
        - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö UUID ‡∏ó‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö‡∏°‡∏µ dash ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏µ dash
        - Retry mechanism + cache clear ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠ retrieval fail
        """
        # (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£ import ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß)

        self._ensure_chroma_client_is_valid()

        if not chunk_uuids:
            logger.info("VSM: No chunk_uuids provided for hydration.")
            return []

        if collection_name is None:
            collection_name = get_doc_type_collection_key(
                doc_type=EVIDENCE_DOC_TYPES, 
                enabler=getattr(self, 'enabler', 'km')
            )

        # Prepare UUIDs: no-dash + attempt 64-char dash formatting
        no_dash = [u.replace("-", "") for u in chunk_uuids if u]
        with_dash = []
        for u in no_dash:
            if len(u) == 32: # UUID4 is 32 chars, not 64
                # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤ UUID ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÉ‡∏ä‡πâ‡∏°‡∏µ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô 8-4-4-4-12 = 32
                try:
                    uuid_obj = uuid.UUID(u, version=4) # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô UUID
                    with_dash.append(str(uuid_obj))
                except ValueError:
                    # ‡∏ñ‡πâ‡∏≤‡πÅ‡∏õ‡∏•‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≤‡∏°
                    pass 
            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô 64-char hash (‡∏ã‡∏∂‡πà‡∏á‡πÑ‡∏°‡πà‡∏ô‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô UUID) ‡πÉ‡∏´‡πâ‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡∏µ‡∏î‡∏ï‡πà‡∏≠‡πÑ‡∏õ
            # ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏ó‡∏≥ 64-char hash ‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ö‡∏ö‡∏Ç‡∏µ‡∏î‡∏Å‡∏•‡∏≤‡∏á ‡∏ã‡∏∂‡πà‡∏á‡∏≠‡∏≤‡∏à‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô UUID
            # ‡πÅ‡∏ï‡πà‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏Å‡∏©‡∏≤‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏î‡∏¥‡∏°:
            elif len(u) == 64:
                 part1, part2, part3, part4, part5 = u[:8], u[8:12], u[12:16], u[16:20], u[20:]
                 with_dash.append(f"{part1}-{part2}-{part3}-{part4}-{part5}")
        
        # All formats
        all_formats = list(set(chunk_uuids + no_dash + with_dash))  # Remove duplicates

        result = {"documents": [], "metadatas": [], "ids": []}
        max_retries = 3

        for attempt in range(1, max_retries + 1):
            chroma = self._load_chroma_instance(collection_name)
            if not chroma:
                logger.warning(f"Cannot load '{collection_name}' (attempt {attempt})")
                if attempt < max_retries and collection_name in self._chroma_cache:
                    logger.warning(f"VSM: Clearing Chroma cache for '{collection_name}' due to load failure.")
                    del self._chroma_cache[collection_name]
                    self._ensure_chroma_client_is_valid()
                continue

            try:
                logger.info(f"Hydration attempt {attempt}/{max_retries} ‚Üí {len(all_formats)} UUIDs from '{collection_name}'")
                result = chroma.get(ids=all_formats, include=["documents", "metadatas", "ids"])

                if result.get("documents"):
                    logger.info(f"Success: Retrieved {len(result['documents'])} chunks on attempt {attempt}")
                    break

                logger.warning(f"Got 0 chunks on attempt {attempt}, retrying...")

            except Exception as e:
                logger.error(f"Hydration failed (attempt {attempt}): {e}")

            # Clear cache only on failure
            if attempt < max_retries and collection_name in self._chroma_cache:
                logger.warning(f"VSM: Clearing Chroma cache for '{collection_name}' for retry.")
                del self._chroma_cache[collection_name]
                self._ensure_chroma_client_is_valid()

        # Build LcDocument objects
        docs = []
        documents_raw = result.get("documents", [])
        metas = result.get("metadatas", [{}] * len(documents_raw))
        ids = result.get("ids", [])

        for i, text in enumerate(documents_raw):
            if not text or not text.strip():
                continue
            meta = metas[i].copy()
            id_clean = ids[i].replace("-", "") if ids[i] else ""
            meta["chunk_uuid"] = id_clean

            # Map to stable_doc_uuid
            stable = self._uuid_to_doc_id.get(id_clean) or meta.get("stable_doc_uuid")
            if stable:
                meta["stable_doc_uuid"] = stable

            docs.append(LcDocument(page_content=text.strip(), metadata=meta))

        logger.info(f"Hydration complete ‚Üí Retrieved {len(docs)} full-text chunks (requested {len(chunk_uuids)})")
        return docs


    def create_hybrid_retriever(self, collection_name: str, top_k: int = INITIAL_TOP_K) -> EnsembleRetriever:
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞ Cache Hybrid Retriever (Vector + BM25) (FIXED LOGIC)
        """
        # 0. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Cache ‡∏Å‡πà‡∏≠‡∏ô (Performance Optimization)
        if collection_name in self._hybrid_retriever_cache:
            self.logger.info(f"Requesting Hybrid Retriever from Manager for {collection_name} (Cached)...")
            return self._hybrid_retriever_cache[collection_name]
            
        self.logger.info(f"Creating NEW Hybrid Retriever for {collection_name}...")

        try:
            # 1. ‡πÇ‡∏´‡∏•‡∏î Chroma Instance (‡πÉ‡∏ä‡πâ Logic ‡πÉ‡∏ô _load_chroma_instance ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á)
            chroma_instance = self._load_chroma_instance(collection_name) 
            if not chroma_instance:
                raise ValueError(f"Chroma instance for '{collection_name}' failed to load.")
            
            # 2. Vector Retriever
            vector_retriever = chroma_instance.as_retriever(
                search_kwargs={"k": top_k}
            )

            # 3. ‡∏î‡∏∂‡∏á Documents ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö BM25 Index (‡πÉ‡∏ä‡πâ Cache ‡∏´‡∏£‡∏∑‡∏≠‡∏î‡∏∂‡∏á‡πÉ‡∏´‡∏°‡πà)
            if collection_name in self._bm25_docs_cache:
                langchain_docs = self._bm25_docs_cache[collection_name]
                self.logger.info(f"Loaded {len(langchain_docs)} documents for BM25 from cache.")
            else:
                self.logger.info("Fetching documents from Chroma for BM25 Indexing...")
                
                # üí° ‡∏î‡∏∂‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏à‡∏≤‡∏Å Chroma Instance ‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß
                docs = chroma_instance._collection.get( # ‡πÉ‡∏ä‡πâ _collection ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Client ‡∏ã‡πâ‡∏≥
                    include=["documents", "metadatas"]
                )
                
                texts = docs["documents"]
                langchain_docs = [
                    Document(page_content=text, metadata=meta)
                    for text, meta in zip(texts, docs["metadatas"])
                ]
                
                self._bm25_docs_cache[collection_name] = langchain_docs
                self.logger.info(f"‚úÖ Fetched and cached {len(langchain_docs)} documents for BM25.")

            # 4. BM25 Retriever
            bm25_retriever = BM25Retriever.from_documents(
                langchain_docs, 
                tokenizer=word_tokenize # üéØ FIX: ‡πÉ‡∏ä‡πâ pythainlp.word_tokenize ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
            )
            bm25_retriever.k = top_k

            # 5. Ensemble Retriever (Hybrid)
            ensemble_retriever = EnsembleRetriever(
                retrievers=[vector_retriever, bm25_retriever],
                weights=[HYBRID_VECTOR_WEIGHT, HYBRID_BM25_WEIGHT]
            )
            
            self._hybrid_retriever_cache[collection_name] = ensemble_retriever
            self.logger.info(f"‚úÖ Hybrid Retriever created successfully for {collection_name}. HYBRID mode activated.")
            return ensemble_retriever
        
        except Exception as e:
            self.logger.error(f"‚ùå Failed to create Hybrid Retriever for '{collection_name}': {e}", exc_info=True)
            raise e # ‡∏¢‡∏Å Exception ‡∏≠‡∏≠‡∏Å‡πÑ‡∏õ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Logic Fallback (‡πÉ‡∏ô get_retriever) ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô
    
    def get_limited_chunks_from_doc_ids(self, stable_doc_ids: Union[str, List[str]], query: Union[str, List[str]], doc_type: str, enabler: Optional[str] = None, limit_per_doc: int = 5) -> List[LcDocument]:
        if isinstance(stable_doc_ids, str):
            stable_doc_ids = [stable_doc_ids]
        stable_doc_ids = [uid for uid in stable_doc_ids if uid]
        if not stable_doc_ids:
            return []
        vector_search_query = query[0] if isinstance(query, list) and query else (query if isinstance(query, str) else "")
        if not vector_search_query:
            logger.warning("Limited chunk search skipped: Query is empty.")
            return []
        # üéØ FIX: ‡πÉ‡∏ä‡πâ get_doc_type_collection_key ‡πÅ‡∏ó‡∏ô _get_collection_name
        collection_name = get_doc_type_collection_key(doc_type, enabler)
        chroma_instance = self._load_chroma_instance(collection_name)
        if not chroma_instance:
            logger.error(f"Collection '{collection_name}' is not loaded.")
            return []
        all_limited_documents: List[LcDocument] = []
        total_chunks_retrieved = 0
        for stable_id in stable_doc_ids:
            stable_id_clean = stable_id.strip()
            # üéØ FIX: ‡πÉ‡∏ä‡πâ 'stable_doc_uuid' ‡πÄ‡∏õ‡πá‡∏ô filter key ‡πÅ‡∏ó‡∏ô 'doc_id' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô
            doc_filter = {"stable_doc_uuid": stable_id_clean}
            try:
                custom_retriever = ChromaRetriever(vectorstore=chroma_instance, k=limit_per_doc, filter=doc_filter)
                limited_docs = custom_retriever.get_relevant_documents(query=vector_search_query)
                for doc in limited_docs:
                    doc.metadata["priority_search_type"] = "limited_vector_search"
                    doc.metadata["priority_limit"] = limit_per_doc
                    all_limited_documents.append(doc)
                total_chunks_retrieved += len(limited_docs)
            except Exception as e:
                logger.error(f"‚ùå Error performing limited vector search for Stable ID '{stable_id_clean}': {e}")
                continue
        logger.info(f"‚úÖ Retrieved {total_chunks_retrieved} limited chunks (max {limit_per_doc}/doc) for {len(stable_doc_ids)} Stable IDs from '{collection_name}'.")
        return all_limited_documents

    def get_retriever(self, collection_name: str, top_k: int = INITIAL_TOP_K, final_k: int = FINAL_K_RERANKED, use_rerank: bool = USE_HYBRID_SEARCH, use_hybrid: bool = True) -> Any:
        # NOTE: Imports ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ñ‡∏π‡∏Å‡∏ô‡∏≥‡∏°‡∏≤‡πÑ‡∏ß‡πâ‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ‡∏ï‡∏≤‡∏°‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÉ‡∏ä
        # ‡πÇ‡∏´‡∏•‡∏î Chroma Instance
        chroma_instance = self._load_chroma_instance(collection_name)
        if not chroma_instance:
            logger.warning(f"Retriever creation failed: Collection '{collection_name}' not loaded.")
            return None

        # 1. Raw Vector Retrieve Function (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°)
        def raw_vector_retrieve(query: str, filter_dict: Optional[dict] = None, k: int = top_k) -> List[LcDocument]:
            try:
                bge_prefix = "‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå: "
                query_with_prefix = f"{bge_prefix}{query.strip()}"
                logger.info(f"[BGE-M3 PREFIX ADDED] Using prefixed query: '{query_with_prefix[:100]}...'")

                search_kwargs = {"k": k}
                if filter_dict:
                    search_kwargs["filter"] = filter_dict
                
                docs = chroma_instance.similarity_search(
                    query=query_with_prefix,
                    k=k,
                    filter=filter_dict
                )
                logger.info(f"Raw vector retrieval: {len(docs)} docs")
                return docs
            except Exception as e:
                logger.error(f"Vector retrieval failed: {e}")
                return []


        # 2. Reranker Wrapper (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°)
        def retrieve_with_rerank(docs: List[LcDocument], query: str) -> List[LcDocument]:
            reranker = get_global_reranker()
            if not (use_rerank and reranker and hasattr(reranker, "compress_documents")):
                return docs[:final_k]

            try:
                reranked = reranker.compress_documents(documents=docs, query=query, top_n=final_k)
                # Inject score (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°)
                scores = getattr(reranker, "scores", None)
                if scores and len(scores) >= len(reranked):
                    doc_scores = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)
                    for i, (doc, score) in enumerate(doc_scores[:len(reranked)]):
                        for r_doc in reranked:
                            if r_doc.page_content == doc.page_content:
                                score = float(score) if score is not None else 0.0
                                r_doc.metadata["_rerank_score_force"] = score
                                orig = r_doc.metadata.get("source_filename", "UNKNOWN")
                                r_doc.metadata["source_filename"] = f"{orig}|SCORE:{score:.4f}"
                                break
                logger.info(f"Reranking success ‚Üí kept {len(reranked)} docs")
                return reranked
            except Exception as e:
                logger.warning(f"Rerank failed: {e}, fallback to raw")
                return docs[:final_k]


        # 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á Vector Retriever (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°)
        vector_retriever = chroma_instance.as_retriever(search_kwargs={"k": top_k})

        # 4. ‡∏™‡∏£‡πâ‡∏≤‡∏á BM25 Retriever (Hybrid)
        if use_hybrid:
            try:
                # üî¥ FIX CRITICAL (Chroma Access): ‡πÉ‡∏ä‡πâ chroma_instance._collection 
                #    ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å self.client.get_collection(collection_name)
                
                if chroma_instance is None or not hasattr(chroma_instance, "_collection"):
                    # Logic ‡∏ô‡∏µ‡πâ‡∏Ñ‡∏ß‡∏£‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡∏´‡∏≤‡∏Å _load_chroma_instance ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
                    logger.warning(f"Chroma Instance for '{collection_name}' is invalid. Skipping Hybrid setup.")
                    raise ValueError("Invalid chroma_instance object for Hybrid setup.")
                
                collection = chroma_instance._collection # üü¢ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ
                
                # ‡∏î‡∏∂‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏à‡∏≤‡∏Å collection
                result = collection.get(include=["documents", "metadatas"])
                texts = result["documents"]
                metadatas = result["metadatas"]

                langchain_docs = [
                    LcDocument(page_content=text, metadata=meta or {})
                    for text, meta in zip(texts, metadatas)
                ]

                # *** KEY FIX: ‡πÄ‡∏û‡∏¥‡πà‡∏° tokenizer ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ***
                bm25_retriever = BM25Retriever.from_documents(
                    langchain_docs,
                    tokenizer=thai_tokenizer_for_bm25 # <-- ‡πÉ‡∏ä‡πâ Tokenizer ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
                )
                bm25_retriever.k = top_k

                # 5. Ensemble (Hybrid) Retriever
                ensemble_retriever = EnsembleRetriever(
                    retrievers=[vector_retriever, bm25_retriever],
                    weights=[HYBRID_VECTOR_WEIGHT, HYBRID_BM25_WEIGHT]  # ‡∏õ‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ: Vector 70%, BM25 30%
                )

                # 6. Ultimate Hybrid Retriever with Rerank
                class UltimateHybridRetriever(BaseRetriever):
                    def _get_relevant_documents(self, query: str, *, run_manager=None) -> List[LcDocument]:
                        docs = ensemble_retriever.get_relevant_documents(query)
                        return retrieve_with_rerank(docs, query)

                    def invoke(self, query: str, config: Optional[dict] = None, **kwargs) -> List[LcDocument]:
                        # NOTE: ‡πÄ‡∏£‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ kwargs ‡πÉ‡∏ô body ‡πÅ‡∏ï‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÉ‡∏ô signature
                        return self._get_relevant_documents(query)
                
                # 7. Return the UltimateHybridRetriever instance
                return UltimateHybridRetriever()


            except Exception as e:
                # üéØ FIX 2: ‡∏õ‡∏£‡∏±‡∏ö Log level ‡πÅ‡∏•‡∏∞ message ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏Å‡∏¥‡∏î Hybrid setup failed
                logger.error(f"Hybrid/BM25/Ensemble Retriever setup failed for '{collection_name}': {e}", exc_info=False)
                # Fallback ‡πÑ‡∏õ‡πÉ‡∏ä‡πâ Vector Retriever ‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤ (‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£ "pass" ‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡πÇ‡∏Ñ‡πâ‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏•‡πà‡∏≤‡∏á)
                pass

        # Fallback (‡∏ñ‡πâ‡∏≤ Hybrid ‡∏ñ‡∏π‡∏Å‡∏õ‡∏¥‡∏î ‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Hybrid ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß)
        if use_rerank and get_global_reranker():
            class SimpleVectorRerankRetriever(BaseRetriever):
                def _get_relevant_documents(self, query: str, *, run_manager=None) -> List[LcDocument]:
                    docs = raw_vector_retrieve(query, filter_dict=None, k=top_k)
                    return retrieve_with_rerank(docs, query)
            
                # üî¥ FIX CRITICAL (TypeError): ‡πÄ‡∏û‡∏¥‡πà‡∏° **kwargs ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Argument 'k'
                def invoke(self, query: str, config: Optional[dict] = None, **kwargs) -> List[LcDocument]:
                    return self._get_relevant_documents(query)
            
            return SimpleVectorRerankRetriever()
        
        # Fallback to simple Chroma vector retriever
        return vector_retriever

    def get_all_collection_names(self) -> List[str]:
        # üéØ FIX: ‡∏•‡∏ö base_path ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å list_vectorstore_folders
        return list_vectorstore_folders(tenant=self.tenant, year=self.year)


    def get_chunks_from_doc_ids(self, stable_doc_ids: Union[str, List[str]], doc_type: str, enabler: Optional[str] = None) -> List[LcDocument]:
        import chromadb # Import locally if not already imported
        from langchain_core.documents import Document as LcDocument

        if isinstance(stable_doc_ids, str):
            stable_doc_ids = [stable_doc_ids]
        stable_doc_ids = [uid for uid in stable_doc_ids if uid]
        if not stable_doc_ids:
            return []
        
        # üéØ FIX: ‡πÉ‡∏ä‡πâ get_doc_type_collection_key ‡πÅ‡∏ó‡∏ô _get_collection_name
        collection_name = get_doc_type_collection_key(doc_type, enabler)
        all_chunk_uuids = []
        skipped_docs = []
        found_stable_ids = []
        
        for stable_id in stable_doc_ids:
            stable_id_clean = stable_id.strip()
            if stable_id_clean in self._doc_id_mapping:
                doc_entry = self._doc_id_mapping[stable_id_clean]
                if isinstance(doc_entry, dict) and "chunk_uuids" in doc_entry and isinstance(doc_entry.get("chunk_uuids"), list):
                    chunk_uuids = doc_entry["chunk_uuids"]
                    if chunk_uuids:
                        all_chunk_uuids.extend(chunk_uuids)
                        found_stable_ids.append(stable_id_clean)
                    else:
                        logger.warning(f"Mapping found for Stable ID '{stable_id_clean}' but 'chunk_uuids' list is empty.")
                else:
                    # [Minor Fix] ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏à‡∏≤‡∏Å stable_doc_clean ‡πÄ‡∏õ‡πá‡∏ô stable_id_clean
                    logger.warning(f"Mapping entry for Stable ID '{stable_id_clean}' is malformed or missing 'chunk_uuids'.") 
            else:
                skipped_docs.append(stable_id_clean)
                
        if skipped_docs:
            logger.warning(f"Skipping Stable IDs not found in mapping: {skipped_docs}")
        if not all_chunk_uuids:
            logger.warning(f"No valid chunk UUIDs found for provided Stable Document IDs: {skipped_docs}. Check doc_id_mapping.json.")
            return []
            
        chroma_instance = self._load_chroma_instance(collection_name)
        if not chroma_instance:
            logger.error(f"Collection '{collection_name}' is not loaded.")
            return []
            
        try:
            collection = chroma_instance._collection
            
            # üéØ FINAL FIX 16.0: ‡∏•‡∏ö "ids" ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å include ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ ChromaDB API Error
            result = collection.get(ids=all_chunk_uuids, include=["documents", "metadatas"]) 
            
            documents: List[LcDocument] = []
            if not result.get("documents"):
                logger.warning(f"Chroma DB returned 0 documents for {len(all_chunk_uuids)} chunk UUIDs in collection '{collection_name}'.")
                return []
                
            for i, text in enumerate(result.get("documents", [])):
                if text:
                    metadata = result.get("metadatas", [{}])[i]
                    # IDs ‡∏ñ‡∏π‡∏Å‡∏Ñ‡∏∑‡∏ô‡∏°‡∏≤‡πÇ‡∏î‡∏¢‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á include
                    chunk_uuid_from_result = result.get("ids", [""])[i] 
                    
                    # NOTE: ‡πÉ‡∏ä‡πâ self._uuid_to_doc_id
                    doc_id = self._uuid_to_doc_id.get(chunk_uuid_from_result, "UNKNOWN") 
                    
                    metadata["chunk_uuid"] = chunk_uuid_from_result
                    metadata["doc_id"] = doc_id
                    metadata["doc_type"] = doc_type
                    documents.append(LcDocument(page_content=text, metadata=metadata))
                    
            logger.info(f"‚úÖ Retrieved {len(documents)} chunks for {len(found_stable_ids)} Stable IDs from '{collection_name}'.")
            return documents
        except Exception as e:
            logger.error(f"‚ùå Error retrieving documents by Chunk UUIDs from collection '{collection_name}': {e}")
            return []

# Helper function
def get_vectorstore_manager(
    doc_type: str = "all",           # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡πà‡∏≤ default
    tenant: str = DEFAULT_TENANT,
    year: Optional[int] = None,
    enabler: Optional[str] = None,
) -> VectorStoreManager:
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏∑‡∏ô VectorStoreManager (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏ó‡∏∏‡∏Å doc_type)
    """
    return VectorStoreManager(
        # doc_type=doc_type,
        tenant=tenant,
        # year=year or DEFAULT_YEAR,
        # enabler=enabler
    )

def load_vectorstore(doc_type: str, enabler: Optional[str] = None) -> Optional[Chroma]:
    collection_name = get_doc_type_collection_key(doc_type, enabler)
    vsm = get_vectorstore_manager(
        doc_type=doc_type,           # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ!
        enabler=enabler
    )
    return vsm._load_chroma_instance(collection_name)

class VectorStoreExecutorSingleton:
    _instance = None
    _is_initialized = False

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            cls._instance = super(VectorStoreExecutorSingleton, cls).__new__(cls)
        return cls._instance

    def __init__(self):
        if not VectorStoreExecutorSingleton._is_initialized:
            self.max_workers = MAX_PARALLEL_WORKERS
            self._executor = ThreadPoolExecutor(max_workers=self.max_workers)
            logger.info(f"Initialized VectorStoreExecutorSingleton (ThreadPoolExecutor with {self.max_workers} workers) for batch tasks.")
            VectorStoreExecutorSingleton._is_initialized = True

    @property
    def executor(self) -> ThreadPoolExecutor:
        return self._executor

    def close(self):
        if self._is_initialized:
            logger.info("Shutting down VectorStoreExecutorSingleton ThreadPoolExecutor...")
            self._executor.shutdown(wait=True)
            VectorStoreExecutorSingleton._is_initialized = False

def get_vectorstore(
    collection_name: str, 
    tenant: str, 
    year: Optional[int],
    # üí° ‡πÄ‡∏û‡∏¥‡πà‡∏° Argument ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á Chroma Client ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
    # ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡πÉ‡∏ä‡πâ Embedding Model ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô VectorStoreExecutorSingleton
    # ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏° embedding_function ‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏î‡πâ‡∏ß‡∏¢
) -> VectorStoreExecutorSingleton:
    """
    Wrapper function ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ VectorStoreExecutorSingleton 
    ‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á‡∏ú‡πà‡∏≤‡∏ô Argument ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏∏ Path ‡πÅ‡∏•‡∏∞ Collection Name.
    """
    
    # üéØ FIX: ‡∏™‡πà‡∏á‡∏ú‡πà‡∏≤‡∏ô Argument ‡πÑ‡∏õ‡∏¢‡∏±‡∏á Constructor ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏•‡∏≤‡∏™‡∏´‡∏•‡∏±‡∏Å
    return VectorStoreExecutorSingleton(
        collection_name=collection_name, 
        tenant=tenant, 
        year=year
    )

# -------------------- Custom Retriever for Chroma --------------------
class ChromaRetriever(BaseRetriever):
    vectorstore: Any
    k: int
    filter: Optional[Dict] = None
    model_config = ConfigDict(arbitrary_types_allowed=True)

    def _get_relevant_documents(self, query: str, *, run_manager=None) -> List[LcDocument]:
        
        # üéØ DEBUG: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Collection Count ‡πÅ‡∏•‡∏∞ Embedding (‡πÇ‡∏Ñ‡πâ‡∏î‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏°)
        try:
            raw_collection = self.vectorstore._collection
            count = raw_collection.count() 
            logger.critical(f"üéØ [DEBUG CHROMA] Collection Count: {count}")
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Query Embedding
            if hasattr(self.vectorstore, '_embedding_function') and self.vectorstore._embedding_function:
                embedding_function = self.vectorstore._embedding_function
                # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö embed query
                query_embedding = embedding_function.embed_query(query)
                logger.critical(f"üéØ [DEBUG CHROMA] Query Embedding Success. Vector size: {len(query_embedding)}")
            else:
                logger.critical("üéØ [DEBUG CHROMA] Cannot access _embedding_function.")

        except Exception as debug_e:
            logger.critical(f"‚ùå [DEBUG CHROMA] Debug check failed (Skip search): {debug_e}")
            return []
        # END DEBUG

        try:
            # ‡∏£‡∏±‡∏ô similarity search
            return self.vectorstore.similarity_search(query=query, k=self.k, filter=self.filter)
        except Exception as e:
            logger.error(f"‚ùå Chroma similarity_search failed in custom retriever: {e}")
            return []

    def get_relevant_documents(self, query: str, **kwargs) -> List[LcDocument]:
        return self._get_relevant_documents(query, **kwargs)

    async def _aget_relevant_documents(self, query: str, *, run_manager=None) -> List[LcDocument]:
        return self._get_relevant_documents(query, run_manager=run_manager)

# -------------------- MultiDoc / Parallel Retriever --------------------
class NamedRetriever(BaseModel):
    """
    Defines a single retriever configuration, mapping a document type/enabler
    to a specific VectorStore collection context (tenant/year).
    """
    doc_id: str
    doc_type: str
    top_k: int = INITIAL_TOP_K
    final_k: int = FINAL_K_RERANKED
    # üéØ FIX: ‡∏•‡∏ö base_path ‡∏≠‡∏≠‡∏Å
    enabler: Optional[str] = None
    tenant: str = DEFAULT_TENANT
    year: Optional[int] = DEFAULT_YEAR # <--- üéØ FIX: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô Optional[int] ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö None
    
    # load_instance ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏Å VSM ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ self.tenant ‡πÅ‡∏•‡∏∞ self.year
    def load_instance(self) -> Any:
        """
        Loads the actual VectorStore Retriever instance using VectorStoreManager 
        with the correct tenant and year context.
        """
        # ‚ö†Ô∏è VSM Singleton ‡∏ï‡πâ‡∏≠‡∏á‡∏ñ‡∏π‡∏Å init ‡∏î‡πâ‡∏ß‡∏¢‡∏õ‡∏µ‡πÄ‡∏™‡∏°‡∏≠ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÇ‡∏´‡∏•‡∏î Doc ID Mapping
        # ‡∏ñ‡πâ‡∏≤ self.year ‡πÄ‡∏õ‡πá‡∏ô None (‡πÄ‡∏ä‡πà‡∏ô ‡πÄ‡∏õ‡πá‡∏ô General Docs ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô‡∏ó‡∏∏‡∏Å‡∏õ‡∏µ) ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ DEFAULT_YEAR
        manager = VectorStoreManager(
            # üéØ FIX: ‡∏•‡∏ö base_path ‡∏≠‡∏≠‡∏Å
            tenant=self.tenant, 
            year=self.year if self.year is not None else DEFAULT_YEAR # <--- üéØ FIX: ‡πÉ‡∏ä‡πâ DEFAULT_YEAR ‡∏ñ‡πâ‡∏≤ self.year ‡πÄ‡∏õ‡πá‡∏ô None
        ) 
        # üéØ FIX: ‡πÉ‡∏ä‡πâ get_doc_type_collection_key ‡πÅ‡∏ó‡∏ô _get_collection_name
        collection_name = get_doc_type_collection_key(self.doc_type, self.enabler)
        
        retriever = manager.get_retriever(collection_name=collection_name, top_k=self.top_k, final_k=self.final_k) 
        
        if not retriever:
            raise ValueError(f"Retriever not found for collection '{collection_name}' at path based on tenant={self.tenant}, year={self.year}")
        
        return retriever

class MultiDocRetriever(BaseRetriever): # FIX: ‡πÑ‡∏°‡πà‡∏°‡∏µ BaseModel ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á Metaclass Conflict
    # üéØ FIX: Pydantic Fields (‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ _ ‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ö Input)
    retrievers_list: List[NamedRetriever] = Field(default_factory=list)
    k_per_doc: int = Field(default=INITIAL_TOP_K)
    doc_ids_filter: Optional[Set[str]] = Field(default=None) 
    
    # Reranking fields 
    compressor: Optional[BaseDocumentCompressor] = Field(default=None)
    final_k: int = Field(default=FINAL_K_RERANKED)
    
    # üéØ Internal Fields (‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏ó‡∏∏‡∏Å‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡πÉ‡∏ô __init__ ‡πÅ‡∏•‡∏∞ exclude=True)
    _executor: Optional[Union[ThreadPoolExecutor, ProcessPoolExecutor]] = Field(default=None, exclude=True)
    _executor_type: Optional[str] = Field(default=None, exclude=True) 
    _executor_mode: Optional[str] = Field(default=None, exclude=True)
    _all_retrievers: Dict[str, Any] = Field(default_factory=dict, exclude=True)
    _doc_ids_filter_list: Optional[List[str]] = Field(default=None, exclude=True) 
    _chroma_filter: Optional[Dict[str, Any]] = Field(default=None, exclude=True)
    _manager: Optional['VectorStoreManager'] = Field(default=None, exclude=True) 
    _is_running: bool = Field(default=False, exclude=True) # ‡πÄ‡∏û‡∏¥‡πà‡∏° is_running ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö cleanup
    _lock: threading.Lock = Field(default_factory=threading.Lock, exclude=True) # ‡πÄ‡∏û‡∏¥‡πà‡∏° lock
    
    # NOTE: _retrievers_list ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô _get_relevant_documents
    # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏°‡∏±‡∏ô‡∏ñ‡∏π‡∏Å‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å self.retrievers_list
    _retrievers_list: List[NamedRetriever] = Field(default_factory=list, exclude=True) 

    class Config:
        arbitrary_types_allowed = True
    
    # -------------------- Property: num_workers (FIXED) --------------------
    @property
    def num_workers(self) -> int:
        """Calculates the optimal number of workers for the current executor type."""
        # ‡∏î‡∏∂‡∏á MAX_PARALLEL_WORKERS ‡∏à‡∏≤‡∏Å globals()
        max_workers_from_config = globals().get('MAX_PARALLEL_WORKERS', 4) 
        
        # ‡∏î‡∏∂‡∏á _executor_type ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡πÅ‡∏•‡πâ‡∏ß
        # ‡πÉ‡∏ä‡πâ getattr ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏ñ‡πâ‡∏≤ __init__ ‡∏¢‡∏±‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏°‡πà‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå
        executor_type = getattr(self, '_executor_type', 'thread') 
        
        if executor_type == "process":
            # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Process Pool ‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏≥‡∏Å‡∏±‡∏î
            return max(1, min(max_workers_from_config, os.cpu_count() - 1 if os.cpu_count() else 4))
        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Thread Pool ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤‡πÑ‡∏î‡πâ
        return max_workers_from_config

    # -------------------- Initializer --------------------
    def __init__(self, **data: Any) -> None:
        """Initializes the MultiDocRetriever and its internal state."""
        
        # 1. ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å Pydantic init ‡∏Å‡πà‡∏≠‡∏ô
        super().__init__(**data)

        # 2. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡πâ‡∏Å‡∏±‡∏ö Internal Fields ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ object.__setattr__
        #    ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏Å‡∏≤‡∏£‡∏ñ‡∏π‡∏Å‡∏î‡∏±‡∏Å‡πÇ‡∏î‡∏¢ Pydantic V1 __setattr__ (FIXED)
        
        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
        object.__setattr__(self, '_retrievers_list', self.retrievers_list)
        
        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Executor Type
        executor_type_val = self._choose_executor()
        object.__setattr__(self, '_executor_type', executor_type_val)
        
        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Executor instance
        object.__setattr__(self, '_executor', self._initialize_executor())
        object.__setattr__(self, '_is_running', True)
        
        # 3. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Retriever
        object.__setattr__(self, '_all_retrievers', {
            r.doc_id: r for r in self.retrievers_list
        })
        
        # 4. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Doc ID Filter
        if self.doc_ids_filter:
            doc_ids_list = list(self.doc_ids_filter)
            object.__setattr__(self, '_doc_ids_filter_list', doc_ids_list)
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á Chroma Filter
            chroma_filter = {"$or": [{"chunk_uuid": {"$in": doc_ids_list}}]}
            object.__setattr__(self, '_chroma_filter', chroma_filter)
        
        # 5. Logging (‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ num_workers ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡πÅ‡∏•‡πâ‡∏ß)
        if self._executor_type == "process":
            logger.info(f"Initialized MultiDocRetriever using ProcessPoolExecutor ({self.num_workers} workers).")
        else:
            logger.info(f"Initialized MultiDocRetriever using ThreadPoolExecutor ({self.num_workers} threads).")
            
    # -------------------- Executor Management --------------------
    
    def _initialize_executor(self) -> Union[ThreadPoolExecutor, ProcessPoolExecutor]:
        """Initializes the appropriate executor."""
        # NOTE: ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å _get_executor ‡∏°‡∏µ logic ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏Å‡πá‡∏ö instance ‡∏≠‡∏¢‡∏π‡πà
        # ‡πÄ‡∏£‡∏≤‡∏à‡∏∂‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏°‡∏±‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏á‡πÜ ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢
        return self._get_executor() 
        
    def _choose_executor(self) -> str:
        # NOTE: ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤ ENV_FORCE_MODE, _detect_system(), _detect_torch_device() ‡∏ñ‡∏π‡∏Å import
        # ‡∏î‡∏∂‡∏á ENV_FORCE_MODE ‡∏à‡∏≤‡∏Å globals()
        ENV_FORCE_MODE = globals().get('ENV_FORCE_MODE', None) 
        
        if ENV_FORCE_MODE == "process":
            return "process"
        if ENV_FORCE_MODE == "thread":
            return "thread"
            
        # NOTE: ‡∏ñ‡πâ‡∏≤ _detect_system, _detect_torch_device ‡πÄ‡∏õ‡πá‡∏ô Global functions
        # ‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏°‡∏±‡∏ô‡∏ú‡πà‡∏≤‡∏ô globals() ‡∏´‡∏£‡∏∑‡∏≠ import ‡∏°‡∏±‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏á‡πÜ
        system = globals().get('_detect_system', lambda: {'platform': 'unknown', 'cpu_count': 1, 'total_ram_gb': 1})()
        _detect_torch_device_func = globals().get('_detect_torch_device', lambda: 'cpu')
        
        if _detect_torch_device_func() == "mps" or system['platform'] == 'darwin':
            return "thread"
        
        if system['cpu_count'] >= 4 and (system['total_ram_gb'] is None or system['total_ram_gb'] > 8):
            return "process"
            
        return "thread" 

    def _get_executor(self) -> Union[ThreadPoolExecutor, ProcessPoolExecutor]:
        if self._executor is None:
            # ‡πÉ‡∏ä‡πâ self.num_workers ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô Property ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÅ‡∏•‡πâ‡∏ß
            workers = self.num_workers 
            
            # üìå FIX: ‡πÉ‡∏ä‡πâ object.__setattr__ ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤ self._executor ‡πÉ‡∏ô lazy init
            if self._executor_type == "process":
                new_executor = ProcessPoolExecutor(max_workers=workers)
                logger.info(f"üõ†Ô∏è Using ProcessPoolExecutor with {workers} workers.")
            else:
                new_executor = ThreadPoolExecutor(max_workers=workers)
                logger.info(f"üõ†Ô∏è Using ThreadPoolExecutor with {workers} workers.")
            
            object.__setattr__(self, '_executor', new_executor)
            
        return self._executor
    
    # (‡πÄ‡∏°‡∏ò‡∏≠‡∏î get_relevant_documents, _choose_executor, shutdown, __del__, 
    # _static_retrieve_task, _thread_retrieve_task, _get_relevant_documents 
    # ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡πÉ‡∏ä‡πâ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÉ‡∏´‡πâ‡∏°‡∏≤‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î)

    # ... (‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏•‡∏≤‡∏™) ...
    
    def shutdown(self):
        with self._lock: # ‡πÉ‡∏ä‡πâ lock ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô race condition
            if self._executor and self._is_running:
                executor_type_name = "ProcessPoolExecutor" if self._executor_type == "process" else "ThreadPoolExecutor"
                # ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ self.num_workers ‡πÑ‡∏î‡πâ‡πÅ‡∏•‡πâ‡∏ß
                workers = self.num_workers
                
                logger.info(f"Shutting down MultiDocRetriever's {executor_type_name} executor ({workers} workers).")
                self._executor.shutdown(wait=True)
                object.__setattr__(self, '_executor', None)
                object.__setattr__(self, '_is_running', False)

    def __del__(self):
        try:
            self.shutdown()
        except Exception:
            pass


    @staticmethod
    def _static_retrieve_task(named_r: NamedRetriever, query: str, chroma_filter: Optional[Dict]):
        """Static task method for ProcessPoolExecutor."""
        # NOTE: ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ñ‡∏π‡∏Å‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å‡∏°‡∏≤‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°
        try:
            # load_instance ensures the correct VSM context is used
            retriever_instance = named_r.load_instance()
            if not retriever_instance:
                return []
                
            # Prepare config for the invoke method of the RerankRetriever
            search_kwargs = {"k": named_r.top_k}
            if chroma_filter:
                # The Chroma filter is applied as 'where' in Chroma's implementation
                # üéØ FIX: ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ key 'where' ‡πÉ‡∏ô config ‡∏Ç‡∏≠‡∏á invoke()
                search_kwargs["where"] = chroma_filter 
                
            # üéØ FIX: ‡∏ï‡πâ‡∏≠‡∏á‡∏™‡πà‡∏á filter ‡πÄ‡∏õ‡πá‡∏ô 'where' ‡πÉ‡∏ô config ‡∏î‡πâ‡∏ß‡∏¢
            config = {"configurable": {"search_kwargs": {"where": chroma_filter}}}
            if not chroma_filter:
                config = {"configurable": {"search_kwargs": {}}} # ‡∏™‡πà‡∏á config ‡πÄ‡∏õ‡∏•‡πà‡∏≤‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ filter
            
            # retriever_instance is a RerankRetriever (which implements Runnable.invoke)
            docs = retriever_instance.invoke(query, config=config)
            
            for doc in docs:
                doc.metadata["retrieval_source"] = named_r.doc_type
                # üéØ FIX: ‡πÉ‡∏ä‡πâ get_doc_type_collection_key ‡πÅ‡∏ó‡∏ô _get_collection_name
                doc.metadata["collection_name"] = get_doc_type_collection_key(named_r.doc_type, named_r.enabler)
                
                # üéØ FIX 1A: Ensure chunk_uuid is present for final filtering.
                chunk_uuid = doc.metadata.get("chunk_uuid") 
                
                if not chunk_uuid:
                    # Try to find the ID from common Langchain/Chroma internal keys
                    potential_uuid = doc.metadata.get("id") or doc.metadata.get("_id") 
                    
                    if potential_uuid:
                        doc.metadata["chunk_uuid"] = str(potential_uuid)
                    else:
                        # Final Fallback: Use a stable hash of the content/metadata for deduplication/ID
                        key_content = f"{doc.page_content}{doc.metadata.get('doc_id')}"
                        hashed_uuid = hashlib.sha256(key_content.encode('utf-8')).hexdigest()
                        doc.metadata["chunk_uuid"] = hashed_uuid[:32] # Use first 32 chars for uniqueness
                        
            return docs
        except Exception as e:
            # Use print here as logger might not be configured correctly in child process
            print(f"‚ùå Child retrieval error for {named_r.doc_id} ({named_r.doc_type}): {e}")
            return []

    def _thread_retrieve_task(self, named_r: NamedRetriever, query: str, chroma_filter: Optional[Dict]):
        """Instance method for ThreadPoolExecutor."""
        # NOTE: ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ñ‡∏π‡∏Å‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å‡∏°‡∏≤‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°
        try:
            # load_instance ensures the correct VSM context is used
            retriever_instance = named_r.load_instance()
            if not retriever_instance:
                return []
                
            # Prepare config for the invoke method of the RerankRetriever
            search_kwargs = {"k": named_r.top_k}
            if chroma_filter:
                # The Chroma filter is applied as 'where' in Chroma's implementation
                # üéØ FIX: ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ key 'where' ‡πÉ‡∏ô config ‡∏Ç‡∏≠‡∏á invoke()
                search_kwargs["where"] = chroma_filter 
                
            # üéØ FIX: ‡∏ï‡πâ‡∏≠‡∏á‡∏™‡πà‡∏á filter ‡πÄ‡∏õ‡πá‡∏ô 'where' ‡πÉ‡∏ô config ‡∏î‡πâ‡∏ß‡∏¢
            config = {"configurable": {"search_kwargs": {"where": chroma_filter}}}
            if not chroma_filter:
                config = {"configurable": {"search_kwargs": {}}} # ‡∏™‡πà‡∏á config ‡πÄ‡∏õ‡∏•‡πà‡∏≤‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ filter

            # retriever_instance is a RerankRetriever (which implements Runnable.invoke)
            docs = retriever_instance.invoke(query, config=config)
            
            for doc in docs:
                doc.metadata["retrieval_source"] = named_r.doc_type
                # üéØ FIX: ‡πÉ‡∏ä‡πâ get_doc_type_collection_key ‡πÅ‡∏ó‡∏ô _get_collection_name
                doc.metadata["collection_name"] = get_doc_type_collection_key(named_r.doc_type, named_r.enabler)

                # üéØ FIX 1A: Ensure chunk_uuid is present for final filtering.
                chunk_uuid = doc.metadata.get("chunk_uuid") 
                
                if not chunk_uuid:
                    # Try to find the ID from common Langchain/Chroma internal keys
                    potential_uuid = doc.metadata.get("id") or doc.metadata.get("_id") 
                    
                    if potential_uuid:
                        doc.metadata["chunk_uuid"] = str(potential_uuid)
                    else:
                        # Final Fallback: Use a stable hash of the content/metadata for deduplication/ID
                        key_content = f"{doc.page_content}{doc.metadata.get('doc_id')}"
                        hashed_uuid = hashlib.sha256(key_content.encode('utf-8')).hexdigest()
                        doc.metadata["chunk_uuid"] = hashed_uuid[:32] # Use first 32 chars for uniqueness
                    
            return docs
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Thread retrieval error for {named_r.doc_id}: {e}")
            return []

    def _get_relevant_documents(self, query: str, *, run_manager: Any = None) -> List[LcDocument]:
        # ‡πÉ‡∏ä‡πâ self.num_workers ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì max_workers ‡πÉ‡∏ô‡πÄ‡∏°‡∏ò‡∏≠‡∏î‡∏ô‡∏µ‡πâ
        max_workers = self.num_workers
        # NOTE: self._retrievers_list ‡∏ñ‡∏π‡∏Å‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡πÉ‡∏ô __init__ ‡πÅ‡∏•‡πâ‡∏ß
        num_retrievers = len(self._retrievers_list) 
        
        # ‡∏õ‡∏£‡∏±‡∏ö max_workers ‡πÉ‡∏´‡πâ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡∏à‡∏≥‡∏ô‡∏ß‡∏ô retriever
        max_workers = min(num_retrievers, self.num_workers)
        if max_workers <= 0:
            max_workers = 1
            
        chosen = self._executor_type
        logger.info(f"‚öôÔ∏è Running MultiDocRetriever with {chosen} executor ({max_workers} workers) [Filter: {bool(self._chroma_filter)}]")
        all_docs: List[LcDocument] = []
        
        executor = self._get_executor()
        futures = []
        for named_r in self._retrievers_list:
            if chosen == "process":
                # Use static method for ProcessPoolExecutor
                future = executor.submit(MultiDocRetriever._static_retrieve_task, named_r, query, self._chroma_filter)
            else:
                # Use instance method for ThreadPoolExecutor
                future = executor.submit(self._thread_retrieve_task, named_r, query, self._chroma_filter)
            futures.append(future)
            
        for f in futures:
            try:
                docs = f.result()
                if docs:
                    all_docs.extend(docs)
            except Exception as e:
                logger.warning(f"Future failed: {e}")
                
        # Deduplication using chunk metadata
        seen = set()
        unique_docs = []
        for d in all_docs:
            src = d.metadata.get("retrieval_source") or ""
            # Use 'chunk_uuid' or 'ids' (which is the UUID from Chroma) for unique identification
            # NOTE: chunk_uuid should now be present due to the fix in the task methods
            chunk_uuid = d.metadata.get("chunk_uuid") or d.metadata.get("ids") or "" 
            
            # Fallback to content if UUIDs are missing (less reliable)
            if not chunk_uuid:
                 # Use a hash or truncated content as a fallback unique key
                 key = f"{src}_{d.page_content[:120]}_{d.metadata.get('doc_id', 'no_doc_id')}"
            else:
                 key = f"{src}_{chunk_uuid}"
                 
            if key not in seen:
                seen.add(key)
                unique_docs.append(d)
                
        logger.info(f"üìù Query='{query[:80]}...' found {len(unique_docs)} unique docs across sources (Executor={chosen})")
        
        for d in unique_docs:
            score = d.metadata.get("relevance_score")
            if score is not None:
                logger.debug(f" - [Reranked] Source={d.metadata.get('doc_type')}, Score={score:.4f}, Content='{d.page_content[:80]}...'")
        
        return unique_docs

    def get_relevant_documents(self, query: str, **kwargs) -> List[LcDocument]:
        return self._get_relevant_documents(query, **kwargs)

# -------------------- END OF MultiDocRetriever --------------------
# -------------------- load_all_vectorstores --------------------
# NOTE: ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£ import constants ‡πÅ‡∏•‡∏∞ classes ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
# ‡πÄ‡∏ä‡πà‡∏ô DEFAULT_TENANT, DEFAULT_YEAR, INITIAL_TOP_K, FINAL_K_RERANKED,
# EVIDENCE_DOC_TYPES, VectorStoreManager, MultiDocRetriever, NamedRetriever, 
# get_global_reranker, list_vectorstore_folders, get_doc_type_collection_key

def load_all_vectorstores(
    tenant: str = DEFAULT_TENANT, 
    year: int = DEFAULT_YEAR, 
    doc_ids: Optional[Set[str]] = None,
    doc_types: Optional[Union[str, List[str]]] = None,
    enabler_filter: Optional[str] = None,
    top_k: int = INITIAL_TOP_K,
    final_k: int = FINAL_K_RERANKED
) -> 'VectorStoreManager': # ‡πÉ‡∏ä‡πâ 'VectorStoreManager' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á Circular Dependency
    """
    Initializes the VSM and the main MultiDocRetriever for the current assessment context.
    """
    # 1. Initialize VSM (Singleton) - This is where the VSM object is created/reused
    manager = VectorStoreManager(
        tenant=tenant, 
        year=year, 
    )
    
    # 2. Prepare the list of target collection keys
    target_collection_keys: Set[str] = set()
    # list_vectorstore_folders() ‡∏à‡∏∞‡∏™‡πÅ‡∏Å‡∏ô‡∏´‡∏≤ collections ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô Tenant/Year
    existing_collections = list_vectorstore_folders(tenant, year, doc_type=None, enabler=None) 
    
    # Filtering Logic
    if doc_types:
        if isinstance(doc_types, str):
            doc_types = [dt.strip() for dt in doc_types.split(',')]
        
        for dt in doc_types:
            dt_norm = dt.lower().strip()
            if dt_norm == EVIDENCE_DOC_TYPES.lower():
                # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô doc_type 'evidence' ‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á enabler
                if enabler_filter:
                    enabler_list = [e.strip().upper() for e in enabler_filter.split(',')]
                    for enabler in enabler_list:
                        key = get_doc_type_collection_key(dt_norm, enabler)
                        if key in existing_collections:
                            target_collection_keys.add(key)
                        else:
                            logger.warning(f"üîç DEBUG: Skipping collection '{key}' (Not found in existing collections).")
                else:
                    # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô doc_type 'evidence' ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á enabler ‡πÉ‡∏´‡πâ‡πÄ‡∏≠‡∏≤ evidence ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
                    for collection in existing_collections:
                        if collection.startswith(f"{EVIDENCE_DOC_TYPES.lower()}_"):
                            target_collection_keys.add(collection)
            else: 
                # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö doc_type ‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏°‡∏õ‡∏µ (document, faq)
                key = get_doc_type_collection_key(dt_norm, None) 
                if key in existing_collections:
                     target_collection_keys.add(key)
                
                # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏£‡∏ì‡∏µ collection ‡∏ä‡∏∑‡πà‡∏≠ doc_type_ALL
                key_all = get_doc_type_collection_key(dt_norm, "ALL") 
                if key_all in existing_collections:
                     target_collection_keys.add(key_all)
    
    else:
        # ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏ doc_types ‡πÄ‡∏•‡∏¢ ‡πÉ‡∏´‡πâ‡πÇ‡∏´‡∏•‡∏î‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏™‡πÅ‡∏Å‡∏ô‡πÄ‡∏à‡∏≠
        target_collection_keys.update(existing_collections)

    logger.info(f"üîç DEBUG: Attempting to load {len(target_collection_keys)} total target collections: {target_collection_keys}")

    # 3. Build NamedRetriever objects
    all_retrievers: List[NamedRetriever] = []
    
    for collection_name in target_collection_keys:
        # ‡πÅ‡∏¢‡∏Å‡∏ä‡∏∑‡πà‡∏≠ collection ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤ doc_type ‡πÅ‡∏•‡∏∞ enabler
        parts = collection_name.split('_')
        doc_type_for_check = parts[0]
        enabler_for_check = parts[1].upper() if len(parts) > 1 else None
        
        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á: evidence ‡πÉ‡∏ä‡πâ‡∏õ‡∏µ, ‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ‡∏õ‡∏µ
        target_year = year
        if doc_type_for_check.lower() != EVIDENCE_DOC_TYPES.lower():
            target_year = None
            
        nr = NamedRetriever(
            doc_id=collection_name, 
            doc_type=doc_type_for_check, 
            enabler=enabler_for_check, 
            top_k=top_k, 
            final_k=final_k, 
            tenant=tenant, 
            year=target_year # ‡∏™‡πà‡∏á‡∏Ñ‡πà‡∏≤‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
        )
        all_retrievers.append(nr)
        logger.info(f"üîç DEBUG: Successfully added retriever for collection '{collection_name}' (Year={target_year}).")

    final_filter_ids = doc_ids
    if doc_ids:
        logger.info(f"‚úÖ Hard Filter Enabled: Using {len(doc_ids)} original 64-char UUIDs for filtering.")
    logger.info(f"üîç DEBUG: Final count of all_retrievers = {len(all_retrievers)}")

    if not all_retrievers:
        raise ValueError(f"No vectorstore collections found matching tenant={tenant}, year={year}, doc_types={doc_types}, enabler={enabler_filter}. Please check your configuration and ensure data exists.")
        
    # 4. Initialize MultiDocRetriever (MDR)
    
    # 4.1 Prepare Reranker (Compressor)
    reranker = None
    if final_k > 0:
        reranker = get_global_reranker()
        if reranker is None:
             # WARNING ‡∏ô‡∏µ‡πâ‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡πÉ‡∏ô traceback ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏∂‡∏á‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
             logger.warning("‚ùå WARNING: Reranker requested but failed to initialize. Reranking disabled.")
             final_k = top_k 
        else:
             logger.info(f"‚úÖ Reranker initialized ({reranker.rerank_model}). Will return top {final_k} documents.")
             

    # 4.2 Create MDR instance
    # üí° ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠ Argument ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÉ‡∏ô MultiDocRetriever Pydantic Fields
    mdr = MultiDocRetriever( 
        retrievers_list=all_retrievers, 
        k_per_doc=top_k, 
        doc_ids_filter=final_filter_ids,
        compressor=reranker, 
        final_k=final_k
    )
    
    # 5. Set MDR in VSM (Singleton)
    # üìå FIX: ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç AttributeError ‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡πâ‡∏Å‡∏±‡∏ö Internal Field ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
    manager._multi_doc_retriever = mdr
    
    # 6. Return the manager
    return manager


def get_multi_doc_retriever(
    tenant: str = DEFAULT_TENANT,
    year: int = DEFAULT_YEAR,
    doc_types: List[str] = [],
    doc_ids: Optional[List[str]] = None,
    evidence_enabler: Optional[str] = None,
    base_path: str = "", # üéØ FIX: base_path ‡∏ñ‡∏π‡∏Å‡∏•‡∏∞‡πÄ‡∏•‡∏¢
    top_k: int = INITIAL_TOP_K,
    final_k: int = FINAL_K_RERANKED
) -> MultiDocRetriever:
    """
    Factory function to create a MultiDocRetriever based on configuration.
    It determines which NamedRetrievers to initialize based on the tenant, year, and doc_types.
    """
    all_retrievers: List[NamedRetriever] = []

    # 1. Dynamic Check for Year-Specific Collections
    # Loop through requested doc_types and check against the target year
    target_year = year
    for doc_type_for_check in doc_types:
        # üéØ FIX: ‡πÉ‡∏ä‡πâ get_doc_type_collection_key ‡πÅ‡∏ó‡∏ô _get_collection_name
        collection_name = get_doc_type_collection_key(doc_type_for_check, evidence_enabler)
        
        enabler_for_check = evidence_enabler
        
        # Check if collection exists for the specific year
        if not vectorstore_exists(tenant=tenant, year=target_year, doc_type=doc_type_for_check, enabler=enabler_for_check):
            logger.warning(f"üîç DEBUG: Skipping collection '{collection_name}' (vectorstore_exists failed at tenant={tenant}, year={target_year}).")
            continue
            
        # üéØ FIX 2C: ‡∏™‡πà‡∏á target_year ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô NamedRetriever
        nr = NamedRetriever(
            doc_id=collection_name, 
            doc_type=doc_type_for_check, 
            enabler=enabler_for_check, 
            top_k=top_k, 
            final_k=final_k, 
            # üéØ FIX: ‡∏•‡∏ö base_path ‡∏≠‡∏≠‡∏Å
            tenant=tenant, 
            year=target_year # <--- ‡∏™‡πà‡∏á‡∏Ñ‡πà‡∏≤‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
        )
        all_retrievers.append(nr)
        logger.info(f"üîç DEBUG: Successfully added retriever for collection '{collection_name}' (Year={target_year}).")

    final_filter_ids = doc_ids
    if doc_ids:
        logger.info(f"‚úÖ Hard Filter Enabled: Using {len(doc_ids)} original 64-char UUIDs for filtering.")
    logger.info(f"üîç DEBUG: Final count of all_retrievers = {len(all_retrievers)}")

    if not all_retrievers:
        raise ValueError(f"No vectorstore collections found matching tenant={tenant}, year={year}, doc_types={doc_types} and evidence_enabler={evidence_enabler}")
        
    mdr = MultiDocRetriever(retrievers_list=all_retrievers, k_per_doc=top_k, doc_ids_filter=final_filter_ids)
    return mdr