# core/vectorstore.py
import os
import platform
import logging
import threading
import multiprocessing
import json
import shutil
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from typing import List, Optional, Union, Sequence, Any, Dict, Set, Tuple

# system utils
try:
    import psutil
except ImportError:
    psutil = None

# LangChain-ish imports (adjust to your project's versions)
from langchain_core.documents import Document as LcDocument
from langchain_core.retrievers import BaseRetriever
from langchain_core.documents import BaseDocumentCompressor

# Pydantic helpers
from pydantic import PrivateAttr, ConfigDict, BaseModel

# Chroma / HF embeddings
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
import chromadb
from chromadb.config import Settings

# Logging
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# Try import CrossEncoder (sentence-transformers)
try:
    from sentence_transformers import CrossEncoder
    _HAS_SENT_TRANS = True
except Exception:
    CrossEncoder = None
    _HAS_SENT_TRANS = False
    logger.warning("‚ö†Ô∏è sentence-transformers CrossEncoder not available. Reranker will be disabled.")

# Configure chromadb telemetry if available
try:
    chromadb.configure(anonymized_telemetry=False)
except Exception:
    try:
        chromadb.settings = Settings(anonymized_telemetry=False)
    except Exception:
        pass

# -------------------- Global Config --------------------
from config.global_vars import (
    VECTORSTORE_DIR,
    MAPPING_BASE_DIR,
    FINAL_K_RERANKED,
    INITIAL_TOP_K,
    EVIDENCE_DOC_TYPES,
    MAX_PARALLEL_WORKERS,
    DEFAULT_TENANT,
    DEFAULT_YEAR,
    DEFAULT_ENABLER
)

# -------------------- Vectorstore Constants --------------------
ENV_FORCE_MODE = os.getenv("VECTOR_MODE", "").lower()  # "thread", "process", or ""
ENV_DISABLE_ACCEL = os.getenv("VECTOR_DISABLE_ACCEL", "").lower() in ("1", "true", "yes")

# Global caches (per process)
_CACHED_EMBEDDINGS = None
_EMBED_LOCK = threading.Lock()
_MPS_WARNING_SHOWN = False

# -------------------- Helper: detect environment & device --------------------
@staticmethod
def _detect_system():
    cpu_count = os.cpu_count() or 4
    total_ram_gb = None
    if psutil:
        try:
            total_ram_gb = psutil.virtual_memory().total / (1024 ** 3)
        except Exception:
            total_ram_gb = None
    return {"cpu_count": cpu_count, "total_ram_gb": total_ram_gb, "platform": platform.system().lower()}

@staticmethod
def _detect_torch_device():
    try:
        import torch
        if ENV_DISABLE_ACCEL:
            return "cpu"
        if torch.cuda.is_available():
            return "cuda"
        if platform.system().lower() == "darwin" and hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
            # MPS can be fragile for multiprocessing; we'll warn and possibly force cpu later
            return "mps"
    except Exception:
        pass
    return "cpu"

def get_hf_embeddings(device_hint: Optional[str] = None):
    global _CACHED_EMBEDDINGS, _MPS_WARNING_SHOWN
    device = device_hint or _detect_torch_device()

    # ... (‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏à‡∏ô‡∏ñ‡∏∂‡∏á‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ)

    if _CACHED_EMBEDDINGS is None:
        with _EMBED_LOCK:
            if _CACHED_EMBEDDINGS is None:
                # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≤‡∏î!!!
                model_name = "intfloat/multilingual-e5-base"  # ‡∏´‡∏£‡∏∑‡∏≠ large ‡∏ñ‡πâ‡∏≤‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÅ‡∏£‡∏á
                
                logger.info(f"Loading BEST Thai RAG embedding 2025: {model_name} on {device}")
                logger.info("This model was used to build ALL PEA 2568 vectorstores (evidence_km, document, etc.)")
                
                try:
                    _CACHED_EMBEDDINGS = HuggingFaceEmbeddings(
                        model_name=model_name,
                        model_kwargs={
                            "device": device,
                            # ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å: e5 ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ prefix!
                            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏™‡πà ‚Üí ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ï‡∏Å‡∏Æ‡∏ß‡∏ö!
                        },
                        encode_kwargs={
                            "normalize_embeddings": True,
                            # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö e5 series ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏™‡πà prefix ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô!!!
                            "prompt": "query: "  # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö query
                            # ‡∏´‡∏£‡∏∑‡∏≠‡∏ñ‡πâ‡∏≤‡∏à‡∏∞ embed ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ ‚Üí ‡πÉ‡∏ä‡πâ "passage: "
                        }
                    )
                except Exception as e:
                    logger.error(f"Failed to load {model_name}: {e}")
                    logger.warning("Falling back to paraphrase-multilingual-MiniLM-L12-v2")
                    _CACHED_EMBEDDINGS = HuggingFaceEmbeddings(
                        model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
                        model_kwargs={"device": "cpu"}
                    )
    return _CACHED_EMBEDDINGS

# =================================================================
# HuggingFace Cross-Encoder Reranker wrapper (singleton)
# =================================================================
class HuggingFaceCrossEncoderCompressor(BaseDocumentCompressor, BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)
    # default cross-encoder recommended model
    rerank_model: str = "cross-encoder/ms-marco-MiniLM-L-6-v2"
    rerank_device: str = "cpu"
    rerank_max_length: int = 512
    _cross_encoder: Any = PrivateAttr(None)

    def __init__(self, **data):
        super().__init__(**data)
        # determine device safely (force cpu for CrossEncoder on mac mps)
        try:
            device = _detect_torch_device()
            if device == "mps":
                # CrossEncoder on MPS can be unstable; force CPU
                device = "cpu"
            self.rerank_device = device
        except Exception:
            self.rerank_device = "cpu"

    def set_encoder_instance(self, encoder: Any):
        self._cross_encoder = encoder

    def compress_documents(self, documents: Sequence[LcDocument], query: str, top_n: int, callbacks: Optional[Any] = None) -> List[LcDocument]:
        if not documents:
            return []
        if self._cross_encoder is None or not hasattr(self._cross_encoder, "predict"):
            logger.error("HuggingFace Cross-Encoder is not initialized. Returning truncated documents.")
            return list(documents)[:top_n]

        # Prepare input pairs
        sentence_pairs = [[query, doc.page_content] for doc in documents]

        try:
            scores = self._cross_encoder.predict(sentence_pairs, show_progress_bar=False)
        except TypeError:
            # Some CrossEncoder versions accept different args
            try:
                scores = self._cross_encoder.predict(sentence_pairs)
            except Exception as e:
                logger.error(f"‚ùå Cross-Encoder prediction failed: {e}. Returning truncated documents.")
                return list(documents)[:top_n]
        except Exception as e:
            logger.error(f"‚ùå Cross-Encoder prediction failed: {e}. Returning truncated documents.")
            return list(documents)[:top_n]

        # sort and return top_n
        doc_scores = sorted(zip(documents, scores), key=lambda x: x[1], reverse=True)
        final_docs = []
        for doc, score in doc_scores[:top_n]:
            # üü¢ FIX: Check and initialize metadata if it is None (prevents TypeError)
            if doc.metadata is None:
                doc.metadata = {}
                logger.warning(f"‚ö†Ô∏è Reranker: Found Document with None metadata. Initializing metadata to {{}}.")
                
            doc.metadata["relevance_score"] = float(score)
            final_docs.append(doc)
        return final_docs

_CACHED_RERANKER_INSTANCE: Optional[HuggingFaceCrossEncoderCompressor] = None
_CACHED_CROSS_ENCODER: Any = None

def get_global_reranker() -> Optional[HuggingFaceCrossEncoderCompressor]:
    """
    Returns a cached HuggingFaceCrossEncoderCompressor instance (singleton).
    """
    global _CACHED_RERANKER_INSTANCE, _CACHED_CROSS_ENCODER

    if _CACHED_RERANKER_INSTANCE is None:
        try:
            if not _HAS_SENT_TRANS:
                logging.warning("sentence-transformers not installed. Cross-Encoder reranker disabled.")
                return None

            instance = HuggingFaceCrossEncoderCompressor(
                rerank_model="mixedbread-ai/mxbai-rerank-xsmall-v1"
            )

            from sentence_transformers import CrossEncoder
            cross_encoder_model = CrossEncoder(
                instance.rerank_model,
                device=instance.rerank_device
            )

            instance.set_encoder_instance(cross_encoder_model)

            _CACHED_RERANKER_INSTANCE = instance
            _CACHED_CROSS_ENCODER = cross_encoder_model

            logging.info(f"Initialized global Cross-Encoder reranker: {instance.rerank_model} on {instance.rerank_device}")

        except Exception as e:
            logging.warning(f"Failed to initialize global reranker: {e}")
            return None

    return _CACHED_RERANKER_INSTANCE


# -------------------- Path Helper Function (REVISED for Lowercase Path Suffix and Optional Year) --------------------
def _build_vectorstore_path_by_doc_type(tenant: str, year: Optional[int], doc_type: str, enabler: Optional[str] = None) -> str:
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á Full Path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Collection ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏ï‡∏£‡∏£‡∏Å‡∏∞ EVIDENCE_DOC_TYPES
    - Evidence (‡∏°‡∏µ year): VECTORSTORE_DIR / tenant / year / collection_name
    - Docs/FAQ (‡πÑ‡∏°‡πà‡∏°‡∏µ year): VECTORSTORE_DIR / tenant / collection_name
    """
    doc_type_lower = doc_type.lower()
    collection_name = _get_collection_name(doc_type, enabler)
    
    path_segments = [VECTORSTORE_DIR, tenant.lower()]
    
    # üéØ FIX: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏õ‡∏µ (year is not None) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Evidence ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
    if doc_type_lower == EVIDENCE_DOC_TYPES.lower() and year is not None:
        # Path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö evidence ‡∏Ñ‡∏∑‡∏≠ /tenant/year/collection_name
        path_segments.append(str(year))
        path_segments.append(collection_name)
    else:
        # Path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö document/faq (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ‡∏õ‡∏µ)
        path_segments.append(collection_name)
        
    return os.path.join(*path_segments)

# -----------------------------------------------------------

def _get_collection_name(doc_type: str, enabler: Optional[str] = None) -> str:
    """
    Calculates the Chroma collection name (Internal identifier).
    """
    doc_type_norm = doc_type.strip().lower()
    
    if doc_type_norm == EVIDENCE_DOC_TYPES.lower():
        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Evidence: ‡∏ä‡∏∑‡πà‡∏≠ Collection ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏ß‡∏° enabler ‡πÄ‡∏™‡∏°‡∏≠
        enabler_norm = (enabler or "km").strip().lower()
        
        # üéØ FIX: ‡∏£‡∏ß‡∏°‡∏ï‡∏£‡∏£‡∏Å‡∏∞‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÄ‡∏î‡∏µ‡∏¢‡∏ß: evidence + enabler 
        collection_name = f"{doc_type_norm}_{enabler_norm}"
        
    else:
        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ: ‡∏ä‡∏∑‡πà‡∏≠ Collection ‡∏Ñ‡∏∑‡∏≠ doc_type ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
        collection_name = doc_type_norm
        
    logger.debug(f"üß≠ DEBUG: _get_collection_name(doc_type={doc_type}, enabler={enabler}) => {collection_name}")
    return collection_name

# üìå REVISED: ‡πÄ‡∏û‡∏¥‡πà‡∏° tenant ‡πÅ‡∏•‡∏∞ year, ‡πÉ‡∏ä‡πâ Logic Centralized KM
def get_vectorstore_path(
    tenant: str, 
    year: Optional[int], 
    doc_type: Optional[str] = None, 
    enabler: Optional[str] = None
) -> str:
    """
    Calculates the full persist directory path for the vector store instance
    based on the Centralized KM Logic.
    """
    if not doc_type:
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ doc_type ‡πÉ‡∏´‡πâ return root ‡∏Ç‡∏≠‡∏á tenant
        return os.path.join(VECTORSTORE_DIR, tenant.lower()) 
        
    # üéØ FIX: ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ _build_vectorstore_path_by_doc_type ‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏•‡πâ‡∏ß
    return _build_vectorstore_path_by_doc_type(tenant, year, doc_type, enabler)

# üìå REVISED: ‡πÄ‡∏û‡∏¥‡πà‡∏° tenant ‡πÅ‡∏•‡∏∞ year, ‡πÉ‡∏ä‡πâ Logic Centralized KM
def vectorstore_exists(
    doc_id: str, # ‡∏£‡∏±‡∏Å‡∏©‡∏≤‡πÑ‡∏ß‡πâ‡∏ï‡∏≤‡∏° Signature ‡πÄ‡∏î‡∏¥‡∏°‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ
    tenant: str,
    year: Optional[int], # <--- ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö None ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö General Docs
    doc_type: Optional[str] = None, 
    enabler: Optional[str] = None, 
    base_path: str = VECTORSTORE_DIR # base_path ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏•‡∏∞‡πÄ‡∏•‡∏¢‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÉ‡∏ä‡πâ global VECTORSTORE_DIR
) -> bool:
    """
    Checks if the Vector Store directory exists for the given context.
    """
    if not doc_type:
        return False
        
    # 1. Get the full path using the updated logic
    path = get_vectorstore_path(tenant, year, doc_type, enabler) 
    
    # 2. Check for the actual data file created by Chroma
    file_path = os.path.join(path, "chroma.sqlite3")
    
    if not os.path.isdir(path):
        logger.warning(f"‚ùå V-Exists Check: Directory not found for doc_type '{doc_type}' at {path}")
        return False
    if os.path.isfile(file_path):
        return True
    logger.error(f"‚ùå V-Exists Check: FAILED to find file chroma.sqlite3 at {file_path}")
    return False

def _get_collection_parent_dir(tenant: str, year: Optional[int], doc_type: str) -> str: 
    """
    Calculates the parent directory where the collection folder resides.
    - evidence: VECTORSTORE_DIR / tenant / year 
    - others:   VECTORSTORE_DIR / tenant 
    """
    doc_type_lower = doc_type.lower()
    path_segments = [VECTORSTORE_DIR, tenant.lower()]
    
    # üéØ FIX: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏õ‡∏µ (year is not None) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Evidence ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
    if doc_type_lower == EVIDENCE_DOC_TYPES.lower() and year is not None:
        # Parent ‡∏Ç‡∏≠‡∏á evidence ‡∏Ñ‡∏∑‡∏≠‡∏°‡∏µ‡∏õ‡∏µ
        path_segments.append(str(year))
        
    return os.path.join(*path_segments)

def list_vectorstore_folders(
    tenant: str, 
    year: int, # NOTE: ‡πÉ‡∏ä‡πâ‡∏õ‡∏µ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ô (int)
    doc_type: Optional[str] = None, 
    enabler: Optional[str] = None, 
    base_path: str = VECTORSTORE_DIR
) -> List[str]:
    """
    Lists the actual collection folder names (e.g., 'evidence_km', 'document')
    that exist under the specified tenant and year context.
    """
    
    # Scenario 1: Specific doc_type/enabler is requested
    if doc_type:
        doc_type_norm = doc_type.lower().strip()
        collection_name = _get_collection_name(doc_type_norm, enabler)
        
        # üéØ FIX 4: ‡πÉ‡∏ä‡πâ get_vectorstore_path ‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏•‡πâ‡∏ß
        full_collection_path = get_vectorstore_path(tenant, year, doc_type_norm, enabler)
        
        if os.path.isdir(full_collection_path):
            return [collection_name] 
        return []

    # Scenario 2: List ALL collections for the given tenant/year context (List All)
    
    collections: Set[str] = set()
    
    # 1. Scan the Year Root (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Doc Type: evidence)
    root_year = _get_collection_parent_dir(tenant, year, EVIDENCE_DOC_TYPES) 
    if os.path.isdir(root_year):
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ evidence_... collections ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏õ‡∏µ
        for sub_dir in os.listdir(root_year):
             # üéØ FIX 5: ‡πÉ‡∏ä‡πâ startsWith ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏ö 'evidence_km' (‡∏´‡∏£‡∏∑‡∏≠ evidence_xxx)
             if sub_dir.lower().startswith(f"{EVIDENCE_DOC_TYPES.lower()}_"): 
                 collections.add(sub_dir.lower()) 

    # 2. Scan the Common Root (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Doc Type: document, faq, ‡∏Ø‡∏•‡∏Ø)
    # Common Root: VECTORSTORE_DIR / tenant
    root_common = _get_collection_parent_dir(tenant, year=None, doc_type="document") 
    if os.path.isdir(root_common):
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå Doc Type
        for sub_dir in os.listdir(root_common):
            sub_dir_lower = sub_dir.lower()
            if sub_dir_lower == EVIDENCE_DOC_TYPES.lower() or sub_dir.isdigit():
                 continue 
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏õ‡πá‡∏ô Collection ‡∏à‡∏£‡∏¥‡∏á (‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå Chroma)
            full_collection_path = os.path.join(root_common, sub_dir)
            
            if os.path.isfile(os.path.join(full_collection_path, "chroma.sqlite3")):
                 collections.add(sub_dir_lower) 
    
    return sorted(list(collections))

# -------------------- VECTORSTORE MANAGER (SINGLETON) --------------------
class VectorStoreManager:
    _instance = None
    _is_initialized = False
    _lock = threading.Lock()

    # ‡πÉ‡∏ä‡πâ default_factory ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏™‡πà {} ‡∏´‡∏£‡∏∑‡∏≠ None ‡∏ï‡∏£‡∏á‡πÜ
    _chroma_cache: Dict[str, Chroma] = PrivateAttr(default_factory=dict)
    _multi_doc_retriever: Optional['MultiDocRetriever'] = PrivateAttr(default=None)
    
    tenant: str = PrivateAttr(default=DEFAULT_TENANT)
    year: int = PrivateAttr(default=DEFAULT_YEAR)

    _doc_id_mapping: Dict[str, Dict[str, Any]] = PrivateAttr(default_factory=dict)
    _uuid_to_doc_id: Dict[str, str] = PrivateAttr(default_factory=dict)

    _embeddings: Any = PrivateAttr(default=None)

    # ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î: ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ default_factory=dict ‡∏´‡∏£‡∏∑‡∏≠ default=None ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô!
    _client: Optional[chromadb.PersistentClient] = PrivateAttr(default=None)

    def __new__(cls, *args, **kwargs):
        with cls._lock:
            if cls._instance is None:
                cls._instance = super(VectorStoreManager, cls).__new__(cls)
        return cls._instance

    def __init__(self, base_path: str = VECTORSTORE_DIR, tenant: str = DEFAULT_TENANT, year: int = DEFAULT_YEAR):
        if not self._is_initialized:
            self._base_path = base_path
            self.tenant = tenant.lower()        # ‡∏ï‡πâ‡∏≠‡∏á .lower() ‡∏î‡πâ‡∏ß‡∏¢‡∏ô‡∏∞!
            self.year = year
            self.enabler = DEFAULT_ENABLER   
            self._chroma_cache = {}
            self._embeddings = get_hf_embeddings()
            
            # ‡πÅ‡∏Å‡πâ‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ: Chroma Client ‡∏ï‡πâ‡∏≠‡∏á‡∏ä‡∏µ‡πâ‡∏ó‡∏µ‡πà root ‡∏Ç‡∏≠‡∏á tenant ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô!
            # chroma_root_path = os.path.join(base_path, self.tenant)   # ‡πÑ‡∏°‡πà‡∏°‡∏µ str(year)!!!
            # chroma_root_path = os.path.join(base_path, self.tenant, str(self.year))  # ‡πÄ‡∏û‡∏¥‡πà‡∏° year ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ!
            # 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠ Collection ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á (evidence_km)
            collection_name = _get_collection_name(EVIDENCE_DOC_TYPES, self.enabler)
            
            # 2. ‡πÉ‡∏ä‡πâ Path Helper ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á Full Path
            #    ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå: /vectorstore/pea/2568/evidence_km
            chroma_root_path = _build_vectorstore_path_by_doc_type(
                tenant=self.tenant, 
                year=self.year, 
                doc_type=EVIDENCE_DOC_TYPES, 
                enabler=self.enabler
            )
            
            self._client = chromadb.PersistentClient(path=chroma_root_path)
            logger.info(f"ChromaDB Client initialized at FULL COLLECTION PATH: {chroma_root_path}")
            # Note: ‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ Chroma Client ‡πÉ‡∏ô‡πÇ‡∏´‡∏°‡∏î‡∏ô‡∏µ‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÇ‡∏´‡∏•‡∏î DB ‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏≤‡∏¢ collection
            
            # ‡πÇ‡∏´‡∏•‡∏î mapping ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ tenant/year ‡πÅ‡∏•‡πâ‡∏ß
            self._load_doc_id_mapping() 
            
            logger.info(f"Initialized VectorStoreManager (Tenant: {self.tenant}, Year: {self.year}). "
                        f"Loaded {len(self._doc_id_mapping)} stable doc IDs.")
            
            VectorStoreManager._is_initialized = True

    def close(self):
        with self._lock:
            if self._multi_doc_retriever and hasattr(self._multi_doc_retriever, "shutdown"):
                logger.info("Closing MultiDocRetriever executor via VSM.")
                self._multi_doc_retriever.shutdown()
                self._multi_doc_retriever = None
            self._chroma_cache = {}
            VectorStoreManager._is_initialized = False

    def __del__(self):
        try:
            self.close()
        except Exception:
            pass

    def _load_doc_id_mapping(self):
        self._doc_id_mapping = {}
        self._uuid_to_doc_id = {}
        
        # NOTE: Doc ID Mapping ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ Path ‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏õ‡∏µ‡πÄ‡∏™‡∏°‡∏≠
        # mapping_filename = f"{self.tenant.lower()}_{self.year}_doc_id_mapping.json"
        mapping_filename = f"{self.tenant.lower()}_{self.year}_{self.enabler.lower()}_doc_id_mapping.json"
        
        doc_id_mapping_path = os.path.join(
            MAPPING_BASE_DIR, 
            self.tenant.lower(), 
            str(self.year), 
            mapping_filename
        )
  
        try:
            with open(doc_id_mapping_path, "r", encoding="utf-8") as f:
                mapping_data: Dict[str, Dict[str, Any]] = json.load(f)
                cleaned_mapping = {k.strip(): v for k, v in mapping_data.items()}
                self._doc_id_mapping = cleaned_mapping
                for doc_id, doc_entry in cleaned_mapping.items():
                    if isinstance(doc_entry, dict) and "chunk_uuids" in doc_entry and isinstance(doc_entry.get("chunk_uuids"), list):
                        for uid in doc_entry["chunk_uuids"]:
                            self._uuid_to_doc_id[uid] = doc_id
            logger.info(f"‚úÖ Loaded Doc ID Mapping from {doc_id_mapping_path}: {len(self._doc_id_mapping)} original documents, {len(self._uuid_to_doc_id)} total chunks.")
        except FileNotFoundError:
            logger.warning(f"‚ö†Ô∏è Doc ID Mapping file not found at {doc_id_mapping_path}.")
        except Exception as e:
            logger.error(f"‚ùå Failed to load Doc ID Mapping from {doc_id_mapping_path}: {e}")

    def _re_parse_collection_name(self, collection_name: str) -> Tuple[str, Optional[str]]:
        collection_name_lower = collection_name.strip().lower()
        if collection_name_lower.startswith(f"{EVIDENCE_DOC_TYPES}_"):
            parts = collection_name_lower.split("_", 1)
            return EVIDENCE_DOC_TYPES, parts[1].upper() if len(parts) == 2 else None
        return collection_name_lower, None

    def _load_chroma_instance(self, collection_name: str) -> Optional[Chroma]:
        """
        ‡πÇ‡∏´‡∏•‡∏î (‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏∑‡∏ô‡∏à‡∏≤‡∏Å cache) Chroma instance ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ PersistentClient ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á
        ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á year-specific (evidence_*) ‡πÅ‡∏•‡∏∞ general/policy collections
        """
        # 1. Fast cache hit
        if collection_name in self._chroma_cache:
            return self._chroma_cache[collection_name]

        # 2. Thread-safe double-check
        with self._lock:
            if collection_name in self._chroma_cache:
                return self._chroma_cache[collection_name]

            # ------------------------------------------------------------------
            # 3. ‡πÅ‡∏¢‡∏Å doc_type ‡∏Å‡∏±‡∏ö enabler ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤
            # ------------------------------------------------------------------
            doc_type, enabler = self._re_parse_collection_name(collection_name)

            # ------------------------------------------------------------------
            # 4. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î target_year ‚Üí ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡∏Ü‡πà‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏°‡∏≤‡∏ï‡∏•‡∏≠‡∏î!!!
            # ------------------------------------------------------------------
            # evidence_* ‡∏ó‡∏∏‡∏Å‡∏ï‡∏±‡∏ß (‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏°‡∏µ enabler ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà) ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ year ‡∏Ç‡∏≠‡∏á tenant ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô!!!
            if doc_type.startswith("evidence"):
                target_year: Optional[int] = self.year
            else:
                # general / policy / ‡∏´‡∏£‡∏∑‡∏≠ collection ‡∏ó‡∏µ‡πà‡∏°‡∏µ enabler (‡πÄ‡∏ä‡πà‡∏ô risk_control_xxx)
                # ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô general (‡πÑ‡∏°‡πà‡∏°‡∏µ year)
                target_year = None

            # ------------------------------------------------------------------
            # 5. ‡∏™‡∏£‡πâ‡∏≤‡∏á persist_directory ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á 100%
            # ------------------------------------------------------------------
            persist_directory = get_vectorstore_path(
                tenant=self.tenant,
                year=target_year,
                doc_type=doc_type,
                enabler=enabler,
            )

            # ------------------------------------------------------------------
            # 6. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ folder ‡∏à‡∏£‡∏¥‡∏á‡πÑ‡∏´‡∏° (debug ‡∏™‡∏∞‡∏î‡∏ß‡∏Å‡∏™‡∏∏‡∏î ‡πÜ)
            # ------------------------------------------------------------------
            if not os.path.exists(persist_directory):
                logger.warning(
                    f"Vectorstore directory NOT FOUND!\n"
                    f"   Collection   : {collection_name}\n"
                    f"   Expected path: {persist_directory}\n"
                    f"   tenant={self.tenant} | year={self.year} | doc_type={doc_type} | enabler={enabler or 'None'}"
                )
                # ‡∏•‡∏≠‡∏á‡πÄ‡∏ä‡πá‡∏Ñ path ‡∏≠‡∏µ‡∏Å‡πÅ‡∏ö‡∏ö‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡πÉ‡∏Ñ‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠ folder ‡∏ú‡∏¥‡∏î
                alt_path = get_vectorstore_path(self.tenant, self.year, doc_type, enabler)
                if os.path.exists(alt_path):
                    logger.warning(f"   BUT found at alternative path: {alt_path} ‚Üê ‡∏≠‡∏≤‡∏à‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ year ‡∏ú‡∏¥‡∏î?")
                return None

            # ------------------------------------------------------------------
            # 7. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ client ‡∏ñ‡∏π‡∏Å init ‡πÅ‡∏•‡πâ‡∏ß
            # ------------------------------------------------------------------
            if self._client is None:
                logger.error("Chroma PersistentClient is None! ‡∏ï‡πâ‡∏≠‡∏á init ‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")
                return None

            try:
                # ------------------------------------------------------------------
                # 8. ‡∏™‡∏£‡πâ‡∏≤‡∏á Chroma instance (‡πÉ‡∏ä‡πâ client ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô ‚Üí ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏∏‡∏î!)
                # ------------------------------------------------------------------
                vectordb = Chroma(
                    client=self._client,                     # ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ client ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á!!!
                    embedding_function=self._embeddings,
                    collection_name=collection_name,
                )

                # Cache ‡πÑ‡∏ß‡πâ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ï‡πà‡∏≠‡πÑ‡∏õ
                self._chroma_cache[collection_name] = vectordb

                logger.info(
                    f"Loaded Chroma collection '{collection_name}' ‚Üí {persist_directory}"
                )
                return vectordb

            except Exception as e:
                logger.error(
                    f"FAILED to load collection '{collection_name}' from {persist_directory}\n"
                    f"Error: {e}",
                    exc_info=True,
                )
                return None

    def get_documents_by_id(self, stable_doc_ids: Union[str, List[str]], doc_type: str = "default_collection", enabler: Optional[str] = None) -> List[LcDocument]:
        """
        Retrieve documents from Chroma collection by stable_doc_ids.
        Compatible with latest Chroma where 'ids' is not a valid include field.
        """
        if isinstance(stable_doc_ids, str):
            stable_doc_ids = [stable_doc_ids]
        stable_doc_ids = [uid.strip() for uid in stable_doc_ids if uid]
        if not stable_doc_ids:
            return []

        collection_name = _get_collection_name(doc_type, enabler)
        chroma_instance = self._load_chroma_instance(collection_name)
        if not chroma_instance:
            logger.warning(f"Cannot retrieve documents: Collection '{collection_name}' is not loaded.")
            return []

        try:
            collection = chroma_instance._collection
            # ‡πÄ‡∏≠‡∏≤ 'ids' ‡∏≠‡∏≠‡∏Å ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ Chroma ‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏°‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö
            result = collection.get(
                where={"stable_doc_uuid": {"$in": stable_doc_ids}},
                include=["documents", "metadatas"]
            )

            documents: List[LcDocument] = []
            docs = result.get("documents", [])
            metadatas = result.get("metadatas", [{}] * len(docs))

            for i, text in enumerate(docs):
                if not text:
                    continue
                metadata = metadatas[i]
                # map chunk_uuid ‡∏à‡∏≤‡∏Å stable_doc_ids ‡πÄ‡∏≠‡∏á
                metadata["chunk_uuid"] = stable_doc_ids[i] if i < len(stable_doc_ids) else f"unknown_{i}"
                metadata["doc_id"] = metadata.get("doc_id", "UNKNOWN")
                metadata["doc_type"] = doc_type
                documents.append(LcDocument(page_content=text, metadata=metadata))

            logger.info(f"‚úÖ Retrieved {len(documents)} documents for {len(stable_doc_ids)} Stable IDs from '{collection_name}'.")
            return documents

        except Exception as e:
            logger.error(f"‚ùå Error retrieving documents by Stable IDs from collection '{collection_name}': {e}")
            return []

    def retrieve_by_chunk_uuids(self, chunk_uuids: List[str], collection_name: Optional[str] = None) -> List[LcDocument]:
            """
            Retrieves documents from Chroma collection based on a list of unique chunk_uuids (IDs).
            Includes de-duplication logic to prevent ChromaDB DuplicateIDError.
            """
            if not chunk_uuids:
                logger.info("VSM: No chunk_uuids provided for hydration.")
                return []
            
            # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ä‡∏∑‡πà‡∏≠ collection ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏∞‡∏ö‡∏∏
            if collection_name is None:
                collection_name = f"evidence_{getattr(self, 'enabler', 'km').lower()}"

            # ‡πÇ‡∏´‡∏•‡∏î Chroma instance
            chroma_instance = self._load_chroma_instance(collection_name)
            if not chroma_instance:
                logger.warning(f"VSM: Cannot load collection '{collection_name}' for hydration.")
                return []

            # DEBUG (‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà)
            logger.info(f"VSM: Attempting hydration with {len(chunk_uuids)} UUIDs from '{collection_name}'")
            logger.info(f"VSM: First 5 UUIDs ‚Üí {chunk_uuids[:5]}")

            try:
                collection = chroma_instance._collection

                # 1. Clean IDs (‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô str ‡πÅ‡∏•‡∏∞ strip)
                clean_ids = [str(uuid).strip() for uuid in chunk_uuids if uuid and str(uuid).strip()]

                # 2. üéØ FIX: ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£ De-duplicate IDs ‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ ChromaDB
                unique_chunk_uuids = list(set(clean_ids))
                
                if len(unique_chunk_uuids) < len(clean_ids):
                    duplicated_count = len(clean_ids) - len(unique_chunk_uuids)
                    logger.warning(f"VSM: De-duplicated {duplicated_count} repeated UUIDs before calling ChromaDB get.")
                
                if not unique_chunk_uuids:
                    logger.warning("VSM: All UUIDs became empty after cleaning or duplication removal!")
                    return []

                # 3. ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ ChromaDB ‡∏î‡πâ‡∏ß‡∏¢ IDs ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô
                results = collection.get(
                    ids=unique_chunk_uuids,
                    include=["documents", "metadatas"]
                )

                found_count = len(results["ids"]) if results["ids"] else 0
                logger.info(f"VSM: Successfully retrieved {found_count}/{len(unique_chunk_uuids)} chunks by UUID from '{collection_name}'")

                # 4. ‡πÅ‡∏õ‡∏•‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏õ‡πá‡∏ô LcDocument
                docs = []
                for i, doc_id in enumerate(results["ids"]):
                    content = results["documents"][i]
                    meta = results["metadatas"][i] if results["metadatas"] else {}
                    doc = LcDocument(page_content=content, metadata=meta.copy())
                    # ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: doc_id ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å Chroma ‡∏Ñ‡∏∑‡∏≠ chunk_uuid
                    doc.metadata["chunk_uuid"] = doc_id 
                    docs.append(doc)
                
                return docs

            except Exception as e:
                logger.error(f"VSM: FATAL Error in retrieve_by_chunk_uuids: {e}", exc_info=True)
                return []

    def get_limited_chunks_from_doc_ids(self, stable_doc_ids: Union[str, List[str]], query: Union[str, List[str]], doc_type: str, enabler: Optional[str] = None, limit_per_doc: int = 5) -> List[LcDocument]:
        if isinstance(stable_doc_ids, str):
            stable_doc_ids = [stable_doc_ids]
        stable_doc_ids = [uid for uid in stable_doc_ids if uid]
        if not stable_doc_ids:
            return []
        vector_search_query = query[0] if isinstance(query, list) and query else (query if isinstance(query, str) else "")
        if not vector_search_query:
            logger.warning("Limited chunk search skipped: Query is empty.")
            return []
        collection_name = _get_collection_name(doc_type, enabler)
        chroma_instance = self._load_chroma_instance(collection_name)
        if not chroma_instance:
            logger.error(f"Collection '{collection_name}' is not loaded.")
            return []
        all_limited_documents: List[LcDocument] = []
        total_chunks_retrieved = 0
        for stable_id in stable_doc_ids:
            stable_id_clean = stable_id.strip()
            doc_filter = {"stable_doc_uuid": stable_id_clean}
            try:
                custom_retriever = ChromaRetriever(vectorstore=chroma_instance, k=limit_per_doc, filter=doc_filter)
                limited_docs = custom_retriever.get_relevant_documents(query=vector_search_query)
                for doc in limited_docs:
                    doc.metadata["priority_search_type"] = "limited_vector_search"
                    doc.metadata["priority_limit"] = limit_per_doc
                    all_limited_documents.append(doc)
                total_chunks_retrieved += len(limited_docs)
            except Exception as e:
                logger.error(f"‚ùå Error performing limited vector search for Stable ID '{stable_id_clean}': {e}")
                continue
        logger.info(f"‚úÖ Retrieved {total_chunks_retrieved} limited chunks (max {limit_per_doc}/doc) for {len(stable_doc_ids)} Stable IDs from '{collection_name}'.")
        return all_limited_documents

    # -------------------- Retriever Creation --------------------
    def get_retriever(self, collection_name: str, top_k: int = INITIAL_TOP_K, final_k: int = FINAL_K_RERANKED, use_rerank: bool = True) -> Any:
        chroma_instance = self._load_chroma_instance(collection_name)
        if not chroma_instance:
            logger.warning(f"Retriever creation failed: Collection '{collection_name}' not loaded.")
            return None

        # 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á retriever ‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏° k ‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á
        def raw_retrieve(query: str, filter_dict: Optional[dict] = None, k: int = top_k) -> List[LcDocument]:
            try:
                original_query = query
                
                # ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å: PEA 2568 ingest ‡∏î‡πâ‡∏ß‡∏¢ paraphrase-multilingual-MiniLM-L12-v2 ‚Üí ‡πÑ‡∏°‡πà‡∏°‡∏µ prefix
                # ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏™‡πà query: ‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≤‡∏î!
                query = query.strip()
                logger.critical(f"[NO PREFIX FOR PEA] Using raw query: '{query[:100]}...'")

                search_kwargs = {"k": k}
                if filter_dict:
                    search_kwargs["filter"] = filter_dict
                
                docs = chroma_instance.similarity_search(
                    query=query,
                    k=k,
                    filter=filter_dict
                )
                logger.info(f"Raw retrieval: {len(docs)} docs (k={k}, filter={bool(filter_dict)})")
                return docs
            except Exception as e:
                logger.error(f"Raw retrieval failed: {e}", exc_info=True)
                return []

        # 2. Reranker wrapper (‡∏â‡∏ö‡∏±‡∏ö‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡πÅ‡∏ö‡∏ö)
        def retrieve_with_rerank(query: str, config: Optional[dict] = None):
            filter_dict = None
            if config and isinstance(config, dict):
                filter_dict = config.get("configurable", {}).get("search_kwargs", {}).get("filter")

            # ‡∏î‡∏∂‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏î‡πâ‡∏ß‡∏¢ k ‡πÄ‡∏ï‡πá‡∏°
            docs = raw_retrieve(query, filter_dict, k=top_k)

            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ reranker ‚Üí ‡∏Ñ‡∏∑‡∏ô‡∏ï‡∏≤‡∏° top_k
            reranker = get_global_reranker()
            if not (use_rerank and reranker and hasattr(reranker, "compress_documents")):
                return docs[:final_k]

            try:
                reranked = reranker.compress_documents(
                    documents=docs,
                    query=query,
                    top_n=final_k
                )
                # ‡∏î‡∏∂‡∏á score ‡∏à‡∏≤‡∏Å reranker (‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î)
                scores = getattr(reranker, "scores", None)
                if scores and len(scores) >= len(reranked):
                    for doc, score in zip(reranked, scores[:len(reranked)]):
                        score = float(score) if score is not None else 0.0
                        # ‡∏â‡∏µ‡∏î‡πÅ‡∏Ñ‡πà key ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô ‚Üí _rerank_score_force
                        doc.metadata["_rerank_score_force"] = score
                        # ‡πÅ‡∏•‡∏∞ source_filename (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö extraction)
                        orig = doc.metadata.get("source_filename", "UNKNOWN")
                        doc.metadata["source_filename"] = f"{orig}|SCORE:{score:.4f}"

                logger.info(f"Reranking success ‚Üí kept {len(reranked)} docs")
                return reranked

            except Exception as e:
                logger.warning(f"Rerank failed ({e}), fallback to raw")
                return docs[:final_k]

        # 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á LangChain-compatible Retriever
        class UltimateRetriever(BaseRetriever):
            def _get_relevant_documents(self, query: str, *, run_manager=None) -> List[LcDocument]:
                return retrieve_with_rerank(query, config=None)

            def invoke(self, query: str, config: Optional[dict] = None) -> List[LcDocument]:
                return retrieve_with_rerank(query, config=config)

        logger.info(f"Ultimate Retriever ready ‚Üí {collection_name} | top_k={top_k} ‚Üí final_k={final_k} | rerank={use_rerank}")
        return UltimateRetriever()

    def get_all_collection_names(self) -> List[str]:
        return list_vectorstore_folders(tenant=self.tenant, year=self.year, base_path=self._base_path)

    def get_chunks_from_doc_ids(self, stable_doc_ids: Union[str, List[str]], doc_type: str, enabler: Optional[str] = None) -> List[LcDocument]:
        if isinstance(stable_doc_ids, str):
            stable_doc_ids = [stable_doc_ids]
        stable_doc_ids = [uid for uid in stable_doc_ids if uid]
        if not stable_doc_ids:
            return []
        collection_name = _get_collection_name(doc_type, enabler)
        all_chunk_uuids = []
        skipped_docs = []
        found_stable_ids = []
        for stable_id in stable_doc_ids:
            stable_id_clean = stable_id.strip()
            if stable_id_clean in self._doc_id_mapping:
                doc_entry = self._doc_id_mapping[stable_id_clean]
                if isinstance(doc_entry, dict) and "chunk_uuids" in doc_entry and isinstance(doc_entry.get("chunk_uuids"), list):
                    chunk_uuids = doc_entry["chunk_uuids"]
                    if chunk_uuids:
                        all_chunk_uuids.extend(chunk_uuids)
                        found_stable_ids.append(stable_id_clean)
                    else:
                        logger.warning(f"Mapping found for Stable ID '{stable_id_clean}' but 'chunk_uuids' list is empty.")
                else:
                    logger.warning(f"Mapping entry for Stable ID '{stable_id_clean}' is malformed or missing 'chunk_uuids'.")
            else:
                skipped_docs.append(stable_id_clean)
        if skipped_docs:
            logger.warning(f"Skipping Stable IDs not found in mapping: {skipped_docs}")
        if not all_chunk_uuids:
            logger.warning(f"No valid chunk UUIDs found for provided Stable Document IDs: {skipped_docs}. Check doc_id_mapping.json.")
            return []
        chroma_instance = self._load_chroma_instance(collection_name)
        if not chroma_instance:
            logger.error(f"Collection '{collection_name}' is not loaded.")
            return []
        try:
            collection = chroma_instance._collection
            result = collection.get(ids=all_chunk_uuids, include=["documents", "metadatas", "ids"])
            documents: List[LcDocument] = []
            if not result.get("documents"):
                logger.warning(f"Chroma DB returned 0 documents for {len(all_chunk_uuids)} chunk UUIDs in collection '{collection_name}'.")
                return []
            for i, text in enumerate(result.get("documents", [])):
                if text:
                    metadata = result.get("metadatas", [{}])[i]
                    chunk_uuid_from_result = result.get("ids", [""])[i]
                    doc_id = self._uuid_to_doc_id.get(chunk_uuid_from_result, "UNKNOWN")
                    metadata["chunk_uuid"] = chunk_uuid_from_result
                    metadata["doc_id"] = doc_id
                    metadata["doc_type"] = doc_type
                    documents.append(LcDocument(page_content=text, metadata=metadata))
            logger.info(f"‚úÖ Retrieved {len(documents)} chunks for {len(found_stable_ids)} Stable IDs from '{collection_name}'.")
            return documents
        except Exception as e:
            logger.error(f"‚ùå Error retrieving documents by Chunk UUIDs from collection '{collection_name}': {e}")
            return []

# Helper function
def get_vectorstore_manager() -> VectorStoreManager:
    return VectorStoreManager()

def load_vectorstore(doc_type: str, enabler: Optional[str] = None) -> Optional[Chroma]:
    collection_name = _get_collection_name(doc_type, enabler)
    return get_vectorstore_manager()._load_chroma_instance(collection_name)

class VectorStoreExecutorSingleton:
    _instance = None
    _is_initialized = False

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            cls._instance = super(VectorStoreExecutorSingleton, cls).__new__(cls)
        return cls._instance

    def __init__(self):
        if not VectorStoreExecutorSingleton._is_initialized:
            self.max_workers = MAX_PARALLEL_WORKERS
            self._executor = ThreadPoolExecutor(max_workers=self.max_workers)
            logger.info(f"Initialized VectorStoreExecutorSingleton (ThreadPoolExecutor with {self.max_workers} workers) for batch tasks.")
            VectorStoreExecutorSingleton._is_initialized = True

    @property
    def executor(self) -> ThreadPoolExecutor:
        return self._executor

    def close(self):
        if self._is_initialized:
            logger.info("Shutting down VectorStoreExecutorSingleton ThreadPoolExecutor...")
            self._executor.shutdown(wait=True)
            VectorStoreExecutorSingleton._is_initialized = False

def get_vectorstore() -> VectorStoreExecutorSingleton:
    return VectorStoreExecutorSingleton()

# -------------------- Custom Retriever for Chroma --------------------
class ChromaRetriever(BaseRetriever):
    vectorstore: Any
    k: int
    filter: Optional[Dict] = None
    model_config = ConfigDict(arbitrary_types_allowed=True)

    def _get_relevant_documents(self, query: str, *, run_manager=None) -> List[LcDocument]:
        
        # üéØ DEBUG: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Collection Count ‡πÅ‡∏•‡∏∞ Embedding (‡πÇ‡∏Ñ‡πâ‡∏î‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏°)
        try:
            raw_collection = self.vectorstore._collection
            count = raw_collection.count() 
            logger.critical(f"üéØ [DEBUG CHROMA] Collection Count: {count}")
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Query Embedding
            if hasattr(self.vectorstore, '_embedding_function') and self.vectorstore._embedding_function:
                embedding_function = self.vectorstore._embedding_function
                # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö embed query
                query_embedding = embedding_function.embed_query(query)
                logger.critical(f"üéØ [DEBUG CHROMA] Query Embedding Success. Vector size: {len(query_embedding)}")
            else:
                logger.critical("üéØ [DEBUG CHROMA] Cannot access _embedding_function.")

        except Exception as debug_e:
            logger.critical(f"‚ùå [DEBUG CHROMA] Debug check failed (Skip search): {debug_e}")
            return []
        # END DEBUG

        try:
            # ‡∏£‡∏±‡∏ô similarity search
            return self.vectorstore.similarity_search(query=query, k=self.k, filter=self.filter)
        except Exception as e:
            logger.error(f"‚ùå Chroma similarity_search failed in custom retriever: {e}")
            return []

    def get_relevant_documents(self, query: str, **kwargs) -> List[LcDocument]:
        return self._get_relevant_documents(query, **kwargs)

    async def _aget_relevant_documents(self, query: str, *, run_manager=None) -> List[LcDocument]:
        return self._get_relevant_documents(query, run_manager=run_manager)

# -------------------- MultiDoc / Parallel Retriever --------------------
class NamedRetriever(BaseModel):
    doc_id: str
    doc_type: str
    top_k: int = INITIAL_TOP_K
    final_k: int = FINAL_K_RERANKED
    base_path: str = VECTORSTORE_DIR
    enabler: Optional[str] = None
    tenant: str = DEFAULT_TENANT
    year: Optional[int] = DEFAULT_YEAR # <--- üéØ FIX: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô Optional[int] ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö None
    
    # load_instance ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏Å VSM ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ self.tenant ‡πÅ‡∏•‡∏∞ self.year
    def load_instance(self) -> Any:
        # VSM ‡∏ñ‡∏π‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏î‡πâ‡∏ß‡∏¢ context ‡∏Ç‡∏≠‡∏á collection ‡∏ô‡∏µ‡πâ (year ‡∏≠‡∏≤‡∏à‡πÄ‡∏õ‡πá‡∏ô None)
        # ‚ö†Ô∏è VSM Singleton ‡∏ï‡πâ‡∏≠‡∏á‡∏ñ‡∏π‡∏Å init ‡∏î‡πâ‡∏ß‡∏¢‡∏õ‡∏µ‡πÄ‡∏™‡∏°‡∏≠ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÇ‡∏´‡∏•‡∏î Doc ID Mapping
        manager = VectorStoreManager(base_path=self.base_path, tenant=self.tenant, year=self.year if self.year is not None else DEFAULT_YEAR) 
        collection_name = _get_collection_name(self.doc_type, self.enabler)
        retriever = manager.get_retriever(collection_name=collection_name, top_k=self.top_k, final_k=self.final_k) 
        if not retriever:
            raise ValueError(f"Retriever not found for collection '{collection_name}' at path based on tenant={self.tenant}, year={self.year}")
        return retriever

class MultiDocRetriever(BaseRetriever):
    model_config = ConfigDict(arbitrary_types_allowed=True)
    _all_retrievers: Dict[str, Any] = PrivateAttr(default_factory=dict)
    _retrievers_list: list[NamedRetriever] = PrivateAttr()
    _k_per_doc: int = PrivateAttr()
    _manager: VectorStoreManager = PrivateAttr()
    _doc_ids_filter: Optional[List[str]] = PrivateAttr()
    _chroma_filter: Optional[Dict[str, Any]] = PrivateAttr()
    _executor_type: str = PrivateAttr()
    _executor: Union[ThreadPoolExecutor, ProcessPoolExecutor, None] = PrivateAttr(None)

    def __init__(self, retrievers_list: list[NamedRetriever], k_per_doc: int = INITIAL_TOP_K, doc_ids_filter: Optional[List[str]] = None):
        super().__init__()
        self._retrievers_list = retrievers_list
        self._k_per_doc = k_per_doc

        # üéØ FIX 2: ‡∏î‡∏∂‡∏á Context ‡∏à‡∏≤‡∏Å NamedRetriever ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å
        tenant_context = retrievers_list[0].tenant if retrievers_list else DEFAULT_TENANT
        year_context = retrievers_list[0].year if retrievers_list else DEFAULT_YEAR

        # üéØ FIX 2A: ‡∏™‡∏£‡πâ‡∏≤‡∏á VSM ‡∏î‡πâ‡∏ß‡∏¢ Context ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Doc ID Mapping)
        # ‡∏ñ‡πâ‡∏≤ year_context ‡πÄ‡∏õ‡πá‡∏ô None (‡∏à‡∏≤‡∏Å NamedRetriever ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô General doc) ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ DEFAULT_YEAR
        if year_context is None:
             year_context = DEFAULT_YEAR 
             
        self._manager = VectorStoreManager(tenant=tenant_context, year=year_context)
        
        self._all_retrievers = {}
        for named_r in retrievers_list:
            collection_name = _get_collection_name(named_r.doc_type, named_r.enabler)
            try:
                retriever_instance = named_r.load_instance()
                if retriever_instance:
                    self._all_retrievers[collection_name] = retriever_instance
                    logger.info(f"‚úÖ MultiDocRetriever cached collection: {collection_name}")
                else:
                    logger.warning(f"‚ö†Ô∏è Failed to load instance for {collection_name} during MDR init.")
            except Exception as e:
                logger.error(f"‚ùå Error loading instance {collection_name} into MDR cache: {e}")

        self._doc_ids_filter = doc_ids_filter
        self._chroma_filter = None
        if doc_ids_filter:
            self._chroma_filter = {"doc_id": {"$in": doc_ids_filter}}
            logger.info(f"‚úÖ MultiDocRetriever initialized with doc_ids filter for {len(doc_ids_filter)} Stable IDs.")

        self._executor_type = self._choose_executor()
        logger.info(f"MultiDocRetriever will use executor type: {self._executor_type} (workers={MAX_PARALLEL_WORKERS})")
    
    def _choose_executor(self) -> str:
        sys_info = _detect_system()
        device = _detect_torch_device()
        force = ENV_FORCE_MODE
        if force in ("thread", "process"):
            logger.info(f"VECTOR_MODE override: forcing '{force}' executor")
            return force
        if sys_info["platform"] == "darwin" and device == "mps":
            logger.warning("‚ö†Ô∏è Detected MPS on macOS: forcing executor -> thread to avoid multi-process failures.")
            return "thread"
        if sys_info["total_ram_gb"] and sys_info["total_ram_gb"] < 12:
            logger.warning(f"‚ö†Ô∏è Detected low RAM ({sys_info['total_ram_gb']:.1f}GB): forcing executor -> thread.")
            return "thread"
        if sys_info["cpu_count"] >= 8 and (sys_info["total_ram_gb"] or 0) >= 16:
            logger.info("High-resources machine detected -> choosing 'process' executor")
            return "process"
        logger.info("Defaulting to 'thread' executor")
        return "thread"

    def shutdown(self):
        if self._executor:
            executor_type_name = "ProcessPoolExecutor" if self._executor_type == "process" else "ThreadPoolExecutor"
            workers = self._executor._max_workers if hasattr(self._executor, "_max_workers") else "N/A"
            logger.info(f"Shutting down MultiDocRetriever's {executor_type_name} executor ({workers} workers).")
            self._executor.shutdown(wait=True)
            self._executor = None

    def __del__(self):
        try:
            self.shutdown()
        except Exception:
            pass

    def _get_executor(self) -> Union[ThreadPoolExecutor, ProcessPoolExecutor]:
        if self._executor is None:
            workers = MAX_PARALLEL_WORKERS
            if self._executor_type == "process":
                self._executor = ProcessPoolExecutor(max_workers=workers)
                logger.info(f"üõ†Ô∏è Using ProcessPoolExecutor with {workers} workers.")
            else:
                self._executor = ThreadPoolExecutor(max_workers=workers)
                logger.info(f"üõ†Ô∏è Using ThreadPoolExecutor with {workers} workers.")
        return self._executor

    @staticmethod
    def _static_retrieve_task(named_r: NamedRetriever, query: str, chroma_filter: Optional[Dict]):
        try:
            retriever_instance = named_r.load_instance()
            search_kwargs = {"k": named_r.top_k, "filter": chroma_filter} if chroma_filter else {"k": named_r.top_k}
            config = {"configurable": {"search_kwargs": search_kwargs}}
            docs = retriever_instance.invoke(query, config=config)
            for doc in docs:
                doc.metadata["retrieval_source"] = named_r.doc_type
                doc.metadata["collection_name"] = _get_collection_name(named_r.doc_type, named_r.enabler)
            return docs
        except Exception as e:
            print(f"‚ùå Child retrieval error for {named_r.doc_id} ({named_r.doc_type}): {e}")
            return []

    def _thread_retrieve_task(self, named_r: NamedRetriever, query: str, chroma_filter: Optional[Dict]):
        try:
            retriever_instance = named_r.load_instance()
            search_kwargs = {"k": named_r.top_k, "filter": chroma_filter} if chroma_filter else {"k": named_r.top_k}
            config = {"configurable": {"search_kwargs": search_kwargs}}
            docs = retriever_instance.invoke(query, config=config)
            for doc in docs:
                doc.metadata["retrieval_source"] = named_r.doc_type
                doc.metadata["collection_name"] = _get_collection_name(named_r.doc_type, named_r.enabler)
            return docs
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Thread retrieval error for {named_r.doc_id}: {e}")
            return []

    def _get_relevant_documents(self, query: str, *, run_manager: Any = None) -> List[LcDocument]:
        max_workers = min(len(self._retrievers_list), MAX_PARALLEL_WORKERS)
        if max_workers <= 0:
            max_workers = 1
        chosen = self._executor_type
        logger.info(f"‚öôÔ∏è Running MultiDocRetriever with {chosen} executor ({max_workers} workers) [Filter: {bool(self._chroma_filter)}]")
        all_docs: List[LcDocument] = []
        executor = self._get_executor()
        futures = []
        for named_r in self._retrievers_list:
            if chosen == "process":
                future = executor.submit(MultiDocRetriever._static_retrieve_task, named_r, query, self._chroma_filter)
            else:
                future = executor.submit(self._thread_retrieve_task, named_r, query, self._chroma_filter)
            futures.append(future)
        for f in futures:
            try:
                docs = f.result()
                if docs:
                    all_docs.extend(docs)
            except Exception as e:
                logger.warning(f"Future failed: {e}")
        seen = set()
        unique_docs = []
        for d in all_docs:
            src = d.metadata.get("retrieval_source") or ""
            chunk_uuid = d.metadata.get("chunk_uuid") or d.metadata.get("ids") or ""
            key = f"{src}_{chunk_uuid}_{d.page_content[:120]}"
            if key not in seen:
                seen.add(key)
                unique_docs.append(d)
        logger.info(f"üìù Query='{query[:80]}...' found {len(unique_docs)} unique docs across sources (Executor={chosen})")
        for d in unique_docs:
            score = d.metadata.get("relevance_score")
            if score is not None:
                logger.debug(f" - [Reranked] Source={d.metadata.get('doc_type')}, Score={score:.4f}, Content='{d.page_content[:80]}...'")
        return unique_docs

    def get_relevant_documents(self, query: str, **kwargs) -> List[LcDocument]:
        return self._get_relevant_documents(query, **kwargs)

# -------------------- load_all_vectorstores --------------------
def load_vectorstore_retriever(
    doc_id: str, 
    top_k: int = INITIAL_TOP_K, 
    final_k: int = FINAL_K_RERANKED, 
    doc_types: Union[list, str] = "default_collection", 
    base_path: str = VECTORSTORE_DIR, 
    enabler: Optional[str] = None, 
    tenant: str = DEFAULT_TENANT,
    year: int = DEFAULT_YEAR      
):
    if isinstance(doc_types, str):
        target_doc_type = doc_types
    elif isinstance(doc_types, list) and doc_types:
        target_doc_type = doc_types[0]
    else:
        raise ValueError("doc_types must be a single string or a non-empty list containing the target doc_type.")
        
    collection_name = _get_collection_name(target_doc_type, enabler)
    
    # üéØ FIX 3: ‡∏™‡πà‡∏á tenant ‡πÅ‡∏•‡∏∞ year ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô VectorStoreManager
    manager = VectorStoreManager(base_path=base_path, tenant=tenant, year=year) 
    retriever = None
    
    # NOTE: load_vectorstore_retriever ‡πÉ‡∏ä‡πâ‡∏õ‡∏µ‡πÄ‡∏™‡∏°‡∏≠ ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏õ‡πá‡∏ô context ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏°‡∏≤
    if vectorstore_exists(doc_id="N/A", base_path=base_path, doc_type=target_doc_type, enabler=enabler, tenant=tenant, year=year):
        retriever = manager.get_retriever(collection_name, top_k, final_k)
        
    if retriever is None:
        raise ValueError(f"‚ùå Vectorstore for collection '{collection_name}' not found.")
    return retriever

def load_all_vectorstores(
    tenant: str, 
    year: int,    
    doc_types: Optional[Union[str, List[str]]] = None, 
    top_k: int = INITIAL_TOP_K, 
    final_k: int = FINAL_K_RERANKED, 
    base_path: str = VECTORSTORE_DIR, 
    evidence_enabler: Optional[str] = None, 
    doc_ids: Optional[List[str]] = None
) -> VectorStoreManager:
    
    doc_types = [doc_types] if isinstance(doc_types, str) else doc_types or []
    doc_type_filter = {dt.strip().lower() for dt in doc_types}
    
    # üéØ FIX 1: VSM ‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏î‡πâ‡∏ß‡∏¢ tenant/year ‡∏Ç‡∏≠‡∏á RUN (‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Doc ID Mapping)
    manager = VectorStoreManager(base_path=base_path, tenant=tenant, year=year) 
    
    all_retrievers: List[NamedRetriever] = []
    target_collection_names: Set[str] = set()

    # --- 1. Collection Discovery ---
    if not doc_type_filter:
        logger.error("Must specify doc_types for multi-year compatibility.")
        raise ValueError("Must specify doc_types when using multi-tenant setup.")
    
    # üéØ NEW: ‡πÄ‡∏£‡∏≤‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ list collections ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÑ‡∏´‡∏ô‡πÉ‡∏ä‡πâ‡∏õ‡∏µ/‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ‡∏õ‡∏µ 
    # ‡πÄ‡∏£‡∏≤‡∏à‡∏∂‡∏á‡πÉ‡∏ä‡πâ doc_type_filter ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á collection_name ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£ list ‡∏à‡∏≤‡∏Å folder
    for dt_norm in doc_type_filter:
        if dt_norm == EVIDENCE_DOC_TYPES.lower(): 
            if evidence_enabler:
                # Specific evidence collection: ‡πÉ‡∏ä‡πâ year
                collection_name = _get_collection_name(EVIDENCE_DOC_TYPES, evidence_enabler)
                target_collection_names.add(collection_name)
                logger.info(f"üîç Added specific evidence collection: {collection_name} (Year-Specific)")
            else:
                # All evidence collections: ‡∏ï‡πâ‡∏≠‡∏á list ‡∏à‡∏≤‡∏Å folder ‡∏†‡∏≤‡∏¢‡πÉ‡∏ï‡πâ tenant/year
                evidence_collections = list_vectorstore_folders(tenant=tenant, year=year, doc_type=EVIDENCE_DOC_TYPES, base_path=base_path)
                target_collection_names.update(evidence_collections)
                logger.info(f"üîç Added all evidence collections found: {evidence_collections} (Year-Specific)")
        else:
            # Standard Collections: ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ year
            collection_name = _get_collection_name(dt_norm, None)
            target_collection_names.add(collection_name)
            logger.info(f"üîç Added standard collection: {collection_name} (Shared/General)")
    
    logger.info(f"üîç DEBUG: Attempting to load {len(target_collection_names)} total target collections: {target_collection_names}")
    
    # --- 2. Retriever List Creation & Existence Check ---
    for collection_name in target_collection_names:
        doc_type_for_check, enabler_for_check = manager._re_parse_collection_name(collection_name)
        
        # üéØ FIX 2A: Logic ‡∏Å‡∏≥‡∏´‡∏ô‡∏î target_year (None ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö General Docs)
        target_year: Optional[int] = year
        if doc_type_for_check.lower() != EVIDENCE_DOC_TYPES.lower() and enabler_for_check is None:
            # ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£‡∏ñ‡∏π‡∏Å‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏°‡∏õ‡∏µ
            target_year = None # <--- ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏õ‡πá‡∏ô None
            
        # üéØ FIX 2B: ‡∏™‡πà‡∏á target_year ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏Ç‡∏≠‡∏á Vectorstore
        if not vectorstore_exists(doc_id="N/A", tenant=tenant, year=target_year, doc_type=doc_type_for_check, enabler=enabler_for_check):
            logger.warning(f"üîç DEBUG: Skipping collection '{collection_name}' (vectorstore_exists failed at tenant={tenant}, year={target_year}).")
            continue
            
        # üéØ FIX 2C: ‡∏™‡πà‡∏á target_year ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô NamedRetriever
        nr = NamedRetriever(
            doc_id=collection_name, 
            doc_type=doc_type_for_check, 
            enabler=enabler_for_check, 
            top_k=top_k, 
            final_k=final_k, 
            base_path=base_path,
            tenant=tenant, 
            year=target_year # <--- ‡∏™‡πà‡∏á None ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö General Docs
        )
        all_retrievers.append(nr)
        logger.info(f"üîç DEBUG: Successfully added retriever for collection '{collection_name}' (Year={target_year}).")

    final_filter_ids = doc_ids
    if doc_ids:
        logger.info(f"‚úÖ Hard Filter Enabled: Using {len(doc_ids)} original 64-char UUIDs for filtering.")
    logger.info(f"üîç DEBUG: Final count of all_retrievers = {len(all_retrievers)}")

    if not all_retrievers:
        raise ValueError(f"No vectorstore collections found matching tenant={tenant}, year={year}, doc_types={doc_types} and evidence_enabler={evidence_enabler}")
        
    mdr = MultiDocRetriever(retrievers_list=all_retrievers, k_per_doc=top_k, doc_ids_filter=final_filter_ids)
    manager._multi_doc_retriever = mdr
    logger.info(f"‚úÖ MultiDocRetriever loaded with {len(mdr._all_retrievers)} collections and cached in VSM.")
    return manager