# core/vectorstore.py
import os
import platform
import logging
import threading
import multiprocessing
import json
import shutil
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from typing import List, Optional, Union, Sequence, Any, Dict, Set, Tuple
from pathlib import Path
import hashlib

# system utils
try:
    import psutil
except ImportError:
    psutil = None

# LangChain-ish imports (adjust to your project's versions)
from langchain_core.documents import Document as LcDocument
from langchain_core.retrievers import BaseRetriever
from langchain_core.documents import BaseDocumentCompressor
from langchain_core.callbacks.manager import CallbackManagerForRetrieverRun
from langchain_core.runnables import Runnable 

# Pydantic helpers
from pydantic import PrivateAttr, ConfigDict, BaseModel

# Chroma / HF embeddings
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
import chromadb
from chromadb.config import Settings

# Logging
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)


# Try import CrossEncoder (sentence-transformers)
try:
    from sentence_transformers import CrossEncoder
    _HAS_SENT_TRANS = True
except Exception:
    CrossEncoder = None
    _HAS_SENT_TRANS = False
    logger.warning("‚ö†Ô∏è sentence-transformers CrossEncoder not available. Reranker will be disabled.")

# Configure chromadb telemetry if available
try:
    chromadb.configure(anonymized_telemetry=False)
except Exception:
    try:
        chromadb.settings = Settings(anonymized_telemetry=False)
    except Exception:
        pass

# -------------------- Global Config --------------------
from config.global_vars import (
    VECTORSTORE_DIR,
    MAPPING_BASE_DIR,
    FINAL_K_RERANKED,
    INITIAL_TOP_K,
    EVIDENCE_DOC_TYPES,
    MAX_PARALLEL_WORKERS,
    DEFAULT_TENANT,
    DEFAULT_YEAR,
    DEFAULT_ENABLER
)

# -------------------- Vectorstore Constants --------------------
ENV_FORCE_MODE = os.getenv("VECTOR_MODE", "").lower()  # "thread", "process", or ""
ENV_DISABLE_ACCEL = os.getenv("VECTOR_DISABLE_ACCEL", "").lower() in ("1", "true", "yes")

# Global caches (per process)
_CACHED_EMBEDDINGS = None
_EMBED_LOCK = threading.Lock()
_MPS_WARNING_SHOWN = False

# -------------------- Helper: detect environment & device --------------------
@staticmethod
def _detect_system():
    cpu_count = os.cpu_count() or 4
    total_ram_gb = None
    if psutil:
        try:
            total_ram_gb = psutil.virtual_memory().total / (1024 ** 3)
        except Exception:
            total_ram_gb = None
    return {"cpu_count": cpu_count, "total_ram_gb": total_ram_gb, "platform": platform.system().lower()}

@staticmethod
def _detect_torch_device():
    try:
        import torch
        if ENV_DISABLE_ACCEL:
            return "cpu"
        if torch.cuda.is_available():
            return "cuda"
        if platform.system().lower() == "darwin" and hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
            # MPS can be fragile for multiprocessing; we'll warn and possibly force cpu later
            return "mps"
    except Exception:
        pass
    return "cpu"

def get_hf_embeddings(device_hint: Optional[str] = None):
    global _CACHED_EMBEDDINGS, _MPS_WARNING_SHOWN
    device = device_hint or _detect_torch_device()

    # ... (‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏à‡∏ô‡∏ñ‡∏∂‡∏á‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ)

    if _CACHED_EMBEDDINGS is None:
        with _EMBED_LOCK:
            if _CACHED_EMBEDDINGS is None:
                # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≤‡∏î!!!
                model_name = "intfloat/multilingual-e5-base"  # ‡∏´‡∏£‡∏∑‡∏≠ large ‡∏ñ‡πâ‡∏≤‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÅ‡∏£‡∏á
                
                logger.info(f"Loading BEST Thai RAG embedding 2025: {model_name} on {device}")
                logger.info("This model was used to build ALL PEA 2568 vectorstores (evidence_km, document, etc.)")
                
                try:
                    _CACHED_EMBEDDINGS = HuggingFaceEmbeddings(
                        model_name=model_name,
                        model_kwargs={
                            "device": device,
                            # ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å: e5 ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ prefix!
                            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏™‡πà ‚Üí ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ï‡∏Å‡∏Æ‡∏ß‡∏ö!
                        },
                        encode_kwargs={
                            "normalize_embeddings": True,
                            # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö e5 series ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏™‡πà prefix ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô!!!
                            "prompt": "query: "  # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö query
                            # ‡∏´‡∏£‡∏∑‡∏≠‡∏ñ‡πâ‡∏≤‡∏à‡∏∞ embed ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ ‚Üí ‡πÉ‡∏ä‡πâ "passage: "
                        }
                    )
                except Exception as e:
                    logger.error(f"Failed to load {model_name}: {e}")
                    logger.warning("Falling back to paraphrase-multilingual-MiniLM-L12-v2")
                    _CACHED_EMBEDDINGS = HuggingFaceEmbeddings(
                        model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
                        model_kwargs={"device": "cpu"}
                    )
    return _CACHED_EMBEDDINGS

# =================================================================
# HuggingFace Cross-Encoder Reranker wrapper (singleton)
# =================================================================
class HuggingFaceCrossEncoderCompressor(BaseDocumentCompressor, BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)
    # default cross-encoder recommended model
    rerank_model: str = "cross-encoder/ms-marco-MiniLM-L-6-v2"
    rerank_device: str = "cpu"
    rerank_max_length: int = 512
    _cross_encoder: Any = PrivateAttr(None)

    def __init__(self, **data):
        super().__init__(**data)
        # determine device safely (force cpu for CrossEncoder on mac mps)
        try:
            device = _detect_torch_device()
            if device == "mps":
                # CrossEncoder on MPS can be unstable; force CPU
                device = "cpu"
            self.rerank_device = device
        except Exception:
            self.rerank_device = "cpu"

    def set_encoder_instance(self, encoder: Any):
        self._cross_encoder = encoder

    def compress_documents(self, documents: Sequence[LcDocument], query: str, top_n: int, callbacks: Optional[Any] = None) -> List[LcDocument]:
        if not documents:
            return []
        if self._cross_encoder is None or not hasattr(self._cross_encoder, "predict"):
            logger.error("HuggingFace Cross-Encoder is not initialized. Returning truncated documents.")
            return list(documents)[:top_n]

        # Prepare input pairs
        sentence_pairs = [[query, doc.page_content] for doc in documents]

        try:
            scores = self._cross_encoder.predict(sentence_pairs, show_progress_bar=False)
        except TypeError:
            # Some CrossEncoder versions accept different args
            try:
                scores = self._cross_encoder.predict(sentence_pairs)
            except Exception as e:
                logger.error(f"‚ùå Cross-Encoder prediction failed: {e}. Returning truncated documents.")
                return list(documents)[:top_n]
        except Exception as e:
            logger.error(f"‚ùå Cross-Encoder prediction failed: {e}. Returning truncated documents.")
            return list(documents)[:top_n]

        # sort and return top_n
        doc_scores = sorted(zip(documents, scores), key=lambda x: x[1], reverse=True)
        final_docs = []
        for doc, score in doc_scores[:top_n]:
            # üü¢ FIX: Check and initialize metadata if it is None (prevents TypeError)
            if doc.metadata is None:
                doc.metadata = {}
                logger.warning(f"‚ö†Ô∏è Reranker: Found Document with None metadata. Initializing metadata to {{}}.")
                
            doc.metadata["relevance_score"] = float(score)
            final_docs.append(doc)
        return final_docs

_CACHED_RERANKER_INSTANCE: Optional[HuggingFaceCrossEncoderCompressor] = None
_CACHED_CROSS_ENCODER: Any = None

def get_global_reranker() -> Optional[HuggingFaceCrossEncoderCompressor]:
    """
    Returns a cached HuggingFaceCrossEncoderCompressor instance (singleton).
    """
    global _CACHED_RERANKER_INSTANCE, _CACHED_CROSS_ENCODER

    if _CACHED_RERANKER_INSTANCE is None:
        try:
            if not _HAS_SENT_TRANS:
                logging.warning("sentence-transformers not installed. Cross-Encoder reranker disabled.")
                return None

            instance = HuggingFaceCrossEncoderCompressor(
                rerank_model="mixedbread-ai/mxbai-rerank-xsmall-v1"
            )

            from sentence_transformers import CrossEncoder
            cross_encoder_model = CrossEncoder(
                instance.rerank_model,
                device=instance.rerank_device
            )

            instance.set_encoder_instance(cross_encoder_model)

            _CACHED_RERANKER_INSTANCE = instance
            _CACHED_CROSS_ENCODER = cross_encoder_model

            logging.info(f"Initialized global Cross-Encoder reranker: {instance.rerank_model} on {instance.rerank_device}")

        except Exception as e:
            logging.warning(f"Failed to initialize global reranker: {e}")
            return None

    return _CACHED_RERANKER_INSTANCE



# -------------------- Path Helper Function (REVISED for Lowercase Path Suffix and Optional Year) --------------------
def _build_vectorstore_path_by_doc_type(tenant: str, year: Optional[int], doc_type: str, enabler: Optional[str] = None) -> str:
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á Full Path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Collection ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏ï‡∏£‡∏£‡∏Å‡∏∞ EVIDENCE_DOC_TYPES
    - Evidence (‡∏°‡∏µ year): VECTORSTORE_DIR / tenant / year / collection_name
    - Docs/FAQ (‡πÑ‡∏°‡πà‡∏°‡∏µ year): VECTORSTORE_DIR / tenant / collection_name
    """
    doc_type_lower = doc_type.lower()
    collection_name = _get_collection_name(doc_type, enabler)
    
    path_segments = [VECTORSTORE_DIR, tenant.lower()]
    
    # üéØ FIX: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏õ‡∏µ (year is not None) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Evidence ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
    if doc_type_lower == EVIDENCE_DOC_TYPES.lower() and year is not None:
        # Path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö evidence ‡∏Ñ‡∏∑‡∏≠ /tenant/year/collection_name
        path_segments.append(str(year))
        
    path_segments.append(collection_name)
    
    return os.path.join(*path_segments)


def _get_collection_name(doc_type: str, enabler: Optional[str] = None) -> str:
    """
    Calculates the Chroma collection name (Internal identifier).
    """
    doc_type_norm = doc_type.strip().lower()
    
    if doc_type_norm == EVIDENCE_DOC_TYPES.lower():
        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Evidence: ‡∏ä‡∏∑‡πà‡∏≠ Collection ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏ß‡∏° enabler ‡πÄ‡∏™‡∏°‡∏≠
        enabler_norm = (enabler or "km").strip().lower()
        collection_name = f"{doc_type_norm}_{enabler_norm}"
        
    else:
        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ: ‡∏ä‡∏∑‡πà‡∏≠ Collection ‡∏Ñ‡∏∑‡∏≠ doc_type ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
        collection_name = doc_type_norm
        
    logger.debug(f"üß≠ DEBUG: _get_collection_name(doc_type={doc_type}, enabler={enabler}) => {collection_name}")
    return collection_name

def _build_vectorstore_path_by_doc_type(tenant: str, year: Optional[int], doc_type: str, enabler: Optional[str] = None) -> str:
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á Full Path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Collection ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏ï‡∏£‡∏£‡∏Å‡∏∞ EVIDENCE_DOC_TYPES
    - Evidence (‡∏°‡∏µ year): VECTORSTORE_DIR / tenant / year / collection_name
    - Docs/FAQ (‡πÑ‡∏°‡πà‡∏°‡∏µ year): VECTORSTORE_DIR / tenant / collection_name
    """
    doc_type_lower = doc_type.lower()
    collection_name = _get_collection_name(doc_type, enabler)
    
    path_segments = [VECTORSTORE_DIR, tenant.lower()]
    
    # üéØ FIX: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏õ‡∏µ (year is not None) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Evidence ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
    if doc_type_lower == EVIDENCE_DOC_TYPES.lower() and year is not None:
        # Path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö evidence ‡∏Ñ‡∏∑‡∏≠ /tenant/year/collection_name
        path_segments.append(str(year))
        
    path_segments.append(collection_name)
    
    return os.path.join(*path_segments)

def get_vectorstore_path(
    tenant: str, 
    year: Optional[int], 
    doc_type: Optional[str] = None, 
    enabler: Optional[str] = None
) -> str:
    """
    Calculates the full persist directory path for the vector store instance
    based on the Centralized KM Logic.
    """
    if not doc_type:
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ doc_type ‡πÉ‡∏´‡πâ return root ‡∏Ç‡∏≠‡∏á tenant
        return os.path.join(VECTORSTORE_DIR, tenant.lower()) 
        
    return _build_vectorstore_path_by_doc_type(tenant, year, doc_type, enabler)

def vectorstore_exists(
    doc_id: str = "N/A", # ‡∏£‡∏±‡∏Å‡∏©‡∏≤‡πÑ‡∏ß‡πâ‡∏ï‡∏≤‡∏° Signature ‡πÄ‡∏î‡∏¥‡∏°
    tenant: str = DEFAULT_TENANT,
    year: Optional[int] = DEFAULT_YEAR, # <--- ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö None ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö General Docs
    doc_type: Optional[str] = None, 
    enabler: Optional[str] = None, 
    base_path: str = VECTORSTORE_DIR # base_path ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏•‡∏∞‡πÄ‡∏•‡∏¢‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÉ‡∏ä‡πâ global VECTORSTORE_DIR
) -> bool:
    """
    Checks if the Vector Store directory exists for the given context.
    """
    if not doc_type:
        return False
        
    # 1. Get the full path using the updated logic
    # path ‡∏à‡∏∞‡∏°‡∏µ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏µ‡∏Å‡πá‡πÑ‡∏î‡πâ ‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö doc_type ‡πÅ‡∏•‡∏∞ year
    path = get_vectorstore_path(tenant, year, doc_type, enabler) 
    
    # 2. Check for the actual data file created by Chroma
    file_path = os.path.join(path, "chroma.sqlite3")
    
    if not os.path.isdir(path):
        logger.warning(f"‚ùå V-Exists Check: Directory not found for doc_type '{doc_type}' at {path}")
        return False
    if os.path.isfile(file_path):
        return True
    logger.error(f"‚ùå V-Exists Check: FAILED to find file chroma.sqlite3 at {file_path} in {path}")
    return False

def _get_collection_parent_dir(tenant: str, year: Optional[int], doc_type: str) -> str: 
    """
    Calculates the parent directory where the collection folder resides.
    - evidence: VECTORSTORE_DIR / tenant / year 
    - others:   VECTORSTORE_DIR / tenant 
    """
    doc_type_lower = doc_type.lower()
    path_segments = [VECTORSTORE_DIR, tenant.lower()]
    
    # üéØ FIX: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏õ‡∏µ (year is not None) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Evidence ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
    if doc_type_lower == EVIDENCE_DOC_TYPES.lower() and year is not None:
        # Parent ‡∏Ç‡∏≠‡∏á evidence ‡∏Ñ‡∏∑‡∏≠‡∏°‡∏µ‡∏õ‡∏µ
        path_segments.append(str(year))
        
    return os.path.join(*path_segments)

def list_vectorstore_folders(
    tenant: str, 
    year: int, # NOTE: ‡πÉ‡∏ä‡πâ‡∏õ‡∏µ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ô (int)
    doc_type: Optional[str] = None, 
    enabler: Optional[str] = None, 
    base_path: str = VECTORSTORE_DIR
) -> List[str]:
    """
    Lists the actual collection folder names (e.g., 'evidence_km', 'document')
    that exist under the specified tenant and year context.
    """
    
    # Scenario 1: Specific doc_type/enabler is requested
    if doc_type:
        doc_type_norm = doc_type.lower().strip()
        collection_name = _get_collection_name(doc_type_norm, enabler)
        
        # üéØ FIX: ‡πÉ‡∏ä‡πâ get_vectorstore_path ‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏•‡πâ‡∏ß (‡∏ï‡πâ‡∏≠‡∏á‡∏™‡πà‡∏á‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á)
        # year ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÉ‡∏ä‡πâ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö doc_type ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô get_vectorstore_path
        full_collection_path = get_vectorstore_path(tenant, year, doc_type_norm, enabler)
        
        if os.path.isdir(full_collection_path) and os.path.isfile(os.path.join(full_collection_path, "chroma.sqlite3")):
            return [collection_name] 
        return []

    # Scenario 2: List ALL collections for the given tenant/year context (List All)
    
    collections: Set[str] = set()
    
    # 1. Scan the Year Root (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Doc Type: evidence)
    root_year = _get_collection_parent_dir(tenant, year, EVIDENCE_DOC_TYPES) 
    if os.path.isdir(root_year):
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ evidence_... collections ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏õ‡∏µ
        for sub_dir in os.listdir(root_year):
             sub_dir_lower = sub_dir.lower()
             if sub_dir_lower.startswith(f"{EVIDENCE_DOC_TYPES.lower()}_"): 
                 full_collection_path = os.path.join(root_year, sub_dir)
                 if os.path.isfile(os.path.join(full_collection_path, "chroma.sqlite3")):
                    collections.add(sub_dir_lower) 

    # 2. Scan the Common Root (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Doc Type: document, faq, ‡∏Ø‡∏•‡∏Ø)
    # Common Root: VECTORSTORE_DIR / tenant
    root_common = _get_collection_parent_dir(tenant, year=None, doc_type="document") 
    if os.path.isdir(root_common):
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå Doc Type
        for sub_dir in os.listdir(root_common):
            sub_dir_lower = sub_dir.lower()
            # ‡∏Ç‡πâ‡∏≤‡∏°‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå evidence ‡πÅ‡∏•‡∏∞‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç (‡∏õ‡∏µ)
            if sub_dir_lower == EVIDENCE_DOC_TYPES.lower() or sub_dir.isdigit():
                 continue 
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏õ‡πá‡∏ô Collection ‡∏à‡∏£‡∏¥‡∏á (‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå Chroma)
            full_collection_path = os.path.join(root_common, sub_dir)
            
            if os.path.isfile(os.path.join(full_collection_path, "chroma.sqlite3")):
                 collections.add(sub_dir_lower) 
    
    return sorted(list(collections))


# -------------------- VECTORSTORE MANAGER (SINGLETON) --------------------
class VectorStoreManager:
    _instance = None
    _is_initialized = False
    _lock = threading.Lock()

    # ‡πÉ‡∏ä‡πâ default_factory ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏™‡πà {} ‡∏´‡∏£‡∏∑‡∏≠ None ‡∏ï‡∏£‡∏á‡πÜ
    _chroma_cache: Dict[str, Chroma] = PrivateAttr(default_factory=dict)
    _multi_doc_retriever: Optional['MultiDocRetriever'] = PrivateAttr(default=None)
    
    tenant: str = PrivateAttr(default=DEFAULT_TENANT)
    year: int = PrivateAttr(default=DEFAULT_YEAR)

    _doc_id_mapping: Dict[str, Dict[str, Any]] = PrivateAttr(default_factory=dict)
    _uuid_to_doc_id: Dict[str, str] = PrivateAttr(default_factory=dict)

    _embeddings: Any = PrivateAttr(default=None)

    # ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î: ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ default_factory=dict ‡∏´‡∏£‡∏∑‡∏≠ default=None ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô!
    _client: Optional[chromadb.PersistentClient] = PrivateAttr(default=None)

    def __new__(cls, *args, **kwargs):
        with cls._lock:
            if cls._instance is None:
                cls._instance = super(VectorStoreManager, cls).__new__(cls)
        return cls._instance

    def __init__(self, base_path: str = VECTORSTORE_DIR, tenant: str = DEFAULT_TENANT, year: int = DEFAULT_YEAR):
        if not self._is_initialized:
            self._base_path = base_path
            self.tenant = tenant.lower()        # ‡∏ï‡πâ‡∏≠‡∏á .lower() ‡∏î‡πâ‡∏ß‡∏¢‡∏ô‡∏∞!
            self.year = year
            self.enabler = DEFAULT_ENABLER   
            self._chroma_cache = {}
            self._embeddings = get_hf_embeddings()
            
            # 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠ Collection ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á (evidence_km)
            collection_name = _get_collection_name(EVIDENCE_DOC_TYPES, self.enabler)
            
            # 2. ‡πÉ‡∏ä‡πâ Path Helper ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á Full Path
            chroma_root_path = _build_vectorstore_path_by_doc_type(
                tenant=self.tenant, 
                year=self.year, 
                doc_type=EVIDENCE_DOC_TYPES, 
                enabler=self.enabler
            )
            
            self._client = chromadb.PersistentClient(path=chroma_root_path)
            logger.info(f"ChromaDB Client initialized at FULL COLLECTION PATH: {chroma_root_path}")
            
            # ‡πÇ‡∏´‡∏•‡∏î mapping ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ tenant/year ‡πÅ‡∏•‡πâ‡∏ß
            self._load_doc_id_mapping() 
            
            logger.info(f"Initialized VectorStoreManager (Tenant: {self.tenant}, Year: {self.year}). "
                        f"Loaded {len(self._doc_id_mapping)} stable doc IDs.")
            
            VectorStoreManager._is_initialized = True
    
    @property
    def doc_id_map(self) -> Dict[str, Dict[str, Any]]:
        """Provides access to the Stable Doc ID -> Chunk UUIDs mapping."""
        return self._doc_id_mapping

    @property
    def uuid_to_doc_id_map(self) -> Dict[str, str]:
        """Provides access to the Chunk UUID -> Stable Doc ID mapping."""
        return self._uuid_to_doc_id
    
    def close(self):
        with self._lock:
            if self._multi_doc_retriever and hasattr(self._multi_doc_retriever, "shutdown"):
                logger.info("Closing MultiDocRetriever executor via VSM.")
                self._multi_doc_retriever.shutdown()
                self._multi_doc_retriever = None
            self._chroma_cache = {}
            VectorStoreManager._is_initialized = False

    def __del__(self):
        try:
            self.close()
        except Exception:
            pass

    def _load_doc_id_mapping(self):
        self._doc_id_mapping = {}
        self._uuid_to_doc_id = {}
        
        # NOTE: Doc ID Mapping ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ Path ‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏õ‡∏µ‡πÄ‡∏™‡∏°‡∏≠
        mapping_filename = f"{self.tenant.lower()}_{self.year}_{self.enabler.lower()}_doc_id_mapping.json"
        
        doc_id_mapping_path = os.path.join(
            MAPPING_BASE_DIR, 
            self.tenant.lower(), 
            str(self.year), 
            mapping_filename
        )
  
        try:
            with open(doc_id_mapping_path, "r", encoding="utf-8") as f:
                mapping_data: Dict[str, Dict[str, Any]] = json.load(f)
                cleaned_mapping = {k.strip(): v for k, v in mapping_data.items()}
                self._doc_id_mapping = cleaned_mapping
                for doc_id, doc_entry in cleaned_mapping.items():
                    if isinstance(doc_entry, dict) and "chunk_uuids" in doc_entry and isinstance(doc_entry.get("chunk_uuids"), list):
                        for uid in doc_entry["chunk_uuids"]:
                            self._uuid_to_doc_id[uid] = doc_id
            logger.info(f"‚úÖ Loaded Doc ID Mapping from {doc_id_mapping_path}: {len(self._doc_id_mapping)} original documents, {len(self._uuid_to_doc_id)} total chunks.")
        except FileNotFoundError:
            logger.warning(f"‚ö†Ô∏è Doc ID Mapping file not found at {doc_id_mapping_path}.")
        except Exception as e:
            logger.error(f"‚ùå Failed to load Doc ID Mapping from {doc_id_mapping_path}: {e}")

    def _re_parse_collection_name(self, collection_name: str) -> Tuple[str, Optional[str]]:
        collection_name_lower = collection_name.strip().lower()
        if collection_name_lower.startswith(f"{EVIDENCE_DOC_TYPES}_"):
            parts = collection_name_lower.split("_", 1)
            return EVIDENCE_DOC_TYPES, parts[1].upper() if len(parts) == 2 else None
        return collection_name_lower, None

    def _load_chroma_instance(self, collection_name: str) -> Optional[Chroma]:
        # ... (‡πÇ‡∏Ñ‡πâ‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á)
        # 1. Fast cache hit
        if collection_name in self._chroma_cache:
            return self._chroma_cache[collection_name]

        # 2. Thread-safe double-check
        with self._lock:
            if collection_name in self._chroma_cache:
                return self._chroma_cache[collection_name]

            # ------------------------------------------------------------------
            # 3. ‡πÅ‡∏¢‡∏Å doc_type ‡∏Å‡∏±‡∏ö enabler ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤
            # ------------------------------------------------------------------
            doc_type, enabler = self._re_parse_collection_name(collection_name)

            # ------------------------------------------------------------------
            # 4. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î target_year
            # ------------------------------------------------------------------
            if doc_type.startswith("evidence"):
                target_year: Optional[int] = self.year
            else:
                target_year = None

            # ------------------------------------------------------------------
            # 5. ‡∏™‡∏£‡πâ‡∏≤‡∏á persist_directory ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á 100%
            # ------------------------------------------------------------------
            persist_directory = get_vectorstore_path(
                tenant=self.tenant,
                year=target_year,
                doc_type=doc_type,
                enabler=enabler,
            )

            # ------------------------------------------------------------------
            # 6. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ folder ‡∏à‡∏£‡∏¥‡∏á‡πÑ‡∏´‡∏°
            # ------------------------------------------------------------------
            if not os.path.exists(persist_directory):
                logger.warning(
                    f"Vectorstore directory NOT FOUND!\n"
                    f"   Collection   : {collection_name}\n"
                    f"   Expected path: {persist_directory}\n"
                    f"   tenant={self.tenant} | year={self.year} | doc_type={doc_type} | enabler={enabler or 'None'}"
                )
                alt_path = get_vectorstore_path(self.tenant, self.year, doc_type, enabler)
                if os.path.exists(alt_path):
                    logger.warning(f"   BUT found at alternative path: {alt_path} ‚Üê ‡∏≠‡∏≤‡∏à‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ year ‡∏ú‡∏¥‡∏î?")
                return None

            # ------------------------------------------------------------------
            # 7. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ client ‡∏ñ‡∏π‡∏Å init ‡πÅ‡∏•‡πâ‡∏ß
            # ------------------------------------------------------------------
            if self._client is None:
                logger.error("Chroma PersistentClient is None! ‡∏ï‡πâ‡∏≠‡∏á init ‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")
                return None
            
            # üéØ FIX: ‡∏î‡∏∂‡∏á Global Embedding Model (768-dim) ‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
            try:
                # üí° ‡πÉ‡∏ä‡πâ get_hf_embeddings() ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ
                correct_embeddings = get_hf_embeddings() 
            except Exception as e:
                logger.error(f"FATAL: Failed to get correct embeddings for Chroma init: {e}")
                return None

            try:
                # ------------------------------------------------------------------
                # 8. ‡∏™‡∏£‡πâ‡∏≤‡∏á Chroma instance (‡πÉ‡∏ä‡πâ client ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô ‚Üí ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏∏‡∏î!)
                # ------------------------------------------------------------------
                vectordb = Chroma(
                    client=self._client,                     # ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ client ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á!!!
                    embedding_function=correct_embeddings,
                    collection_name=collection_name,
                )

                # Cache ‡πÑ‡∏ß‡πâ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ï‡πà‡∏≠‡πÑ‡∏õ
                self._chroma_cache[collection_name] = vectordb

                logger.info(
                    f"Loaded Chroma collection '{collection_name}' ‚Üí {persist_directory}"
                )
                return vectordb

            except Exception as e:
                logger.error(
                    f"FAILED to load collection '{collection_name}' from {persist_directory}\n"
                    f"Error: {e}",
                    exc_info=True,
                )
                return None

    def get_documents_by_id(self, stable_doc_ids: Union[str, List[str]], doc_type: str = "default_collection", enabler: Optional[str] = None) -> List[LcDocument]:
        """
        Retrieve documents from Chroma collection by stable_doc_ids (64-char hash) 
        or by full chunk UUID (64-char hash + _index).

        Automatically uses the Doc ID Map to find all related chunk UUIDs.
        
        *** FIX: ‡∏•‡∏ö 'ids' ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å include ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ ChromaDB ValueError ***
        """
        if isinstance(stable_doc_ids, str):
            stable_doc_ids = [stable_doc_ids]
            
        if not stable_doc_ids:
            return []

        # 1. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ä‡∏∑‡πà‡∏≠ Collection ‡πÅ‡∏•‡∏∞‡πÇ‡∏´‡∏•‡∏î Instance
        collection_name = _get_collection_name(doc_type=doc_type, enabler=enabler, year=self.year)
        chroma_instance = self._load_chroma_instance(collection_name)
        
        if not chroma_instance:
            logger.warning(f"VSM: Cannot load collection '{collection_name}' for document retrieval.")
            return []

        # 2. ‡πÅ‡∏õ‡∏•‡∏á Stable Doc IDs ‡πÄ‡∏õ‡πá‡∏ô Chunk UUIDs
        search_ids: List[str] = []
        search_key: str = "ids" # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏õ‡πá‡∏ô 'ids' ‡∏Å‡πà‡∏≠‡∏ô
        
        for stable_id in stable_doc_ids:
            # ‡πÉ‡∏ä‡πâ Doc ID Map ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤ Chunk UUIDs ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
            map_entry = self.doc_id_map.get(stable_id)
            if map_entry and map_entry.get("chunk_uuids"):
                search_ids.extend(map_entry["chunk_uuids"])
            else:
                # ‡∏ñ‡πâ‡∏≤‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡πÉ‡∏ô map ‡πÉ‡∏´‡πâ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô Chunk UUID ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡πá‡∏ô Stable ID ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ
                search_ids.append(stable_id) 
                
        # 3. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ ID ‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Query
        search_ids = list(set([str(i).strip() for i in search_ids if str(i).strip()]))


        try:
            collection = chroma_instance._collection
            documents: List[LcDocument] = []
            
            # --- ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å ChromaDB ---
            if search_key == "ids": # ‡πÉ‡∏ä‡πâ Primary Key Search (Chunk UUIDs)
                 # FIX: ‡∏•‡∏ö "ids" ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å include
                 result = collection.get(
                    ids=search_ids,
                    include=["documents", "metadatas"] 
                )
            else: # Fallback: ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ï‡∏≤‡∏° metadata (‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡πÉ‡∏ô Logic ‡∏ô‡∏µ‡πâ)
                 # FIX: ‡∏•‡∏ö "ids" ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å include
                 result = collection.get(
                    where={search_key: {"$in": search_ids}},
                    include=["documents", "metadatas"]
                )

            # --- ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå ---
            docs = result.get("documents", [])
            metadatas = result.get("metadatas", [{}] * len(docs))
            # NOTE: ‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á‡∏ß‡πà‡∏≤ "ids" ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô‡∏°‡∏≤‡πÄ‡∏™‡∏°‡∏≠
            ids = result.get("ids", [""] * len(docs)) 

            for i, text in enumerate(docs):
                meta = metadatas[i].copy() if metadatas and metadatas[i] else {}
                chunk_uuid = ids[i] if ids else (meta.get("chunk_uuid") or "")
                
                # ‡πÄ‡∏û‡∏¥‡πà‡∏°/‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï chunk_uuid
                if chunk_uuid:
                    meta["chunk_uuid"] = chunk_uuid

                # ‡πÄ‡∏û‡∏¥‡πà‡∏° stable_doc_uuid ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ map (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
                stable_doc_id = self.uuid_to_doc_id_map.get(chunk_uuid) or meta.get("stable_doc_uuid") or meta.get("doc_id")
                if stable_doc_id:
                     meta["stable_doc_uuid"] = stable_doc_id

                doc = LcDocument(page_content=text, metadata=meta)
                documents.append(doc)
                
            logger.info(f"‚úÖ Retrieved {len(documents)} documents for {len(stable_doc_ids)} Stable IDs from '{collection_name}' (Search Mode: {search_key}).")
            return documents

        except Exception as e:
            logger.error(f"‚ùå Error retrieving documents by Stable/Chunk IDs from collection '{collection_name}': {e}")
            return []
        
    def retrieve_by_chunk_uuids(self, chunk_uuids: List[str], collection_name: Optional[str] = None) -> List[LcDocument]:
        """
        Retrieves documents from Chroma collection based on a list of unique chunk_uuids (IDs).
        Includes de-duplication logic to prevent ChromaDB DuplicateIDError.
        
        *** FIX: ‡∏•‡∏ö 'ids' ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å include ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ ChromaDB ValueError ***
        """
        if not chunk_uuids:
            logger.info("VSM: No chunk_uuids provided for hydration.")
            return []
        
        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ä‡∏∑‡πà‡∏≠ collection ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏∞‡∏ö‡∏∏
        if collection_name is None:
            collection_name = f"evidence_{getattr(self, 'enabler', 'km').lower()}"

        # ‡πÇ‡∏´‡∏•‡∏î Chroma instance
        chroma_instance = self._load_chroma_instance(collection_name)
        if not chroma_instance:
            logger.warning(f"VSM: Cannot load collection '{collection_name}' for hydration.")
            return []

        # DEBUG (‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà)
        logger.info(f"VSM: Attempting hydration with {len(chunk_uuids)} UUIDs from '{collection_name}'")
        logger.info(f"VSM: First 5 UUIDs ‚Üí {chunk_uuids[:5]}")

        try:
            collection = chroma_instance._collection

            # 1. Clean IDs (‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô str ‡πÅ‡∏•‡∏∞ strip)
            clean_ids = [str(uuid).strip() for uuid in chunk_uuids if uuid and str(uuid).strip()]

            # 2. üéØ FIX: ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£ De-duplicate IDs ‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ ChromaDB
            unique_chunk_uuids = list(set(clean_ids))
            
            if len(unique_chunk_uuids) < len(clean_ids):
                duplicated_count = len(clean_ids) - len(unique_chunk_uuids)
                logger.warning(f"VSM: De-duplicated {duplicated_count} repeated UUIDs before calling ChromaDB get.")
            
            if not unique_chunk_uuids:
                logger.warning("VSM: All UUIDs became empty after cleaning or duplication removal!")
                return []

            # 3. ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ ChromaDB ‡∏î‡πâ‡∏ß‡∏¢ IDs ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô
            # üéØ FIX: ‡∏•‡∏ö "ids" ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å include ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ ValueError
            results = collection.get(
                ids=unique_chunk_uuids,
                include=["documents", "metadatas"] 
            )

            # NOTE: ‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á‡∏ß‡πà‡∏≤ "ids" ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô‡∏°‡∏≤‡πÄ‡∏™‡∏°‡∏≠ ‡πÅ‡∏°‡πâ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏∞‡∏ö‡∏∏‡πÉ‡∏ô include
            found_count = len(results["ids"]) if results.get("ids") else 0
            logger.info(f"VSM: Successfully retrieved {found_count}/{len(unique_chunk_uuids)} chunks by UUID from '{collection_name}'")

            # 4. ‡πÅ‡∏õ‡∏•‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏õ‡πá‡∏ô LcDocument
            docs = []
            # ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏ú‡πà‡∏≤‡∏ô IDs ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤
            for i, doc_id in enumerate(results["ids"]): 
                content = results["documents"][i]
                meta = results["metadatas"][i] if results.get("metadatas") else {}
                doc = LcDocument(page_content=content, metadata=meta.copy())
                
                # ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: doc_id ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å Chroma ‡∏Ñ‡∏∑‡∏≠ chunk_uuid
                doc.metadata["chunk_uuid"] = doc_id 
                # ‡πÄ‡∏û‡∏¥‡πà‡∏° stable_doc_uuid ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ map
                stable_doc_id = self.uuid_to_doc_id_map.get(doc_id) or meta.get("stable_doc_uuid") or meta.get("doc_id")
                if stable_doc_id:
                     doc.metadata["stable_doc_uuid"] = stable_doc_id
                     
                docs.append(doc)
            
            return docs

        except Exception as e:
            logger.error(f"VSM: FATAL Error in retrieve_by_chunk_uuids: {e}", exc_info=True)
            return []
            
    def get_limited_chunks_from_doc_ids(self, stable_doc_ids: Union[str, List[str]], query: Union[str, List[str]], doc_type: str, enabler: Optional[str] = None, limit_per_doc: int = 5) -> List[LcDocument]:
        if isinstance(stable_doc_ids, str):
            stable_doc_ids = [stable_doc_ids]
        stable_doc_ids = [uid for uid in stable_doc_ids if uid]
        if not stable_doc_ids:
            return []
        vector_search_query = query[0] if isinstance(query, list) and query else (query if isinstance(query, str) else "")
        if not vector_search_query:
            logger.warning("Limited chunk search skipped: Query is empty.")
            return []
        collection_name = _get_collection_name(doc_type, enabler)
        chroma_instance = self._load_chroma_instance(collection_name)
        if not chroma_instance:
            logger.error(f"Collection '{collection_name}' is not loaded.")
            return []
        all_limited_documents: List[LcDocument] = []
        total_chunks_retrieved = 0
        for stable_id in stable_doc_ids:
            stable_id_clean = stable_id.strip()
            doc_filter = {"stable_doc_uuid": stable_id_clean}
            try:
                custom_retriever = ChromaRetriever(vectorstore=chroma_instance, k=limit_per_doc, filter=doc_filter)
                limited_docs = custom_retriever.get_relevant_documents(query=vector_search_query)
                for doc in limited_docs:
                    doc.metadata["priority_search_type"] = "limited_vector_search"
                    doc.metadata["priority_limit"] = limit_per_doc
                    all_limited_documents.append(doc)
                total_chunks_retrieved += len(limited_docs)
            except Exception as e:
                logger.error(f"‚ùå Error performing limited vector search for Stable ID '{stable_id_clean}': {e}")
                continue
        logger.info(f"‚úÖ Retrieved {total_chunks_retrieved} limited chunks (max {limit_per_doc}/doc) for {len(stable_doc_ids)} Stable IDs from '{collection_name}'.")
        return all_limited_documents

    # -------------------- Retriever Creation --------------------
    def get_retriever(self, collection_name: str, top_k: int = INITIAL_TOP_K, final_k: int = FINAL_K_RERANKED, use_rerank: bool = True) -> Any:
        chroma_instance = self._load_chroma_instance(collection_name)
        if not chroma_instance:
            logger.warning(f"Retriever creation failed: Collection '{collection_name}' not loaded.")
            return None

        # 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á retriever ‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏° k ‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á
        def raw_retrieve(query: str, filter_dict: Optional[dict] = None, k: int = top_k) -> List[LcDocument]:
            try:
                original_query = query
                
                # ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å: PEA 2568 ingest ‡∏î‡πâ‡∏ß‡∏¢ paraphrase-multilingual-MiniLM-L12-v2 ‚Üí ‡πÑ‡∏°‡πà‡∏°‡∏µ prefix
                # ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏™‡πà query: ‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≤‡∏î!
                query = query.strip()
                logger.critical(f"[NO PREFIX FOR PEA] Using raw query: '{query[:100]}...'")

                search_kwargs = {"k": k}
                if filter_dict:
                    search_kwargs["filter"] = filter_dict
                
                docs = chroma_instance.similarity_search(
                    query=query,
                    k=k,
                    filter=filter_dict
                )
                logger.info(f"Raw retrieval: {len(docs)} docs (k={k}, filter={bool(filter_dict)})")
                return docs
            except Exception as e:
                logger.error(f"Raw retrieval failed: {e}", exc_info=True)
                return []

        # 2. Reranker wrapper (‡∏â‡∏ö‡∏±‡∏ö‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡πÅ‡∏ö‡∏ö)
        def retrieve_with_rerank(query: str, config: Optional[dict] = None):
            filter_dict = None
            if config and isinstance(config, dict):
                filter_dict = config.get("configurable", {}).get("search_kwargs", {}).get("filter")

            # ‡∏î‡∏∂‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏î‡πâ‡∏ß‡∏¢ k ‡πÄ‡∏ï‡πá‡∏°
            docs = raw_retrieve(query, filter_dict, k=top_k)

            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ reranker ‚Üí ‡∏Ñ‡∏∑‡∏ô‡∏ï‡∏≤‡∏° top_k
            reranker = get_global_reranker()
            if not (use_rerank and reranker and hasattr(reranker, "compress_documents")):
                return docs[:final_k]

            try:
                reranked = reranker.compress_documents(
                    documents=docs,
                    query=query,
                    top_n=final_k
                )
                # ‡∏î‡∏∂‡∏á score ‡∏à‡∏≤‡∏Å reranker (‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î)
                scores = getattr(reranker, "scores", None)
                if scores and len(scores) >= len(reranked):
                    for doc, score in zip(reranked, scores[:len(reranked)]):
                        score = float(score) if score is not None else 0.0
                        # ‡∏â‡∏µ‡∏î‡πÅ‡∏Ñ‡πà key ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô ‚Üí _rerank_score_force
                        doc.metadata["_rerank_score_force"] = score
                        # ‡πÅ‡∏•‡∏∞ source_filename (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö extraction)
                        orig = doc.metadata.get("source_filename", "UNKNOWN")
                        doc.metadata["source_filename"] = f"{orig}|SCORE:{score:.4f}"

                logger.info(f"Reranking success ‚Üí kept {len(reranked)} docs")
                return reranked

            except Exception as e:
                logger.warning(f"Rerank failed ({e}), fallback to raw")
                return docs[:final_k]

        # 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á LangChain-compatible Retriever
        class UltimateRetriever(BaseRetriever):
            def _get_relevant_documents(self, query: str, *, run_manager=None) -> List[LcDocument]:
                return retrieve_with_rerank(query, config=None)

            def invoke(self, query: str, config: Optional[dict] = None) -> List[LcDocument]:
                return retrieve_with_rerank(query, config=config)

        logger.info(f"Ultimate Retriever ready ‚Üí {collection_name} | top_k={top_k} ‚Üí final_k={final_k} | rerank={use_rerank}")
        return UltimateRetriever()

    def get_all_collection_names(self) -> List[str]:
        return list_vectorstore_folders(tenant=self.tenant, year=self.year, base_path=self._base_path)

    def get_chunks_from_doc_ids(self, stable_doc_ids: Union[str, List[str]], doc_type: str, enabler: Optional[str] = None) -> List[LcDocument]:
        if isinstance(stable_doc_ids, str):
            stable_doc_ids = [stable_doc_ids]
        stable_doc_ids = [uid for uid in stable_doc_ids if uid]
        if not stable_doc_ids:
            return []
        collection_name = _get_collection_name(doc_type, enabler)
        all_chunk_uuids = []
        skipped_docs = []
        found_stable_ids = []
        for stable_id in stable_doc_ids:
            stable_id_clean = stable_id.strip()
            if stable_id_clean in self._doc_id_mapping:
                doc_entry = self._doc_id_mapping[stable_id_clean]
                if isinstance(doc_entry, dict) and "chunk_uuids" in doc_entry and isinstance(doc_entry.get("chunk_uuids"), list):
                    chunk_uuids = doc_entry["chunk_uuids"]
                    if chunk_uuids:
                        all_chunk_uuids.extend(chunk_uuids)
                        found_stable_ids.append(stable_id_clean)
                    else:
                        logger.warning(f"Mapping found for Stable ID '{stable_id_clean}' but 'chunk_uuids' list is empty.")
                else:
                    logger.warning(f"Mapping entry for Stable ID '{stable_id_clean}' is malformed or missing 'chunk_uuids'.")
            else:
                skipped_docs.append(stable_id_clean)
        if skipped_docs:
            logger.warning(f"Skipping Stable IDs not found in mapping: {skipped_docs}")
        if not all_chunk_uuids:
            logger.warning(f"No valid chunk UUIDs found for provided Stable Document IDs: {skipped_docs}. Check doc_id_mapping.json.")
            return []
        chroma_instance = self._load_chroma_instance(collection_name)
        if not chroma_instance:
            logger.error(f"Collection '{collection_name}' is not loaded.")
            return []
        try:
            collection = chroma_instance._collection
            result = collection.get(ids=all_chunk_uuids, include=["documents", "metadatas", "ids"])
            documents: List[LcDocument] = []
            if not result.get("documents"):
                logger.warning(f"Chroma DB returned 0 documents for {len(all_chunk_uuids)} chunk UUIDs in collection '{collection_name}'.")
                return []
            for i, text in enumerate(result.get("documents", [])):
                if text:
                    metadata = result.get("metadatas", [{}])[i]
                    chunk_uuid_from_result = result.get("ids", [""])[i]
                    doc_id = self._uuid_to_doc_id.get(chunk_uuid_from_result, "UNKNOWN")
                    metadata["chunk_uuid"] = chunk_uuid_from_result
                    metadata["doc_id"] = doc_id
                    metadata["doc_type"] = doc_type
                    documents.append(LcDocument(page_content=text, metadata=metadata))
            logger.info(f"‚úÖ Retrieved {len(documents)} chunks for {len(found_stable_ids)} Stable IDs from '{collection_name}'.")
            return documents
        except Exception as e:
            logger.error(f"‚ùå Error retrieving documents by Chunk UUIDs from collection '{collection_name}': {e}")
            return []

# Helper function
def get_vectorstore_manager() -> VectorStoreManager:
    return VectorStoreManager()

def load_vectorstore(doc_type: str, enabler: Optional[str] = None) -> Optional[Chroma]:
    collection_name = _get_collection_name(doc_type, enabler)
    return get_vectorstore_manager()._load_chroma_instance(collection_name)

class VectorStoreExecutorSingleton:
    _instance = None
    _is_initialized = False

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            cls._instance = super(VectorStoreExecutorSingleton, cls).__new__(cls)
        return cls._instance

    def __init__(self):
        if not VectorStoreExecutorSingleton._is_initialized:
            self.max_workers = MAX_PARALLEL_WORKERS
            self._executor = ThreadPoolExecutor(max_workers=self.max_workers)
            logger.info(f"Initialized VectorStoreExecutorSingleton (ThreadPoolExecutor with {self.max_workers} workers) for batch tasks.")
            VectorStoreExecutorSingleton._is_initialized = True

    @property
    def executor(self) -> ThreadPoolExecutor:
        return self._executor

    def close(self):
        if self._is_initialized:
            logger.info("Shutting down VectorStoreExecutorSingleton ThreadPoolExecutor...")
            self._executor.shutdown(wait=True)
            VectorStoreExecutorSingleton._is_initialized = False

def get_vectorstore() -> VectorStoreExecutorSingleton:
    return VectorStoreExecutorSingleton()

# -------------------- Custom Retriever for Chroma --------------------
class ChromaRetriever(BaseRetriever):
    vectorstore: Any
    k: int
    filter: Optional[Dict] = None
    model_config = ConfigDict(arbitrary_types_allowed=True)

    def _get_relevant_documents(self, query: str, *, run_manager=None) -> List[LcDocument]:
        
        # üéØ DEBUG: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Collection Count ‡πÅ‡∏•‡∏∞ Embedding (‡πÇ‡∏Ñ‡πâ‡∏î‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏°)
        try:
            raw_collection = self.vectorstore._collection
            count = raw_collection.count() 
            logger.critical(f"üéØ [DEBUG CHROMA] Collection Count: {count}")
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Query Embedding
            if hasattr(self.vectorstore, '_embedding_function') and self.vectorstore._embedding_function:
                embedding_function = self.vectorstore._embedding_function
                # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö embed query
                query_embedding = embedding_function.embed_query(query)
                logger.critical(f"üéØ [DEBUG CHROMA] Query Embedding Success. Vector size: {len(query_embedding)}")
            else:
                logger.critical("üéØ [DEBUG CHROMA] Cannot access _embedding_function.")

        except Exception as debug_e:
            logger.critical(f"‚ùå [DEBUG CHROMA] Debug check failed (Skip search): {debug_e}")
            return []
        # END DEBUG

        try:
            # ‡∏£‡∏±‡∏ô similarity search
            return self.vectorstore.similarity_search(query=query, k=self.k, filter=self.filter)
        except Exception as e:
            logger.error(f"‚ùå Chroma similarity_search failed in custom retriever: {e}")
            return []

    def get_relevant_documents(self, query: str, **kwargs) -> List[LcDocument]:
        return self._get_relevant_documents(query, **kwargs)

    async def _aget_relevant_documents(self, query: str, *, run_manager=None) -> List[LcDocument]:
        return self._get_relevant_documents(query, run_manager=run_manager)

# -------------------- MultiDoc / Parallel Retriever --------------------
class NamedRetriever(BaseModel):
    """
    Defines a single retriever configuration, mapping a document type/enabler
    to a specific VectorStore collection context (tenant/year).
    """
    doc_id: str
    doc_type: str
    top_k: int = INITIAL_TOP_K
    final_k: int = FINAL_K_RERANKED
    base_path: str = VECTORSTORE_DIR
    enabler: Optional[str] = None
    tenant: str = DEFAULT_TENANT
    year: Optional[int] = DEFAULT_YEAR # <--- üéØ FIX: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô Optional[int] ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö None
    
    # load_instance ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏Å VSM ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ self.tenant ‡πÅ‡∏•‡∏∞ self.year
    def load_instance(self) -> Any:
        """
        Loads the actual VectorStore Retriever instance using VectorStoreManager 
        with the correct tenant and year context.
        """
        # ‚ö†Ô∏è VSM Singleton ‡∏ï‡πâ‡∏≠‡∏á‡∏ñ‡∏π‡∏Å init ‡∏î‡πâ‡∏ß‡∏¢‡∏õ‡∏µ‡πÄ‡∏™‡∏°‡∏≠ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÇ‡∏´‡∏•‡∏î Doc ID Mapping
        # ‡∏ñ‡πâ‡∏≤ self.year ‡πÄ‡∏õ‡πá‡∏ô None (‡πÄ‡∏ä‡πà‡∏ô ‡πÄ‡∏õ‡πá‡∏ô General Docs ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô‡∏ó‡∏∏‡∏Å‡∏õ‡∏µ) ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ DEFAULT_YEAR
        manager = VectorStoreManager(
            base_path=self.base_path, 
            tenant=self.tenant, 
            year=self.year if self.year is not None else DEFAULT_YEAR # <--- üéØ FIX: ‡πÉ‡∏ä‡πâ DEFAULT_YEAR ‡∏ñ‡πâ‡∏≤ self.year ‡πÄ‡∏õ‡πá‡∏ô None
        ) 
        collection_name = _get_collection_name(self.doc_type, self.enabler)
        
        retriever = manager.get_retriever(collection_name=collection_name, top_k=self.top_k, final_k=self.final_k) 
        
        if not retriever:
            raise ValueError(f"Retriever not found for collection '{collection_name}' at path based on tenant={self.tenant}, year={self.year}")
        
        return retriever


class MultiDocRetriever(BaseRetriever):
    model_config = ConfigDict(arbitrary_types_allowed=True)
    _all_retrievers: Dict[str, Any] = PrivateAttr(default_factory=dict)
    _retrievers_list: list[NamedRetriever] = PrivateAttr()
    _k_per_doc: int = PrivateAttr()
    _manager: VectorStoreManager = PrivateAttr()
    _doc_ids_filter: Optional[List[str]] = PrivateAttr()
    _chroma_filter: Optional[Dict[str, Any]] = PrivateAttr()
    _executor_type: str = PrivateAttr()
    _executor: Union[ThreadPoolExecutor, ProcessPoolExecutor, None] = PrivateAttr(None)

    def __init__(self, retrievers_list: list[NamedRetriever], k_per_doc: int = INITIAL_TOP_K, doc_ids_filter: Optional[List[str]] = None):
        super().__init__()
        self._retrievers_list = retrievers_list
        self._k_per_doc = k_per_doc

        # üéØ FIX 2: ‡∏î‡∏∂‡∏á Context ‡∏à‡∏≤‡∏Å NamedRetriever ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å
        tenant_context = retrievers_list[0].tenant if retrievers_list else DEFAULT_TENANT
        year_context = retrievers_list[0].year if retrievers_list else DEFAULT_YEAR

        # üéØ FIX 2A: ‡∏™‡∏£‡πâ‡∏≤‡∏á VSM ‡∏î‡πâ‡∏ß‡∏¢ Context ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Doc ID Mapping)
        # ‡∏ñ‡πâ‡∏≤ year_context ‡πÄ‡∏õ‡πá‡∏ô None (‡∏à‡∏≤‡∏Å NamedRetriever ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô General doc) ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ DEFAULT_YEAR
        if year_context is None:
             year_context = DEFAULT_YEAR 
             
        self._manager = VectorStoreManager(tenant=tenant_context, year=year_context)
        
        self._all_retrievers = {}
        for named_r in retrievers_list:
            collection_name = _get_collection_name(named_r.doc_type, named_r.enabler)
            try:
                # Load the RerankRetriever instance
                retriever_instance = named_r.load_instance()
                if retriever_instance:
                    self._all_retrievers[collection_name] = retriever_instance
                    logger.info(f"‚úÖ MultiDocRetriever cached collection: {collection_name}")
                else:
                    logger.warning(f"‚ö†Ô∏è Failed to load instance for {collection_name} during MDR init.")
            except Exception as e:
                logger.error(f"‚ùå Error loading instance {collection_name} into MDR cache: {e}")

        self._doc_ids_filter = doc_ids_filter
        self._chroma_filter = None
        if doc_ids_filter:
            # Chroma filter applied during retrieval
            self._chroma_filter = {"doc_id": {"$in": doc_ids_filter}}
            logger.info(f"‚úÖ MultiDocRetriever initialized with doc_ids filter for {len(doc_ids_filter)} Stable IDs.")

        # Using a simpler executor choice that doesn't rely on undefined imports
        self._executor_type = self._choose_executor() 
        logger.info(f"MultiDocRetriever will use executor type: {self._executor_type} (workers={MAX_PARALLEL_WORKERS})")
    
    def _choose_executor(self) -> str:
        """Selects the executor type based on basic platform info."""
        # Simplify executor choice to avoid dependency on undefined imports
        if platform.system() == "Windows":
             return "process"
        # Defaulting to thread pool for efficiency on other platforms unless specified otherwise
        return "thread" 

    def shutdown(self):
        if self._executor:
            executor_type_name = "ProcessPoolExecutor" if self._executor_type == "process" else "ThreadPoolExecutor"
            workers = self._executor._max_workers if hasattr(self._executor, "_max_workers") else "N/A"
            logger.info(f"Shutting down MultiDocRetriever's {executor_type_name} executor ({workers} workers).")
            self._executor.shutdown(wait=True)
            self._executor = None

    def __del__(self):
        try:
            self.shutdown()
        except Exception:
            pass

    def _get_executor(self) -> Union[ThreadPoolExecutor, ProcessPoolExecutor]:
        if self._executor is None:
            workers = MAX_PARALLEL_WORKERS
            if self._executor_type == "process":
                self._executor = ProcessPoolExecutor(max_workers=workers)
                logger.info(f"üõ†Ô∏è Using ProcessPoolExecutor with {workers} workers.")
            else:
                self._executor = ThreadPoolExecutor(max_workers=workers)
                logger.info(f"üõ†Ô∏è Using ThreadPoolExecutor with {workers} workers.")
        return self._executor

    @staticmethod
    def _static_retrieve_task(named_r: NamedRetriever, query: str, chroma_filter: Optional[Dict]):
        """Static task method for ProcessPoolExecutor."""
        try:
            # load_instance ensures the correct VSM context is used
            retriever_instance = named_r.load_instance()
            if not retriever_instance:
                return []
                
            # Prepare config for the invoke method of the RerankRetriever
            search_kwargs = {"k": named_r.top_k}
            if chroma_filter:
                # The Chroma filter is applied as 'where' in Chroma's implementation
                search_kwargs["where"] = chroma_filter 
                
            config = {"configurable": {"search_kwargs": search_kwargs}}
            
            # retriever_instance is a RerankRetriever (which implements Runnable.invoke)
            docs = retriever_instance.invoke(query, config=config)
            
            for doc in docs:
                doc.metadata["retrieval_source"] = named_r.doc_type
                doc.metadata["collection_name"] = _get_collection_name(named_r.doc_type, named_r.enabler)
                
                # üéØ FIX 1A: Ensure chunk_uuid is present for final filtering.
                chunk_uuid = doc.metadata.get("chunk_uuid") 
                
                if not chunk_uuid:
                    # Try to find the ID from common Langchain/Chroma internal keys
                    potential_uuid = doc.metadata.get("id") or doc.metadata.get("_id") 
                    
                    if potential_uuid:
                        doc.metadata["chunk_uuid"] = str(potential_uuid)
                    else:
                        # Final Fallback: Use a stable hash of the content/metadata for deduplication/ID
                        key_content = f"{doc.page_content}{doc.metadata.get('doc_id')}"
                        hashed_uuid = hashlib.sha256(key_content.encode('utf-8')).hexdigest()
                        doc.metadata["chunk_uuid"] = hashed_uuid[:32] # Use first 32 chars for uniqueness
                        
            return docs
        except Exception as e:
            # Use print here as logger might not be configured correctly in child process
            print(f"‚ùå Child retrieval error for {named_r.doc_id} ({named_r.doc_type}): {e}")
            return []

    def _thread_retrieve_task(self, named_r: NamedRetriever, query: str, chroma_filter: Optional[Dict]):
        """Instance method for ThreadPoolExecutor."""
        try:
            # load_instance ensures the correct VSM context is used
            retriever_instance = named_r.load_instance()
            if not retriever_instance:
                return []
                
            # Prepare config for the invoke method of the RerankRetriever
            search_kwargs = {"k": named_r.top_k}
            if chroma_filter:
                # The Chroma filter is applied as 'where' in Chroma's implementation
                search_kwargs["where"] = chroma_filter 
                
            config = {"configurable": {"search_kwargs": search_kwargs}}

            # retriever_instance is a RerankRetriever (which implements Runnable.invoke)
            docs = retriever_instance.invoke(query, config=config)
            
            for doc in docs:
                doc.metadata["retrieval_source"] = named_r.doc_type
                doc.metadata["collection_name"] = _get_collection_name(named_r.doc_type, named_r.enabler)

                # üéØ FIX 1A: Ensure chunk_uuid is present for final filtering.
                chunk_uuid = doc.metadata.get("chunk_uuid") 
                
                if not chunk_uuid:
                    # Try to find the ID from common Langchain/Chroma internal keys
                    potential_uuid = doc.metadata.get("id") or doc.metadata.get("_id") 
                    
                    if potential_uuid:
                        doc.metadata["chunk_uuid"] = str(potential_uuid)
                    else:
                        # Final Fallback: Use a stable hash of the content/metadata for deduplication/ID
                        key_content = f"{doc.page_content}{doc.metadata.get('doc_id')}"
                        hashed_uuid = hashlib.sha256(key_content.encode('utf-8')).hexdigest()
                        doc.metadata["chunk_uuid"] = hashed_uuid[:32] # Use first 32 chars for uniqueness
                    
            return docs
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Thread retrieval error for {named_r.doc_id}: {e}")
            return []

    def _get_relevant_documents(self, query: str, *, run_manager: Any = None) -> List[LcDocument]:
        max_workers = min(len(self._retrievers_list), MAX_PARALLEL_WORKERS)
        if max_workers <= 0:
            max_workers = 1
        chosen = self._executor_type
        logger.info(f"‚öôÔ∏è Running MultiDocRetriever with {chosen} executor ({max_workers} workers) [Filter: {bool(self._chroma_filter)}]")
        all_docs: List[LcDocument] = []
        
        executor = self._get_executor()
        futures = []
        for named_r in self._retrievers_list:
            if chosen == "process":
                # Use static method for ProcessPoolExecutor
                future = executor.submit(MultiDocRetriever._static_retrieve_task, named_r, query, self._chroma_filter)
            else:
                # Use instance method for ThreadPoolExecutor
                future = executor.submit(self._thread_retrieve_task, named_r, query, self._chroma_filter)
            futures.append(future)
            
        for f in futures:
            try:
                docs = f.result()
                if docs:
                    all_docs.extend(docs)
            except Exception as e:
                logger.warning(f"Future failed: {e}")
                
        # Deduplication using chunk metadata
        seen = set()
        unique_docs = []
        for d in all_docs:
            src = d.metadata.get("retrieval_source") or ""
            # Use 'chunk_uuid' or 'ids' (which is the UUID from Chroma) for unique identification
            # NOTE: chunk_uuid should now be present due to the fix in the task methods
            chunk_uuid = d.metadata.get("chunk_uuid") or d.metadata.get("ids") or "" 
            
            # Fallback to content if UUIDs are missing (less reliable)
            if not chunk_uuid:
                 # Use a hash or truncated content as a fallback unique key
                 key = f"{src}_{d.page_content[:120]}_{d.metadata.get('doc_id', 'no_doc_id')}"
            else:
                 key = f"{src}_{chunk_uuid}"
                 
            if key not in seen:
                seen.add(key)
                unique_docs.append(d)
                
        logger.info(f"üìù Query='{query[:80]}...' found {len(unique_docs)} unique docs across sources (Executor={chosen})")
        
        for d in unique_docs:
            score = d.metadata.get("relevance_score")
            if score is not None:
                logger.debug(f" - [Reranked] Source={d.metadata.get('doc_type')}, Score={score:.4f}, Content='{d.page_content[:80]}...'")
        
        return unique_docs

    def get_relevant_documents(self, query: str, **kwargs) -> List[LcDocument]:
        return self._get_relevant_documents(query, **kwargs)

# -------------------- load_all_vectorstores --------------------
def load_vectorstore_retriever(
    doc_id: str, 
    top_k: int = INITIAL_TOP_K, 
    final_k: int = FINAL_K_RERANKED, 
    doc_types: Union[list, str] = "default_collection", 
    base_path: str = VECTORSTORE_DIR, 
    enabler: Optional[str] = None, 
    tenant: str = DEFAULT_TENANT,
    year: int = DEFAULT_YEAR      
):
    if isinstance(doc_types, str):
        target_doc_type = doc_types
    elif isinstance(doc_types, list) and doc_types:
        target_doc_type = doc_types[0]
    else:
        raise ValueError("doc_types must be a single string or a non-empty list containing the target doc_type.")
        
    collection_name = _get_collection_name(target_doc_type, enabler)
    
    # üéØ FIX 3: ‡∏™‡πà‡∏á tenant ‡πÅ‡∏•‡∏∞ year ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô VectorStoreManager
    manager = VectorStoreManager(base_path=base_path, tenant=tenant, year=year) 
    retriever = None
    
    # NOTE: load_vectorstore_retriever ‡πÉ‡∏ä‡πâ‡∏õ‡∏µ‡πÄ‡∏™‡∏°‡∏≠ ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏õ‡πá‡∏ô context ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏°‡∏≤
    if vectorstore_exists(doc_id="N/A", base_path=base_path, doc_type=target_doc_type, enabler=enabler, tenant=tenant, year=year):
        retriever = manager.get_retriever(collection_name, top_k, final_k)
        
    if retriever is None:
        raise ValueError(f"‚ùå Vectorstore for collection '{collection_name}' not found.")
    return retriever

def load_all_vectorstores( # <-- ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: ‡πÄ‡∏û‡∏¥‡πà‡∏° 'd' ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÄ‡∏õ‡πá‡∏ô 'def'
    tenant: str, 
    year: int,    
    doc_types: Optional[Union[str, List[str]]] = None, 
    top_k: int = INITIAL_TOP_K, 
    final_k: int = FINAL_K_RERANKED, 
    base_path: Path = VECTORSTORE_DIR, 
    evidence_enabler: Optional[str] = None, 
    doc_ids: Optional[List[str]] = None
) -> VectorStoreManager:
    """
    Loads all relevant vectorstore collections based on tenant, year, and document types.
    Handles segregation logic for year-specific (evidence) and general (standard) documents.
    """
    
    doc_types = [doc_types] if isinstance(doc_types, str) else doc_types or []
    doc_type_filter = {dt.strip().lower() for dt in doc_types}
    
    # üéØ FIX 1: VSM ‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏î‡πâ‡∏ß‡∏¢ tenant/year ‡∏Ç‡∏≠‡∏á RUN (‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Doc ID Mapping)
    manager = VectorStoreManager(base_path=base_path, tenant=tenant, year=year) 
    
    all_retrievers: List[NamedRetriever] = []
    target_collection_names: Set[str] = set()

    # --- 1. Collection Discovery ---
    if not doc_type_filter:
        logger.error("Must specify doc_types for multi-year compatibility.")
        raise ValueError("Must specify doc_types when using multi-tenant setup.")
    
    # üéØ NEW: ‡πÄ‡∏£‡∏≤‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ list collections ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÑ‡∏´‡∏ô‡πÉ‡∏ä‡πâ‡∏õ‡∏µ/‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ‡∏õ‡∏µ 
    # ‡πÄ‡∏£‡∏≤‡∏à‡∏∂‡∏á‡πÉ‡∏ä‡πâ doc_type_filter ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á collection_name ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£ list ‡∏à‡∏≤‡∏Å folder
    for dt_norm in doc_type_filter:
        if dt_norm == EVIDENCE_DOC_TYPES.lower(): 
            if evidence_enabler:
                # Specific evidence collection: ‡πÉ‡∏ä‡πâ year
                collection_name = _get_collection_name(EVIDENCE_DOC_TYPES, evidence_enabler)
                target_collection_names.add(collection_name)
                logger.info(f"üîç Added specific evidence collection: {collection_name} (Year-Specific)")
            else:
                # All evidence collections: ‡∏ï‡πâ‡∏≠‡∏á list ‡∏à‡∏≤‡∏Å folder ‡∏†‡∏≤‡∏¢‡πÉ‡∏ï‡πâ tenant/year
                evidence_collections = list_vectorstore_folders(tenant=tenant, year=year, doc_type=EVIDENCE_DOC_TYPES, base_path=base_path)
                target_collection_names.update(evidence_collections)
                logger.info(f"üîç Added all evidence collections found: {evidence_collections} (Year-Specific)")
        else:
            # Standard Collections: ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ year
            collection_name = _get_collection_name(dt_norm, None)
            target_collection_names.add(collection_name)
            logger.info(f"üîç Added standard collection: {collection_name} (Shared/General)")
    
    logger.info(f"üîç DEBUG: Attempting to load {len(target_collection_names)} total target collections: {target_collection_names}")
    
    # --- 2. Retriever List Creation & Existence Check ---
    for collection_name in target_collection_names:
        doc_type_for_check, enabler_for_check = manager._re_parse_collection_name(collection_name)
        
        # üéØ FIX 2A: Logic ‡∏Å‡∏≥‡∏´‡∏ô‡∏î target_year (None ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö General Docs)
        target_year: Optional[int] = year
        if doc_type_for_check.lower() != EVIDENCE_DOC_TYPES.lower() and enabler_for_check is None:
            # ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£‡∏ñ‡∏π‡∏Å‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏°‡∏õ‡∏µ
            target_year = None # <--- ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏õ‡πá‡∏ô None
            
        # üéØ FIX 2B: ‡∏™‡πà‡∏á target_year ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏Ç‡∏≠‡∏á Vectorstore
        if not vectorstore_exists(doc_id="N/A", tenant=tenant, year=target_year, doc_type=doc_type_for_check, enabler=enabler_for_check):
            logger.warning(f"üîç DEBUG: Skipping collection '{collection_name}' (vectorstore_exists failed at tenant={tenant}, year={target_year}).")
            continue
            
        # üéØ FIX 2C: ‡∏™‡πà‡∏á target_year ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô NamedRetriever
        nr = NamedRetriever(
            doc_id=collection_name, 
            doc_type=doc_type_for_check, 
            enabler=enabler_for_check, 
            top_k=top_k, 
            final_k=final_k, 
            base_path=base_path,
            tenant=tenant, 
            year=target_year # <--- ‡∏™‡πà‡∏á None ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö General Docs
        )
        all_retrievers.append(nr)
        logger.info(f"üîç DEBUG: Successfully added retriever for collection '{collection_name}' (Year={target_year}).")

    final_filter_ids = doc_ids
    if doc_ids:
        logger.info(f"‚úÖ Hard Filter Enabled: Using {len(doc_ids)} original 64-char UUIDs for filtering.")
    logger.info(f"üîç DEBUG: Final count of all_retrievers = {len(all_retrievers)}")

    if not all_retrievers:
        raise ValueError(f"No vectorstore collections found matching tenant={tenant}, year={year}, doc_types={doc_types} and evidence_enabler={evidence_enabler}")
        
    mdr = MultiDocRetriever(retrievers_list=all_retrievers, k_per_doc=top_k, doc_ids_filter=final_filter_ids)
    manager._multi_doc_retriever = mdr
    logger.info(f"‚úÖ MultiDocRetriever loaded with {len(mdr._all_retrievers)} collections and cached in VSM.")
    return manager


def get_multi_doc_retriever(
    tenant: str = DEFAULT_TENANT,
    year: int = DEFAULT_YEAR,
    doc_types: List[str] = [],
    doc_ids: Optional[List[str]] = None,
    evidence_enabler: Optional[str] = None,
    base_path: str = VECTORSTORE_DIR,
    top_k: int = INITIAL_TOP_K,
    final_k: int = FINAL_K_RERANKED
) -> MultiDocRetriever:
    """
    Factory function to create a MultiDocRetriever based on configuration.
    It determines which NamedRetrievers to initialize based on the tenant, year, and doc_types.
    """
    all_retrievers: List[NamedRetriever] = []

    # 1. Dynamic Check for Year-Specific Collections
    # Loop through requested doc_types and check against the target year
    target_year = year
    for doc_type_for_check in doc_types:
        collection_name = _get_collection_name(doc_type_for_check, evidence_enabler)
        
        enabler_for_check = evidence_enabler
        
        # Check if collection exists for the specific year
        if not vectorstore_exists(base_path=base_path, tenant=tenant, year=target_year, doc_type=doc_type_for_check, enabler=enabler_for_check):
            logger.warning(f"üîç DEBUG: Skipping collection '{collection_name}' (vectorstore_exists failed at tenant={tenant}, year={target_year}).")
            continue
            
        # üéØ FIX 2C: ‡∏™‡πà‡∏á target_year ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô NamedRetriever
        nr = NamedRetriever(
            doc_id=collection_name, 
            doc_type=doc_type_for_check, 
            enabler=enabler_for_check, 
            top_k=top_k, 
            final_k=final_k, 
            base_path=base_path,
            tenant=tenant, 
            year=target_year # <--- ‡∏™‡πà‡∏á‡∏Ñ‡πà‡∏≤‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
        )
        all_retrievers.append(nr)
        logger.info(f"üîç DEBUG: Successfully added retriever for collection '{collection_name}' (Year={target_year}).")

    final_filter_ids = doc_ids
    if doc_ids:
        logger.info(f"‚úÖ Hard Filter Enabled: Using {len(doc_ids)} original 64-char UUIDs for filtering.")
    logger.info(f"üîç DEBUG: Final count of all_retrievers = {len(all_retrievers)}")

    if not all_retrievers:
        raise ValueError(f"No vectorstore collections found matching tenant={tenant}, year={year}, doc_types={doc_types} and evidence_enabler={evidence_enabler}")
        
    mdr = MultiDocRetriever(retrievers_list=all_retrievers, k_per_doc=top_k, doc_ids_filter=final_filter_ids)
    return mdr
