"""
llm_data_utils.py
Robust LLM + RAG utilities for SEAM assessment (CLEAN FINAL VERSION)
"""

import logging

logger = logging.getLogger(__name__)
if not logger.handlers:
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")


import time
import json
import hashlib
import uuid
import re
from datetime import datetime
from typing import List, Dict, Any, Optional, Union, Callable, TypeVar, Set
import json5
from utils.enabler_keyword_map import ENABLER_KEYWORD_MAP, DEFAULT_KEYWORDS
from langchain.retrievers import EnsembleRetriever
from langchain_core.documents import Document
from langchain_community.retrievers import BM25Retriever # FIX: Import BM25 ‡∏à‡∏≤‡∏Å community
import os
from pydantic import ValidationError
import unicodedata

# --- ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° JSON Schema ---
try:
    from core.action_plan_schema import get_clean_action_plan_schema
    schema_json = json.dumps(get_clean_action_plan_schema(), ensure_ascii=False, indent=2)
except Exception as e:
    logger.error(f"Schema load failed: {e}")


# Optional: regex ‡πÅ‡∏ó‡∏ô re (‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤) ‚Äî ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡πá‡πÉ‡∏ä‡πâ re ‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤
try:
    import regex as re  # type: ignore
except ImportError:
    pass  # ‡πÉ‡∏ä‡πâ re ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ

# ===================================================================
# 1. Core Configuration (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô)
# ===================================================================
from config.global_vars import (
    INITIAL_TOP_K,
    MAX_EVAL_CONTEXT_LENGTH,
    DEFAULT_EMBED_BATCH_SIZE,
    RERANK_THRESHOLD,
    ANALYSIS_FINAL_K,
    ACTION_PLAN_STEP_MAX_WORDS,
    LLM_TEMPERATURE,
    MAX_ACTION_PLAN_TOKENS
)

# ===================================================================
# 2. Critical Utilities (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏à‡∏£‡∏¥‡∏á ‚Äî ‡πÑ‡∏°‡πà‡∏°‡∏µ fallback)
# ===================================================================
# üéØ FIX 1: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô Import ‡∏à‡∏≤‡∏Å _get_collection_name ‡πÑ‡∏õ‡πÄ‡∏õ‡πá‡∏ô get_doc_type_collection_key
from core.vectorstore import get_hf_embeddings
from utils.path_utils import (
    get_doc_type_collection_key, 
    _n, get_mapping_file_path, # <--- ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà‡∏à‡∏≤‡∏Å utils/path_utils
    get_vectorstore_collection_path,
    get_vectorstore_tenant_root_path,
    get_rubric_file_path
)
from core.json_extractor import (
    _robust_extract_json,
    _normalize_keys,
    _safe_int_parse,
    _extract_normalized_dict
)

# ===================================================================
# 3. Project Modules (‡∏ñ‡πâ‡∏≤‡∏´‡∏≤‡∏¢ ‚Üí ‡∏õ‡∏•‡πà‡∏≠‡∏¢‡πÉ‡∏´‡πâ error ‡∏ä‡∏±‡∏î ‡πÜ ‡πÑ‡∏õ‡πÄ‡∏•‡∏¢ ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏á‡∏µ‡∏¢‡∏ö)
# ===================================================================
from core.seam_prompts import (
    SYSTEM_ASSESSMENT_PROMPT,
    USER_ASSESSMENT_PROMPT,
    SYSTEM_ACTION_PLAN_PROMPT,
    ACTION_PLAN_PROMPT,
    SYSTEM_EVIDENCE_DESCRIPTION_PROMPT,
    EVIDENCE_DESCRIPTION_PROMPT,
    SYSTEM_LOW_LEVEL_PROMPT,
    USER_LOW_LEVEL_PROMPT,
    USER_LOW_LEVEL_PROMPT_TEMPLATE,
    USER_EVIDENCE_DESCRIPTION_TEMPLATE,
    EXCELLENCE_ADVICE_PROMPT, 
    SYSTEM_EXCELLENCE_PROMPT,
    SYSTEM_QUALITY_PROMPT,
    QUALITY_REFINEMENT_PROMPT
)

from core.vectorstore import VectorStoreManager, get_global_reranker, ChromaRetriever
from core.assessment_schema import CombinedAssessment, EvidenceSummary
from core.action_plan_schema import ActionPlanActions, ActionPlanResult

try:
    from core.assessment_schema import StatementAssessment
except ImportError:
    from pydantic import BaseModel
    class StatementAssessment(BaseModel):
        score: int = 0
        reason: str = ""

from langchain_core.documents import Document as LcDocument
# ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î Import ‡πÉ‡∏ô routers/llm_router.py

# ===================================================================
# 4. Constants
# ===================================================================
LOW_LEVEL_K: int = 3
_MOCK_FLAG = False
_MAX_LLM_RETRIES = 3

def set_mock_control_mode(enable: bool):
    global _MOCK_FLAG
    _MOCK_FLAG = bool(enable)
    logger.info(f"Mock control mode: {_MOCK_FLAG}")

def _create_where_filter(
    stable_doc_ids: Optional[Union[Set[str], List[str]]] = None,
    subject: Optional[str] = None,
    sub_topic: Optional[str] = None,
    year: Optional[Union[int, str]] = None,
    enabler: Optional[str] = None,
    tenant: Optional[str] = None,
    **kwargs
) -> Dict[str, Any]:
    """
    [PRODUCTION VERSION] ‡∏™‡∏£‡πâ‡∏≤‡∏á Filter ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ChromaDB ‡∏ó‡∏µ‡πà‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥
    ‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á Data Type Mismatch (Int/Str) ‡πÅ‡∏•‡∏∞‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Multi-tenant
    """
    filters: List[Dict[str, Any]] = []

    # --- 1. ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Stable Doc IDs (‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î) ---
    if stable_doc_ids:
        ids_list = [str(i).strip() for i in (stable_doc_ids if isinstance(stable_doc_ids, (list, set)) else [stable_doc_ids]) if i]
        if ids_list:
            if len(ids_list) == 1:
                # ‡∏Å‡∏£‡∏ì‡∏µ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ $and
                return {"stable_doc_uuid": ids_list[0]}
            else:
                return {"stable_doc_uuid": {"$in": ids_list}}

    # --- 2. ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Year (‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ Local Mac ‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠) ---
    if year is not None:
        year_str = str(year).strip()
        if year_str and year_str.lower() != "none":
            # üéØ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: ‡∏™‡πà‡∏á‡∏ó‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö Int ‡πÅ‡∏•‡∏∞ Str ‡∏´‡∏£‡∏∑‡∏≠‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô Int ‡∏ï‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà Peek ‡πÄ‡∏à‡∏≠‡πÉ‡∏ô Mac
            try:
                # ‡∏ñ‡πâ‡∏≤‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ‡∏™‡πà‡∏á‡πÅ‡∏ö‡∏ö Integer (ChromaDB Local ‡∏°‡∏±‡∏Å‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏õ‡πá‡∏ô Int)
                val_year = int(year_str)
                filters.append({"year": val_year})
            except (ValueError, TypeError):
                # ‡∏ñ‡πâ‡∏≤‡πÅ‡∏õ‡∏•‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á‡πÜ ‡πÉ‡∏´‡πâ‡∏™‡πà‡∏á‡πÅ‡∏ö‡∏ö String
                filters.append({"year": year_str})
    
    # --- 3. ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Tenant (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ç‡πâ‡∏≤‡∏°‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó) ---
    effective_tenant = tenant or kwargs.get("tenant")
    if effective_tenant and str(effective_tenant).strip():
        filters.append({"tenant": str(effective_tenant).strip()})

    # --- 4. ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Enabler (KM, IM, etc.) ---
    if enabler and str(enabler).strip():
        filters.append({"enabler": enabler.strip().upper()})

    # --- 5. ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Subject (‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÄ‡∏Å‡∏ì‡∏ë‡πå) ---
    if subject and str(subject).strip():
        filters.append({"subject": str(subject).strip()})

    # --- 6. ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Sub Topic ---
    if sub_topic and str(sub_topic).strip():
        filters.append({"sub_topic": str(sub_topic).strip()})

    # --- ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Filter ---
    if not filters:
        return {}

    # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢‡∏≠‡∏±‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏î‡πâ‡∏ß‡∏¢ $and
    return filters[0] if len(filters) == 1 else {"$and": filters}


def retrieve_context_for_endpoint(
    vectorstore_manager,
    query: str = "",
    tenant: Optional[str] = None,
    year: Optional[Union[int, str]] = None,
    stable_doc_ids: Optional[Set[str]] = None,
    doc_type: Optional[str] = None,
    enabler: Optional[str] = None,
    subject: Optional[str] = None,
    sub_topic: Optional[str] = None,
    k_to_retrieve: int = 150, 
    k_to_rerank: int = 30,    
    strict_filter: bool = False,
    **kwargs
) -> Dict[str, Any]:
    """
    [ULTIMATE REVISED] Retrieval for Search Endpoint
    - ‡∏¢‡∏∂‡∏î Metadata ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢ (No Pydantic Error)
    - ‡πÉ‡∏ä‡πâ Deterministic MD5 Hash ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Deduplication
    - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Anchor Chunks ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á Context ‡∏ó‡∏µ‡πà‡∏°‡∏±‡πà‡∏ô‡∏Ñ‡∏á
    """
    start_time = time.time()
    vsm = vectorstore_manager

    # 1. Resolve collection
    clean_doc_type = str(doc_type or "document").strip().lower()
    collection_name = get_doc_type_collection_key(doc_type=clean_doc_type, enabler=enabler)
    
    chroma = vsm._load_chroma_instance(collection_name)
    if not chroma:
        logger.error(f"‚ùå Collection {collection_name} not found.")
        return {"top_evidences": [], "aggregated_context": "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", "retrieval_time": 0}

    # 2. Create where_filter
    where_filter = _create_where_filter(
        stable_doc_ids=list(stable_doc_ids) if stable_doc_ids else None, 
        subject=subject, 
        sub_topic=sub_topic, 
        year=year
    )

    # Dictionary ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Deduplication (‡πÉ‡∏ä‡πâ MD5 ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏õ‡πá‡∏ô Key)
    unique_map: Dict[str, LcDocument] = {}

    # =====================================================
    # ‚öì 2.1 ANCHOR RETRIEVAL (‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå)
    # =====================================================
    if stable_doc_ids:
        # ‡∏î‡∏∂‡∏á‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏£‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        anchors = chroma.get(where=where_filter, limit=10) 
        if anchors and anchors.get('documents'):
            for i in range(len(anchors['documents'])):
                content = anchors['documents'][i]
                md = anchors['metadatas'][i]
                
                # Deterministic MD5 Hash
                c_hash = hashlib.md5(content.encode()).hexdigest()
                uid = md.get("chunk_uuid") or f"anchor-{c_hash}"
                
                if uid not in unique_map:
                    # ‡∏â‡∏µ‡∏î‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÉ‡∏´‡πâ Anchor ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ï‡∏¥‡∏î‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡∏ï‡πâ‡∏ô‡πÜ ‡∏´‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á
                    md["score"] = 0.5 
                    md["is_anchor"] = True
                    unique_map[uid] = LcDocument(page_content=content, metadata=md)

    # =====================================================
    # üîç 2.2 SEMANTIC SEARCH
    # =====================================================
    search_query = query if (query and query != "*" and len(query) > 2) else ""
    
    # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö‡∏à‡∏≤‡∏Å Vector DB
    if search_query:
        docs = chroma.similarity_search(search_query, k=k_to_retrieve, filter=where_filter)
    else:
        # Fallback ‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏°‡πà‡∏°‡∏µ Query ‡πÉ‡∏´‡πâ‡∏î‡∏∂‡∏á‡πÅ‡∏ö‡∏ö‡∏Å‡∏ß‡∏≤‡∏î
        docs = chroma.similarity_search("*", k=k_to_retrieve, filter=where_filter)

    for d in docs:
        c_hash = hashlib.md5(d.page_content.encode()).hexdigest()
        md = d.metadata or {}
        uid = md.get("chunk_uuid") or c_hash
        if uid not in unique_map:
            unique_map[uid] = d

    candidates = list(unique_map.values())

    # =====================================================
    # üöÄ 3. BATCH RERANKING
    # =====================================================
    final_scored_docs = []
    reranker = getattr(vsm, "reranker", None)
    
    if reranker and candidates and search_query:
        try:
            batch_size = 100 
            logger.info(f"üöÄ Reranking {len(candidates)} candidates in batches...")
            for i in range(0, len(candidates), batch_size):
                batch = candidates[i : i + batch_size]
                # ‡πÉ‡∏ä‡πâ Reranker ‡∏Ç‡∏≠‡∏á LangChain (Compressor)
                scored_batch = reranker.compress_documents(documents=batch, query=search_query)
                final_scored_docs.extend(scored_batch)
        except Exception as e:
            logger.error(f"‚ö†Ô∏è Rerank failed: {e}")
            final_scored_docs = candidates
    else:
        final_scored_docs = candidates

    # =====================================================
    # 4. SORTING & SCORE INJECTION (‡∏´‡∏±‡∏ß‡πÉ‡∏à‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ 0.0000)
    # =====================================================
    def get_score(d) -> float:
        m = d.metadata or {}
        # ‡∏î‡∏∂‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏à‡∏≤‡∏Å‡∏ó‡∏∂‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ
        s = m.get("relevance_score") or m.get("score") or m.get("rerank_score") or 0.0
        try: return float(s)
        except: return 0.0

    final_scored_docs.sort(key=get_score, reverse=True)

    # =====================================================
    # 5. RESPONSE BUILD
    # =====================================================
    top_evidences = []
    aggregated_parts = []
    
    for doc in final_scored_docs[:k_to_rerank]:
        md = doc.metadata or {}
        text = doc.page_content.strip()
        score = get_score(doc)
        
        # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏•‡∏∞‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏≤‡πÉ‡∏´‡πâ Robust
        p_val = md.get("page") or md.get("page_label") or md.get("page_number") or "N/A"
        source_name = md.get('source') or md.get('source_filename') or md.get('file_name') or 'Unknown'
        s_uuid = md.get("stable_doc_uuid") or md.get("doc_id") or ""
        
        # SYNC SCORE ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤ Metadata ‡∏ó‡∏∏‡∏Å‡∏ï‡∏±‡∏ß
        md["score"] = score
        md["relevance_score"] = score
        
        evidence_item = {
            "doc_id": str(s_uuid),
            "chunk_uuid": str(md.get("chunk_uuid") or ""),
            "source": source_name,
            "text": text,
            "page": str(p_val),
            "score": score,
            "pdca_tag": md.get("pdca_tag", "Other"),
            "metadata": md
        }
        
        top_evidences.append(evidence_item)
        aggregated_parts.append(f"[‡πÑ‡∏ü‡∏•‡πå: {source_name}, ‡∏´‡∏ô‡πâ‡∏≤: {p_val}] {text}")

    retrieval_time = round(time.time() - start_time, 3)
    logger.info(f"üèÅ Finished: {len(top_evidences)} chunks in {retrieval_time}s")

    return {
        "top_evidences": top_evidences,
        "aggregated_context": "\n\n---\n\n".join(aggregated_parts) if aggregated_parts else "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•",
        "retrieval_time": retrieval_time,
        "total_candidates": len(candidates)
    }

# ------------------------
# Retrieval: retrieve_context_with_filter (Revised)
# ------------------------
def retrieve_context_with_filter(
    query: Union[str, List[str]],
    doc_type: str,
    tenant: Optional[str] = None,
    year: Optional[Union[int, str]] = None,
    enabler: Optional[str] = None,
    subject: Optional[str] = None,
    vectorstore_manager: Optional[Any] = None,
    mapped_uuids: Optional[List[str]] = None,
    stable_doc_ids: Optional[List[str]] = None,
    priority_docs_input: Optional[List[Any]] = None,
    sequential_chunk_uuids: Optional[List[str]] = None,
    sub_id: Optional[str] = None,
    level: Optional[int] = None,
    get_previous_level_docs: Optional[Callable[[int, str], List[Any]]] = None,
    top_k: int = 150, 
) -> Dict[str, Any]:
    """
    [FIXED VERSION] ‡∏¢‡∏∂‡∏î‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ (No Error) 
    ‡πÅ‡∏ï‡πà‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏¢‡∏±‡∏î‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏•‡∏á Metadata ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Rerank 0.0000
    """
    start_time = time.time()
    manager = vectorstore_manager
    queries_to_run = [query] if isinstance(query, str) else list(query or [""])
    
    # 1. Resolve Collection & Filter
    # ‡πÉ‡∏ä‡πâ helper ‡∏à‡∏≤‡∏Å utils ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Ç‡∏≠‡∏á‡∏ä‡∏∑‡πà‡∏≠ collection
    collection_name = get_doc_type_collection_key(doc_type, enabler or "KM")
    
    target_ids = set()
    if stable_doc_ids: target_ids.update([str(i) for i in stable_doc_ids])
    if mapped_uuids: target_ids.update([str(i) for i in mapped_uuids])
    if sequential_chunk_uuids: target_ids.update([str(i) for i in sequential_chunk_uuids])
    
    where_filter = _create_where_filter(
        stable_doc_ids=list(target_ids) if target_ids else None,
        subject=subject,
        year=year
    )

    # 2. Collect Chunks
    all_source_chunks = []

    # 2.1 Priority Docs
    if priority_docs_input:
        for doc in priority_docs_input:
            if not doc: continue
            if isinstance(doc, dict):
                pc = doc.get('page_content') or doc.get('text') or ''
                meta = doc.get('metadata') or {}
                if pc.strip():
                    all_source_chunks.append(LcDocument(page_content=pc, metadata=meta))
            elif hasattr(doc, 'page_content'):
                all_source_chunks.append(doc)

    # 2.2 Vector Retrieval
    try:
        full_retriever = manager.get_retriever(collection_name=collection_name)
        base_retriever = getattr(full_retriever, "base_retriever", full_retriever)
        
        search_kwargs = {"k": top_k}
        if where_filter: 
            search_kwargs["where"] = where_filter

        for q in queries_to_run:
            if not q: continue
            docs = base_retriever.invoke(q, config={"configurable": {"search_kwargs": search_kwargs}})
            if docs: all_source_chunks.extend(docs)
    except Exception as e:
        logger.error(f"Retrieval error: {e}")

    # 3. Deduplicate (‡πÉ‡∏ä‡πâ Hash ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥)
    unique_map: Dict[str, LcDocument] = {}
    for doc in all_source_chunks:
        if not doc or not doc.page_content.strip(): continue
        md = doc.metadata or {}
        # ‡πÉ‡∏ä‡πâ hashlib ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ ID ‡∏ó‡∏µ‡πà‡∏Ñ‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô‡∏ó‡∏∏‡∏Å‡∏£‡∏≠‡∏ö
        c_hash = hashlib.md5(doc.page_content.encode()).hexdigest()
        uid = str(md.get("chunk_uuid") or f"{md.get('stable_doc_uuid', 'unknown')}-{c_hash}")
        
        if uid not in unique_map:
            unique_map[uid] = doc

    candidates = list(unique_map.values())

    # 4. Batch Reranking
    final_scored_docs = []
    batch_size = 100 
    reranker = getattr(manager, "reranker", None)

    if reranker and candidates:
        main_query = queries_to_run[0]
        for i in range(0, len(candidates), batch_size):
            batch = candidates[i : i + batch_size]
            try:
                # üìå ‡∏Å‡∏∏‡∏ç‡πÅ‡∏à‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: Reranker ‡∏à‡∏∞‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÉ‡∏ô metadata
                scored_batch = reranker.compress_documents(batch, main_query)
                final_scored_docs.extend(scored_batch)
            except Exception as e:
                logger.error(f"Rerank Error: {e}")
                final_scored_docs.extend(batch)
    else:
        final_scored_docs = candidates

    # 5. Sorting & Score Injection (‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ 0.0000)
    def get_score(d) -> float:
        m = d.metadata or {}
        # ‡πÑ‡∏•‡πà‡∏´‡∏≤‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ
        s = m.get("relevance_score") or m.get("score") or m.get("rerank_score") or 0.0
        try: return float(s)
        except: return 0.0

    final_scored_docs.sort(key=get_score, reverse=True)

    # 6. Final Formatting (‡∏Ñ‡∏±‡∏î‡πÄ‡∏•‡∏∑‡∏≠‡∏Å K ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å)
    top_evidences = []
    aggregated_parts = []
    final_k = ANALYSIS_FINAL_K # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å config

    for doc in final_scored_docs[:final_k]:
        score = get_score(doc)
        # ‡∏Å‡∏£‡∏≠‡∏á Threshold ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡∏±‡πâ‡∏á‡πÑ‡∏ß‡πâ‡πÉ‡∏ô .env
        if score < RERANK_THRESHOLD and RERANK_THRESHOLD > 0:
            continue

        md = doc.metadata or {}
        text = doc.page_content.strip()
        
        # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö ingest.py
        page = str(md.get("page_label") or md.get("page_number") or md.get("page") or "N/A")
        source = md.get("source") or md.get("source_filename") or "Unknown"
        pdca = md.get("pdca_tag", "Other")

        # üéØ SYNC SCORE ‡πÄ‡∏Ç‡πâ‡∏≤ Metadata (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Loop ‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡πÅ‡∏•‡πâ‡∏ß‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ 0)
        md["score"] = score
        md["relevance_score"] = score
        md["rerank_score"] = score

        top_evidences.append({
            "doc_id": str(md.get("stable_doc_uuid") or md.get("doc_id") or ""),
            "chunk_uuid": str(md.get("chunk_uuid") or ""),
            "source": source,
            "text": text,
            "page": page,
            "pdca_tag": pdca,
            "score": score,
            "metadata": md
        })
        aggregated_parts.append(f"[{pdca}] [‡πÑ‡∏ü‡∏•‡πå: {source} ‡∏´‡∏ô‡πâ‡∏≤: {page}] {text}")

    return {
        "top_evidences": top_evidences,
        "aggregated_context": "\n\n---\n\n".join(aggregated_parts) if aggregated_parts else "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô",
        "retrieval_time": round(time.time() - start_time, 3),
        "total_candidates": len(candidates)
    }

# =====================================================================
# üõ† Helper: check_rubric_readiness (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏á‡∏µ‡∏¢‡∏ö‡∏•‡∏á)
# =====================================================================
def is_rubric_ready(tenant: str) -> bool:
    """ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏Ç‡∏≠‡∏á seam collection ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏û‡πà‡∏ô Warning ‡∏Å‡∏ß‡∏ô‡πÉ‡∏à """
    if not tenant:
        return False
    
    tenant_vs_root = get_vectorstore_tenant_root_path(tenant)
    chroma_path = os.path.join(tenant_vs_root, "seam")
    
    # ‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ True/False ‡πÄ‡∏á‡∏µ‡∏¢‡∏ö‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏î‡∏∂‡∏á Rubric ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    return os.path.exists(chroma_path)


# =====================================================================
# üöÄ Ultimate Version: retrieve_context_with_rubric (FIXED & REVISED)
# =====================================================================
def retrieve_context_with_rubric(
    vectorstore_manager,
    query: str,
    doc_type: str,
    enabler: Optional[str] = None,
    stable_doc_ids: Optional[Set[str]] = None,
    tenant: Optional[str] = None,
    year: Optional[Union[int, str]] = None,
    subject: Optional[str] = None,
    rubric_vectorstore_name: str = "seam", 
    top_k: int = 150,         # üöÄ ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏î‡∏∂‡∏á‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Reranker ‡∏°‡∏µ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô
    rubric_top_k: int = 15,  
    strict_filter: bool = True,
    k_to_rerank: int = 30    
) -> Dict[str, Any]:
    """
    [REVISED VERSION] ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Rubric + Evidence Retrieval ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏£‡∏∞‡∏ö‡∏ö Batch Reranking 
    ‡πÅ‡∏•‡∏∞ Content-Based Deduplication ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô Level 5 ‡∏´‡∏≤‡∏¢
    """
    start_time = time.time()
    vsm = vectorstore_manager

    # --- 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏™‡∏•‡∏±‡∏ö Collection ---
    if hasattr(vsm, 'doc_type') and vsm.doc_type != doc_type:
        logger.info(f"üîÑ Switching VSM doc_type to: {doc_type}")
        vsm.close()
        vsm.__init__(tenant=tenant, year=year, doc_type=doc_type, enabler=enabler)

    evidence_collection = get_doc_type_collection_key(doc_type, enabler or "KM")
    
    rubric_results = []
    # ‡πÉ‡∏ä‡πâ Dict ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥ Deduplication ‡∏î‡πâ‡∏ß‡∏¢ Content Hash ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ã‡πâ‡∏≥‡πÅ‡∏ï‡πà ID ‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô
    unique_evidence_map: Dict[str, LcDocument] = {}

    # --- 2. ‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á Rubrics (‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô) ---
    try:
        rubric_chroma = vsm._load_chroma_instance(rubric_vectorstore_name)
        if rubric_chroma:
            rubric_query = f"‡πÄ‡∏Å‡∏ì‡∏ë‡πå SE-AM {enabler} {subject or ''}: {query}"
            r_docs = rubric_chroma.similarity_search(rubric_query, k=rubric_top_k)
            for rd in r_docs:
                rubric_results.append({
                    "text": rd.page_content, 
                    "metadata": rd.metadata, 
                    "is_rubric": True
                })
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Rubric Retrieval Error: {e}")

    # --- 3. ‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á Evidence (‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô) ---
    try:
        evidence_chroma = vsm._load_chroma_instance(evidence_collection)
        if not evidence_chroma:
            return {"top_evidences": [], "rubric_context": rubric_results, "retrieval_time": 0}

        where_filter = None
        if stable_doc_ids:
            ids_list = [str(i).strip().lower() for i in stable_doc_ids if i]
            where_filter = {"stable_doc_uuid": ids_list[0]} if len(ids_list) == 1 else {"stable_doc_uuid": {"$in": ids_list}}
            
            # ‚öì 3.1 Fetch Anchor Chunks (‡∏™‡πà‡∏ß‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå ‡πÄ‡∏ä‡πà‡∏ô ‡∏´‡∏ô‡πâ‡∏≤ 1-5)
            anchors = evidence_chroma.get(where=where_filter, limit=10)
            if anchors and anchors.get('documents'):
                for i in range(len(anchors['documents'])):
                    content = anchors['documents'][i]
                    md = dict(anchors['metadatas'][i] or {}) # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô metadata ‡πÄ‡∏õ‡πá‡∏ô None
                    
                    # üéØ ‡πÅ‡∏Å‡πâ‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà 1: ‡πÉ‡∏ä‡πâ MD5 ‡πÅ‡∏ó‡∏ô hash() ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ ID ‡∏Ñ‡∏á‡∏ó‡∏µ‡πà (Deterministic)
                    content_hash = hashlib.md5(content.encode()).hexdigest()
                    uid = str(md.get("chunk_uuid") or f"anchor-{content_hash}")
                    
                    if uid not in unique_evidence_map:
                        # üéØ ‡πÅ‡∏Å‡πâ‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà 2: ‡∏â‡∏µ‡∏î‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏•‡∏á‡πÉ‡∏ô metadata ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á (Safe Injection)
                        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÉ‡∏´‡πâ Anchor ‡∏°‡∏µ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á (0.95) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏´‡∏•‡∏±‡∏Å
                        md["score"] = 0.95
                        md["relevance_score"] = 0.95
                        md["is_anchor"] = True
                        
                        unique_evidence_map[uid] = LcDocument(
                            page_content=content,
                            metadata=md
                        )

        # üîç 3.2 Semantic Search (‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢)
        search_results = evidence_chroma.similarity_search(query, k=top_k, filter=where_filter)
        for d in search_results:
            content_hash = str(hash(d.page_content))
            uid = d.metadata.get("chunk_uuid") or content_hash
            if uid not in unique_evidence_map:
                unique_evidence_map[uid] = d

        candidates = list(unique_evidence_map.values())

        # --- 4. BATCH RERANKING (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô OOM ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥) ---
        evidence_results = []
        reranker = get_global_reranker()
        
        if reranker and candidates and query:
            try:
                batch_size = 100 # üöÄ ‡πÅ‡∏ö‡πà‡∏á Batch ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏Ç‡∏≠‡∏á VRAM
                scored_candidates = []
                
                logger.info(f"üöÄ Batch Reranking {len(candidates)} chunks...")
                for i in range(0, len(candidates), batch_size):
                    batch = candidates[i : i + batch_size]
                    reranked_batch = reranker.compress_documents(documents=batch, query=query)
                    scored_candidates.extend(reranked_batch)
                
                # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å Batch
                scored_candidates = sorted(
                    scored_candidates, 
                    key=lambda x: getattr(x, "relevance_score", 0), 
                    reverse=True
                )
                
                for r in scored_candidates[:k_to_rerank]:
                    doc = r.document if hasattr(r, "document") else r
                    m = doc.metadata or {}
                    score = getattr(r, "relevance_score", 0.0)
                    
                    # üéØ Sync Score ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤ Metadata ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ
                    m["rerank_score"] = score
                    m["score"] = score
                    
                    evidence_results.append({
                        "text": doc.page_content,
                        "source_filename": m.get("source_filename") or m.get("source") or "Evidence",
                        "page_label": str(m.get("page_label") or m.get("page_number") or m.get("page") or "N/A"),
                        "doc_id": m.get("stable_doc_uuid") or m.get("doc_id"),
                        "chunk_uuid": m.get("chunk_uuid") or str(uuid.uuid4()),
                        "pdca_tag": m.get("pdca_tag") or "Content",
                        "rerank_score": score,
                        "is_evidence": True,
                        "metadata": m
                    })
            except Exception as e:
                logger.error(f"‚ö†Ô∏è Rerank failed: {e}")
                candidates = candidates[:k_to_rerank] # Fallback
        
        # ‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏°‡πà‡∏°‡∏µ Reranker ‡∏´‡∏£‡∏∑‡∏≠ Error ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏õ‡∏Å‡∏ï‡∏¥
        if not evidence_results:
            for d in candidates[:k_to_rerank]:
                m = d.metadata or {}
                evidence_results.append({
                    "text": d.page_content,
                    "source_filename": m.get("source_filename") or m.get("source") or "Evidence",
                    "page_label": str(m.get("page_label") or m.get("page_number") or m.get("page") or "N/A"),
                    "doc_id": m.get("stable_doc_uuid") or m.get("doc_id"),
                    "chunk_uuid": m.get("chunk_uuid") or str(uuid.uuid4()),
                    "pdca_tag": m.get("pdca_tag") or "Content",
                    "rerank_score": 0.0,
                    "is_evidence": True,
                    "metadata": m
                })

    except Exception as e:
        logger.error(f"‚ùå Evidence Retrieval Error: {e}", exc_info=True)

    retrieval_time = round(time.time() - start_time, 3)
    logger.info(f"‚úÖ Success: Retrieved {len(evidence_results)} evidence chunks in {retrieval_time}s")

    return {
        "top_evidences": evidence_results,
        "rubric_context": rubric_results,
        "retrieval_time": retrieval_time,
        "used_chunk_uuids": [e["chunk_uuid"] for e in evidence_results if e.get("chunk_uuid")]
    }

# ========================
#  retrieve_context_by_doc_ids (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö hydration ‡πÉ‡∏ô router)
# ========================
def retrieve_context_by_doc_ids(
    doc_uuids: List[str],
    doc_type: str,
    enabler: Optional[str] = None,
    vectorstore_manager = None,
    limit: int = 100, # üöÄ ‡πÄ‡∏û‡∏¥‡πà‡∏° limit ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô
    tenant: Optional[str] = None,
    year: Optional[Union[int, str]] = None,
) -> Dict[str, Any]:
    """
    [REVISED] ‡∏î‡∏∂‡∏á chunks ‡∏à‡∏≤‡∏Å stable_doc_uuid ‡∏´‡∏•‡∏≤‡∏¢‡∏ï‡∏±‡∏ß (‡πÉ‡∏ä‡πâ‡∏ï‡∏≠‡∏ô hydration sources)
    ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏∏‡∏õ‡∏µ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤ Collection ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏Å‡∏©‡∏≤ Metadata ‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô
    """
    start_time = time.time()
    vsm = vectorstore_manager or VectorStoreManager(tenant=tenant, year=year)
    
    # Resolve collection name ‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏õ‡∏µ‡πÅ‡∏•‡∏∞ enabler
    collection_name = get_doc_type_collection_key(doc_type=doc_type, enabler=enabler)

    chroma = vsm._load_chroma_instance(collection_name)
    if not chroma:
        logger.error(f"‚ùå Collection {collection_name} not found for hydration")
        return {"top_evidences": []}

    if not doc_uuids:
        return {"top_evidences": []}

    logger.info(f"üíß Hydration ‚Üí {len(doc_uuids)} doc IDs from {collection_name}")

    try:
        # ‡πÉ‡∏ä‡πâ Metadata filter ‡∏î‡∏∂‡∏á chunks ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏•‡∏¥‡∏™‡∏ï‡πå ID ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
        # ‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô list comprehension ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏ß‡∏£‡πå‡∏Ç‡∏≠‡∏á Type
        ids_to_query = [str(u) for u in doc_uuids if u]
        
        results = chroma._collection.get(
            where={"stable_doc_uuid": {"$in": ids_to_query}},
            limit=limit,
            include=["documents", "metadatas"]
        )
    except Exception as e:
        logger.error(f"‚ö†Ô∏è Hydration query failed: {e}")
        return {"top_evidences": []}

    evidences = []
    # üéØ ‡πÉ‡∏ä‡πâ Content-based Deduplication ‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô‡∏Å‡∏±‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ã‡πâ‡∏≥
    seen_contents = set()

    for doc_content, meta in zip(results.get("documents", []), results.get("metadatas", [])):
        if not doc_content or not doc_content.strip():
            continue
            
        content_hash = str(hash(doc_content))
        if content_hash in seen_contents:
            continue
        seen_contents.add(content_hash)

        p_val = meta.get("page_label") or meta.get("page_number") or meta.get("page") or "N/A"
        
        # üéØ Sync Score ‡∏´‡∏•‡∏≠‡∏Å (‡πÄ‡∏û‡∏£‡∏≤‡∏∞ Hydration ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ú‡πà‡∏≤‡∏ô Rerank ‡πÅ‡∏ï‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Logic ‡∏≠‡∏∑‡πà‡∏ô‡πÑ‡∏°‡πà‡∏û‡∏±‡∏á)
        score = meta.get("score") or meta.get("rerank_score") or 0.85

        evidences.append({
            "doc_id": meta.get("stable_doc_uuid") or meta.get("doc_id"),
            "chunk_uuid": meta.get("chunk_uuid"),
            "source": meta.get("source") or meta.get("source_filename") or "Unknown",
            "page": str(p_val),
            "text": doc_content.strip(),
            "pdca_tag": meta.get("pdca_tag", "Other"),
            "score": score,
            "metadata": meta # ‡πÅ‡∏ô‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏ï‡πá‡∏°‡πÑ‡∏ß‡πâ‡πÄ‡∏™‡∏°‡∏≠
        })

    logger.info(f"‚úÖ Hydration success: {len(evidences)} chunks from {len(doc_uuids)} docs")
    return {
        "top_evidences": evidences,
        "retrieval_time": round(time.time() - start_time, 3)
    }


def retrieve_context_for_low_levels(query: str, doc_type: str, enabler: Optional[str]=None,
                                 vectorstore_manager: Optional['VectorStoreManager']=None,
                                 top_k: int=LOW_LEVEL_K, initial_k: int=INITIAL_TOP_K,
                                 # üü¢ NEW: ‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠ arguments
                                 mapped_uuids: Optional[List[str]]=None,
                                 priority_docs_input: Optional[List[Any]] = None,
                                 sequential_chunk_uuids: Optional[List[str]] = None, 
                                 sub_id: Optional[str]=None, level: Optional[int]=None) -> Dict[str, Any]:
    """
    Retrieves a small, focused context for low levels (L1, L2) using a reduced k (LOW_LEVEL_K).
    """
    # ‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å ‡πÅ‡∏ï‡πà‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏ä‡πâ k ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
    return retrieve_context_with_filter(
        query=query,
        doc_type=doc_type,
        enabler=enabler,
        vectorstore_manager=vectorstore_manager,
        top_k=LOW_LEVEL_K,
        initial_k=initial_k,
        mapped_uuids=mapped_uuids,
        priority_docs_input=priority_docs_input,
        sequential_chunk_uuids=sequential_chunk_uuids, 
        sub_id=sub_id,
        level=level
    )

# ------------------------
# LLM fetcher
# ------------------------
def _fetch_llm_response(
    system_prompt: str, 
    user_prompt: str, 
    max_retries: int = 3,
    llm_executor: Any = None 
) -> str:
    """
    Ultimate Robust LLM Fetcher v2026.1.19-final
    - ‡∏Ñ‡∏∑‡∏ô STRING ‡πÄ‡∏™‡∏°‡∏≠ ‡πÑ‡∏°‡πà‡πÄ‡∏Ñ‡∏¢‡∏Ñ‡∏∑‡∏ô None
    - ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô NoneType.strip() ‡∏ó‡∏∏‡∏Å‡∏à‡∏∏‡∏î
    - Log ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠ debug
    """

    if llm_executor is None and not _MOCK_FLAG:
        raise ConnectionError("LLM instance not initialized.")

    # Enforced prompt (‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏µ‡∏Å‡∏ô‡∏¥‡∏î)
    enforced_system_prompt = (system_prompt or "").strip() + (
        "\n\n### STRICT OUTPUT RULES - FOLLOW EXACTLY ###\n"
        "1. Respond with ONLY valid JSON object. No other text.\n"
        "2. Start with '{' and end with '}'.\n"
        "3. No markdown, no explanations, no prefixes.\n"
        "4. If no evidence: {\"score\": 0, \"reason\": \"No evidence\", \"is_passed\": false}"
    )

    messages = [
        {"role": "system", "content": enforced_system_prompt},
        {"role": "user",   "content": (user_prompt or "").strip()}
    ]

    for attempt in range(1, max_retries + 1):
        try:
            if _MOCK_FLAG:
                mock = '{"score": 1, "reason": "Mock active", "is_passed": true}'
                logger.critical(f"LLM RAW (MOCK): {mock}")
                return mock

            # LLM CALL
            response = llm_executor.invoke(messages, config={"temperature": 0.0})

            # SAFE EXTRACTION - ‡∏£‡∏ß‡∏° patch ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
            raw_text = ""
            if response is None:
                logger.warning("Response object is None")
            elif hasattr(response, "content"):
                raw_text = str(response.content or "")
            elif hasattr(response, "text"):
                raw_text = str(response.text or "")
            elif isinstance(response, str):
                raw_text = response
            else:
                raw_text = str(response or "")

            # Log raw ‡∏Å‡πà‡∏≠‡∏ô clean
            preview = (raw_text[:1000] + "...") if len(raw_text) > 1000 else raw_text
            logger.critical(f"LLM RAW RESPONSE (attempt {attempt}): {preview}")

            # Clean - ‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô
            raw_text_stripped = (raw_text or "").strip()

            # ‡∏´‡∏≤ JSON block
            json_match = re.search(r'\{[\s\S]*?\}', raw_text_stripped, re.DOTALL)
            if json_match:
                extracted = json_match.group(0)
                try:
                    json.loads(extracted)  # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡πà‡∏≠‡∏ô
                    return extracted
                except json.JSONDecodeError:
                    logger.warning(f"Extracted not valid JSON: {extracted[:120]}...")

            # Fallback: ‡∏•‡∏≠‡∏á parse ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
            try:
                parsed = json.loads(raw_text_stripped)
                return json.dumps(parsed, ensure_ascii=False)
            except json.JSONDecodeError:
                pass

            # Ultimate safe return - ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö string ‡πÄ‡∏™‡∏°‡∏≠ (‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥)
            final_return = str(raw_text_stripped or "").strip()
            logger.debug(f"No valid JSON ‚Üí returning cleaned string (len={len(final_return)})")
            return final_return

        except Exception as e:
            logger.error(f"Attempt {attempt} failed: {str(e)}", exc_info=True)
            if attempt < max_retries:
                time.sleep(2 ** attempt)
            else:
                logger.critical("All retries failed")
                return '{"score": 0, "reason": "LLM failed after retries", "is_passed": false}'

    return '{"score": 0, "reason": "Unknown error", "is_passed": false}'

# ------------------------
# Evaluation
# ------------------------
T = TypeVar("T", bound=BaseModel)

def _check_and_handle_empty_context(context: str, sub_id: str, level: int) -> Optional[Dict[str, Any]]:
    """
    Returns Failure result if context is empty or contains known error strings.
    Auto-fail with PDCA keys all set to 0.
    """
    if not context or "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á" in context or "ERROR:" in context.upper():
        logger.warning(f"Auto-FAIL L{level} for {sub_id}: Empty or Error Context detected from RAG.")
        context_preview = context.strip()[:100].replace("\n", " ") if context else "Empty Context"
        return {
            "score": 0,
            "reason": f"‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏î‡πâ‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á (Context: {context_preview}).",
            "is_passed": False,
            "P_Plan_Score": 0,
            "D_Do_Score": 0,
            "C_Check_Score": 0,
            "A_Act_Score": 0,
        }
    return None

def _get_context_for_level(
    context: str,
    level: int,
    chunks: list = None,  # ‡∏™‡πà‡∏á chunks ‡∏°‡∏≤‡∏à‡∏≤‡∏Å _run_single_assessment
    max_chars_l1_l2: int = 15000,  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 6000
    max_chars_l3_up: int = 10000,
    max_chunks_l1_l2: int = 40,
    max_chunks_l3_up: int = 25
) -> str:
    if not context:
        return "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô"

    # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ chunks ‚Üí ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏° score ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å top
    if chunks:
        sorted_chunks = sorted(
            chunks,
            key=lambda c: float(c.get('rerank_score', 0) or c.get('score', 0)),
            reverse=True
        )
        max_chunks = max_chunks_l1_l2 if level <= 2 else max_chunks_l3_up
        selected = sorted_chunks[:max_chunks]

        parts = []
        for i, c in enumerate(selected, 1):
            score = c.get('rerank_score', 'N/A')
            source = c.get('source', '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏')
            text = c.get('text', '').strip()
            if text:
                parts.append(f"[Chunk {i} | Score: {score} | {source}]\n{text}\n{'-'*80}\n")

        final = "".join(parts)
        max_chars = max_chars_l1_l2 if level <= 2 else max_chars_l3_up
        if len(final) > max_chars:
            final = final[:max_chars] + "\n... (‡∏ï‡∏±‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°)"
        return final

    # fallback ‡πÄ‡∏î‡∏¥‡∏°
    max_chars = max_chars_l1_l2 if level <= 2 else max_chars_l3_up
    return context[:max_chars] + ("... [truncated]" if len(context) > max_chars else "")

def evaluate_with_llm(
    context: str, 
    sub_criteria_name: str, 
    level: int, 
    statement_text: str, 
    sub_id: str, 
    llm_executor: Any = None, 
    required_phases: List[str] = None,
    specific_contextual_rule: str = "‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô",
    ai_confidence: str = "MEDIUM",
    confidence_reason: str = "N/A",
    pdca_context: str = "", # <--- [ADD] ‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏¢‡∏Å‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà P-D-C-A
    **kwargs
) -> Dict[str, Any]:
    """
    [REVISED v2026.3.5 ‚Äî PDCA Block Enabled]
    - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏â‡∏µ‡∏î pdca_context ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏Å‡∏±‡∏î‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô
    - ‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏£‡∏∞‡∏ö‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Parse JSON ‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Multi-Enabler
    """
    logger = logging.getLogger(__name__)

    # 1. Safe casting + defaults
    ctx_raw = str(context or "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô")
    pdca_ctx = str(pdca_context or "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏¢‡∏Å‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà (‡πÇ‡∏õ‡∏£‡∏î‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏à‡∏≤‡∏Å Full Context)")
    s_name = str(sub_criteria_name or "N/A")
    sid = str(sub_id or "N/A")
    s_text = str(statement_text or "N/A")
    
    # Enabler info
    enabler_full_name = str(kwargs.get("enabler_full_name", "Unknown Enabler"))
    enabler_code = str(kwargs.get("enabler_code", "UNK"))
    
    logger.info(f"[EVAL START] Enabler: {enabler_full_name} ({enabler_code}) | Sub: {sid} | L{level}")

    # ‡∏î‡∏∂‡∏á Context ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏ï‡∏≤‡∏°‡∏£‡∏∞‡∏î‡∏±‡∏ö (Slice ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î)
    context_to_send_eval = _get_context_for_level(ctx_raw, level) or ""
    
    # 2. Safe phases
    phases_str = ", ".join(str(p).strip() for p in (required_phases or [])) if required_phases else "P, D, C, A"

    # 3. Baseline clean
    baseline_raw = kwargs.get("baseline_summary")
    baseline_summary = str(baseline_raw or "").strip()

    try:
        # Build prompt with PDCA Context support
        # ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡πÉ‡∏ô USER_ASSESSMENT_PROMPT ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ placeholder {pdca_context}
        full_prompt = USER_ASSESSMENT_PROMPT.format(
            sub_criteria_name=s_name,
            sub_id=sid,
            level=int(level),
            statement_text=s_text,
            context=context_to_send_eval[:28000], # ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ PDCA Block
            pdca_context=pdca_ctx[:8000],         # [CRITICAL] ‡∏â‡∏µ‡∏î‡∏ö‡∏•‡πá‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏¢‡∏Å‡∏´‡∏°‡∏ß‡∏î
            required_phases=phases_str,
            specific_contextual_rule=str(specific_contextual_rule or "‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå"),
            ai_confidence=str(ai_confidence or "MEDIUM"),
            confidence_reason=str(confidence_reason or "N/A"),
            enabler_full_name=enabler_full_name,
            enabler_code=enabler_code
        )

        if baseline_summary:
            full_prompt += f"\n\n--- BASELINE DATA (‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤) ---\n{baseline_summary}"

        # 4. LLM call with guard
        if llm_executor is None:
            raise ValueError("No LLM executor provided")

        raw_response = _fetch_llm_response(None, full_prompt, llm_executor=llm_executor)
        raw_response = str(raw_response or "").strip()

        # 5. Parse with fallback
        parsed = _robust_extract_json(raw_response)
        if not parsed or not isinstance(parsed, dict):
            logger.warning(f"[JSON PARSE FAIL] Sub {sid} L{level} - Using heuristic fallback")
            parsed = _heuristic_fallback_parse(raw_response)

        # 6. Build final result object
        result = _build_audit_result_object(
            parsed, raw_response, context_to_send_eval, ai_confidence, 
            level=level, sub_id=sid, enabler_full_name=enabler_full_name, enabler_code=enabler_code
        )
        return result

    except Exception as e:
        logger.error(f"üõë Evaluation Error Enabler:{enabler_code} Sub:{sid} L{level}: {str(e)}", exc_info=True)
        return _create_fallback_error(sid, level, e, context_to_send_eval, enabler_full_name, enabler_code)


def evaluate_with_llm_low_level(
    context: str,
    sub_criteria_name: str,
    level: int,
    statement_text: str,
    sub_id: str,
    llm_executor: Any = None,
    required_phases: List[str] = None,
    specific_contextual_rule: str = "‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô",
    ai_confidence: str = "MEDIUM",
    pdca_context: str = "", # <--- [ADD]
    **kwargs
) -> Dict[str, Any]:
    """
    [REVISED v2026.3.5 ‚Äî Low Level PDCA Enabled]
    - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö pdca_context ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏∞‡∏î‡∏±‡∏ö L1-L2
    - ‡πÄ‡∏ô‡πâ‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà '‡πÅ‡∏ú‡∏ô' ‡πÅ‡∏•‡∏∞ '‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°'
    """
    logger = logging.getLogger(__name__)

    # 1. Safe casting
    ctx = str(context or "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô")
    pdca_ctx = str(pdca_context or "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏¢‡∏Å‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà")
    s_name = str(sub_criteria_name or "N/A")
    s_text = str(statement_text or "N/A")
    sid = str(sub_id or "N/A")
    
    # Enabler info
    enabler_full_name = str(kwargs.get("enabler_full_name", "Unknown Enabler"))
    enabler_code = str(kwargs.get("enabler_code", "UNK"))
    
    logger.info(f"[LOW EVAL START] Enabler: {enabler_full_name} ({enabler_code}) | Sub: {sid} | L{level}")

    plan_kws = str(kwargs.get("plan_keywords") or "‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢, ‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô, ‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå")
    baseline_summary = str(kwargs.get("baseline_summary") or "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤").strip()
    conf_reason = str(kwargs.get("confidence_reason") or "‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ï‡∏≤‡∏°‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏á‡∏≤‡∏ô")
    
    phases_str = ", ".join(str(p) for p in (required_phases or [])) if required_phases else "P, D"

    try:
        full_prompt = USER_LOW_LEVEL_PROMPT.format(
            sub_id=sid,
            sub_criteria_name=s_name,
            level=int(level),
            statement_text=s_text,
            context=ctx[:28000],
            pdca_context=pdca_ctx[:8000], # [CRITICAL]
            required_phases=phases_str,
            plan_keywords=plan_kws,
            baseline_summary=baseline_summary,
            specific_contextual_rule=str(specific_contextual_rule or "‡∏ï‡∏£‡∏ß‡∏à‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå"),
            ai_confidence=str(ai_confidence or "MEDIUM"),
            confidence_reason=conf_reason,
            enabler_full_name=enabler_full_name,
            enabler_code=enabler_code
        )

        if llm_executor is None:
            raise ValueError("No LLM executor provided")

        raw_response = _fetch_llm_response(None, full_prompt, llm_executor=llm_executor)
        raw_response = str(raw_response or "").strip()

        parsed = _robust_extract_json(raw_response)
        if not parsed or not isinstance(parsed, dict):
            logger.warning(f"[LOW JSON PARSE FAIL] Sub {sid} L{level} - Using heuristic fallback")
            parsed = _heuristic_fallback_parse(raw_response)

        result = _build_audit_result_object(
            parsed, raw_response, ctx, ai_confidence, 
            level=level, sub_id=sid, enabler_full_name=enabler_full_name, enabler_code=enabler_code
        )
        return result

    except Exception as e:
        logger.error(f"üõë Low-Level Eval Error Enabler:{enabler_code} Sub:{sid} L{level}: {str(e)}", exc_info=True)
        return _create_fallback_error(sid, level, e, ctx, enabler_full_name, enabler_code)
    
def _build_audit_result_object(parsed: Dict, raw_response: str, context: str, confidence: str, **kwargs) -> Dict[str, Any]:
    """
    [ULTIMATE-SYNC v2026.1.22] ‚Äî Multi-Enabler + Zero-Error
    - ‡πÄ‡∏û‡∏¥‡πà‡∏° enabler_full_name & enabler_code ‡πÉ‡∏ô result
    - Robust string handling (str(val or "") ‡∏Å‡πà‡∏≠‡∏ô strip)
    - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö key ‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏à‡∏≤‡∏Å LLM
    """
    level = kwargs.get('level', 1)
    sub_id = kwargs.get('sub_id', 'Unknown')
    enabler_full_name = kwargs.get('enabler_full_name', 'Unknown Enabler')
    enabler_code = kwargs.get('enabler_code', 'UNK')

    def clean_score(val, default=0.0):
        if val is None: return default
        try:
            return float(val)
        except (ValueError, TypeError):
            return default

    if not isinstance(parsed, dict):
        parsed = {}

    score = clean_score(parsed.get("score"))
    is_passed = parsed.get("is_passed")
    if is_passed is None:
        is_passed = score >= 0.7 if level <= 2 else score >= 1.0

    # Robust extraction (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô NoneType.strip)
    ext_p = str(parsed.get("Extraction_P") or parsed.get("‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô P") or parsed.get("p_plan_extraction") or "-").strip()
    ext_d = str(parsed.get("Extraction_D") or parsed.get("‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô D") or parsed.get("d_do_extraction") or "-").strip()
    ext_c = str(parsed.get("Extraction_C") or parsed.get("‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô C") or parsed.get("c_check_extraction") or "-").strip()
    ext_a = str(parsed.get("Extraction_A") or parsed.get("‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô A") or parsed.get("a_act_extraction") or "-").strip()

    # PDCA scores ‚Äî ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö key ‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢
    p_plan_score = clean_score(parsed.get("P_Plan_Score") or parsed.get("P_Score") or parsed.get("plan_score") or parsed.get("P"))
    d_do_score = clean_score(parsed.get("D_Do_Score") or parsed.get("D_Score") or parsed.get("do_score") or parsed.get("D"))
    c_check_score = clean_score(parsed.get("C_Check_Score") or parsed.get("C_Score") or parsed.get("check_score") or parsed.get("C"))
    a_act_score = clean_score(parsed.get("A_Act_Score") or parsed.get("A_Score") or parsed.get("act_score") or parsed.get("A"))

    # Fallback ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö L1-L2
    if bool(is_passed) and level <= 2 and p_plan_score == 0:
        p_plan_score = score

    return {
        "sub_id": str(sub_id),
        "level": int(level),
        "score": score,
        "is_passed": bool(is_passed),
        "reason": str(parsed.get("reason") or parsed.get("‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•") or "‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏à‡∏≤‡∏Å LLM").strip(),
        "summary_thai": str(parsed.get("summary_thai") or parsed.get("‡∏ö‡∏ó‡∏™‡∏£‡∏∏‡∏õ") or "").strip(),
        "coaching_insight": str(parsed.get("coaching_insight") or parsed.get("‡∏Ç‡πâ‡∏≠‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥") or "").strip(),
        
        "P_Plan_Score": p_plan_score,
        "D_Do_Score": d_do_score,
        "C_Check_Score": c_check_score,
        "A_Act_Score": a_act_score,

        "Extraction_P": ext_p,
        "Extraction_D": ext_d,
        "Extraction_C": ext_c,
        "Extraction_A": ext_a,
        
        "final_llm_context": str(context or ""),
        "raw_llm_response": str(raw_response or ""),
        "ai_confidence_at_eval": str(confidence or "MEDIUM"),
        "consistency_check": bool(parsed.get("consistency_check", True)),
        
        # Multi-Enabler Traceability
        "enabler_at_eval": f"{enabler_full_name} ({enabler_code})"
    }


def _create_fallback_error(sub_id: str, level: int, error: Exception, context: str, 
                          enabler_full_name: str = "Unknown", enabler_code: str = "UNK") -> Dict[str, Any]:
    """[SAFETY NET] ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏ì‡∏µ LLM ‡∏´‡∏£‡∏∑‡∏≠ Prompt ‡∏û‡∏±‡∏á"""
    logger = logging.getLogger(__name__)
    logger.error(f"üõë Critical Audit Failure Enabler:{enabler_code} Sub:{sub_id} L{level}: {str(error)}")
    
    return {
        "sub_id": sub_id,
        "level": level,
        "score": 0.0,
        "reason": f"Audit Engine Error: {str(error)}",
        "is_passed": False,
        "consistency_check": False,
        "P_Plan_Score": 0.0, "D_Do_Score": 0.0, "C_Check_Score": 0.0, "A_Act_Score": 0.0,
        "Extraction_P": "-", "Extraction_D": "-", "Extraction_C": "-", "Extraction_A": "-",
        "final_llm_context": str(context or ""),
        "raw_llm_response": "",
        "ai_confidence_at_eval": "ERROR",
        "enabler_at_eval": f"{enabler_full_name} ({enabler_code})"
    }


def _heuristic_fallback_parse(raw_text: str) -> Dict:
    """
    [ENHANCED v2026.1.22] Fallback parse ‚Äî ‡∏´‡∏≤ score/PDCA ‡∏à‡∏≤‡∏Å raw text ‡∏î‡πâ‡∏ß‡∏¢ regex + keyword
    """
    parsed = {
        "score": 0.0,
        "is_passed": False,
        "reason": "JSON parse failed - fallback heuristic",
        "summary_thai": "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏¢‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå",
        "P_Plan_Score": 0.0,
        "D_Do_Score": 0.0,
        "C_Check_Score": 0.0,
        "A_Act_Score": 0.0,
        "consistency_check": False
    }

    import re

    # ‡∏´‡∏≤ score ‡∏´‡∏•‡∏±‡∏Å
    score_match = re.search(r"(?:score|‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô|total score)\D*([\d\.]+)", raw_text, re.IGNORECASE)
    if score_match:
        try:
            parsed["score"] = float(score_match.group(1))
            parsed["is_passed"] = parsed["score"] >= 0.7
        except:
            pass

    # ‡∏´‡∏≤ PDCA scores
    pdca_patterns = {
        "P_Plan_Score": r"(?:P_Plan|P|Plan|‡πÅ‡∏ú‡∏ô)\D*([\d\.]+)",
        "D_Do_Score": r"(?:D_Do|D|Do|‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥)\D*([\d\.]+)",
        "C_Check_Score": r"(?:C_Check|C|Check|‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö)\D*([\d\.]+)",
        "A_Act_Score": r"(?:A_Act|A|Act|‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á)\D*([\d\.]+)"
    }

    for key, pattern in pdca_patterns.items():
        match = re.search(pattern, raw_text, re.IGNORECASE)
        if match:
            try:
                parsed[key] = float(match.group(1))
            except:
                pass

    parsed["reason"] += f" | Raw snippet: {raw_text[:300]}..."
    return parsed

# ------------------------
# Summarize (FULL VERSION - v2026.4 Ultra-Robust & Zero-Error)
# ------------------------
def create_context_summary_llm(
    context: str,
    sub_criteria_name: str,
    level: int,
    sub_id: str,
    statement_text: str = "",           # Default ‡∏ß‡πà‡∏≤‡∏á ‚Üí ‡πÑ‡∏°‡πà error ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏™‡πà‡∏á
    next_level: int = None,             # Default ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏≠‡∏á
    llm_executor: Any = None
) -> Dict[str, Any]:
    """
    [SUMMARIZER v2026.4 ‚Äî Ultra-Robust & Zero-Error]
    - ‡πÄ‡∏û‡∏¥‡πà‡∏° default ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö statement_text ‡πÅ‡∏•‡∏∞ next_level ‚Üí ‡πÑ‡∏°‡πà error ‡∏ñ‡πâ‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö
    - Fallback ‡πÉ‡∏ô prompt ‡∏ñ‡πâ‡∏≤ statement_text ‡∏ß‡πà‡∏≤‡∏á (‡∏™‡∏£‡∏∏‡∏õ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ)
    - Log ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ fallback + clean ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á
    - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö retry 4 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á + hint ‡πÉ‡∏ô prompt ‡∏£‡∏≠‡∏ö retry
    """
    logger = logging.getLogger("AssessmentApp")

    # 1. Validation ‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô
    if llm_executor is None:
        logger.warning("‚ö†Ô∏è LLM executor is None - returning fallback")
        return {
            "summary": "‡∏£‡∏∞‡∏ö‡∏ö LLM ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô",
            "suggestion_for_next_level": "‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ LLM",
            "compliance_note": "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏î‡πâ",
            "evidence_integrity_score": 0.0
        }

    context_safe = (context or "").strip()
    if len(context_safe) < 30:
        return {
            "summary": "‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏Å‡∏¥‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏™‡∏£‡∏∏‡∏õ‡πÑ‡∏î‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô",
            "suggestion_for_next_level": "‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á",
            "compliance_note": "‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô",
            "evidence_integrity_score": 0.1
        }

    # Fallback next_level ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏™‡πà‡∏á
    if next_level is None:
        next_level = min(level + 1, 5)
        logger.debug(f"[SUMMARY] next_level fallback to {next_level} for {sub_id} L{level}")

    # 2. Clean context ‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á (‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏à‡∏≤‡∏Å‡πÄ‡∏î‡∏¥‡∏°)
    context_to_send = re.sub(r'[\x00-\x1F\x7F-\x9F]', ' ', context_safe)[:6500]

    # 3. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Prompt
    try:
        fallback_statement = statement_text or "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏ statement (‡∏™‡∏£‡∏∏‡∏õ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°)"
        human_prompt = USER_EVIDENCE_DESCRIPTION_TEMPLATE.format(
            sub_id=f"{sub_id} - {sub_criteria_name}",
            sub_criteria_name=sub_criteria_name,
            level=level,
            statement_text=fallback_statement,
            next_level=next_level,
            context=context_to_send
        )
    except Exception as e:
        logger.error(f"‚ùå Formatting Error in Summary Prompt: {e}")
        return {
            "summary": "‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö prompt",
            "suggestion_for_next_level": "N/A",
            "compliance_note": "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏î‡πâ",
            "evidence_integrity_score": 0.0
        }

    # System Instruction ‡πÄ‡∏Ç‡πâ‡∏°‡∏á‡∏ß‡∏î + ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö fallback
    system_instruction = (
        f"{SYSTEM_EVIDENCE_DESCRIPTION_PROMPT}\n"
        "### STRICT RULES (‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏ï‡∏≤‡∏°‡∏ó‡∏∏‡∏Å‡∏Ç‡πâ‡∏≠) ###\n"
        "1. RETURN ONLY VALID JSON OBJECT. ‡∏´‡πâ‡∏≤‡∏°‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏î ‡πÜ ‡∏ô‡∏≠‡∏Å JSON\n"
        "2. ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏ä‡πâ ```json ‡∏´‡∏£‡∏∑‡∏≠ markdown block\n"
        "3. ‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏•‡πâ‡∏ß‡∏ô‡πÉ‡∏ô‡∏ó‡∏∏‡∏Å value\n"
        "4. ‡∏´‡πâ‡∏≤‡∏°‡∏°‡πÇ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô context\n"
        "5. ‡∏ñ‡πâ‡∏≤ statement_text ‡∏ß‡πà‡∏≤‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡πá‡∏ô '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏' ‡πÉ‡∏´‡πâ‡∏™‡∏£‡∏∏‡∏õ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô compliance\n"
        "6. ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ statement_text ‡πÉ‡∏´‡πâ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô compliance ‡∏Å‡∏±‡∏ö statement ‡∏à‡∏£‡∏¥‡∏á ‡πÜ"
    )

    # 4. Execution Loop with Advanced Parsing + Retry
    max_retries = 4  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏õ‡πá‡∏ô 4 ‡∏£‡∏≠‡∏ö
    for attempt in range(1, max_retries + 1):
        try:
            logger.info(f"üîÑ Generating Summary {sub_id} L{level} (Attempt {attempt})")

            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å LLM (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á LangChain & Ollama-style)
            if hasattr(llm_executor, 'generate'):
                raw_response = llm_executor.generate(system=system_instruction, prompts=[human_prompt])
            elif hasattr(llm_executor, 'invoke'):
                raw_response = llm_executor.invoke(human_prompt)
            else:
                raw_response = llm_executor(system_instruction + "\n" + human_prompt)

            # Robust Text Extraction
            res_text = ""
            if hasattr(raw_response, 'generations'):
                res_text = raw_response.generations[0][0].text.strip()
            elif hasattr(raw_response, 'content'):
                res_text = str(raw_response.content).strip()
            else:
                res_text = str(raw_response).strip()

            # 5. ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (Thai-safe)
            if res_text:
                res_text = res_text.replace('\xa0', ' ').replace('\u200b', '')
                res_text = "".join(c for c in res_text if ord(c) >= 32 or c in "\n\r\t")
                res_text = re.sub(r'```(?:json)?\s*|\s*```', '', res_text).strip()
                res_text = re.sub(r'^[^{\[]+', '', res_text).strip()

            # 6. Robust JSON Extraction
            parsed = _extract_normalized_dict(res_text)

            if parsed and isinstance(parsed, dict):
                summary_val = parsed.get("summary") or parsed.get("‡∏™‡∏£‡∏∏‡∏õ") or ""
                suggestion = parsed.get("suggestion_for_next_level") or parsed.get("‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ñ‡∏±‡∏î‡πÑ‡∏õ") or ""
                compliance = parsed.get("compliance_note") or parsed.get("‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á") or "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏"
                score = float(parsed.get("evidence_integrity_score", 0.5))

                if summary_val.strip():
                    logger.info(f"‚úÖ Summary Generated Successfully (Attempt {attempt})")
                    return {
                        "summary": str(summary_val).strip(),
                        "suggestion_for_next_level": str(suggestion).strip() or "‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ñ‡∏±‡∏î‡πÑ‡∏õ",
                        "compliance_note": str(compliance).strip(),
                        "evidence_integrity_score": max(0.0, min(1.0, score))
                    }

            logger.warning(f"‚ö†Ô∏è Attempt {attempt}: Invalid/empty JSON. Retrying...")
            human_prompt += "\n\n(‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å: ‡∏ï‡∏≠‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞ JSON ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ { ‡πÅ‡∏•‡∏∞‡∏à‡∏ö‡∏î‡πâ‡∏ß‡∏¢ } ‡∏´‡πâ‡∏≤‡∏°‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏∑‡πà‡∏ô‡πÉ‡∏î)"

            time.sleep(0.8)  # ‡∏û‡∏±‡∏Å‡∏ô‡∏≤‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô‡∏ô‡∏¥‡∏î‡∏´‡∏ô‡πà‡∏≠‡∏¢

        except Exception as e:
            logger.error(f"‚ùå Attempt {attempt} Error: {str(e)}")
            time.sleep(1.2)

    # 7. Ultimate Fallback
    logger.error(f"‚ùå All attempts failed for {sub_id} L{level}")
    return {
        "summary": f"‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö {level} ‡πÅ‡∏ï‡πà‡∏£‡∏∞‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡∏∏‡∏õ‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå",
        "suggestion_for_next_level": f"‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏£‡∏∞‡∏î‡∏±‡∏ö {next_level or level+1}",
        "compliance_note": "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡πÑ‡∏î‡πâ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î",
        "evidence_integrity_score": 0.3
    }


def create_structured_action_plan(
    recommendation_statements: List[Dict[str, Any]],
    sub_id: str,
    sub_criteria_name: str,
    enabler: str = "KM",
    target_level: int = 5,
    llm_executor: Any = None,
    logger: logging.Logger = None,
    max_retries: int = 3,
    enabler_rules: Dict[str, Any] = {}
) -> List[Dict[str, Any]]:
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏¥‡∏á‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô
    Rev v36.9.9: Robustness & better fallback
    """
    if logger is None:
        logger = logging.getLogger(__name__)

    if llm_executor is None:
        logger.error("No llm_executor provided ‚Üí returning emergency fallback plan")
        return _get_emergency_fallback_plan(
            sub_id, sub_criteria_name, target_level,
            is_sustain_mode=not recommendation_statements,
            is_quality_refinement=False,
            enabler=enabler,
            recommendation_statements=recommendation_statements
        )

    # --- 1. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡πÅ‡∏•‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏´‡∏°‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô ---
    is_sustain_mode = not recommendation_statements
    is_quality_refinement = False

    if recommendation_statements:
        scores = [float(s.get('score', 0.0)) for s in recommendation_statements]
        avg_score = sum(scores) / len(scores) if scores else 0.0
        types = [str(s.get('recommendation_type', '')) for s in recommendation_statements]
        if all(t not in ['FAILED_REMEDIATION'] for t in types) and avg_score < 0.8:
            is_quality_refinement = True

    # --- 2. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Advice Focus ---
    specific_rule = enabler_rules.get(enabler, enabler_rules.get("DEFAULT", ""))
    
    if is_sustain_mode:
        mode_label = "SUSTAIN (Maintenance)"
        advice_focus = "‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏•‡∏¥‡∏® (Best Practice) ‡πÅ‡∏•‡∏∞‡∏Ç‡∏¢‡∏≤‡∏¢‡∏ú‡∏•‡∏™‡∏π‡πà‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°"
        dynamic_max_phases, max_steps = 1, 4
    elif is_quality_refinement:
        mode_label = "QUALITY REFINEMENT"
        advice_focus = "‡∏¢‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô (Evidence Quality) ‡πÅ‡∏•‡∏∞ KPI ‡πÉ‡∏´‡πâ‡∏Ñ‡∏°‡∏ä‡∏±‡∏î‡∏Ç‡∏∂‡πâ‡∏ô"
        dynamic_max_phases, max_steps = 1, 3
    else:
        mode_label = "GAP REMEDIATION"
        advice_focus = "‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏¥‡∏î‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á (Gap Remediation) ‡∏ï‡∏≤‡∏°‡∏ß‡∏á‡∏à‡∏£ PDCA ‡πÅ‡∏•‡∏∞‡∏à‡∏µ‡πâ‡∏à‡∏∏‡∏î‡∏ö‡∏Å‡∏û‡∏£‡πà‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á"
        dynamic_max_phases = 3 if target_level >= 4 else 2
        max_steps = 3

    if specific_rule:
        advice_focus += f" (‡∏Å‡∏é‡πÄ‡∏â‡∏û‡∏≤‡∏∞: {specific_rule})"

    logger.info(f"üöÄ [ACTION-PLAN START] {sub_id} | Mode: {mode_label} | Target: L{target_level}")

    # --- 3. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Gaps & Insights + REAL FILE SOURCES ---
    stmt_list = []
    real_files = set()
    insight_count = 0

    if is_sustain_mode:
        stmt_content = f"‡∏ö‡∏£‡∏£‡∏•‡∏∏‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏£‡∏∞‡∏î‡∏±‡∏ö {target_level} ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡πÅ‡∏ö‡∏ö (Sustain Mode)"
    else:
        for s in sorted(recommendation_statements, key=lambda x: x.get('level', 0)):
            lvl = s.get('level', 0)
            reason = (s.get('context') or s.get('reason') or "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á").strip()
            insight = s.get('coaching_insight', '').strip()
            source = s.get('source', s.get('file_name', '')).strip()

            if source and source != "-":
                real_files.add(source)

            context_text = f"Level {lvl}: {reason}"
            if insight:
                context_text += f" | ‡∏ö‡∏ó‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏∂‡∏Å: {insight}"
                insight_count += 1
            if source:
                context_text += f" | ‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÑ‡∏ü‡∏•‡πå: {source}"

            stmt_list.append(f"- {context_text}")

        file_list_str = f"\n\n[Available Real Files]: {', '.join(real_files)}" if real_files else ""
        stmt_content = "\n".join(stmt_list) + file_list_str

    logger.info(f"üìä [GAP-STAT] Levels: {len(stmt_list)} | Insights: {insight_count} | Real Files: {len(real_files)}")

    # --- 4. ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö Prompt ---
    human_prompt = ACTION_PLAN_PROMPT.format(
        enabler=enabler,
        sub_id=sub_id,
        sub_criteria_name=sub_criteria_name,
        target_level=target_level,
        recommendation_statements_list=stmt_content,
        advice_focus=advice_focus,
        max_phases=dynamic_max_phases,
        max_steps=max_steps,
        max_words_per_step=ACTION_PLAN_STEP_MAX_WORDS,
        language="‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢"
    )

    # --- 5. Execution Loop (Revised) ---
    for attempt in range(1, max_retries + 1):
        try:
            logger.info(f"ü§ñ Generating Action Plan (Attempt {attempt}/{max_retries})...")

            response = llm_executor.generate(
                system=SYSTEM_ACTION_PLAN_PROMPT,
                prompts=[human_prompt],
                temperature=LLM_TEMPERATURE,
                max_tokens=MAX_ACTION_PLAN_TOKENS
            )
            raw_text = getattr(response, 'content', str(response)).strip()

            # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏°‡∏≤‡πÉ‡∏ä‡πâ Single Entry Point ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏£‡∏ß‡∏° Extractor + Pydantic ‡πÑ‡∏ß‡πâ‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô
            # ‡∏ï‡∏±‡∏ß‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô List[ActionPlanActions] (Pydantic Objects)
            validated_items = build_action_plan_from_llm(raw_text, logger)

            if validated_items:
                logger.info(f"‚úÖ Strategic Roadmap built for {sub_id} with {len(validated_items)} valid phases")
                # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô Dict ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏ö‡∏î‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏¥‡∏° (‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô)
                return [item.model_dump() for item in validated_items]
            else:
                logger.warning(f"[RETRY] Validation failed or empty result (Attempt {attempt})")

        except Exception as e:
            logger.error(f"üí• Attempt {attempt} failed: {str(e)}")
            time.sleep(0.7 * attempt)

    # --- 6. Fallback ---
    logger.critical(f"‚ùå [MAX-RETRIES] Failed to build Action Plan for {sub_id}. Using fallback.")
    return _get_emergency_fallback_plan(
        sub_id, sub_criteria_name, target_level,
        is_sustain_mode=is_sustain_mode,
        is_quality_refinement=is_quality_refinement,
        enabler=enabler,
        recommendation_statements=recommendation_statements
    )

# =================================================================
# 2. Key Normalizer: ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤ LLM ‡∏û‡πà‡∏ô Key ‡πÑ‡∏°‡πà‡∏ô‡∏¥‡πà‡∏á
# =================================================================
def action_plan_normalize_keys(obj: Any) -> Any:
    """
    [FINAL PRODUCTION v2026.3.25]
    - Kill whitespace / newline / invisible char bugs (e.g. '\\n    phase')
    - Canonical key normalization (LLM-unstable safe)
    - Robust numeric coercion (step / failed_level)
    - Coaching insight auto-detection (fuzzy)
    - Recursive + order-safe
    """

    # -----------------------------
    # Recursive list handling
    # -----------------------------
    if isinstance(obj, list):
        return [action_plan_normalize_keys(i) for i in obj]

    # -----------------------------
    # Dict handling
    # -----------------------------
    if isinstance(obj, dict):

        # Canonical schema map (normalized_key -> target_key)
        FIELD_MAPPING = {
            # Phase structure
            "phase": "phase",
            "goal": "goal",
            "actions": "actions",

            # Action level
            "statementid": "statement_id",
            "failedlevel": "failed_level",
            "recommendation": "recommendation",

            # Coaching / Insight (CRITICAL)
            "coachinginsight": "coaching_insight",
            "coaching": "coaching_insight",
            "insight": "coaching_insight",
            "coachingsuggestion": "coaching_insight",
            "auditnote": "coaching_insight",
            "note": "coaching_insight",

            # Evidence & metric
            "targetevidencetype": "target_evidence_type",
            "keymetric": "key_metric",

            # Steps
            "steps": "steps",
            "step": "step",
            "description": "description",
            "responsible": "responsible",
            "verificationoutcome": "verification_outcome",

            # Optional
            "toolstemplates": "tools_templates"
        }

        new_obj = {}

        for raw_key, raw_value in obj.items():

            # -----------------------------
            # 1) HARD CLEAN KEY (anti '\n    phase')
            # -----------------------------
            k = str(raw_key)
            k = k.strip()                      # remove \n \t spaces
            k = k.lower()
            k = re.sub(r'[\s_]+', '', k)       # remove spaces + underscores
            k = re.sub(r'[^a-z0-9]', '', k)    # remove symbols

            # -----------------------------
            # 2) Resolve target key
            # -----------------------------
            target_key = FIELD_MAPPING.get(k)

            # Fuzzy coaching detection
            if not target_key:
                if "coach" in k or "insight" in k or "audit" in k:
                    target_key = "coaching_insight"
                else:
                    # fallback: preserve sanitized key (never raw)
                    target_key = k

            # -----------------------------
            # 3) Numeric coercion
            # -----------------------------
            if target_key in {"failed_level", "step"}:
                try:
                    if isinstance(raw_value, (int, float)):
                        value = int(raw_value)
                    else:
                        nums = re.findall(r"\d+", str(raw_value))
                        value = int(nums[0]) if nums else 0
                except Exception:
                    value = 0
            else:
                value = raw_value

            # -----------------------------
            # 4) Recursive normalize value
            # -----------------------------
            new_obj[target_key] = action_plan_normalize_keys(value)

        return new_obj

    # -----------------------------
    # Primitive passthrough
    # -----------------------------
    return obj


# =================================================================
# 3. JSON Extractor: ‡∏£‡∏∞‡∏ö‡∏ö‡∏Å‡∏π‡πâ‡∏Ñ‡∏∑‡∏ô JSON ‡∏ó‡∏µ‡πà‡∏û‡∏±‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏à‡∏ô‡πÑ‡∏°‡πà‡∏à‡∏ö
# =================================================================
def _get_emergency_fallback_plan(
    sub_id: str, 
    sub_criteria_name: str, 
    target_level: int, 
    is_sustain_mode: bool, 
    is_quality_refinement: bool, 
    enabler: str = "KM",
    recommendation_statements: List[Dict] = None # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏ô‡∏µ‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤
) -> List[Dict[str, Any]]:
    
    # ‡∏î‡∏∂‡∏á Insight ‡∏à‡∏£‡∏¥‡∏á‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å‡∏°‡∏≤‡∏ó‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
    top_insight = "‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå SE-AM ‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πà‡∏ß‡∏á‡∏ï‡∏≤‡∏°‡∏ß‡∏á‡∏à‡∏£ PDCA"
    if recommendation_statements:
        top_insight = recommendation_statements[0].get('coaching_insight') or \
                      recommendation_statements[0].get('reason') or top_insight

    title = "‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô‡∏ü‡∏∑‡πâ‡∏ô‡∏ü‡∏π‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏õ‡∏¥‡∏î‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á"
    if is_sustain_mode: title = "‡πÅ‡∏ú‡∏ô‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏•‡∏¥‡∏®"
    
    return [{
        "phase": f"Phase 1: {title}",
        "goal": f"‡∏¢‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö {sub_criteria_name} ‡∏™‡∏π‡πà‡∏£‡∏∞‡∏î‡∏±‡∏ö {target_level}",
        "actions": [{
            "statement_id": sub_id, 
            "failed_level": target_level,
            "recommendation": f"‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏ï‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞: {top_insight}", 
            "target_evidence_type": "‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏• / Evidence Package",
            "key_metric": "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô 100%",
            "steps": [
                {
                    "step": 1, 
                    "description": "‡∏à‡∏±‡∏î‡∏ó‡∏≥‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå SE-AM", 
                    "responsible": "‡∏Ñ‡∏ì‡∏∞‡∏ó‡∏≥‡∏á‡∏≤‡∏ô KM", 
                    "verification_outcome": "assessment_report.pdf"
                }
            ]
        }]
    }]

def build_action_plan_from_llm(raw_llm_text: str, logger_instance: Optional[logging.Logger] = None) -> List[ActionPlanActions]:
    """
    [SINGLE ENTRY POINT] ‡∏õ‡∏£‡∏∞‡∏™‡∏≤‡∏ô‡∏á‡∏≤‡∏ô Extractor ‡πÅ‡∏•‡∏∞ Pydantic
    """
    log = logger_instance or logger
    extracted_data = _extract_json_array_for_action_plan(raw_llm_text, log)
    try:
        validated_result = ActionPlanResult.validate_flexible(extracted_data)
        return validated_result.root
    except ValidationError as e:
        log.error(f"‚ùå Validation Error: {e.json()}")
        return []
    
def _extract_json_array_for_action_plan(
    raw_text: Any,
    logger: logging.Logger
) -> List[Dict[str, Any]]:
    """
    [ULTIMATE FINAL v2026.3.24]
    Robust JSON Array extractor for Action Plan

    Capabilities:
    - Strip markdown / prose noise
    - Recover truncated or malformed JSON
    - Normalize key casing (Phase vs phase)
    - High-signal debug logging when extraction fails
    """

    try:
        # --------------------------------------------------
        # 0) Normalize input
        # --------------------------------------------------
        if not isinstance(raw_text, str):
            raw_text = str(raw_text) if raw_text is not None else ""

        raw_text = raw_text.strip()
        if not raw_text:
            return []

        # --------------------------------------------------
        # 1) Pre-cleaning (LLM noise removal)
        # --------------------------------------------------
        # Remove markdown code fences
        text = re.sub(r"```(?:json)?", "", raw_text, flags=re.IGNORECASE)

        # Remove control characters (except whitespace)
        text = "".join(
            ch for ch in text
            if unicodedata.category(ch)[0] != "C" or ch in "\n\r\t"
        ).strip()

        # --------------------------------------------------
        # 2) Candidate JSON segment extraction
        # --------------------------------------------------
        candidate = None

        # Case A: Proper JSON array [ { ... } ]
        array_match = re.search(
            r"\[\s*\{.*?\}\s*\]",
            text,
            flags=re.DOTALL | re.MULTILINE
        )
        if array_match:
            candidate = array_match.group(0)

        # Case B: Single object { "phase": ... }
        if candidate is None:
            obj_match = re.search(
                r"\{\s*['\"]?phase['\"]?\s*:\s*.*?\}",
                text,
                flags=re.DOTALL | re.MULTILINE
            )
            if obj_match:
                candidate = f"[{obj_match.group(0)}]"

        # Case C: Fallback ‚Äì try entire text
        if candidate is None:
            candidate = text

        # --------------------------------------------------
        # 3) Structural cleanup (trailing commas, whitespace)
        # --------------------------------------------------
        candidate = re.sub(r",\s*([\]}])", r"\1", candidate).strip()

        # --------------------------------------------------
        # 4) JSON parsing (json5 tolerant)
        # --------------------------------------------------
        def try_parse(payload: str):
            try:
                return json5.loads(payload)
            except Exception:
                return None

        result = try_parse(candidate)

        # --------------------------------------------------
        # 5) Deep recovery (truncated JSON)
        # --------------------------------------------------
        if result is None:
            logger.debug("üîß Attempting deep JSON recovery...")

            open_braces = candidate.count("{")
            close_braces = candidate.count("}")

            fixes = []
            if open_braces > close_braces:
                diff = open_braces - close_braces
                fixes.append(candidate + ("}" * diff) + "]")
                fixes.append(candidate + ("}" * diff))

            for fixed in fixes:
                result = try_parse(fixed)
                if result is not None:
                    logger.info("‚úÖ JSON recovery successful via brace completion")
                    break

        if result is None:
            logger.warning(
                "‚ö†Ô∏è JSON parse failed entirely. "
                f"Raw length: {len(raw_text)}"
            )
            logger.debug(f"üîç Raw preview (500):\n{raw_text[:500]}")
            return []

        # --------------------------------------------------
        # 6) Normalize to list
        # --------------------------------------------------
        if isinstance(result, dict):
            result = [result]
        elif not isinstance(result, list):
            return []

        # --------------------------------------------------
        # 7) Final validation (phase / goal / actions guard)
        # --------------------------------------------------
        final_items: List[Dict[str, Any]] = []

        for item in result:
            if not isinstance(item, dict):
                continue

            # Normalize keys (case-insensitive)
            lowered = {str(k).lower(): v for k, v in item.items()}

            if any(k in lowered for k in ("phase", "goal", "actions")):
                final_items.append(item)

        if not final_items:
            logger.warning(
                "‚ö†Ô∏è Extraction finished but no valid phases found. "
                f"Raw length: {len(raw_text)}"
            )
            logger.debug(
                f"üîç Failed Content Preview (first 500):\n{raw_text[:500]}"
            )
            logger.debug(
                f"üîç Candidate Segment Used (first 500):\n{candidate[:500]}"
            )

        return final_items

    except Exception as e:
        logger.error(f"üí• Critical Extraction Error: {str(e)}")
        logger.debug(f"üîç Raw text preview:\n{str(raw_text)[:500]}")
        return []
