"""
llm_data_utils.py
Robust LLM + RAG utilities for SEAM assessment (CLEAN FINAL VERSION)
"""

import logging

logger = logging.getLogger(__name__)
if not logger.handlers:
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")


import time
import json
import hashlib
import uuid
import re
from datetime import datetime
from typing import List, Dict, Any, Optional, Union, Callable, TypeVar, Set
import json5
from utils.enabler_keyword_map import ENABLER_KEYWORD_MAP, DEFAULT_KEYWORDS
from langchain.retrievers import EnsembleRetriever
from langchain_core.documents import Document
from langchain_community.retrievers import BM25Retriever # FIX: Import BM25 ‡∏à‡∏≤‡∏Å community
import os
# --- ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° JSON Schema ---
try:
    from core.action_plan_schema import get_clean_action_plan_schema
    schema_json = json.dumps(get_clean_action_plan_schema(), ensure_ascii=False, indent=2)
except Exception as e:
    logger.error(f"Schema load failed: {e}")


# Optional: regex ‡πÅ‡∏ó‡∏ô re (‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤) ‚Äî ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡πá‡πÉ‡∏ä‡πâ re ‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤
try:
    import regex as re  # type: ignore
except ImportError:
    pass  # ‡πÉ‡∏ä‡πâ re ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ


# ===================================================================
# 1. Core Configuration (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô)
# ===================================================================
from config.global_vars import (
    DEFAULT_ENABLER,
    INITIAL_TOP_K,
    MAX_EVAL_CONTEXT_LENGTH,
    USE_HYBRID_SEARCH, 
    HYBRID_VECTOR_WEIGHT, 
    HYBRID_BM25_WEIGHT,
    MAX_ACTION_PLAN_PHASES,
    MAX_STEPS_PER_ACTION,
    ACTION_PLAN_STEP_MAX_WORDS,
    ACTION_PLAN_LANGUAGE,
    QUERY_INITIAL_K
)

# ===================================================================
# 2. Critical Utilities (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏à‡∏£‡∏¥‡∏á ‚Äî ‡πÑ‡∏°‡πà‡∏°‡∏µ fallback)
# ===================================================================
# üéØ FIX 1: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô Import ‡∏à‡∏≤‡∏Å _get_collection_name ‡πÑ‡∏õ‡πÄ‡∏õ‡πá‡∏ô get_doc_type_collection_key
from core.vectorstore import get_hf_embeddings
from utils.path_utils import (
    get_doc_type_collection_key, 
    _n, get_mapping_file_path, # <--- ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà‡∏à‡∏≤‡∏Å utils/path_utils
    get_vectorstore_collection_path,
    get_vectorstore_tenant_root_path,
    get_rubric_file_path
)
from core.json_extractor import (
    _robust_extract_json,
    _normalize_keys,
    _safe_int_parse,
    _extract_normalized_dict
)

# ===================================================================
# 3. Project Modules (‡∏ñ‡πâ‡∏≤‡∏´‡∏≤‡∏¢ ‚Üí ‡∏õ‡∏•‡πà‡∏≠‡∏¢‡πÉ‡∏´‡πâ error ‡∏ä‡∏±‡∏î ‡πÜ ‡πÑ‡∏õ‡πÄ‡∏•‡∏¢ ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏á‡∏µ‡∏¢‡∏ö)
# ===================================================================
from core.seam_prompts import (
    SYSTEM_ASSESSMENT_PROMPT,
    USER_ASSESSMENT_PROMPT,
    SYSTEM_ACTION_PLAN_PROMPT,
    ACTION_PLAN_PROMPT,
    SYSTEM_EVIDENCE_DESCRIPTION_PROMPT,
    EVIDENCE_DESCRIPTION_PROMPT,
    SYSTEM_LOW_LEVEL_PROMPT,
    USER_LOW_LEVEL_PROMPT,
    USER_LOW_LEVEL_PROMPT_TEMPLATE,
    USER_EVIDENCE_DESCRIPTION_TEMPLATE
)

from core.vectorstore import VectorStoreManager, get_global_reranker, ChromaRetriever
from core.assessment_schema import CombinedAssessment, EvidenceSummary
from core.action_plan_schema import ActionPlanActions

try:
    from core.assessment_schema import StatementAssessment
except ImportError:
    from pydantic import BaseModel
    class StatementAssessment(BaseModel):
        score: int = 0
        reason: str = ""

from langchain_core.documents import Document as LcDocument
# ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î Import ‡πÉ‡∏ô routers/llm_router.py

# ===================================================================
# 4. Constants
# ===================================================================
LOW_LEVEL_K: int = 3
_MOCK_FLAG = False
_MAX_LLM_RETRIES = 3

def set_mock_control_mode(enable: bool):
    global _MOCK_FLAG
    _MOCK_FLAG = bool(enable)
    logger.info(f"Mock control mode: {_MOCK_FLAG}")

# ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÉ‡∏ô core/llm_data_utils.py

def _create_where_filter(
    stable_doc_ids: Optional[Union[Set[str], List[str]]] = None,
    subject: Optional[str] = None,
    sub_topic: Optional[str] = None,  # üëà ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏°‡∏≤
    year: Optional[Union[int, str]] = None,
    enabler: Optional[str] = None,
    **kwargs  # üëà ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏° **kwargs ‡πÑ‡∏ß‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏±‡∏ô‡πÄ‡∏´‡∏ô‡∏µ‡∏¢‡∏ß‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï
) -> Dict[str, Any]:
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á Filter ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ChromaDB ‡∏ó‡∏µ‡πà‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô:
    """
    filters: List[Dict[str, Any]] = []

    # --- 1. ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Stable Doc IDs (‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î) ---
    if stable_doc_ids:
        ids_list = [str(i).strip() for i in (stable_doc_ids if isinstance(stable_doc_ids, (list, set)) else [stable_doc_ids]) if i]
        if ids_list:
            if len(ids_list) == 1:
                return {"stable_doc_uuid": ids_list[0]}
            else:
                return {"stable_doc_uuid": {"$in": ids_list}}

    # --- 2. ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Metadata ‡∏≠‡∏∑‡πà‡∏ô‡πÜ ---
    if year and str(year).strip():
        filters.append({"year": str(year).strip()})
    
    if enabler and str(enabler).strip():
        filters.append({"enabler": enabler.strip().upper()})

    if subject and str(subject).strip():
        filters.append({"subject": str(subject).strip()})

    # --- 3. ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ sub_topic (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏™‡πà‡∏á‡∏°‡∏≤) ---
    if sub_topic and str(sub_topic).strip():
        filters.append({"sub_topic": str(sub_topic).strip()})

    if not filters:
        return {}

    return filters[0] if len(filters) == 1 else {"$and": filters}

def retrieve_context_for_endpoint(
    vectorstore_manager,
    query: str = "",
    tenant: Optional[str] = None,
    year: Optional[Union[int, str]] = None,
    stable_doc_ids: Optional[Set[str]] = None,
    doc_type: Optional[str] = None,
    enabler: Optional[str] = None,
    subject: Optional[str] = None,
    sub_topic: Optional[str] = None,
    k_to_retrieve: int = 150, # üöÄ ‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°
    k_to_rerank: int = 30,    # üöÄ ‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ AI ‡πÄ‡∏´‡πá‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô
    strict_filter: bool = False,
    **kwargs
) -> Dict[str, Any]:
    start_time = time.time()
    vsm = vectorstore_manager

    # 1. Resolve collection
    clean_doc_type = str(doc_type or "document").strip().lower()
    collection_name = get_doc_type_collection_key(doc_type=clean_doc_type, enabler=enabler)
    
    chroma = vsm._load_chroma_instance(collection_name)
    if not chroma:
        return {"top_evidences": [], "aggregated_context": "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", "retrieval_time": 0}

    # 2. Create where_filter
    where_filter = _create_where_filter(
        stable_doc_ids=stable_doc_ids, subject=subject, sub_topic=sub_topic, year=year
    )

    final_chunks: List[LcDocument] = []
    seen_contents: Set[str] = set() # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Chunk ‡∏ã‡πâ‡∏≥

    # =====================================================
    # üéØ NEW: ANCHOR RETRIEVAL (‡∏î‡∏∂‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏™‡∏≤‡∏£‡∏ö‡∏±‡∏ç/‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á)
    # =====================-================================
    if stable_doc_ids:
        logger.info(f"‚öì Fetching Anchor Chunks for structure...")
        # ‡∏î‡∏∂‡∏á 5 Chunks ‡πÅ‡∏£‡∏Å‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÑ‡∏ü‡∏•‡πå (‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡∏ç‡πà‡∏Ñ‡∏∑‡∏≠‡∏´‡∏ô‡πâ‡∏≤ 1-5)
        anchors = chroma.get(where=where_filter, limit=8) 
        if anchors and anchors.get('documents'):
            for i in range(len(anchors['documents'])):
                content = anchors['documents'][i]
                if content not in seen_contents:
                    final_chunks.append(LcDocument(
                        page_content=content,
                        metadata={**anchors['metadatas'][i], "score": 1.0, "is_anchor": True}
                    ))
                    seen_contents.add(content)

    # =====================================================
    # CASE A/B: SEMANTIC & HYBRID SEARCH
    # =====================================================
    search_query = query if (query and query != "*" and len(query) > 2) else ""
    
    # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢
    if search_query:
        docs = chroma.similarity_search(search_query, k=k_to_retrieve, filter=where_filter)
        for d in docs:
            if d.page_content not in seen_contents:
                final_chunks.append(d)
                seen_contents.add(d.page_content)
    elif not final_chunks: # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ query ‡πÅ‡∏•‡∏∞‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ anchor ‡πÉ‡∏´‡πâ‡∏î‡∏∂‡∏á‡πÅ‡∏ö‡∏ö‡∏Å‡∏ß‡∏≤‡∏î
        docs = chroma.similarity_search("*", k=k_to_retrieve, filter=where_filter)
        final_chunks.extend(docs)

    # üéØ Double Check Guardrail (Filter ID)
    if stable_doc_ids:
        target_ids = {str(i).lower() for i in stable_doc_ids}
        final_chunks = [
            d for d in final_chunks 
            if str(d.metadata.get("stable_doc_uuid") or d.metadata.get("doc_id")).lower() in target_ids
        ]

    # =====================================================
    # 3. RERANKING (‡∏Ñ‡∏±‡∏î‡πÄ‡∏≠‡∏≤‡∏ï‡∏±‡∏ß‡∏ó‡πá‡∏≠‡∏õ‡∏ï‡∏≤‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô K ‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏°‡∏≤‡∏à‡∏≤‡∏Å Router)
    # =====================================================
    reranker = get_global_reranker()
    if reranker and final_chunks and search_query:
        try:
            top_n = min(len(final_chunks), k_to_rerank)
            reranked = reranker.compress_documents(documents=final_chunks, query=search_query, top_n=top_n)
            final_chunks = [r.document if hasattr(r, "document") else r for r in reranked]
            for i, res in enumerate(reranked):
                final_chunks[i].metadata["rerank_score"] = getattr(res, "relevance_score", 0)
        except Exception as e:
            logger.error(f"‚ö†Ô∏è Rerank failed: {e}")
            final_chunks = final_chunks[:k_to_rerank]
    else:
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ reranker ‡πÉ‡∏´‡πâ‡∏ï‡∏±‡∏î‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏î‡∏¥‡∏°
        final_chunks = final_chunks[:k_to_rerank]

    # 4. Response Build
    top_evidences = []
    aggregated_parts = []
    
    for doc in final_chunks:
        md = doc.metadata or {}
        text = doc.page_content.strip()
        s_uuid = md.get("stable_doc_uuid") or md.get("doc_id")
        p_val = md.get("page_label") or md.get("page_number") or md.get("page") or "N/A"
        
        top_evidences.append({
            "doc_id": s_uuid,
            "chunk_uuid": md.get("chunk_uuid"),
            "source": md.get("source") or md.get("file_name") or "Unknown",
            "text": text,
            "page": str(p_val),
            "score": md.get("rerank_score") or md.get("score") or 0.0,
            "pdca_tag": md.get("pdca_tag", "Other")
        })
        source_name = md.get('source') or md.get('file_name') or 'Unknown'
        aggregated_parts.append(f"[‡πÑ‡∏ü‡∏•‡πå: {source_name}, ‡∏´‡∏ô‡πâ‡∏≤: {p_val}] {text}")

    retrieval_time = round(time.time() - start_time, 3)
    logger.info(f"üèÅ Finished: {len(top_evidences)} chunks in {retrieval_time}s")

    return {
        "top_evidences": top_evidences,
        "aggregated_context": "\n\n".join(aggregated_parts) if aggregated_parts else "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á",
        "retrieval_time": retrieval_time,
        "used_chunk_uuids": [e["chunk_uuid"] for e in top_evidences if e.get("chunk_uuid")]
    }

# ------------------------
# Retrieval: retrieve_context_with_filter (Revised)
# ------------------------
import time
import uuid
import re
import logging
from typing import List, Dict, Any, Optional, Union, Callable
from langchain_core.documents import Document as LcDocument

logger = logging.getLogger(__name__)

def retrieve_context_with_filter(
    query: Union[str, List[str]],
    doc_type: str,
    tenant: Optional[str] = None,
    year: Optional[Union[int, str]] = None,
    enabler: Optional[str] = None,
    subject: Optional[str] = None,
    vectorstore_manager: Optional[Any] = None, 
    mapped_uuids: Optional[List[str]] = None,
    stable_doc_ids: Optional[List[str]] = None,
    priority_docs_input: Optional[List[Any]] = None,
    sequential_chunk_uuids: Optional[List[str]] = None,
    sub_id: Optional[str] = None,
    level: Optional[int] = None,
    get_previous_level_docs: Optional[Callable[[int, str], List[Any]]] = None,
    top_k: int = 12,
) -> Dict[str, Any]:
    """
    [FULL OPTIMIZED VERSION] Retrieval + Deduplication + Batch Reranking
    ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤ CUDA OOM ‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡πÅ‡∏ö‡πà‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• Reranker ‡πÄ‡∏õ‡πá‡∏ô Batch ‡∏¢‡πà‡∏≠‡∏¢‡πÜ
    """
    start_time = time.time()
    
    # 1. Setup Manager & Configuration
    # ‡πÄ‡∏£‡∏≤‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏î‡∏∂‡∏á VectorStoreManager ‡πÄ‡∏î‡∏¥‡∏°‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡πÇ‡∏´‡∏•‡∏î‡∏ã‡πâ‡∏≥
    manager = vectorstore_manager
    queries_to_run = [query] if isinstance(query, str) else list(query or [""])
    # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô get_doc_type_collection_key ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß
    from config.global_vars import get_doc_type_collection_key
    collection_name = get_doc_type_collection_key(doc_type, enabler or "KM")
    
    # 2. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Filter ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ Retrieval (Vector Search)
    target_ids = set()
    if stable_doc_ids: target_ids.update([str(i) for i in stable_doc_ids])
    if mapped_uuids: target_ids.update([str(i) for i in mapped_uuids])
    if sequential_chunk_uuids: target_ids.update([str(i) for i in sequential_chunk_uuids])
    
    # ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô _create_where_filter ‡∏ï‡πâ‡∏≠‡∏á‡∏ñ‡∏π‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏à‡∏≤‡∏Å‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
    # (‡πÉ‡∏ô‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ï‡∏≤‡∏° Logic ‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ)
    where_filter = _create_where_filter(
        stable_doc_ids=list(target_ids) if target_ids else None,
        subject=subject,
        year=year
    )

    # 3. ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô (Base Chunks)
    all_source_chunks = []

    # 3.1 Priority Docs (‡∏à‡∏≤‡∏Å Baseline ‡∏´‡∏£‡∏∑‡∏≠ Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤)
    if priority_docs_input:
        for doc in priority_docs_input:
            if not doc: continue
            if isinstance(doc, dict):
                pc = doc.get('page_content') or doc.get('text') or ''
                meta = doc.get('metadata') or {}
                # ‡∏£‡∏±‡∏Å‡∏©‡∏≤ ID ‡πÄ‡∏î‡∏¥‡∏°‡πÑ‡∏ß‡πâ
                meta['chunk_uuid'] = doc.get('chunk_uuid') or meta.get('chunk_uuid')
                meta['stable_doc_uuid'] = doc.get('doc_id') or meta.get('stable_doc_uuid')
                if pc.strip():
                    all_source_chunks.append(LcDocument(page_content=pc, metadata=meta))
            elif hasattr(doc, 'page_content'):
                all_source_chunks.append(doc)

    # 3.2 L3 Fallback (‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å L2)
    if level == 3 and callable(get_previous_level_docs):
        try:
            fallback_chunks = get_previous_level_docs(level - 1, sub_id) or []
            all_source_chunks.extend(fallback_chunks)
            logger.info(f"L3 Fallback: Added {len(fallback_chunks)} chunks from L2")
        except Exception as e:
            logger.warning(f"L3 Fallback failed: {e}")

    # 3.3 Vector Search Retrieval (‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å ChromaDB/Pinecone)
    try:
        full_retriever = manager.get_retriever(collection_name=collection_name)
        # üõ°Ô∏è ‡∏à‡∏∏‡∏î‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Base Retriever ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥ Vector Search ‡∏Å‡πà‡∏≠‡∏ô (‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà Rerank)
        # ‡∏´‡∏≤‡∏Å get_retriever ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ ContextualCompressionRetriever ‡πÉ‡∏´‡πâ‡∏î‡∏∂‡∏á .base_retriever
        base_retriever = getattr(full_retriever, "base_retriever", full_retriever)
        
        search_kwargs = {"k": top_k} # top_k ‡∏ô‡∏µ‡πâ‡∏≠‡∏≤‡∏à‡πÄ‡∏õ‡πá‡∏ô 100-1000
        if where_filter: search_kwargs["where"] = where_filter

        for q in queries_to_run:
            if not q: continue
            docs = base_retriever.invoke(q, config={"configurable": {"search_kwargs": search_kwargs}})
            all_source_chunks.extend(docs or [])
    except Exception as e:
        logger.error(f"Retrieval error for {collection_name}: {e}")

    # 4. Deduplicate (‡∏ï‡∏±‡∏î Chunk ‡∏ã‡πâ‡∏≥) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î Memory ‡∏Å‡πà‡∏≠‡∏ô Rerank
    unique_map: Dict[str, LcDocument] = {}
    for doc in all_source_chunks:
        if not doc or not doc.page_content.strip(): continue
        md = doc.metadata or {}
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á UID ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ä‡πá‡∏Ñ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡πâ‡∏≥
        uid = str(md.get("chunk_uuid") or md.get("stable_doc_uuid") or hash(doc.page_content))
        if uid not in unique_map:
            # L3 Truncate Logic ‡∏ï‡∏≤‡∏°‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏î‡∏¥‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
            if level == 3:
                doc.page_content = doc.page_content[:1000]
            unique_map[uid] = doc

    candidates = list(unique_map.values())

    # 5. [CRITICAL] BATCH RERANKING (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô OOM)
    # ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏™‡πà‡∏á candidates ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏Ç‡πâ‡∏≤ Reranker ‡∏ó‡∏µ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
    final_scored_docs = []
    batch_size = 150  # ‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏ó‡∏µ‡πà‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö GPU 
    
    # ‡∏î‡∏∂‡∏á Reranker ‡∏à‡∏≤‡∏Å Manager (‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡πÉ‡∏´‡∏°‡πà‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ)
    reranker_compressor = getattr(manager, "reranker", None)

    if reranker_compressor and len(candidates) > 0:
        logger.info(f"üöÄ Performing Batch Reranking: {len(candidates)} chunks in batches of {batch_size}")
        for i in range(0, len(candidates), batch_size):
            batch = candidates[i : i + batch_size]
            try:
                # ‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô compress_documents ‡∏Ç‡∏≠‡∏á LangChain Reranker
                scored_batch = reranker_compressor.compress_documents(batch, queries_to_run[0])
                final_scored_docs.extend(scored_batch)
            except Exception as e:
                logger.error(f"Rerank Batch Error at index {i}: {e}")
                # ‡∏ñ‡πâ‡∏≤ batch ‡∏ô‡∏µ‡πâ‡∏û‡∏±‡∏á ‡πÉ‡∏´‡πâ‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô rerank ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏≤‡∏¢
                final_scored_docs.extend(batch)
    else:
        # ‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏°‡πà‡∏°‡∏µ Reranker ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å Vector Search ‡∏ï‡∏£‡∏á‡πÜ
        final_scored_docs = candidates

    # 6. ‡∏Ñ‡∏±‡∏î‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå (Final Output Formatting)
    top_evidences = []
    aggregated_parts = []
    used_uuids = []
    VALID_ID = re.compile(r"^[0-9a-f\-]{36}$|^[0-9a-f]{64}$", re.IGNORECASE)

    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å Reranker
    final_scored_docs = sorted(
        final_scored_docs, 
        key=lambda x: getattr(x, "relevance_score", x.metadata.get("relevance_score", 0.0)), 
        reverse=True
    )

    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞ K ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á (‡πÄ‡∏ä‡πà‡∏ô 12-15 ‡∏ï‡∏±‡∏ß)
    final_k = getattr(globals(), 'QA_FINAL_K', 15) 
    
    for doc in final_scored_docs[:final_k]:
        md = doc.metadata or {}
        text = doc.page_content.strip()
        
        # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ IDs
        c_uuid = str(md.get("chunk_uuid", ""))
        s_uuid = str(md.get("stable_doc_uuid") or md.get("doc_id") or "")
        best_id = s_uuid if VALID_ID.match(s_uuid) else (c_uuid if VALID_ID.match(c_uuid) else f"temp-{uuid.uuid4().hex[:8]}")
        
        if not best_id.startswith("temp-"): 
            used_uuids.append(best_id)
            
        source = md.get("source") or md.get("source_filename") or "Unknown"
        pdca = md.get("pdca_tag", "Other")
        page = str(md.get("page_label") or md.get("page_number") or md.get("page") or "N/A")
        
        # ‡∏î‡∏∂‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô (‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å attribute ‡∏´‡∏£‡∏∑‡∏≠ metadata)
        score = float(getattr(doc, "relevance_score", md.get("relevance_score", 0.0)))

        top_evidences.append({
            "doc_id": s_uuid or best_id,
            "chunk_uuid": c_uuid or best_id,
            "source": source,
            "text": text,
            "page": page,
            "pdca_tag": pdca,
            "score": score
        })
        aggregated_parts.append(f"[{pdca}] [‡πÑ‡∏ü‡∏•‡πå: {source} ‡∏´‡∏ô‡πâ‡∏≤: {page}] {text}")

    return {
        "top_evidences": top_evidences,
        "aggregated_context": "\n\n---\n\n".join(aggregated_parts) if aggregated_parts else "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô",
        "retrieval_time": round(time.time() - start_time, 3),
        "used_chunk_uuids": list(set(used_uuids))
    }


# =====================================================================
# üõ† Helper: check_rubric_readiness (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏á‡∏µ‡∏¢‡∏ö‡∏•‡∏á)
# =====================================================================
def is_rubric_ready(tenant: str) -> bool:
    """ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏Ç‡∏≠‡∏á seam collection ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏û‡πà‡∏ô Warning ‡∏Å‡∏ß‡∏ô‡πÉ‡∏à """
    if not tenant:
        return False
    
    tenant_vs_root = get_vectorstore_tenant_root_path(tenant)
    chroma_path = os.path.join(tenant_vs_root, "seam")
    
    # ‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ True/False ‡πÄ‡∏á‡∏µ‡∏¢‡∏ö‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏î‡∏∂‡∏á Rubric ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    return os.path.exists(chroma_path)


# =====================================================================
# üöÄ Ultimate Version: retrieve_context_with_rubric (FIXED & REVISED)
# =====================================================================

def retrieve_context_with_rubric(
    vectorstore_manager,
    query: str,
    doc_type: str,
    enabler: Optional[str] = None,
    stable_doc_ids: Optional[Set[str]] = None,
    tenant: Optional[str] = None,
    year: Optional[Union[int, str]] = None,
    subject: Optional[str] = None,
    rubric_vectorstore_name: str = "seam", 
    top_k: int = 50,         # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ï‡∏±‡πâ‡∏á‡∏ï‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å Vector Store
    rubric_top_k: int = 15,  
    strict_filter: bool = True,
    k_to_rerank: int = 30    # ‚úÖ FIX: ‡πÄ‡∏û‡∏¥‡πà‡∏° Parameter ‡∏ô‡∏µ‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å Router
) -> Dict[str, Any]:
    """
    ‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå: ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Direct/Global Mode, Anchor Chunks ‡πÅ‡∏•‡∏∞ Reranking
    """
    start_time = time.time()
    vsm = vectorstore_manager
    from utils.path_utils import get_doc_type_collection_key
    from core.vectorstore import get_global_reranker # ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤ Reranker

    # --- 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏™‡∏•‡∏±‡∏ö Collection ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ ---
    if hasattr(vsm, 'doc_type') and vsm.doc_type != doc_type:
        logger.info(f"üîÑ Switching VSM doc_type to: {doc_type}")
        vsm.close()
        vsm.__init__(tenant=tenant, year=year, doc_type=doc_type, enabler=enabler)

    evidence_collection = get_doc_type_collection_key(doc_type, enabler or "KM")
    
    evidence_results = []
    rubric_results = []
    seen_contents = set()

    # --- 2. ‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á Rubrics (‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô SE-AM) ---
    try:
        rubric_chroma = vsm._load_chroma_instance(rubric_vectorstore_name)
        if rubric_chroma:
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á Query ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
            rubric_query = f"‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô SE-AM ‡∏î‡πâ‡∏≤‡∏ô {enabler} ‡∏Ç‡πâ‡∏≠ {subject or ''}: {query}"
            r_docs = rubric_chroma.similarity_search(rubric_query, k=rubric_top_k)
            for rd in r_docs:
                rubric_results.append({
                    "text": rd.page_content, 
                    "metadata": rd.metadata, 
                    "is_rubric": True
                })
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Rubric Retrieval Error: {e}")

    # --- 3. ‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á Evidence (‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô) ---
    try:
        evidence_chroma = vsm._load_chroma_instance(evidence_collection)
        if not evidence_chroma:
            return {"top_evidences": [], "rubric_context": rubric_results, "retrieval_time": 0}

        raw_evidence_chunks = []
        where_filter = None

        # üéØ [DIRECT MODE] ‡∏´‡∏≤‡∏Å‡∏£‡∏∞‡∏ö‡∏∏‡πÑ‡∏ü‡∏•‡πå ‡πÉ‡∏´‡πâ‡∏î‡∏∂‡∏á Anchor Chunks (‡∏´‡∏ô‡πâ‡∏≤ 1-5) ‡∏°‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏™‡∏°‡∏≠
        if stable_doc_ids:
            ids_list = [str(i).strip().lower() for i in stable_doc_ids if i]
            if len(ids_list) == 1:
                where_filter = {"stable_doc_uuid": ids_list[0]}
            else:
                where_filter = {"stable_doc_uuid": {"$in": ids_list}}
            
            # ‚öì Fetch Anchor Chunks (‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏£‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å)
            logger.info(f"‚öì Fetching Anchor Chunks for IDs: {ids_list}")
            anchors = evidence_chroma.get(where=where_filter, limit=10)
            if anchors and anchors.get('documents'):
                for i in range(len(anchors['documents'])):
                    content = anchors['documents'][i]
                    if content not in seen_contents:
                        m = anchors['metadatas'][i]
                        raw_evidence_chunks.append(Document(
                            page_content=content,
                            metadata={**m, "rerank_score": 0.99, "is_anchor": True}
                        ))
                        seen_contents.add(content)

        # üåê Search Chunks ‡∏ï‡∏≤‡∏°‡∏õ‡∏Å‡∏ï‡∏¥
        search_results = evidence_chroma.similarity_search(
            query, 
            k=top_k, 
            filter=where_filter
        )
        for d in search_results:
            if d.page_content not in seen_contents:
                raw_evidence_chunks.append(d)
                seen_contents.add(d.page_content)

        # --- 4. RERANKING (‡∏´‡∏±‡∏ß‡πÉ‡∏à‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥) ---
        reranker = get_global_reranker()
        if reranker and raw_evidence_chunks and query:
            try:
                top_n = min(len(raw_evidence_chunks), k_to_rerank)
                reranked_docs = reranker.compress_documents(
                    documents=raw_evidence_chunks, 
                    query=query, 
                    top_n=top_n
                )
                # ‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏´‡∏•‡∏±‡∏á Rerank
                for r in reranked_docs:
                    doc = r.document if hasattr(r, "document") else r
                    m = doc.metadata or {}
                    evidence_results.append({
                        "text": doc.page_content,
                        "source_filename": m.get("source_filename") or m.get("source") or "Evidence",
                        "page_label": str(m.get("page_label") or m.get("page") or "N/A"),
                        "doc_id": m.get("stable_doc_uuid") or m.get("doc_id"),
                        "pdca_tag": m.get("pdca_tag") or "Content",
                        "rerank_score": getattr(r, "relevance_score", 0.0),
                        "is_evidence": True
                    })
            except Exception as e:
                logger.error(f"‚ö†Ô∏è Rerank failed: {e}")
                # Fallback: ‡∏ï‡∏±‡∏î‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏õ‡∏Å‡∏ï‡∏¥
                raw_evidence_chunks = raw_evidence_chunks[:k_to_rerank]
        
        # ‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏°‡πà‡∏°‡∏µ Reranker ‡∏´‡∏£‡∏∑‡∏≠ Rerank ‡∏û‡∏±‡∏á ‡πÉ‡∏´‡πâ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏Å‡∏ï‡∏¥
        if not evidence_results:
            for d in raw_evidence_chunks[:k_to_rerank]:
                m = d.metadata or {}
                evidence_results.append({
                    "text": d.page_content,
                    "source_filename": m.get("source_filename") or m.get("source") or "Evidence",
                    "page_label": str(m.get("page_label") or m.get("page") or "N/A"),
                    "doc_id": m.get("stable_doc_uuid") or m.get("doc_id"),
                    "pdca_tag": m.get("pdca_tag") or "Content",
                    "rerank_score": 0.0,
                    "is_evidence": True
                })

    except Exception as e:
        logger.error(f"‚ùå Evidence Retrieval Error: {e}", exc_info=True)

    retrieval_time = round(time.time() - start_time, 3)
    logger.info(f"‚úÖ Success: Retrieved {len(evidence_results)} evidence chunks in {retrieval_time}s")

    return {
        "top_evidences": evidence_results,
        "rubric_context": rubric_results,
        "retrieval_time": retrieval_time
    }

# ========================
#  retrieve_context_by_doc_ids (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö hydration ‡πÉ‡∏ô router)
# ========================
def retrieve_context_by_doc_ids(
    doc_uuids: List[str],
    doc_type: str,
    enabler: Optional[str] = None,
    vectorstore_manager = None,
    limit: int = 50,
    tenant: Optional[str] = None,
    year: Optional[Union[int, str]] = None,
) -> Dict[str, Any]:
    """
    ‡∏î‡∏∂‡∏á chunks ‡∏à‡∏≤‡∏Å stable_doc_uuid ‡∏´‡∏•‡∏≤‡∏¢‡∏ï‡∏±‡∏ß (‡πÉ‡∏ä‡πâ‡∏ï‡∏≠‡∏ô hydration sources)
    ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏∏‡∏õ‡∏µ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤ Collection ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
    """
    start_time = time.time()
    vsm = vectorstore_manager or VectorStoreManager()
    
    # üü¢ Resolve collection name ‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏õ‡∏µ‡πÅ‡∏•‡∏∞ enabler
    collection_name = get_doc_type_collection_key(doc_type=doc_type, enabler=enabler)

    chroma = vsm._load_chroma_instance(collection_name)
    if not chroma:
        logger.error(f"Collection {collection_name} not found for hydration")
        return {"top_evidences": []}

    if not doc_uuids:
        return {"top_evidences": []}

    logger.info(f"Hydration ‚Üí {len(doc_uuids)} doc IDs from {collection_name}")

    try:
        # ‡πÉ‡∏ä‡πâ Metadata filter ‡∏î‡∏∂‡∏á chunks ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏•‡∏¥‡∏™‡∏ï‡πå ID ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
        results = chroma._collection.get(
            where={"stable_doc_uuid": {"$in": [str(u) for u in doc_uuids]}},
            limit=limit,
            include=["documents", "metadatas"]
        )
    except Exception as e:
        logger.error(f"Hydration query failed: {e}")
        return {"top_evidences": []}

    evidences = []
    for doc, meta in zip(results.get("documents", []), results.get("metadatas", [])):
        if not doc.strip():
            continue

        p_val = meta.get("page_label") or meta.get("page_number") or meta.get("page") or "N/A"
        evidences.append({
            "doc_id": meta.get("stable_doc_uuid") or meta.get("doc_id"),
            "chunk_uuid": meta.get("chunk_uuid"),
            "source": meta.get("source") or meta.get("source_filename") or "Unknown",
            "page": str(p_val), # üü¢ ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ
            "text": doc,
            "pdca_tag": meta.get("pdca_tag", "Other"),
        })

    logger.info(f"Hydration success: {len(evidences)} chunks from {len(doc_uuids)} docs")
    return {"top_evidences": evidences}


def retrieve_context_for_low_levels(query: str, doc_type: str, enabler: Optional[str]=None,
                                 vectorstore_manager: Optional['VectorStoreManager']=None,
                                 top_k: int=LOW_LEVEL_K, initial_k: int=INITIAL_TOP_K,
                                 # üü¢ NEW: ‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠ arguments
                                 mapped_uuids: Optional[List[str]]=None,
                                 priority_docs_input: Optional[List[Any]] = None,
                                 sequential_chunk_uuids: Optional[List[str]] = None, 
                                 sub_id: Optional[str]=None, level: Optional[int]=None) -> Dict[str, Any]:
    """
    Retrieves a small, focused context for low levels (L1, L2) using a reduced k (LOW_LEVEL_K).
    """
    # ‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å ‡πÅ‡∏ï‡πà‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏ä‡πâ k ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
    return retrieve_context_with_filter(
        query=query,
        doc_type=doc_type,
        enabler=enabler,
        vectorstore_manager=vectorstore_manager,
        top_k=LOW_LEVEL_K,
        initial_k=initial_k,
        mapped_uuids=mapped_uuids,
        priority_docs_input=priority_docs_input,
        sequential_chunk_uuids=sequential_chunk_uuids, 
        sub_id=sub_id,
        level=level
    )

# ----------------------------------------------------
# Helper function: Summarize evidence list (minimal stub)
# ----------------------------------------------------
def _summarize_evidence_list_short(evidences: list, max_sentences: int = 3) -> str:
    """
    Provides a concise summary of evidence items.
    """
    if not evidences:
        return ""
    
    parts = []
    for ev in evidences[:max(1, min(len(evidences), max_sentences))]:
        if isinstance(ev, dict):
            fn = ev.get("source_filename") or ev.get("source") or ev.get("doc_id", "unknown")
            txt = ev.get("text") or ev.get("content") or ""
        else:
            fn = str(ev)
            txt = str(ev)
        txt_short = txt[:120].replace("\n", " ").strip()
        if txt_short:
            parts.append(f"‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå `{fn}`: {txt_short}...")
        else:
            parts.append(f"‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå `{fn}`")
    return " | ".join(parts)


# ULTIMATE FINAL VERSION: build_multichannel_context_for_level (OPTIMIZED)
# ‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó‡πÉ‡∏´‡∏°‡πà: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏Ñ‡πà BASELINE ‡πÅ‡∏•‡∏∞ AUXILIARY summaries ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
# ----------------------------------------------------
def build_multichannel_context_for_level(
    level: int,
    top_evidences: List[Dict[str, Any]],
    previous_levels_map: Optional[Dict[str, Any]] = None,
    previous_levels_evidence: Optional[List[Dict[str, Any]]] = None, # List ‡∏Ç‡∏≠‡∏á Chunks ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏à‡∏≤‡∏Å Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤
    max_main_context_tokens: int = 3000, 
    max_summary_sentences: int = 4,
    max_context_length: Optional[int] = None, 
    **kwargs
) -> Dict[str, Any]:
    """
    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏±‡∏Å‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Context Summary ‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏°‡∏≤
    ‡πÇ‡∏î‡∏¢‡πÄ‡∏ô‡πâ‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Baseline Summary (‡∏à‡∏≤‡∏Å Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤) ‡πÅ‡∏•‡∏∞ Auxiliary Summary (‡∏à‡∏≤‡∏Å Level ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô)
    """
    logger = logging.getLogger(__name__)
    K_MAIN = 5
    MIN_RELEVANCE_FOR_AUX = 0.4  # ‡∏Å‡∏£‡∏≠‡∏á aux ‡∏ó‡∏µ‡πà‡∏ï‡πà‡∏≥‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ

    # --- 1) Baseline Summary ---
    # ‡πÉ‡∏ä‡πâ List ‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏°‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß (previous_levels_evidence_list) ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å
    baseline_evidence = previous_levels_evidence or [] 

    summarizable_baseline = [
        item for item in baseline_evidence
        if isinstance(item, dict) and (item.get("text") or item.get("content"))
    ]
    
    # üü¢ FIX: ‡∏õ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô baseline
    if not summarizable_baseline:
        baseline_summary = "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏à‡∏≤‡∏Å Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤"
    else:
        baseline_summary = _summarize_evidence_list_short(
            summarizable_baseline,
            max_sentences=max_summary_sentences
        )

    # --- 2) Auxiliary Summary ---
    direct, aux_candidates = [], []

    for ev in top_evidences:
        if not isinstance(ev, dict):
            # ‡∏≠‡∏≤‡∏à‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå
            continue # ‡∏Ç‡πâ‡∏≤‡∏° chunks ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô dict ‡πÑ‡∏õ‡πÄ‡∏•‡∏¢

        # NEW: ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö tag ‡∏ó‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö‡πÄ‡∏ï‡πá‡∏°‡πÅ‡∏•‡∏∞‡∏¢‡πà‡∏≠
        tag = (ev.get("pdca_tag") or ev.get("PDCA") or "Other").upper()
        relevance = ev.get("rerank_score") or ev.get("score", 0.0)

        # PDCA Chunks ‡∏ñ‡∏π‡∏Å‡∏™‡πà‡∏á‡πÑ‡∏õ‡πÄ‡∏õ‡πá‡∏ô Direct Context (‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏ô Engine)
        # ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ô‡∏µ‡πâ‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡πÅ‡∏Ñ‡πà‡πÅ‡∏¢‡∏Å‡πÅ‡∏¢‡∏∞ ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Direct Context
        if tag in {"P", "PLAN", "D", "DO", "C", "CHECK", "A", "ACT"}:
            direct.append(ev)
        elif relevance >= MIN_RELEVANCE_FOR_AUX:  # ‡∏Å‡∏£‡∏≠‡∏á aux ‡∏ó‡∏µ‡πà‡∏ï‡πà‡∏≥‡πÄ‡∏Å‡∏¥‡∏ô
            aux_candidates.append(ev)

    # Logic ‡∏Å‡∏≤‡∏£‡∏¢‡πâ‡∏≤‡∏¢‡∏à‡∏≤‡∏Å aux ‡πÑ‡∏õ direct (K_MAIN) ‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡∏≠‡∏¢‡∏π‡πà (‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏±‡∏î/Debug ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏™‡πà‡∏á Direct ‡∏≠‡∏≠‡∏Å‡πÑ‡∏õ)
    if len(direct) < K_MAIN:
        need = K_MAIN - len(direct)
        direct.extend(aux_candidates[:need])
        aux_candidates = aux_candidates[need:]
        
    if len(direct) < K_MAIN:
        logger.warning(f"L{level}: Direct PDCA chunks ‡∏¢‡∏±‡∏á‡∏ô‡πâ‡∏≠‡∏¢ ({len(direct)}) ‡∏´‡∏•‡∏±‡∏á‡∏¢‡πâ‡∏≤‡∏¢‡∏à‡∏≤‡∏Å aux")

    aux_summary = _summarize_evidence_list_short(aux_candidates, max_sentences=3) if aux_candidates else "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏£‡∏≠‡∏á"

    # --- 3) Return ---
    debug_meta = {
        "level": level,
        "direct_count": len(direct),
        "aux_count": len(aux_candidates),
        # üü¢ FIX: ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏™‡∏£‡∏∏‡∏õ‡∏à‡∏£‡∏¥‡∏á
        "baseline_count": len(summarizable_baseline), 
        "max_context_length_received": max_context_length 
    }
    logger.info(f"Context L{level} ‚Üí Direct:{len(direct)} | Aux:{len(aux_candidates)} | Baseline:{len(summarizable_baseline)}")

    return {
        "baseline_summary": baseline_summary,
        "direct_context": "",  
        "aux_summary": aux_summary,
        "debug_meta": debug_meta,
    }

# ------------------------
# LLM fetcher
# ------------------------
def _fetch_llm_response(
    system_prompt: str, 
    user_prompt: str, 
    max_retries: int = 3,
    llm_executor: Any = None 
) -> str:
    """
    ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å LLM ‡∏ú‡πà‡∏≤‡∏ô LangChain/Ollama ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Format ‡∏ú‡∏¥‡∏î‡πÄ‡∏û‡∏µ‡πâ‡∏¢‡∏ô:
    - ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö JSON output ‡∏î‡πâ‡∏ß‡∏¢ Strict English Prompt
    - ‡πÉ‡∏ä‡πâ Regex Extraction ‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡πà‡∏ß‡∏ô { ... } ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡∏≠‡∏≠‡∏Å
    - Log raw response ‡πÄ‡∏ï‡πá‡∏°‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Debug
    - Retry ‡∏û‡∏£‡πâ‡∏≠‡∏° Exponential Backoff ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏Å‡∏¥‡∏î Error
    """
    global _MOCK_FLAG

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ LLM Instance ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    if llm_executor is None and not _MOCK_FLAG: 
        raise ConnectionError("LLM instance not initialized (Missing llm_executor).")

    # 1. üõ†Ô∏è ENFORCED PROMPT (‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡∏°‡∏±‡∏Å‡∏Ñ‡∏∏‡∏° Format ‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏•‡πá‡∏Å)
    enforced_system_prompt = system_prompt.strip() + (
        "\n\n"
        "### STRICT OUTPUT RULES ###\n"
        "1. ANSWER IN VALID JSON OBJECT ONLY.\n"
        "2. NO EXPLANATIONS, NO PREFACE, NO CONVERSATION.\n"
        "3. START WITH '{' AND END WITH '}'.\n"
        "4. DO NOT USE MARKDOWN CODE BLOCKS (```json).\n"
        "5. IF NO EVIDENCE FOUND, RETURN: {\"score\": 0, \"reason\": \"No evidence\", \"is_passed\": false}"
    )

    messages = [
        {"role": "system", "content": enforced_system_prompt},
        {"role": "user",   "content": user_prompt}
    ]

    for attempt in range(1, max_retries + 1):
        try:
            # --- MOCK MODE CASE ---
            if _MOCK_FLAG:
                mock_json = '{"score": 1, "reason": "Mock mode active", "is_passed": true}'
                logger.critical(f"LLM RAW RESPONSE (DEBUG MOCK): {mock_json}")
                return mock_json

            # --- ACTUAL LLM CALL (OLLAMA / LANGCHAIN) ---
            # ‡πÉ‡∏ä‡πâ temperature=0.0 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
            response = llm_executor.invoke(messages, config={"temperature": 0.0})
            
            # ‡∏î‡∏∂‡∏á Text ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏à‡∏≤‡∏Å Response Object
            raw_text = ""
            if hasattr(response, "content"):
                raw_text = str(response.content)
            elif isinstance(response, str):
                raw_text = response
            else:
                raw_text = str(response)

            # üîç ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏´‡πá‡∏ô‡πÉ‡∏ô Log ‡∏Ñ‡∏∑‡∏≠‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ (Log ‡∏Å‡πà‡∏≠‡∏ô Clean ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°‡πÇ‡∏°‡πÄ‡∏î‡∏•)
            logger.critical(f"LLM RAW RESPONSE (DEBUG): {raw_text[:1000]}{'...' if len(raw_text) > 1000 else ''}")

            # 2. üßπ CLEANING LOGIC (Regex Extraction)
            # ‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ LLM ‡∏ï‡∏≠‡∏ö "Based on the text... { ... }"
            raw_text_stripped = raw_text.strip()
            
            # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏õ‡∏µ‡∏Å‡∏Å‡∏≤‡∏Ñ‡∏π‡πà‡πÅ‡∏£‡∏Å { ... }
            json_match = re.search(r'(\{.*\})', raw_text_stripped, re.DOTALL)
            
            if json_match:
                extracted_json = json_match.group(1)
                try:
                    # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô JSON ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                    json.loads(extracted_json) 
                    return extracted_json
                except json.JSONDecodeError:
                    logger.warning(f"Extracted string is not valid JSON: {extracted_json[:100]}")
            
            # 3. üõ°Ô∏è FALLBACK: ‡∏ñ‡πâ‡∏≤ Regex ‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ ‡∏´‡∏£‡∏∑‡∏≠ Parse ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏î‡∏π‡∏ß‡πà‡∏≤ raw_text (‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏î‡∏´‡∏±‡∏ß‡∏ó‡πâ‡∏≤‡∏¢) ‡∏û‡∏≠‡∏•‡∏∏‡πâ‡∏ô‡πÄ‡∏õ‡πá‡∏ô JSON ‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°
            return raw_text_stripped

        except Exception as e:
            logger.error(f"LLM call failed (attempt {attempt}/{max_retries}): {e}")
            if attempt < max_retries:
                # Exponential backoff: 2s, 4s, 8s...
                time.sleep(2 ** attempt)  
            else:
                logger.critical("All LLM attempts failed ‚Äì returning safe fallback JSON")
                return '{"score": 0, "reason": "LLM_TIMEOUT_OR_FAILURE", "is_passed": false}'

    return '{"score": 0, "reason": "Unknown execution error"}'

# ------------------------
# Evaluation
# ------------------------
T = TypeVar("T", bound=BaseModel)

def _check_and_handle_empty_context(context: str, sub_id: str, level: int) -> Optional[Dict[str, Any]]:
    """
    Returns Failure result if context is empty or contains known error strings.
    Auto-fail with PDCA keys all set to 0.
    """
    if not context or "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á" in context or "ERROR:" in context.upper():
        logger.warning(f"Auto-FAIL L{level} for {sub_id}: Empty or Error Context detected from RAG.")
        context_preview = context.strip()[:100].replace("\n", " ") if context else "Empty Context"
        return {
            "score": 0,
            "reason": f"‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏î‡πâ‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á (Context: {context_preview}).",
            "is_passed": False,
            "P_Plan_Score": 0,
            "D_Do_Score": 0,
            "C_Check_Score": 0,
            "A_Act_Score": 0,
        }
    return None

def _get_context_for_level(context: str, level: int) -> str:
    """Return context string with appropriate length limit for each level."""
    if not context:
        return ""
    # L1-L2 ‡πÉ‡∏ä‡πâ context ‡∏¢‡∏≤‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô
    if level <= 2:
        return context[:6000]  
    # L3-L5 ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÉ‡∏ô global_vars ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î Latency
    return context[:MAX_EVAL_CONTEXT_LENGTH]  

# =========================
# Main Evaluation Function
# =========================

def evaluate_with_llm(
    context: str, 
    sub_criteria_name: str, 
    level: int, 
    statement_text: str, 
    sub_id: str, 
    llm_executor: Any = None, 
    pdca_phase: str = "",
    level_constraint: str = "",
    must_include_keywords: str = "",
    avoid_keywords: str = "",
    max_rerank_score: float = 0.0,
    max_evidence_strength: float = 10.0,
    **kwargs
) -> Dict[str, Any]:
    """Standard Evaluation for L3+ with robust handling."""
    
    # üéØ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: ‡πÉ‡∏ä‡πâ logic ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î Context ‡∏ï‡∏≤‡∏° Level
    context_to_send_eval = _get_context_for_level(context, level)
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö context ‡∏ß‡πà‡∏≤‡∏á
    failure_result = _check_and_handle_empty_context(context, sub_id, level)
    if failure_result:
        return failure_result

    # ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å kwargs (‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÉ‡∏ä‡πâ)
    baseline_summary = kwargs.get("baseline_summary", "")
    aux_summary = kwargs.get("aux_summary", "")

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á User Prompt
    try:
        user_prompt = USER_ASSESSMENT_PROMPT.format(
            sub_criteria_name=sub_criteria_name,
            sub_id=sub_id,
            level=level,
            pdca_phase=pdca_phase,
            statement_text=statement_text,
            context=context_to_send_eval, # ‡πÉ‡∏ä‡πâ Context ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡∏±‡∏î‡πÅ‡∏•‡πâ‡∏ß
            level_constraint=level_constraint,
            must_include_keywords=must_include_keywords or "‡πÑ‡∏°‡πà‡∏°‡∏µ",
            avoid_keywords=avoid_keywords or "‡πÑ‡∏°‡πà‡∏°‡∏µ",
            max_rerank_score=max_rerank_score,
            max_evidence_strength=max_evidence_strength
        )
    except KeyError as e:
        logger.error(f"Missing placeholder in prompt template: {e}")
        user_prompt = f"‡πÄ‡∏Å‡∏ì‡∏ë‡πå: {sub_criteria_name} L{level}\n‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {statement_text}\n‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô: {context_to_send_eval}"

    # ‡πÄ‡∏û‡∏¥‡πà‡∏° summary ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
    if baseline_summary:
        user_prompt += f"\n\n--- Baseline summary (‡∏à‡∏≤‡∏Å Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤): ---\n{baseline_summary}"
    if aux_summary:
        user_prompt += f"\n\n--- Auxiliary evidence summary: ---\n{aux_summary}"

    # System Prompt (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ placeholder)
    try:
        # üéØ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠ key ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö SYSTEM_ASSESSMENT_PROMPT (‡∏Ñ‡∏∑‡∏≠ max_evidence_strength)
        system_prompt = SYSTEM_ASSESSMENT_PROMPT.format(
            max_evidence_strength=max_evidence_strength 
        )
    except KeyError:
        system_prompt = SYSTEM_ASSESSMENT_PROMPT  # fallback ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ

    # ‡πÄ‡∏û‡∏¥‡πà‡∏° schema
    try:
        schema_json = json.dumps(CombinedAssessment.model_json_schema(), ensure_ascii=False, indent=2)
    except Exception:
        schema_json = '{"score":0,"reason":"string"}'

    system_prompt += "\n\n--- JSON SCHEMA ---\n" + schema_json + "\nIMPORTANT: Respond only with valid JSON."

    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å LLM
    try:
        raw = _fetch_llm_response(system_prompt, user_prompt, _MAX_LLM_RETRIES, llm_executor=llm_executor)
        parsed = _robust_extract_json(raw)
        
        if not isinstance(parsed, dict):
            logger.error(f"Parsed result is not dict: {type(parsed)}")
            parsed = {}

        return {
            "score": int(parsed.get("score", 0)),
            "reason": parsed.get("reason", "No reason provided."),
            "is_passed": parsed.get("is_passed", False),
            "P_Plan_Score": int(parsed.get("P_Plan_Score", 0)),
            "D_Do_Score": int(parsed.get("D_Do_Score", 0)),
            "C_Check_Score": int(parsed.get("C_Check_Score", 0)),
            "A_Act_Score": int(parsed.get("A_Act_Score", 0)),
        }

    except Exception as e:
        logger.exception(f"evaluate_with_llm failed for {sub_id} L{level}: {e}")
        return {
            "score": 0,
            "reason": f"LLM error: {str(e)}",
            "is_passed": False,
            "P_Plan_Score": 0,
            "D_Do_Score": 0,
            "C_Check_Score": 0,
            "A_Act_Score": 0,
        }
    
# =========================
# Patch for L1-L2 evaluation
# =========================

# 1Ô∏è‚É£ ‡πÄ‡∏û‡∏¥‡πà‡∏° context limit ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö L1/L2
def _get_context_for_level(context: str, level: int) -> str:
    """Return context string with appropriate length limit for each level."""
    if not context:
        return ""
    if level <= 2:
        return context[:6000]  # L1-L2 ‡πÉ‡∏ä‡πâ context ‡∏¢‡∏≤‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô
    return context[:MAX_EVAL_CONTEXT_LENGTH]  # L3-L5

def _extract_combined_assessment(parsed: Dict[str, Any], score_default_key: str = "score") -> Dict[str, Any]:
    """Helper to safely extract combined assessment results."""
    # üü¢ NEW: Extract all scores needed by seam_assessment.py (Action #1 logic)
    score = int(parsed.get(score_default_key, 0))
    is_passed = parsed.get("is_passed", score >= 1) # ‡πÉ‡∏ä‡πâ score >= 1 ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤ default ‡∏ñ‡πâ‡∏≤ LLM ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏™‡πà‡∏á is_passed

    result = {
        "score": score,
        "reason": parsed.get("reason", "No reason provided by LLM."),
        "is_passed": is_passed,
        "P_Plan_Score": int(parsed.get("P_Plan_Score", 0)),
        "D_Do_Score": int(parsed.get("D_Do_Score", 0)),
        "C_Check_Score": int(parsed.get("C_Check_Score", 0)),
        "A_Act_Score": int(parsed.get("A_Act_Score", 0)),
    }
    return result


# =================================================================
# ULTIMATE PRODUCTION: evaluate_with_llm_low_level (L1/L2 Multi-Enabler)
# =================================================================
def evaluate_with_llm_low_level(
    context: str,
    sub_criteria_name: str,
    level: int,
    statement_text: str,
    sub_id: str,
    llm_executor: Any = None,
    pdca_phase: str = "",
    level_constraint: str = "",
    must_include_keywords: str = "",
    avoid_keywords: str = "",
    max_rerank_score: float = 0.0,
    max_evidence_strength: float = 10.0,
    contextual_rules_map: Optional[Dict[str, Any]] = None,
    enabler_id: str = "KM",
    **kwargs
) -> Dict[str, Any]:
    """
    Standard Evaluation for L1/L2 using LOW_LEVEL_PROMPT (Dynamic Multi-Enabler)
    - ‡∏î‡∏∂‡∏á plan_keywords ‡∏à‡∏≤‡∏Å contextual_rules_map (‡∏à‡∏≤‡∏Å pea_km_contextual_rules.json)
    - ‡πÑ‡∏°‡πà hardcode ‡∏≠‡∏µ‡∏Å‡∏ï‡πà‡∏≠‡πÑ‡∏õ
    - ‡∏™‡πà‡∏á‡∏Ñ‡πà‡∏≤ P/D/C/A ‡∏î‡∏¥‡∏ö‡∏à‡∏≤‡∏Å LLM ‚Üí ‡πÉ‡∏´‡πâ _run_single_assessment ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏Å‡∏é L1/L2 ‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
    """
    
    # -------------------- 1. Setup & Context Check --------------------
    context_to_send_eval = context[:MAX_EVAL_CONTEXT_LENGTH] if context else "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á"
    
    failure_result = _check_and_handle_empty_context(context, sub_id, level)
    if failure_result:
        return failure_result

    # -------------------- 2. ‡∏î‡∏∂‡∏á plan_keywords ‡∏à‡∏≤‡∏Å pea_km_contextual_rules.json --------------------
    plan_keywords = "‡∏ß‡∏¥‡∏™‡∏±‡∏¢‡∏ó‡∏±‡∏®‡∏ô‡πå, ‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢, ‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á, ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢"  # fallback ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô

    if contextual_rules_map:
        # 2.1 ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å sub-criteria ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Å‡πà‡∏≠‡∏ô (‡πÄ‡∏ä‡πà‡∏ô 1.1 ‚Üí L1)
        sub_rules = contextual_rules_map.get(sub_id, {})
        l1_rules = sub_rules.get("L1", {})
        if l1_rules and "plan_keywords" in l1_rules:
            plan_keywords = l1_rules["plan_keywords"]
        else:
            # 2.2 Fallback ‡πÑ‡∏õ‡πÉ‡∏ä‡πâ _enabler_defaults (‡πÄ‡∏ä‡πà‡∏ô KM, DX)
            default_rules = contextual_rules_map.get("_enabler_defaults", {})
            if "plan_keywords" in default_rules:
                plan_keywords = default_rules["plan_keywords"]

    logger.debug(f"[L{level}] Using plan_keywords: {plan_keywords}")

    # -------------------- 3. Prompt Building --------------------
    try:
        # ‡∏™‡πà‡∏á‡∏ó‡∏±‡πâ‡∏á plan_keywords ‡πÅ‡∏•‡∏∞ avoid_keywords ‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà Template ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
        system_prompt = SYSTEM_LOW_LEVEL_PROMPT.format(
            plan_keywords=plan_keywords,
            avoid_keywords=avoid_keywords or "‡πÑ‡∏°‡πà‡∏°‡∏µ"
        )
        system_prompt += "\n\nIMPORTANT: Respond only with valid JSON."

        user_prompt = USER_LOW_LEVEL_PROMPT_TEMPLATE.format(
            sub_id=sub_id,
            sub_criteria_name=sub_criteria_name,
            level=level,
            statement_text=statement_text,
            level_constraint=level_constraint or "‡πÑ‡∏°‡πà‡∏°‡∏µ",
            must_include_keywords=must_include_keywords or "‡πÑ‡∏°‡πà‡∏°‡∏µ",
            avoid_keywords=avoid_keywords or "‡πÑ‡∏°‡πà‡∏°‡∏µ",
            context=context_to_send_eval
        )

    except Exception as e:
        logger.error(f"Error formatting LOW_LEVEL_PROMPT: {e}. Using fallback prompt.")
        system_prompt = SYSTEM_LOW_LEVEL_PROMPT + "\n\nIMPORTANT: Respond only with valid JSON."
        user_prompt = f"‡πÄ‡∏Å‡∏ì‡∏ë‡πå: {sub_id} L{level}\n‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô: {context_to_send_eval}\n‡∏ï‡∏≠‡∏ö JSON ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô"

    # -------------------- 4. LLM Call --------------------
    try:
        raw = _fetch_llm_response(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            max_retries=_MAX_LLM_RETRIES,
            llm_executor=llm_executor
        )
        
        parsed = _robust_extract_json(raw)
        
        if not isinstance(parsed, dict):
            logger.error(f"LLM L{level} response parsed to non-dict: {type(parsed)}. Using empty dict.")
            parsed = {}

        # -------------------- 5. ‡∏™‡πà‡∏á‡∏Ñ‡πà‡∏≤‡∏î‡∏¥‡∏ö‡∏à‡∏≤‡∏Å LLM ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (‡πÑ‡∏°‡πà‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö C/A=0 ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà) --------------------
        return {
            "score": int(parsed.get("score", 0)),
            "reason": parsed.get("reason", "‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏à‡∏≤‡∏Å LLM"),
            "is_passed": parsed.get("is_passed", False),
            "P_Plan_Score": int(parsed.get("P_Plan_Score", 0)),
            "D_Do_Score": int(parsed.get("D_Do_Score", 0)),
            "C_Check_Score": int(parsed.get("C_Check_Score", 0)),
            "A_Act_Score": int(parsed.get("A_Act_Score", 0)),
        }

    except Exception as e:
        logger.exception(f"evaluate_with_llm_low_level failed for {sub_id} L{level}: {e}")
        return {
            "score": 0,
            "reason": f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô LLM: {str(e)}",
            "is_passed": False,
            "P_Plan_Score": 0,
            "D_Do_Score": 0,
            "C_Check_Score": 0,
            "A_Act_Score": 0,
        }
    
# ------------------------
# Summarize (FULL VERSION)
# ------------------------
def create_context_summary_llm(
    context: str, 
    sub_criteria_name: str, 
    level: int, 
    sub_id: str, 
    llm_executor: Any 
) -> Dict[str, Any]:
    logger = logging.getLogger("AssessmentApp")

    if llm_executor is None: 
        return {
            "summary": "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡∏∏‡∏õ‡πÑ‡∏î‡πâ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏ö‡∏ö LLM ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô",
            "suggestion_for_next_level": "‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ LLM"
        }

    context_safe = context or ""
    context_limited = context_safe.strip()
    
    if not context_limited or len(context_limited) < 50:
        return {
            "summary": "‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏î‡πâ‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡πâ‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô",
            "suggestion_for_next_level": "‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"
        }

    # Cap context ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Å‡∏±‡∏ö‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏° Model (4000-8000 chars)
    context_to_send = context_limited[:6000] 
    next_level = min(level + 1, 5)

    try:
        human_prompt = USER_EVIDENCE_DESCRIPTION_TEMPLATE.format(
            sub_id=f"{sub_id} - {sub_criteria_name}",
            level=level,
            next_level=next_level,
            context=context_to_send
        )
    except Exception as e:
        logger.error(f"Error formatting prompt: {e}")
        return {"summary": "Error formatting prompt", "suggestion_for_next_level": "Check template"}

    # ‡∏õ‡∏£‡∏±‡∏ö System Instruction ‡πÉ‡∏´‡πâ‡∏î‡∏∏‡∏î‡∏±‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î Invalid Format
    system_instruction = (
        f"{SYSTEM_EVIDENCE_DESCRIPTION_PROMPT}\n"
        "STRICT RULE: ‡∏ï‡∏≠‡∏ö‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö JSON ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡∏´‡πâ‡∏≤‡∏°‡∏°‡∏µ Markdown ‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏≥‡πÄ‡∏Å‡∏£‡∏¥‡πà‡∏ô‡∏ô‡∏≥\n"
        "EXPECTED FORMAT: {\"summary\": \"...\", \"suggestion_for_next_level\": \"...\"}"
    )

    max_retries = 2
    for attempt in range(1, max_retries + 1):
        try:
            logger.info(f"Generating Thai Summary for {sub_id} L{level} (Attempt {attempt})")
            
            raw_response_obj = llm_executor.generate(
                system=system_instruction, 
                prompts=[human_prompt]
            )

            # ‡∏î‡∏∂‡∏á Text ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å Response Object
            raw_response_str = ""
            if hasattr(raw_response_obj, 'generations'): 
                raw_response_str = raw_response_obj.generations[0][0].text
            elif hasattr(raw_response_obj, 'content'):   
                raw_response_str = raw_response_obj.content
            else:
                raw_response_str = str(raw_response_obj)

            # ‡πÉ‡∏ä‡πâ Regex Extract JSON (‡πÄ‡∏ú‡∏∑‡πà‡∏≠ LLM ‡πÉ‡∏™‡πà‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ñ‡∏°‡∏°‡∏≤)
            parsed = _extract_normalized_dict(raw_response_str)
            
            if parsed and isinstance(parsed, dict):
                # ‡πÉ‡∏ä‡πâ .get() ‡∏û‡∏£‡πâ‡∏≠‡∏° Default Value ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô KeyMissingError
                sum_text = parsed.get("summary") or parsed.get("‡∏™‡∏£‡∏∏‡∏õ") or ""
                sug_text = parsed.get("suggestion_for_next_level") or parsed.get("‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥") or ""

                if sum_text:
                    return {
                        "summary": str(sum_text).strip(),
                        "suggestion_for_next_level": str(sug_text).strip() if sug_text else "‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏ñ‡∏±‡∏î‡πÑ‡∏õ"
                    }
            
            logger.warning(f"Attempt {attempt}: LLM returned invalid summary format.")
            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏≠‡∏ö‡πÅ‡∏£‡∏Å‡πÅ‡∏•‡πâ‡∏ß‡∏û‡∏±‡∏á ‡∏£‡∏≠‡∏ö‡∏™‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏¢‡πâ‡∏≥ Force JSON ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡∏≠‡∏µ‡∏Å‡πÉ‡∏ô prompt
            human_prompt += "\nReminder: Return ONLY JSON."
            
        except Exception as e:
            logger.error(f"Attempt {attempt} failed: {str(e)}")
            time.sleep(0.5)

    return {
        "summary": f"‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö {level} ‡πÅ‡∏ï‡πà‡∏£‡∏∞‡∏ö‡∏ö‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏á (Parse Error)",
        "suggestion_for_next_level": f"‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Ç‡∏≠‡∏á Level {next_level} ‡πÉ‡∏ô‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠"
    }


# =================================================================
# 1. JSON Extractor (‡∏ó‡∏ô‡∏ó‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î)
# =================================================================
def _extract_json_array_for_action_plan(text: Any, logger: logging.Logger) -> List[Dict[str, Any]]:
    """
    ‡∏™‡∏Å‡∏±‡∏î JSON Array ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å Text ‡πÇ‡∏î‡∏¢‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ã‡πà‡∏≠‡∏°‡πÅ‡∏ã‡∏°‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á (Auto-Repair)
    ‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Delimiter Error/Control Characters
    """
    try:
        if not isinstance(text, str):
            text = str(text) if text is not None else ""

        if not text.strip():
            return []

        # 1. ‡∏•‡∏ö Markdown Block (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
        clean_text = re.sub(r'```(?:json)?\s*([\s\S]*?)\s*```', r'\1', text, flags=re.IGNORECASE).strip()

        # 2. ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï‡∏ó‡∏µ‡πà‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á [ ] ‡∏´‡∏£‡∏∑‡∏≠ { }
        start_idx = clean_text.find('[')
        end_idx = clean_text.rfind(']')

        if start_idx == -1:
            # ‡∏Å‡∏£‡∏ì‡∏µ LLM ‡∏™‡πà‡∏á‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô Object ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (Single Phase)
            start_idx = clean_text.find('{')
            end_idx = clean_text.rfind('}')
            if start_idx == -1: return []
            json_candidate = clean_text[start_idx:end_idx + 1]
        else:
            json_candidate = clean_text[start_idx:end_idx + 1]

        # 3. ‡∏•‡πâ‡∏≤‡∏á‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏° (Control Characters) ‡∏ó‡∏µ‡πà‡∏°‡∏±‡∏Å‡∏ó‡∏≥‡πÉ‡∏´‡πâ JSON Parse ‡∏û‡∏±‡∏á
        # ‡∏•‡∏ö ASCII 0-31 ‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô newline, tab, carriage return
        json_candidate = "".join(char for char in json_candidate if ord(char) >= 32 or char in "\n\r\t")

        # 4. ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏¢‡πà‡∏≠‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏° Parse
        def try_parse(content):
            try:
                # json5 ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Trailing Comma ‡πÅ‡∏•‡∏∞ Single Quote
                data = json5.loads(content)
                return data if isinstance(data, list) else [data]
            except Exception:
                return None

        # --- ‡∏•‡∏≠‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà 1: Parse ‡∏õ‡∏Å‡∏ï‡∏¥ ---
        result = try_parse(json_candidate)
        if result: return result

        # --- ‡∏•‡∏≠‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà 2: ‡∏ã‡πà‡∏≠‡∏°‡πÅ‡∏ã‡∏°‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏Ñ‡∏≥‡∏û‡∏π‡∏î (Smart Quotes) ---
        repaired_quotes = json_candidate.replace('‚Äú', '"').replace('‚Äù', '"').replace("'", '"')
        result = try_parse(repaired_quotes)
        if result: return result

        # --- ‡∏•‡∏≠‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà 3: ‡∏Å‡∏£‡∏ì‡∏µ JSON ‡∏ï‡∏±‡∏î‡∏à‡∏ö (Truncated Repair) ---
        # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏õ‡∏¥‡∏î Bracket ‡∏ó‡∏µ‡πà LLM ‡πÄ‡∏à‡∏ô‡πÑ‡∏°‡πà‡∏à‡∏ö
        logger.warning("JSON truncated or malformed, attempting brute-force closure...")
        for suffix in ["]", "}", "}]", "}]}]", "}\n]"]:
            result = try_parse(json_candidate + suffix)
            if result:
                logger.info(f"‚úÖ Auto-repaired JSON success with suffix: {suffix}")
                return result

        # --- ‡∏•‡∏≠‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà 4: ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ ‡πÉ‡∏ä‡πâ Regex ‡∏î‡∏∂‡∏á Object ‡∏ó‡∏µ‡∏•‡∏∞‡∏ï‡∏±‡∏ß (Fallback) ---
        logger.warning("Falling back to Regex Object Extraction...")
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ pattern { ... } ‡∏ó‡∏µ‡πà‡∏î‡∏π‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô object
        objects = re.findall(r'\{(?:[^{}]|(?R))*\}', json_candidate) # ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ regex module ‡∏û‡∏¥‡πÄ‡∏®‡∏© ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢:
        if not objects:
            objects = re.findall(r'\{[\s\S]*?\}', json_candidate)
            
        fallback_results = []
        for obj_str in objects:
            try:
                obj_data = json5.loads(obj_str)
                if isinstance(obj_data, dict):
                    fallback_results.append(obj_data)
            except:
                continue
        
        if fallback_results:
            logger.info(f"‚úÖ Recovered {len(fallback_results)} objects via regex")
            return fallback_results

        logger.error(f"Failed to parse JSON. Snippet: {json_candidate[:200]}...")
        return []

    except Exception as e:
        logger.error(f"Extraction logic failed: {str(e)}", exc_info=True)
        return []

# =================================================================
# 2. Key Normalizer (‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö schema ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î)
# =================================================================

def action_plan_normalize_keys(obj: Any) -> Any:
    """
    ‡πÅ‡∏õ‡∏•‡∏á key ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö schema ‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Data Type Enforcement)
    ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞ failed_level ‡πÅ‡∏•‡∏∞ Step ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô Integer 100%
    """
    if isinstance(obj, list):
        return [action_plan_normalize_keys(i) for i in obj]
    
    if isinstance(obj, dict):
        field_mapping = {
            # Phase & Action level
            'phase': 'phase', 'Phase': 'phase',
            'goal': 'goal', 'Goal': 'goal',
            'actions': 'actions', 'Actions': 'actions',
            
            'statement_id': 'statement_id', 'Statement_ID': 'statement_id',
            'statement id': 'statement_id', 'title': 'statement_id', 'id': 'statement_id',
            
            'failed_level': 'failed_level', 'Failed_Level': 'failed_level',
            'failed level': 'failed_level', 'level': 'failed_level',
            
            'recommendation': 'recommendation', 'Recommendation': 'recommendation',
            'recommend': 'recommendation',
            
            'target_evidence_type': 'target_evidence_type', 'Target_Evidence_Type': 'target_evidence_type',
            'evidence_type': 'target_evidence_type', 'evidence': 'target_evidence_type',
            
            'key_metric': 'key_metric', 'Key_Metric': 'key_metric',
            'metric': 'key_metric',
            
            'steps': 'steps', 'Steps': 'steps',
            
            # StepDetail (Capitalized per schema)
            'step': 'Step', 'Step': 'Step',
            'description': 'Description', 'Description': 'Description', 'desc': 'Description',
            'responsible': 'Responsible', 'Responsible': 'Responsible', 'owner': 'Responsible',
            'tools_templates': 'Tools_Templates', 'Tools_Templates': 'Tools_Templates', 'tools': 'Tools_Templates',
            'verification_outcome': 'Verification_Outcome', 'Verification_Outcome': 'Verification_Outcome', 'outcome': 'Verification_Outcome',
        }
        
        new_obj = {}
        for k, v in obj.items():
            # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î Key
            k_clean = k.lower().replace('_', ' ').replace('-', ' ').strip()
            k_no_space = k_clean.replace(' ', '')
            target_key = field_mapping.get(k_clean) or field_mapping.get(k_no_space) or k
            
            # --- [CRITICAL FIX] ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç ---
            if target_key in ['failed_level', 'Step']:
                try:
                    if isinstance(v, str):
                        # ‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠‡πÉ‡∏ô string ‡πÄ‡∏ä‡πà‡∏ô "Level 3" -> 3
                        nums = re.findall(r'\d+', v)
                        v = int(nums[0]) if nums else 0
                    else:
                        v = int(v) if v is not None else 0
                except (ValueError, IndexError):
                    v = 0 # Fallback default
            
            new_obj[target_key] = action_plan_normalize_keys(v)
        
        return new_obj
    
    return obj


# =================================================================
# 3. Main Function: create_structured_action_plan
# =================================================================
def create_structured_action_plan(
    recommendation_statements: List[Dict[str, Any]],
    sub_id: str,
    sub_criteria_name: str,
    target_level: int,
    llm_executor: Any,
    logger: logging.Logger,
    max_retries: int = 3
) -> List[Dict[str, Any]]:
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á Action Plan ‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô Pydantic validation 100%
    ‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏à‡∏≤‡∏Å config.global_vars ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ï‡πá‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö:
    - ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Phase ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
    - ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Steps ‡∏ï‡πà‡∏≠ Action
    - ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß Step (‡∏Ñ‡∏≥)
    - ‡∏†‡∏≤‡∏©‡∏≤
    """
    from config import global_vars as gv

    # --- Sustain Mode (‡πÑ‡∏°‡πà‡∏°‡∏µ Gap) ---
    if not recommendation_statements:
        logger.info(f"[Sustain Mode] No gaps found ‚Üí Level {target_level}")
        return [{
            "phase": f"Level {target_level} Sustain & Innovation",
            "goal": f"‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏¢‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö {sub_criteria_name} ‡∏™‡∏π‡πà‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏•‡∏¥‡∏®‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á",
            "actions": [{
                "statement_id": f"SUSTAIN_L{target_level}",
                "failed_level": target_level,
                "recommendation": "‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ó‡∏≥ Benchmarking ‡∏Å‡∏±‡∏ö Best Practice ‡∏™‡∏≤‡∏Å‡∏•",
                "target_evidence_type": "Internal Audit Report / External Benchmarking Report",
                "key_metric": f"Maintain Maturity ‚â• Level {target_level}",
                "steps": [{
                    "Step": "1",
                    "Description": "‡∏ó‡∏ö‡∏ó‡∏ß‡∏ô‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏•‡∏∞ KPI ‡∏£‡∏≤‡∏¢‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™ ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏ï‡∏≤‡∏° PDCA",
                    "Responsible": "KM Committee / Top Management",
                    "Tools_Templates": "PDCA Dashboard / Quarterly Review Template",
                    "Verification_Outcome": "Quarterly KM Review Report"
                }, {
                    "Step": "2",
                    "Description": "‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏Ñ‡πâ‡∏ô‡∏Ñ‡∏ß‡πâ‡∏≤ Best Practices ‡∏à‡∏≤‡∏Å‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£‡∏ä‡∏±‡πâ‡∏ô‡∏ô‡∏≥‡∏ó‡∏±‡πâ‡∏á‡πÉ‡∏ô‡πÅ‡∏•‡∏∞‡∏ï‡πà‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®",
                    "Responsible": "KM Team",
                    "Tools_Templates": "Benchmarking Framework",
                    "Verification_Outcome": "Benchmarking Study Report"
                }]
            }]
        }]

    # --- ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Gap ‡πÅ‡∏•‡∏∞‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Phase ‡∏ï‡∏≤‡∏°‡∏£‡∏∞‡∏î‡∏±‡∏ö ---
    max_failed_level = max([s.get('level', 0) for s in recommendation_statements] or [1])

    if max_failed_level >= 5:
        advice_focus = "Innovation, External Benchmarking, Digital Transformation ‡πÅ‡∏•‡∏∞ Continuous Improvement"
    elif max_failed_level >= 3:
        advice_focus = "Standardization, KPI Monitoring, PDCA Cycle ‡πÅ‡∏•‡∏∞ Evidence Strengthening"
    else:
        advice_focus = "Policy Establishment, Resource Allocation, Communication ‡πÅ‡∏•‡∏∞ Basic Training"

    stmt_blocks = [
        f"- [Level {s.get('level')}] {s.get('statement')} (Gap: {s.get('reason')})"
        for s in recommendation_statements
    ]

    # --- ‡∏™‡∏£‡πâ‡∏≤‡∏á Prompt ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡πà‡∏á config ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö LLM ---
    human_prompt = ACTION_PLAN_PROMPT.format(
        sub_id=sub_id,
        sub_criteria_name=sub_criteria_name,
        target_level=target_level,
        advice_focus=advice_focus,
        recommendation_statements_list="\n".join(stmt_blocks),
        json_schema=schema_json,
        max_phases=gv.MAX_ACTION_PLAN_PHASES,
        max_steps=gv.MAX_STEPS_PER_ACTION,
        max_words_per_step=gv.ACTION_PLAN_STEP_MAX_WORDS,
        language="‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢" if gv.ACTION_PLAN_LANGUAGE == "th" else "English"
    )

    # --- Retry Loop ---
    for attempt in range(1, max_retries + 1):
        try:
            logger.info(f"Action Plan Generation | Attempt {attempt}/{gv.OLLAMA_MAX_RETRIES}")
            response = llm_executor.generate(
                system=SYSTEM_ACTION_PLAN_PROMPT,
                prompts=[human_prompt],
                temperature=gv.LLM_TEMPERATURE,
                max_tokens=3000
            )

            raw_text = ""
            if hasattr(response, 'generations') and response.generations:
                raw_text = response.generations[0][0].text
            elif hasattr(response, 'text'):
                raw_text = response.text
            else:
                raw_text = str(response)

            if attempt == 1:
                logger.debug(f"Raw Response (first 800 chars):\n{raw_text[:800]}")

            items = _extract_json_array_for_action_plan(raw_text, logger)
            if not items:
                logger.warning(f"Attempt {attempt}: No JSON extracted")
                continue

            validated_output = []
            for idx, entry in enumerate(items):
                try:
                    clean_entry = action_plan_normalize_keys(entry)
                    validated = ActionPlanActions.model_validate(clean_entry)
                    validated_output.append(validated.model_dump(by_alias=False))
                except Exception as ve:
                    logger.error(f"Entry {idx} validation failed: {ve}")
                    if idx < 3:
                        logger.debug(f"Failed Entry:\n{json.dumps(clean_entry, ensure_ascii=False, indent=2)[:1500]}")

            if validated_output:
                logger.info(f"‚úÖ Success: {len(validated_output)} valid phase(s) on attempt {attempt}")
                return validated_output

        except Exception as e:
            logger.error(f"Attempt {attempt} error: {e}", exc_info=True)

    # --- Emergency Fallback ---
    logger.warning("All attempts failed ‚Üí returning emergency fallback plan")
    return [{
        "phase": "Phase 1: Immediate Action Required",
        "goal": f"‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏´‡∏•‡∏±‡∏Å‡πÉ‡∏ô {sub_criteria_name} ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏£‡πà‡∏á‡∏î‡πà‡∏ß‡∏ô",
        "actions": [{
            "statement_id": f"GAP_L{max_failed_level}",
            "failed_level": max_failed_level,
            "recommendation": f"‡πÅ‡∏ï‡πà‡∏á‡∏ï‡∏±‡πâ‡∏á‡∏ó‡∏µ‡∏°‡∏á‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏ó‡∏≥‡πÅ‡∏ú‡∏ô‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡∏ö‡∏ö ‡πÇ‡∏î‡∏¢‡πÄ‡∏ô‡πâ‡∏ô {advice_focus}",
            "target_evidence_type": "‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÅ‡∏ï‡πà‡∏á‡∏ï‡∏±‡πâ‡∏á / ‡πÅ‡∏ú‡∏ô‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£",
            "key_metric": "‡∏à‡∏±‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏ó‡∏µ‡∏°‡πÅ‡∏•‡∏∞‡∏≠‡∏ô‡∏∏‡∏°‡∏±‡∏ï‡∏¥‡πÅ‡∏ú‡∏ô‡∏†‡∏≤‡∏¢‡πÉ‡∏ô 3 ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô",
            "steps": [
                {"Step": "1", "Description": "‡πÅ‡∏ï‡πà‡∏á‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡∏ì‡∏∞‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏û‡∏±‡∏í‡∏ô‡∏≤ KM ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏ô‡∏µ‡πâ", "Responsible": "‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î", "Tools_Templates": "‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÅ‡∏ï‡πà‡∏á‡∏ï‡∏±‡πâ‡∏á", "Verification_Outcome": "‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÅ‡∏ï‡πà‡∏á‡∏ï‡∏±‡πâ‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£"},
                {"Step": "2", "Description": "‡∏à‡∏±‡∏î‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏° Kick-off ‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Gap ‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô", "Responsible": "‡∏´‡∏±‡∏ß‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡∏° KM", "Tools_Templates": "Gap Analysis Template", "Verification_Outcome": "‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°"}
            ]
        }]
    }]
