"""
llm_data_utils.py
Robust LLM + RAG utilities for SEAM assessment (CLEAN FINAL VERSION)
"""

import logging
import time
import json
import hashlib
import uuid
import re
from datetime import datetime
from typing import List, Dict, Any, Optional, Union, Callable, TypeVar, Set
import json5


# Optional: regex ‡πÅ‡∏ó‡∏ô re (‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤) ‚Äî ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡πá‡πÉ‡∏ä‡πâ re ‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤
try:
    import regex as re  # type: ignore
except ImportError:
    pass  # ‡πÉ‡∏ä‡πâ re ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ

logger = logging.getLogger(__name__)
if not logger.handlers:
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# ===================================================================
# 1. Core Configuration (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô)
# ===================================================================
from config.global_vars import (
    DEFAULT_ENABLER,
    INITIAL_TOP_K,
    FINAL_K_RERANKED,
    MAX_EVAL_CONTEXT_LENGTH,
)

# ===================================================================
# 2. Critical Utilities (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏à‡∏£‡∏¥‡∏á ‚Äî ‡πÑ‡∏°‡πà‡∏°‡∏µ fallback)
# ===================================================================
from core.vectorstore import _get_collection_name, get_hf_embeddings
from core.json_extractor import (
    _robust_extract_json,
    _normalize_keys,
    _safe_int_parse,
    _extract_normalized_dict
)

# ===================================================================
# 3. Project Modules (‡∏ñ‡πâ‡∏≤‡∏´‡∏≤‡∏¢ ‚Üí ‡∏õ‡∏•‡πà‡∏≠‡∏¢‡πÉ‡∏´‡πâ error ‡∏ä‡∏±‡∏î ‡πÜ ‡πÑ‡∏õ‡πÄ‡∏•‡∏¢ ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏á‡∏µ‡∏¢‡∏ö)
# ===================================================================
from core.seam_prompts import (
    SYSTEM_ASSESSMENT_PROMPT,
    USER_ASSESSMENT_PROMPT,
    SYSTEM_ACTION_PLAN_PROMPT,
    ACTION_PLAN_PROMPT,
    SYSTEM_EVIDENCE_DESCRIPTION_PROMPT,
    EVIDENCE_DESCRIPTION_PROMPT,
    SYSTEM_LOW_LEVEL_PROMPT,
    USER_LOW_LEVEL_PROMPT,
)

from core.vectorstore import VectorStoreManager, get_global_reranker, ChromaRetriever
from core.assessment_schema import CombinedAssessment, EvidenceSummary
from core.action_plan_schema import ActionPlanActions

try:
    from core.assessment_schema import StatementAssessment
except ImportError:
    from pydantic import BaseModel
    class StatementAssessment(BaseModel):
        score: int = 0
        reason: str = ""

from langchain_core.documents import Document as LcDocument

# ===================================================================
# 4. Constants
# ===================================================================
LOW_LEVEL_K: int = 3
_MOCK_FLAG = False
_MAX_LLM_RETRIES = 3

def set_mock_control_mode(enable: bool):
    global _MOCK_FLAG
    _MOCK_FLAG = bool(enable)
    logger.info(f"Mock control mode: {_MOCK_FLAG}")

# ------------------------
# Retrieval: retrieve_context_by_doc_ids (Level 2 Hydration)
# ------------------------
def retrieve_context_by_doc_ids(
    doc_uuids: List[str], # <--- Input ‡∏Ñ‡∏∑‡∏≠ Chunk UUIDs (64-char_index) ‡∏´‡∏£‡∏∑‡∏≠ Stable Doc UUID (64-char)
    doc_type: str,
    enabler: Optional[str] = None,
    vectorstore_manager: Optional['VectorStoreManager'] = None
) -> Dict[str, Any]:
    
    # ‡πÑ‡∏°‡πà‡∏°‡∏µ Doc UUID ‚Üí ‡πÑ‡∏°‡πà‡∏°‡∏µ evidence
    if not doc_uuids:
        return {"top_evidences": []}

    # ‡πÉ‡∏ä‡πâ manager ‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤ (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ) ‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà
    manager = vectorstore_manager if vectorstore_manager else VectorStoreManager()
    if manager is None:
        logger.error("VectorStoreManager is None.")
        return {"top_evidences": []}

    # üü¢ NEW FIX: ‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á ID
    chunk_uuids_for_chroma = []
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ Doc ID Map ‡πÑ‡∏´‡∏°
    if not hasattr(manager, 'doc_id_map') or not manager.doc_id_map:
        logger.warning("VSM Doc ID Map is missing or empty! Using input IDs directly (may fail Hydration).")
        # Fallback (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡πÇ‡∏Ñ‡πâ‡∏î‡∏û‡∏±‡∏á)
        chunk_uuids_for_chroma = doc_uuids
        
    else:
        for input_id in doc_uuids:
            input_id_str = str(input_id).strip()
            # 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô Stable Doc ID (64 ‡∏ï‡∏±‡∏ß) ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á Map ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            if len(input_id_str) == 64 and input_id_str in manager.doc_id_map:
                # üéØ ‡πÅ‡∏õ‡∏•‡∏á: ‡πÉ‡∏ä‡πâ Stable Doc ID ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Chunk UUIDs ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
                mapped_info = manager.doc_id_map.get(input_id_str, {})
                full_chunk_list = mapped_info.get('chunk_uuids', [])
                chunk_uuids_for_chroma.extend(full_chunk_list)
            # 2. ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà 64 ‡∏ï‡∏±‡∏ß ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö Stable Doc ID (‡∏≠‡∏≤‡∏à‡πÄ‡∏õ‡πá‡∏ô Chunk ID ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß) ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
            else:
                chunk_uuids_for_chroma.append(input_id_str) 

        # Log ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
        if len(chunk_uuids_for_chroma) > len(doc_uuids):
            logger.info(f"VSM: Mapped {len(doc_uuids)} Stable IDs to {len(chunk_uuids_for_chroma)} full Chunk UUIDs for Chroma.")

    # ‡∏•‡∏ö‡∏ã‡πâ‡∏≥‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤ Chroma
    final_uuids_to_retrieve = list(set(chunk_uuids_for_chroma))
    if not final_uuids_to_retrieve:
        logger.warning("VSM: No valid Chunk UUIDs found after mapping and cleaning.")
        return {"top_evidences": []}
    
    # END OF NEW FIX: ‡πÉ‡∏ä‡πâ final_uuids_to_retrieve ‡πÅ‡∏ó‡∏ô doc_uuids

    try:
        # üéØ FIX: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏õ‡πÉ‡∏ä‡πâ retrieve_by_chunk_uuids ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Chunk ‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (1:1 Hydration)
        collection_name = _get_collection_name(doc_type, enabler or DEFAULT_ENABLER)
        
        # docs ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ LcDocument ‡∏ó‡∏µ‡πà‡∏°‡∏µ page_content ‡πÅ‡∏•‡∏∞ metadata ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö Chunk ID ‡∏ô‡∏±‡πâ‡∏ô ‡πÜ
        # ‡πÉ‡∏ä‡πâ ID ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏õ‡∏•‡∏á‡πÅ‡∏•‡πâ‡∏ß
        docs: List[LcDocument] = manager.retrieve_by_chunk_uuids(final_uuids_to_retrieve, collection_name) 

        top_evidences = []
        for d in docs:
            md = getattr(d, "metadata", {}) or {}
            
            # ‚úÖ FIX: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ chunk_uuid ‡∏ã‡∏∂‡πà‡∏á‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏õ‡πá‡∏ô 64-char_index ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏î‡∏∂‡∏á‡∏°‡∏≤
            # ‡πÉ‡∏ô retrieve_by_chunk_uuids, chunk_uuid ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÉ‡∏™‡πà‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô metadata
            final_chunk_uuid = md.get("chunk_uuid") or md.get("stable_doc_uuid") 
            
            top_evidences.append({
                # doc_id: ID ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏´‡∏•‡∏±‡∏Å (64 ‡∏ï‡∏±‡∏ß)
                "doc_id": md.get("stable_doc_uuid"),
                # chunk_uuid: ID ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á Chunk (64-char_index)
                "chunk_uuid": final_chunk_uuid, 
                "doc_type": md.get("doc_type"),
                "source": md.get("source") or md.get("doc_source"),
                "source_filename": md.get("source") or md.get("doc_source"),  
                "content": getattr(d, "page_content", "").strip(),
                "chunk_index": md.get("chunk_index")
            })

        return {"top_evidences": top_evidences}

    except Exception as e:
        logger.error(f"retrieve_context_by_doc_ids error: {e}")
        return {"top_evidences": []}

# ------------------------
# Retrieval: retrieve_context_with_filter (‡πÅ‡∏Å‡πâ‡∏à‡∏∏‡∏î‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á 2 ‡∏à‡∏∏‡∏î)
# ------------------------
# ------------------------
# Retrieval: retrieve_context_with_filter (‡πÅ‡∏Å‡πâ‡∏à‡∏∏‡∏î‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á 2 ‡∏à‡∏∏‡∏î)
# ------------------------
def retrieve_context_with_filter(
    query: Union[str, List[str]],
    doc_type: str,
    enabler: Optional[str] = None,
    subject: Optional[str] = None, # üü¢ ‡πÄ‡∏û‡∏¥‡πà‡∏° subject
    vectorstore_manager: Optional['VectorStoreManager'] = None,
    mapped_uuids: Optional[List[str]] = None,
    stable_doc_ids: Optional[List[str]] = None, 
    priority_docs_input: Optional[List[Any]] = None,
    sequential_chunk_uuids: Optional[List[str]] = None,
    sub_id: Optional[str] = None,
    level: Optional[int] = None,
    get_previous_level_docs: Optional[Callable[[int, str], List[Any]]] = None,
) -> Dict[str, Any]:
    """
    ‡∏î‡∏∂‡∏á context ‡∏î‡πâ‡∏ß‡∏¢ semantic search + priority + fallback + rerank
    """
    start_time = time.time()
    all_retrieved_chunks: List[Any] = []
    used_chunk_uuids: List[str] = []

    # 1. ‡πÉ‡∏ä‡πâ VectorStoreManager ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å!)
    manager = vectorstore_manager or VectorStoreManager()
    if manager is None or manager._client is None:
        logger.error("VectorStoreManager not initialized!")
        return {"top_evidences": [], "aggregated_context": "", "retrieval_time": 0.0, "used_chunk_uuids": []}

    queries_to_run = [query] if isinstance(query, str) else list(query or [])
    if not queries_to_run:
        queries_to_run = [""]  # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô error

    # ‡∏£‡∏ß‡∏° chunk ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÇ‡∏ú‡∏•‡πà (sequential)
    if sequential_chunk_uuids:
        mapped_uuids = (mapped_uuids or []) + sequential_chunk_uuids

    # 2. Fallback ‡∏à‡∏≤‡∏Å level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Level 3)
    fallback_chunks = []
    if level == 3 and callable(get_previous_level_docs):
        try:
            fallback_chunks = get_previous_level_docs(level - 1, sub_id) or []
            logger.info(f"Fallback from previous level: {len(fallback_chunks)} chunks")
        except Exception as e:
            logger.warning(f"Fallback failed: {e}")

    # 3. Priority chunks (‡πÄ‡∏ä‡πà‡∏ô ‡∏à‡∏≤‡∏Å evidence mapping)
    guaranteed_priority_chunks = []
    if priority_docs_input:
        for doc in priority_docs_input:
            if doc is None:
                continue
            if isinstance(doc, dict):
                pc = doc.get('page_content') or doc.get('text') or ''
                meta = doc.get('metadata') or {}
                
                # üéØ FIX C: ‡∏ô‡∏≥ chunk_uuid ‡πÅ‡∏•‡∏∞ doc_id (stable_doc_uuid) ‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡πÉ‡∏ô metadata
                if 'chunk_uuid' in doc:
                    meta['chunk_uuid'] = doc['chunk_uuid']
                if 'doc_id' in doc:
                    meta['stable_doc_uuid'] = doc['doc_id']
                if 'pdca_tag' in doc:
                     meta['pdca_tag'] = doc['pdca_tag'] # ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Å‡πá‡∏ö PDCA tag ‡∏î‡πâ‡∏ß‡∏¢

                if pc.strip():
                    guaranteed_priority_chunks.append(LcDocument(page_content=pc, metadata=meta))
            elif hasattr(doc, 'page_content'):
                guaranteed_priority_chunks.append(doc)

    # 4. ‡∏î‡∏∂‡∏á collection name ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏ï‡∏±‡∏ß
    collection_name = _get_collection_name(doc_type, enabler or DEFAULT_ENABLER)
    logger.info(f"Requesting retriever ‚Üí collection='{collection_name}' (doc_type={doc_type}, enabler={enabler})")

    # üü¢ Logic ‡∏™‡∏£‡πâ‡∏≤‡∏á Filter WHERE ‡∏à‡∏≤‡∏Å stable_doc_ids ‡πÅ‡∏•‡∏∞ subject
    where_filter: Dict[str, Any] = {}
    doc_id_filter: Dict[str, Any] = {}
    
    # 4.1 Filter: Stable Doc IDs (Hard Filter)
    if stable_doc_ids:
        logger.info(f"Applying Stable Doc ID filter: {len(stable_doc_ids)} IDs")
        doc_id_filter = {"stable_doc_uuid": {"$in": stable_doc_ids}} 
        where_filter = doc_id_filter # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Doc ID Filter

    # 4.2 Filter: Subject (Soft Filter)
    if subject:
        subject_filter = {"subject": {"$eq": subject}}
        
        if where_filter:
            # ‡πÉ‡∏ä‡πâ $and ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏ß‡∏°‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç: (ID ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏£‡∏á AND Subject ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏£‡∏á)
            where_filter = {"$and": [where_filter, subject_filter]}
            logger.info(f"Adding Subject filter (AND logic): {subject}")
        else:
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ stable_doc_ids ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ subject ‡πÄ‡∏õ‡πá‡∏ô filter ‡∏´‡∏•‡∏±‡∏Å
            where_filter = subject_filter
            logger.warning("Applying Subject filter only (no Stable Doc IDs).")


    retriever = manager.get_retriever(collection_name) 
    if not retriever:
        logger.error(f"Retriever NOT FOUND for collection: {collection_name}")
        logger.error(f"Available collections: {list(manager._chroma_cache.keys())}")
        retrieved_chunks = []
    else:
        retrieved_chunks = []
        for q in queries_to_run:
            q_log = q[:120] + "..." if len(q) > 120 else q
            logger.critical(f"[QUERY] Running: '{q_log}' ‚Üí collection='{collection_name}'")

            try:
                # üéØ FIX: ‡∏£‡∏ß‡∏° Filter ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏ô search_kwargs
                search_kwargs = {"k": INITIAL_TOP_K}
                if where_filter:
                    search_kwargs["where"] = where_filter

                if hasattr(retriever, "get_relevant_documents"):
                    docs = retriever.get_relevant_documents(q, search_kwargs=search_kwargs) 
                elif hasattr(retriever, "invoke"):
                    docs = retriever.invoke(q, config={"configurable": {"search_kwargs": search_kwargs}})
                else:
                    docs = []
                retrieved_chunks.extend(docs or [])
            except Exception as e:
                logger.error(f"Retriever invoke failed: {e}", exc_info=True)

    logger.critical(f"[RETRIEVAL] Raw chunks from ChromaDB: {len(retrieved_chunks)} documents")

    # 5. ‡∏£‡∏ß‡∏° + deduplicate ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
    all_chunks = retrieved_chunks + fallback_chunks + guaranteed_priority_chunks
    unique_map: Dict[str, LcDocument] = {}

    for doc in all_chunks:
        if not doc or not hasattr(doc, "page_content"):
            continue
        md = getattr(doc, "metadata", {}) or {}
        pc = str(getattr(doc, "page_content", "") or "").strip()
        if not pc:
            continue

        # ‡∏ï‡∏±‡∏î content ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Level 3
        if level == 3:
            pc = pc[:500]
            doc.page_content = pc

        # üéØ FIX: ‡πÉ‡∏ä‡πâ chunk_uuid ‡∏ã‡∏∂‡πà‡∏á‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠ ID 64-char_index ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Dedup
        # TEMP-ID ‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡∏ñ‡∏π‡∏Å‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö dedup ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß ‡πÅ‡∏ï‡πà‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏Å‡∏£‡∏≠‡∏á‡∏≠‡∏≠‡∏Å‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 7
        chunk_uuid = md.get("chunk_uuid") or md.get("stable_doc_uuid") or f"TEMP-{uuid.uuid4().hex[:12]}"
        if chunk_uuid not in unique_map:
            md["dedup_chunk_uuid"] = chunk_uuid
            unique_map[chunk_uuid] = doc

    dedup_chunks = list(unique_map.values())
    logger.info(f"After dedup: {len(dedup_chunks)} chunks")

    # 6. Rerank (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ reranker ‡πÅ‡∏•‡∏∞‡∏°‡∏µ slot ‡∏ß‡πà‡∏≤‡∏á)
    final_docs = list(guaranteed_priority_chunks)
    slots_left = max(0, FINAL_K_RERANKED - len(final_docs))
    candidates = [d for d in dedup_chunks if d not in final_docs]

    # **NEW:** 6.0. ‡∏™‡∏£‡πâ‡∏≤‡∏á Map ‡πÄ‡∏û‡∏∑‡πà‡∏≠ Patch Metadata ‡∏ó‡∏µ‡πà‡∏´‡∏≤‡∏¢‡πÑ‡∏õ‡∏Å‡∏•‡∏±‡∏ö‡∏Ñ‡∏∑‡∏ô‡∏°‡∏≤ (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Reranker ‡∏•‡πâ‡∏≤‡∏á metadata)
    candidate_metadata_map = {
        doc.page_content: getattr(doc, 'metadata', {}) 
        for doc in candidates if hasattr(doc, 'page_content') and doc.page_content.strip()
    }

    if slots_left > 0 and candidates:
        reranker = get_global_reranker()
        if reranker and hasattr(reranker, "compress_documents"):
            try:
                # 6.1. ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å Reranker (‡∏à‡∏∞‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô DocumentWithScore object)
                reranked_results = reranker.compress_documents(
                    documents=candidates,
                    query=queries_to_run[0],
                    top_n=slots_left
                )
                
                reranked_docs_with_metadata = []
                for result in reranked_results:
                    # üéØ FIX A: ‡πÅ‡∏ï‡∏Å Wrapper Object (‡πÉ‡∏ä‡πâ getattr ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÇ‡∏Ñ‡πâ‡∏î‡∏™‡∏±‡πâ‡∏ô‡∏•‡∏á‡πÅ‡∏•‡∏∞‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô)
                    doc_to_add = getattr(result, 'document', result)
                        
                    # 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
                    if doc_to_add and hasattr(doc_to_add, 'page_content') and doc_to_add.page_content.strip():
                        
                        # 3. **CRITICAL FIX**: Patch Metadata ‡∏ñ‡πâ‡∏≤‡∏û‡∏ö‡∏ß‡πà‡∏≤ ID ‡∏´‡∏≤‡∏¢‡πÑ‡∏õ
                        current_metadata = getattr(doc_to_add, 'metadata', {})
                        chunk_uuid_check = current_metadata.get("chunk_uuid") or current_metadata.get("dedup_chunk_uuid")

                        # ‡∏ñ‡πâ‡∏≤ ID ‡∏´‡∏≤‡∏¢‡πÑ‡∏õ ‡πÅ‡∏•‡∏∞ Content ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ Map ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏´‡∏≤ Original ‡πÑ‡∏î‡πâ
                        if not chunk_uuid_check and doc_to_add.page_content in candidate_metadata_map:
                            original_metadata = candidate_metadata_map[doc_to_add.page_content]
                            
                            # Patch metadata ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô document object
                            if hasattr(doc_to_add, 'metadata'):
                                doc_to_add.metadata = original_metadata
                                logger.debug("Patched metadata back to reranked document.")
                            # ‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô object ‡∏ä‡∏ô‡∏¥‡∏î‡∏≠‡∏∑‡πà‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç metadata ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏õ (‡∏Å‡∏£‡∏ì‡∏µ‡∏ô‡∏µ‡πâ‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£‡πÄ‡∏Å‡∏¥‡∏î)
                        
                        # 4. ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡∏•‡∏¥‡∏™‡∏ï‡πå
                        reranked_docs_with_metadata.append(doc_to_add)
                
                # 6.2. ‡πÉ‡∏ä‡πâ Documents ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏ï‡∏Å‡πÅ‡∏•‡∏∞ Patch Metadata ‡πÅ‡∏•‡πâ‡∏ß
                final_docs.extend(reranked_docs_with_metadata or candidates[:slots_left])
                logger.info(f"Reranker returned {len(reranked_docs_with_metadata)} docs (after extraction and patching)")
                
            except Exception as e:
                logger.warning(f"Reranker failed ({e}), using raw candidates")
                final_docs.extend(candidates[:slots_left])
        else:
            logger.info("No reranker ‚Üí using top-k raw")
            final_docs.extend(candidates[:slots_left])
    else:
        logger.info("No slots left or no candidates ‚Üí priority only")

    # 7. ‡∏™‡∏£‡πâ‡∏≤‡∏á output
    top_evidences = []
    aggregated_parts = []
    used_chunk_uuids: List[str] = [] # ‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞ ID ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÄ‡∏•‡∏∑‡∏≠‡∏Å

    # üü¢ NEW FIX: ‡∏Å‡∏£‡∏≠‡∏á Chunk ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ ID ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏≠‡∏Å‡πÑ‡∏õ
    valid_final_docs = []
    for doc in final_docs[:FINAL_K_RERANKED]:
        md = getattr(doc, "metadata", {}) or {}
        chunk_uuid_candidate = md.get("chunk_uuid") or md.get("dedup_chunk_uuid")
        
        # ‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏Å‡∏≤‡∏£‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö: ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ ID, ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 32 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£ (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏£‡∏≠‡∏á TEMP-), ‡πÅ‡∏•‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô ID ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß (UNKNOWN/TEMP)
        is_valid_hash = bool(chunk_uuid_candidate and len(chunk_uuid_candidate) >= 32 and not re.match(r"^(TEMP|UNKNOWN)-", str(chunk_uuid_candidate)))
        
        if is_valid_hash:
            valid_final_docs.append(doc)
        else:
            logger.warning(
                f"Skipping chunk in final output due to invalid/temporary ID: {chunk_uuid_candidate}. "
                f"Source Doc ID: {md.get('stable_doc_uuid')}"
            )

    # 7.1 ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Chunk ‡∏ó‡∏µ‡πà‡∏°‡∏µ ID ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á Final Output
    for doc in valid_final_docs:
        md = getattr(doc, "metadata", {}) or {}
        pc = str(getattr(doc, "page_content", "") or "").strip()
        
        # üéØ FIX B: ‡πÉ‡∏ä‡πâ ID ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏Å‡∏£‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß (chunk_uuid_final)
        chunk_uuid_final = md.get("chunk_uuid") or md.get("dedup_chunk_uuid")
        
        used_chunk_uuids.append(str(chunk_uuid_final)) # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞ ID ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á

        source = md.get("source") or md.get("filename") or md.get("doc_source") or "Unknown"
        pdca = md.get("pdca_tag", "Other")

        top_evidences.append({
            "doc_id": md.get("stable_doc_uuid"),
            "chunk_uuid": chunk_uuid_final, # ID ‡∏ó‡∏µ‡πà Level 2 ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ (‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô 64-char Hash)
            "source": source,
            "source_filename": source,
            "text": pc,
            "pdca_tag": pdca,
            # üîë CRITICAL FIX: ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô Rerank ‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡πÉ‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
            "rerank_score": md.get("relevance_score", 0.0), 
        })
        aggregated_parts.append(f"[{pdca}] [SOURCE: {source}] {pc}")

    result = {
        "top_evidences": top_evidences,
        "aggregated_context": "\n\n---\n\n".join(aggregated_parts),
        "retrieval_time": round(time.time() - start_time, 3),
        "used_chunk_uuids": used_chunk_uuids
    }

    logger.info(f"Final retrieval L{level or '?'} {sub_id or ''}: {len(top_evidences)} chunks in {result['retrieval_time']:.2f}s")
    return result

# ------------------------------------------------------------------
# Helper Function: Create ChromaDB Where Filter
# ------------------------------------------------------------------
def _create_where_filter(doc_ids: Optional[Set[str]]) -> Dict[str, Any]:
    """
    Creates a ChromaDB 'where' filter dictionary to filter by stable document IDs.
    Assumes the stable document ID is stored in the metadata key 'stable_doc_uuid'.
    """
    if not doc_ids:
        # üü¢ FIX: ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô Dict ‡∏ß‡πà‡∏≤‡∏á ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ ID ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Chroma Error
        return {}
    
    return {
        "stable_doc_uuid": {
            "$in": list(doc_ids)
        }
    }


# ------------------------
# Retrieval: retrieve_context_for_endpoint (Final, Robust Version)
# ------------------------
def retrieve_context_for_endpoint(
    vectorstore_manager: VectorStoreManager, 
    collection_name: Optional[str] = None, 
    query: str = "", 
    stable_doc_ids: Optional[Set[str]] = None, 
    doc_type: Optional[str] = None, 
    enabler: Optional[str] = None, 
    subject: Optional[str] = None, # üü¢ ‡πÄ‡∏û‡∏¥‡πà‡∏° subject ‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡πÉ‡∏ô Signature
    **kwargs: Any, # ‡∏£‡∏±‡∏ö k_to_retrieve ‡πÅ‡∏•‡∏∞ k_to_rerank ‡∏ó‡∏µ‡πà Router ‡∏≠‡∏≤‡∏à‡∏™‡πà‡∏á‡∏°‡∏≤
) -> Dict[str, Any]: 
    """
    Directly query a Chroma collection using stable doc IDs (Hard Filter)
    This is used for endpoints that require specific, already selected documents.
    """
    start_time = time.time() 
    
    # ------------------------------------------------------------------
    # 1. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Collection Name (Fallback Logic)
    # ------------------------------------------------------------------
    if not collection_name and doc_type:
        try:
            # üí° Derive collection name ‡∏à‡∏≤‡∏Å doc_type ‡πÅ‡∏•‡∏∞ enabler
            collection_name = _get_collection_name(doc_type, enabler or DEFAULT_ENABLER)
            logger.info(f"Derived collection_name: '{collection_name}' from doc_type='{doc_type}', enabler='{enabler}'")
        except Exception as e:
            logger.error(f"Cannot derive collection_name from doc_type/enabler: {e}")
            collection_name = None 

    if not collection_name:
        logger.error("FATAL: Cannot determine collection_name. Exiting retrieval.")
        return {"top_evidences": [], "aggregated_context": "", "retrieval_time": 0.0, "used_chunk_uuids": []}
    
    logger.critical(f"[QUERY] Running Endpoint Query: '{query[:50]}...' ‚Üí collection='{collection_name}' (Type: {doc_type or '?'})")

    # 2. ‡πÇ‡∏´‡∏•‡∏î Chroma Instance
    chroma_instance = vectorstore_manager._load_chroma_instance(collection_name)
    if chroma_instance is None:
        logger.error(f"Cannot load Chroma instance for collection: {collection_name}")
        return {"top_evidences": [], "aggregated_context": "", "retrieval_time": 0.0, "used_chunk_uuids": []}
    
    # 3. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Where Filter (‡∏£‡∏ß‡∏° Stable Doc IDs ‡πÅ‡∏•‡∏∞ Subject) üü¢ ‡∏à‡∏∏‡∏î‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç
    # ‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ä‡πà‡∏ß‡∏¢‡∏™‡∏£‡πâ‡∏≤‡∏á filter ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Doc IDs (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
    where_filter = _create_where_filter(stable_doc_ids)

    # 3.2 Filter: Subject (Secondary Safety Filter) 
    # üü¢ FIX: Clean Subject String ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ö‡∏°‡∏≤‡∏ó‡∏±‡∏ô‡∏ó‡∏µ ‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ Exact Match (Final Version)
    cleaned_subject = subject.strip() if subject else None

    if cleaned_subject:
        # üéØ ‡πÉ‡∏ä‡πâ Exact Match: {"subject": value}
        subject_filter = {"subject": cleaned_subject}
        
        if where_filter:
            # ‡πÉ‡∏ä‡πâ $and ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏ß‡∏°‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç: ID ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏£‡∏á AND Subject ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏£‡∏á
            where_filter = {"$and": [where_filter, subject_filter]}
            logger.info(f"Applying combined filter: {len(stable_doc_ids or [])} IDs AND Subject='{cleaned_subject}'")
        else:
            # ‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Doc ID ‡∏°‡∏≤
            where_filter = subject_filter
            logger.warning(f"Applying Subject filter only: '{cleaned_subject}'")
    # ------------------------------------------------------------------
    
    # 4. Embed Query (‡πÅ‡∏Å‡πâ Dimension Mismatch)
    try:
        embedding_func = get_hf_embeddings()
        query_text_with_prefix = "query: " + query
        query_embeddings = embedding_func.embed_query(query_text_with_prefix)
        logger.info("‚úÖ Successfully embedded query with 768 dimension.")
    except Exception as e:
        logger.error(f"FATAL: Failed to embed query with 768-dim model: {e}")
        return {"top_evidences": [], "aggregated_context": "", "retrieval_time": 0.0, "used_chunk_uuids": []}

    # ------------------------------------------------------------------
    # 5. Query Chroma DB ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á (‡πÉ‡∏ä‡πâ Hard Filter ‡∏´‡∏£‡∏∑‡∏≠ Query ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)
    # ------------------------------------------------------------------
    results = {'ids': [[]], 'documents': [[]], 'metadatas': [[]], 'distances': [[]]} # Placeholder
    
    # üí° ‡πÉ‡∏ä‡πâ INITIAL_TOP_K ‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡πà‡∏≤ k_to_retrieve ‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏°‡∏≤‡∏à‡∏≤‡∏Å Router (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
    n_results = kwargs.get("k_to_retrieve", INITIAL_TOP_K)
    
    try:
        query_params = {
            "query_embeddings": [query_embeddings], 
            "n_results": n_results,
            "include": ['documents', 'metadatas', 'distances']
        }
        
        # üéØ FIX: ‡∏™‡πà‡∏á 'where' ‡πÑ‡∏õ‡∏Å‡πá‡∏ï‡πà‡∏≠‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ Filter ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (‡πÅ‡∏Å‡πâ Chroma Error)
        if where_filter: 
            query_params["where"] = where_filter
            filter_summary = f"Doc IDs:{len(stable_doc_ids or [])}"
            if subject:
                 filter_summary += f", Subject:'{subject}'"
            logger.info(f"Running Chroma query with Filter ({filter_summary}) and n_results={n_results}") # üü¢ ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á Log
        else:
            logger.warning("No stable_doc_ids or subject provided. Querying entire collection (may be slow/incorrect usage).")

        results = chroma_instance._collection.query(**query_params)
        
    except Exception as e:
        logger.error(f"Chroma direct query failed (Endpoint): {e}", exc_info=False)
        
    # ------------------------------------------------------------------
    # 6. Post-process: Convert Chroma results to LcDocument
    # ------------------------------------------------------------------
    raw_chunks: List[LcDocument] = []
    if results and results.get('documents') and results['documents'][0]:
        for doc_content, metadata, distance in zip(
            results['documents'][0],
            results['metadatas'][0],
            results['distances'][0]
        ):
            if not metadata:
                metadata = {}
            
            metadata['retrieval_distance'] = float(distance)
            metadata['collection_name'] = collection_name
            
            raw_chunks.append(LcDocument(page_content=doc_content, metadata=metadata))
    
    logger.critical(f"[RETRIEVAL] Raw chunks from ChromaDB (Direct): {len(raw_chunks)} documents")
    final_chunks = list(raw_chunks) 
    
    # ------------------------------------------------------------------
    # 7. Final Output: Convert LcDocument list to expected DICT format
    # ------------------------------------------------------------------
    top_evidences = []
    aggregated_parts = []
    used_chunk_uuids = []
    
    for doc in final_chunks:
        md = getattr(doc, "metadata", {}) or {}
        pc = str(doc.page_content or "").strip()
        
        chunk_uuid = md.get("chunk_uuid") or md.get("dedup_chunk_uuid")
        source = md.get("source") or md.get("filename") or md.get("doc_source") or "Unknown"
        pdca = md.get("pdca_tag", "Other")

        if chunk_uuid:
            used_chunk_uuids.append(str(chunk_uuid))

        top_evidences.append({
            "doc_id": md.get("stable_doc_uuid"),
            "chunk_uuid": chunk_uuid, 
            "source": source,
            "source_filename": source,
            "text": pc,
            "pdca_tag": pdca,
            "retrieval_distance": md.get("retrieval_distance", 0.0),
        })
        aggregated_parts.append(f"[{pdca}] [SOURCE: {source}] {pc}")

    end_time = time.time()
    result = {
        "top_evidences": top_evidences,
        "aggregated_context": "\n\n---\n\n".join(aggregated_parts),
        "retrieval_time": round(end_time - start_time, 3),
        "used_chunk_uuids": used_chunk_uuids 
    }
    
    source_count = len({c.metadata.get('stable_doc_uuid') for c in final_chunks if c.metadata and c.metadata.get('stable_doc_uuid')}) 
    logger.info(f"Final retrieval (Endpoint): {len(top_evidences)} chunks in {result['retrieval_time']:.2f}s (Sources: {source_count})")
    
    return result


def retrieve_context_for_low_levels(query: str, doc_type: str, enabler: Optional[str]=None,
                                 vectorstore_manager: Optional['VectorStoreManager']=None,
                                 top_k: int=LOW_LEVEL_K, initial_k: int=INITIAL_TOP_K,
                                 # üü¢ NEW: ‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠ arguments
                                 mapped_uuids: Optional[List[str]]=None,
                                 priority_docs_input: Optional[List[Any]] = None,
                                 sequential_chunk_uuids: Optional[List[str]] = None, 
                                 sub_id: Optional[str]=None, level: Optional[int]=None) -> Dict[str, Any]:
    """
    Retrieves a small, focused context for low levels (L1, L2) using a reduced k (LOW_LEVEL_K).
    """
    # ‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å ‡πÅ‡∏ï‡πà‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏ä‡πâ k ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
    return retrieve_context_with_filter(
        query=query,
        doc_type=doc_type,
        enabler=enabler,
        vectorstore_manager=vectorstore_manager,
        top_k=LOW_LEVEL_K,
        initial_k=initial_k,
        mapped_uuids=mapped_uuids,
        priority_docs_input=priority_docs_input,
        sequential_chunk_uuids=sequential_chunk_uuids, 
        sub_id=sub_id,
        level=level
    )

# ----------------------------------------------------
# Helper function: Summarize evidence list (minimal stub)
# ----------------------------------------------------
def _summarize_evidence_list_short(evidences: list, max_sentences: int = 3) -> str:
    """
    Provides a concise summary of evidence items.
    """
    if not evidences:
        return ""
    
    parts = []
    for ev in evidences[:max(1, min(len(evidences), max_sentences))]:
        if isinstance(ev, dict):
            fn = ev.get("source_filename") or ev.get("source") or ev.get("doc_id", "unknown")
            txt = ev.get("text") or ev.get("content") or ""
        else:
            fn = str(ev)
            txt = str(ev)
        txt_short = txt[:120].replace("\n", " ").strip()
        if txt_short:
            parts.append(f"‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå `{fn}`: {txt_short}...")
        else:
            parts.append(f"‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå `{fn}`")
    return " | ".join(parts)


# ----------------------------------------------------
# ULTIMATE FINAL VERSION: build_multichannel_context_for_level
# ‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á evidence dicts ‡πÄ‡∏ï‡πá‡∏° ‡πÜ ‡πÅ‡∏•‡∏∞‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Ç‡∏≠‡∏á‡πÄ‡∏Å‡πà‡∏≤‡∏î‡πâ‡∏ß‡∏¢
# ----------------------------------------------------
def build_multichannel_context_for_level(
    level: int,
    top_evidences: list,
    previous_levels_map: dict | None = None,                    # ‡πÄ‡∏Å‡πà‡∏≤: {key: list[dict]} ‡∏´‡∏£‡∏∑‡∏≠ {doc_id: filename}
    previous_levels_evidence: list | None = None,               # ‡πÉ‡∏´‡∏°‡πà: list[dict] ‡∏ó‡∏µ‡πà‡∏°‡∏µ text ‡πÄ‡∏ï‡πá‡∏° ‡πÜ
    max_main_context_tokens: int = 3000,
    max_summary_sentences: int = 4
) -> dict:

    # --- 1) Baseline: ‡πÉ‡∏ä‡πâ previous_levels_evidence ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å (‡∏°‡∏µ text!) ---
    baseline_evidence = previous_levels_evidence or []

    # Fallback ‡πÄ‡∏Å‡πà‡∏≤: ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡∏™‡πà‡∏á‡πÅ‡∏ö‡∏ö‡πÄ‡∏î‡∏¥‡∏°‡∏°‡∏≤ (‡πÄ‡∏ä‡πà‡∏ô‡∏à‡∏≤‡∏Å _run_single_assessment ‡πÄ‡∏Å‡πà‡∏≤)
    if not baseline_evidence and previous_levels_map:
        for items in previous_levels_map.values():
            if isinstance(items, list):
                baseline_evidence.extend(items)
            elif isinstance(items, dict) and (items.get("text") or items.get("content")):
                baseline_evidence.append(items)

    # ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡∏°‡∏µ text
    summarizable_baseline = [
        item for item in baseline_evidence
        if isinstance(item, dict) and (item.get("text") or item.get("content"))
    ]

    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏à‡∏£‡∏¥‡∏á ‡πÜ ‡πÉ‡∏´‡πâ‡πÉ‡∏™‡πà‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ó‡∏ô
    if not summarizable_baseline:
        summarizable_baseline = [{"text": "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏à‡∏≤‡∏Å Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤"}]

    baseline_summary = _summarize_evidence_list_short(
        summarizable_baseline,
        max_sentences=max_summary_sentences
    )

    # --- 2) Direct / Aux classification (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°) ---
    direct, aux = [], []
    K_MAIN = 5

    for ev in top_evidences:
        if not isinstance(ev, dict):
            aux.append(ev)
            continue
        tag = (ev.get("pdca_tag") or ev.get("PDCA") or "P").upper()
        if tag in ("P", "D", "C", "A"):
            direct.append(ev)
        else:
            aux.append(ev)

    if len(direct) < K_MAIN:
        need = K_MAIN - len(direct)
        direct.extend(aux[:need])
        aux = aux[need:]

    direct_for_context = direct[:K_MAIN]

    # --- 3) Join text ---
    def _join_chunks(chunks, max_chars):
        out, used = [], 0
        for c in chunks:
            txt = (c.get("text") or c.get("content") or "").strip()
            if not txt:
                continue
            if used + len(txt) > max_chars:
                remain = max_chars - used
                if remain > 0:
                    out.append(txt[:remain] + "...")
                break
            out.append(txt)
            used += len(txt)
        return "\n\n".join(out)

    direct_context = _join_chunks(direct_for_context, max_main_context_tokens)
    aux_summary = _summarize_evidence_list_short(aux, max_sentences=3) if aux else "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏£‡∏≠‡∏á"

    # --- 4) Debug ---
    debug_meta = {
        "level": level,
        "direct_count": len(direct_for_context),
        "aux_count": len(aux),
        "baseline_count": len(summarizable_baseline),
        "baseline_source": "previous_levels_evidence" if previous_levels_evidence else "fallback_map",
    }

    logger.info(f"Context L{level} ‚Üí Direct:{len(direct_for_context)} | Aux:{len(aux)} | Baseline:{len(summarizable_baseline)}")

    return {
        "baseline_summary": baseline_summary,
        "direct_context": direct_context,
        "aux_summary": aux_summary,
        "debug_meta": debug_meta,
    }

# -------------------- Query Enhancement Functions --------------------
def enhance_query_for_statement(
    statement_text: str,
    sub_id: str,
    statement_id: str, 
    level: int,
    enabler_id: str,
    focus_hint: str,
    llm_executor: Any = None
) -> List[str]:
    """
    Generates a list of tailored queries (Multi-Query strategy) based on the statement 
    ‡πÅ‡∏•‡∏∞ PDCA focus, ‡πÇ‡∏î‡∏¢‡∏Å‡∏≥‡∏´‡∏ô‡∏î Query ‡∏ó‡∏µ‡πà‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏£‡∏∞‡∏î‡∏±‡∏ö
    
    Returns: List[str] of queries.
    """
    
    # Q1: Base Query (P/D Focus) - ‡πÅ‡∏°‡πà‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
    base_query_template = (
        f"{statement_text}. {focus_hint} ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÅ‡∏™‡∏î‡∏á‡πÅ‡∏ú‡∏ô ‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£ ‡πÅ‡∏•‡∏∞‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡∏≠‡∏á {statement_id} "
        f"‡∏ï‡∏≤‡∏°‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Ç‡∏≠‡∏á {enabler_id}"
    )
    
    queries = []

    # 1. Level 5 Query Refinement (‡∏õ‡∏£‡∏±‡∏ö Base Query ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö L5 ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô)
    if level == 5:
        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö L5, ‡∏õ‡∏£‡∏±‡∏ö Base Query Q1 ‡πÉ‡∏´‡πâ‡πÄ‡∏ô‡πâ‡∏ô L5 ‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô
        base_query = base_query_template + ". **‡∏Å‡∏≤‡∏£‡∏ö‡∏π‡∏£‡∏ì‡∏≤‡∏Å‡∏≤‡∏£, ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏±‡πà‡∏á‡∏¢‡∏∑‡∏ô, ‡∏Å‡∏≤‡∏£‡∏Ç‡∏¢‡∏≤‡∏¢‡∏ú‡∏•, ‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡∏£‡πà‡∏≠‡∏á, ‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°**"
        queries.append(base_query)
        
        # Q4 (Innovation/Sustainability Focus) - ‡πÄ‡∏û‡∏¥‡πà‡∏° Query ‡∏ó‡∏µ‡πà 4 ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ L5
        l5_innovation_query = (
            f"‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏° ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏±‡πà‡∏á‡∏¢‡∏∑‡∏ô ‡∏Å‡∏≤‡∏£‡∏Ç‡∏¢‡∏≤‡∏¢‡∏ú‡∏• ‡∏´‡∏£‡∏∑‡∏≠‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡∏£‡πà‡∏≠‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö {statement_id}. "
            f"‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ **Best Practice**, **‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß**, **‡∏Å‡∏≤‡∏£‡∏ö‡∏π‡∏£‡∏ì‡∏≤‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≤‡∏°‡∏™‡∏≤‡∏¢‡∏á‡∏≤‡∏ô**"
        )
        queries.append(l5_innovation_query)
    
    else:
        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö L1-L4, ‡πÉ‡∏ä‡πâ Base Query ‡∏õ‡∏Å‡∏ï‡∏¥
        base_query = base_query_template
        queries.append(base_query)


    # 2. Level 3+ (C/A) Query Refinement (‡πÄ‡∏û‡∏¥‡πà‡∏° C ‡πÅ‡∏•‡∏∞ A ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö L3 ‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏õ)
    if level >= 3:
        
        # üü¢ C (Check/Evaluation) Focus Query
        # ‡πÄ‡∏ô‡πâ‡∏ô‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏±‡∏î‡∏ú‡∏• ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•
        c_query = (
            f"‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏±‡∏î‡∏ú‡∏• ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏• ‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö ‡∏ß‡πà‡∏≤ {statement_id} "
            f"‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≤‡∏°‡πÅ‡∏ú‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏±‡∏î‡∏ú‡∏•‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à "
            f"‡πÅ‡∏ö‡∏ö‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏°‡∏ú‡∏•‡∏ï‡∏≠‡∏ö‡∏£‡∏±‡∏ö ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô"
        )
        queries.append(c_query)

        # üü¢ A (Act/Improvement) Focus Query
        # ‡πÄ‡∏ô‡πâ‡∏ô‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á ‡∏Å‡∏≤‡∏£‡∏ó‡∏ö‡∏ó‡∏ß‡∏ô ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á
        a_query = (
            f"‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á ‡∏Å‡∏≤‡∏£‡∏ó‡∏ö‡∏ó‡∏ß‡∏ô ‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡∏á {statement_id} "
            f"‡∏ï‡∏≤‡∏°‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô ‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞ ‡∏´‡∏£‡∏∑‡∏≠‡∏ú‡∏•‡∏ï‡∏≠‡∏ö‡∏£‡∏±‡∏ö ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô "
            f"‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏≠‡∏ö‡∏ñ‡∏±‡∏î‡πÑ‡∏õ"
        )
        queries.append(a_query)
    
    
    logger.info(f"Generated {len(queries)} queries for {sub_id} L{level} (ID: {statement_id}).")
    return queries


# ------------------------
# LLM fetcher
# ------------------------
def _fetch_llm_response(
    system_prompt: str, 
    user_prompt: str, 
    max_retries: int=_MAX_LLM_RETRIES,
    llm_executor: Any = None 
) -> str:
    global _MOCK_FLAG

    llm = llm_executor
    
    if llm is None and not _MOCK_FLAG: 
        raise ConnectionError("LLM instance not initialized (Missing llm_executor).")

    if _MOCK_FLAG:
        # ‡πÉ‡∏ä‡πâ Mock LLM ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÑ‡∏ß‡πâ
        try:
             resp = llm.invoke([{"role":"system","content":system_prompt},{"role":"user","content":user_prompt}], config={"temperature": 0.0})
             if hasattr(resp, "content"): return resp.content.strip()
             return str(resp).strip()
        except Exception as e:
            logger.error(f"Mock LLM invocation failed: {e}")
            raise ConnectionError("Mock LLM failed to respond.")

    config = {"temperature": 0.0}
    for attempt in range(max_retries):
        try:
            resp = llm.invoke([{"role":"system","content":system_prompt},{"role":"user","content":user_prompt}], config=config)
            if hasattr(resp, "content"): return resp.content.strip()
            if isinstance(resp, dict) and "content" in resp: return resp["content"].strip()
            if isinstance(resp, str): return resp.strip()
            return str(resp).strip()
        except Exception as e:
            logger.warning(f"LLM attempt {attempt+1} failed: {e}")
            time.sleep(0.5)
            
    raise ConnectionError("LLM calls failed after retries")

# ------------------------
# Evaluation
# ------------------------
T = TypeVar("T", bound=BaseModel)

def _check_and_handle_empty_context(context: str, sub_id: str, level: int) -> Optional[Dict[str, Any]]:
    """
    Returns Failure result if context is empty or contains known error strings.
    Auto-fail with PDCA keys all set to 0.
    """
    if not context or "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á" in context or "ERROR:" in context.upper():
        logger.warning(f"Auto-FAIL L{level} for {sub_id}: Empty or Error Context detected from RAG.")
        context_preview = context.strip()[:100].replace("\n", " ") if context else "Empty Context"
        return {
            "score": 0,
            "reason": f"‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏î‡πâ‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á (Context: {context_preview}).",
            "is_passed": False,
            "P_Plan_Score": 0,
            "D_Do_Score": 0,
            "C_Check_Score": 0,
            "A_Act_Score": 0,
        }
    return None


def evaluate_with_llm(
    context: str, 
    sub_criteria_name: str, 
    level: int, 
    statement_text: str, 
    sub_id: str, 
    check_evidence: str = "", 
    act_evidence: str = "", 
    llm_executor: Any = None, 
    max_evidence_strength: float = 10.0, # üü¢ NEW: ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤ Capping ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
    **kwargs
) -> Dict[str, Any]:
    """Standard Evaluation for L3+ with robust handling for missing keys."""
    
    context_to_send_eval = context[:MAX_EVAL_CONTEXT_LENGTH] if context else ""
    # 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Context ‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á LLM
    failure_result = _check_and_handle_empty_context(context, sub_id, level)
    if failure_result:
        return failure_result

    contextual_rules_prompt = kwargs.get("contextual_rules_prompt", "")
    baseline_summary = kwargs.get("baseline_summary", "")
    aux_summary = kwargs.get("aux_summary", "")
    
    # 2. Prepare User & System Prompts
    user_prompt = USER_ASSESSMENT_PROMPT.format(
        sub_criteria_name=sub_criteria_name, 
        level=level, 
        statement_text=statement_text, 
        sub_id=sub_id,
        context=context_to_send_eval or "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á",
        pdca_phase=kwargs.get("pdca_phase",""), 
        level_constraint=kwargs.get("level_constraint",""),
        contextual_rules_prompt=contextual_rules_prompt,
        check_evidence=check_evidence, 
        act_evidence=act_evidence,
        # üü¢ NEW: ‡∏™‡πà‡∏á‡∏Ñ‡πà‡∏≤ Cap ‡πÉ‡∏´‡πâ User Prompt (‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á User)
        max_evi_str_cap_for_llm=max_evidence_strength,
    )

    # Insert baseline_summary into the prompt explicitly:
    if baseline_summary:
        user_prompt = user_prompt + "\n\n--- Baseline summary (‡∏à‡∏≤‡∏Å L1-L2): ---\n" + baseline_summary

    if aux_summary:
        user_prompt = user_prompt + "\n\n--- Auxiliary evidence summary (low-priority): ---\n" + aux_summary

    try:
        schema_json = json.dumps(CombinedAssessment.model_json_schema(), ensure_ascii=False, indent=2)
    except:
        schema_json = '{"score":0,"reason":"string"}'

    # üü¢ FIX: ‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö SYSTEM_ASSESSMENT_PROMPT ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤ Cap ‡∏Å‡πà‡∏≠‡∏ô‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ö Schema
    # (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤ SYSTEM_ASSESSMENT_PROMPT ‡∏°‡∏µ placeholder {max_evi_str_cap_for_llm} ‡πÅ‡∏•‡πâ‡∏ß)
    system_prompt_formatted = SYSTEM_ASSESSMENT_PROMPT.format(
        max_evi_str_cap_for_llm=max_evidence_strength
    )

    system_prompt = system_prompt_formatted + "\n\n--- JSON SCHEMA ---\n" + schema_json + "\nIMPORTANT: Respond only with valid JSON."

    try:
        # 3. ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å LLM
        raw = _fetch_llm_response(system_prompt, user_prompt, _MAX_LLM_RETRIES, llm_executor=llm_executor)
        
        # 4. Extract JSON ‡πÅ‡∏•‡∏∞ normalize keys
        parsed = _robust_extract_json(raw)
        
        # üéØ FIX 1: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏´‡πâ 'parsed' ‡πÄ‡∏õ‡πá‡∏ô dict ‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
        if not isinstance(parsed, dict):
            logger.error(f"LLM L{level} response parsed to non-dict type: {type(parsed).__name__}. Falling back to empty dict.")
            parsed = {}

        # 5. ‡∏Ñ‡∏∑‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå, ‡πÄ‡∏ï‡∏¥‡∏° default ‡∏´‡∏≤‡∏Å key ‡∏Ç‡∏≤‡∏î
        return {
            "score": int(parsed.get("score", 0)),
            "reason": parsed.get("reason", "No reason provided by LLM."),
            "is_passed": parsed.get("is_passed", False),
            "P_Plan_Score": int(parsed.get("P_Plan_Score", 0)),
            "D_Do_Score": int(parsed.get("D_Do_Score", 0)),
            "C_Check_Score": int(parsed.get("C_Check_Score", 0)),
            "A_Act_Score": int(parsed.get("A_Act_Score", 0)),
        }

    except Exception as e:
        logger.exception(f"evaluate_with_llm failed for {sub_id} L{level}: {e}")
        return {
            "score":0,
            "reason":f"LLM error: {e}",
            "is_passed":False,
            "P_Plan_Score": 0,
            "D_Do_Score": 0,
            "C_Check_Score": 0,
            "A_Act_Score": 0,
        }

# =========================
# Patch for L1-L2 evaluation
# =========================

# 1Ô∏è‚É£ ‡πÄ‡∏û‡∏¥‡πà‡∏° context limit ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö L1/L2
def _get_context_for_level(context: str, level: int) -> str:
    """Return context string with appropriate length limit for each level."""
    if not context:
        return ""
    if level <= 2:
        return context[:6000]  # L1-L2 ‡πÉ‡∏ä‡πâ context ‡∏¢‡∏≤‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô
    return context[:MAX_EVAL_CONTEXT_LENGTH]  # L3-L5

def _extract_combined_assessment(parsed: Dict[str, Any], score_default_key: str = "score") -> Dict[str, Any]:
    """Helper to safely extract combined assessment results."""
    # üü¢ NEW: Extract all scores needed by seam_assessment.py (Action #1 logic)
    score = int(parsed.get(score_default_key, 0))
    is_passed = parsed.get("is_passed", score >= 1) # ‡πÉ‡∏ä‡πâ score >= 1 ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤ default ‡∏ñ‡πâ‡∏≤ LLM ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏™‡πà‡∏á is_passed

    result = {
        "score": score,
        "reason": parsed.get("reason", "No reason provided by LLM."),
        "is_passed": is_passed,
        "P_Plan_Score": int(parsed.get("P_Plan_Score", 0)),
        "D_Do_Score": int(parsed.get("D_Do_Score", 0)),
        "C_Check_Score": int(parsed.get("C_Check_Score", 0)),
        "A_Act_Score": int(parsed.get("A_Act_Score", 0)),
    }
    return result

def evaluate_with_llm_low_level(
    context: str, 
    sub_criteria_name: str, 
    level: int, 
    statement_text: str, 
    sub_id: str, 
    llm_executor: Any, 
    max_evidence_strength: float = 10.0, # üü¢ NEW: ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤ Capping ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á (‡πÅ‡∏°‡πâ‡∏à‡∏∞‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ ‡πÅ‡∏ï‡πà‡∏£‡∏±‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô error)
    **kwargs
) -> Dict[str, Any]:
    """
    Evaluation ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö L1/L2 ‡πÅ‡∏ö‡∏ö robust ‡πÅ‡∏•‡∏∞ schema uniform
    """

    failure_result = _check_and_handle_empty_context(context, sub_id, level)
    if failure_result:
        return failure_result

    level_constraint = kwargs.get("level_constraint", "")
    contextual_rules_prompt = kwargs.get("contextual_rules_prompt", "") 

    # ‡∏à‡∏≥‡∏Å‡∏±‡∏î context ‡∏ï‡∏≤‡∏° level
    context_to_send = _get_context_for_level(context, level)

    user_prompt = USER_LOW_LEVEL_PROMPT.format(
        sub_criteria_name=sub_criteria_name,
        level=level,
        statement_text=statement_text,
        sub_id=sub_id,
        context=context_to_send,
        level_constraint=level_constraint,
        contextual_rules_prompt=contextual_rules_prompt
    )

    try:
        schema_json = json.dumps(CombinedAssessment.model_json_schema(), ensure_ascii=False, indent=2)
    except:
        schema_json = '{"score":0,"reason":"string"}'

    # üü¢ FIX: ‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö SYSTEM_LOW_LEVEL_PROMPT ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤ Cap ‡∏Å‡πà‡∏≠‡∏ô‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ö Schema
    system_prompt_formatted = SYSTEM_LOW_LEVEL_PROMPT.format(
        max_evi_str_cap_for_llm=max_evidence_strength
    )
    system_prompt = system_prompt_formatted + "\n\n--- JSON SCHEMA ---\n" + schema_json + "\nIMPORTANT: Respond only with valid JSON."

    try:
        raw = _fetch_llm_response(system_prompt, user_prompt, _MAX_LLM_RETRIES, llm_executor=llm_executor)
        parsed = _robust_extract_json(raw)

        # üéØ FIX 1: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏´‡πâ 'parsed' ‡πÄ‡∏õ‡πá‡∏ô dict ‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á‡πÑ‡∏õ extraction
        if not isinstance(parsed, dict):
            logger.error(f"LLM L{level} response parsed to non-dict type: {type(parsed).__name__}. Falling back to empty dict.")
            parsed = {}
        
        # ‡πÉ‡∏ä‡πâ extraction ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö L1/L2
        return _extract_combined_assessment_low_level(parsed)

    except Exception as e:
        logger.exception(f"evaluate_with_llm_low_level failed for {sub_id} L{level}: {e}")
        return {
            "score":0,
            "reason":f"LLM error: {e}",
            "is_passed":False,
            "P_Plan_Score": 0,
            "D_Do_Score": 0,
            "C_Check_Score": 0,
            "A_Act_Score": 0,
        }

def _extract_combined_assessment_low_level(parsed: dict) -> dict:
    """L1/L2 ‡∏ï‡πâ‡∏≠‡∏á‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö C=A=0 ‡πÅ‡∏•‡∏∞ is_passed ‡∏ï‡∏≤‡∏° score"""
    result = {
        "score": int(parsed.get("score", 0)),
        "reason": parsed.get("reason", "No reason provided by LLM (Low Level)."),
        "is_passed": parsed.get("is_passed", False),
        "P_Plan_Score": int(parsed.get("P_Plan_Score", 0)),
        "D_Do_Score": int(parsed.get("D_Do_Score", 0)),
        "C_Check_Score": 0,  # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö!
        "A_Act_Score": 0,    # üéØ FIX 2: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å 'A_Act_Sure' ‡πÄ‡∏õ‡πá‡∏ô 'A_Act_Score'
    }
    # ‡πÅ‡∏Å‡πâ is_passed ‡∏ñ‡πâ‡∏≤ score >=1 ‡πÅ‡∏ï‡πà LLM ‡∏ö‡∏≠‡∏Å False
    if result["score"] >= 1 and not result["is_passed"]:
        result["is_passed"] = True
    return result

# ------------------------
# Summarize
# ------------------------
def create_context_summary_llm(
    context: str, 
    sub_criteria_name: str, 
    level: int, 
    sub_id: str, 
    llm_executor: Any 
) -> Dict[str, Any]:
    """
    ‡πÉ‡∏ä‡πâ LLM ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ Context...
    """
    # 0. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö llm_executor
    if llm_executor is None: 
        logger.error("LLM instance is None. Cannot summarize context.")
        return {"summary":"LLM not available","suggestion_for_next_level":"Check LLM"}

    # 0.1 ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Context ‡∏™‡∏±‡πâ‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
    context_limited = (context or "").strip()
    if not context_limited or len(context_limited) < 50:
        logger.info(f"Context too short for summarization L{level} {sub_id}. Skipping LLM call.")
        return {
            "summary": "‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏î‡πâ‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡πâ‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á",
            "suggestion_for_next_level": "‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏£‡∏∑‡∏≠‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ RAG"
        }

    # 1. ‡∏à‡∏≥‡∏Å‡∏±‡∏î Context ‡πÉ‡∏´‡πâ‡∏™‡∏±‡πâ‡∏ô‡∏•‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£ (4000 tokens)
    context_to_send = context_limited[:4000]
    
    human_prompt = EVIDENCE_DESCRIPTION_PROMPT.format(
        sub_criteria_name=sub_criteria_name, 
        level=level, 
        context=context_to_send, 
        sub_id=sub_id
    )

    # 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á System Prompt ‡∏û‡∏£‡πâ‡∏≠‡∏° JSON Schema
    try: 
        schema_json = json.dumps(EvidenceSummary.model_json_schema(), ensure_ascii=False, indent=2)
    except: 
        schema_json = '{"summary":"string", "suggestion_for_next_level":"string"}'

    # system_prompt = SYSTEM_EVIDENCE_DESCRIPTION_PROMPT + "\n\n--- JSON SCHEMA ---\n" + schema_json + "\nIMPORTANT: Respond only with valid JSON."
    system_prompt = (
        SYSTEM_EVIDENCE_DESCRIPTION_PROMPT
        + "\n\n--- JSON SCHEMA ---\n"
        + schema_json
        + "\nIMPORTANT: Respond only with valid JSON. ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÉ‡∏ô‡∏ó‡∏∏‡∏Å key ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©."
    )


    # 3. ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ LLM ‡∏û‡∏£‡πâ‡∏≠‡∏° Retries
    try:
        raw = _fetch_llm_response(system_prompt, human_prompt, 2, llm_executor=llm_executor)
        
        # 4. ‡πÅ‡∏õ‡∏•‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå JSON
        parsed = _extract_normalized_dict(raw) or {}
        parsed.setdefault("summary", "Fallback: No summary provided by LLM.")
        parsed.setdefault("suggestion_for_next_level", "Fallback: No suggestion provided.")
        
        # 5. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á Schema ‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô
        if not all(k in parsed for k in ["summary", "suggestion_for_next_level"]):
             logger.warning(f"LLM Summary: Missing expected keys in JSON. Raw: {raw[:100]}...")
             
        return parsed
        
    except Exception as e:
        logger.exception(f"create_context_summary_llm failed for {sub_id} L{level}: {e}")
        # Fallback ‡∏Å‡∏£‡∏ì‡∏µ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î
        return {"summary":f"LLM Error during summarization: {e.__class__.__name__}","suggestion_for_next_level": "Manual review required due to LLM failure."}

# ------------------------
# FINAL: create_structured_action_plan (Production-Ready 100%)
# ------------------------
def _extract_json_array_for_action_plan(llm_response: str) -> List[Dict[str, Any]]:
    """Extract JSON array ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏Ç‡πá‡∏á‡πÅ‡∏Å‡∏£‡πà‡∏á‡∏™‡∏∏‡∏î ‡πÜ ‚Äî ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Action Plan ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô"""
    if not llm_response or not isinstance(llm_response, str):
        return []

    text = llm_response.strip()

    # 1. ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡πÉ‡∏ô code block ‡∏Å‡πà‡∏≠‡∏ô (```json ‡∏´‡∏£‡∏∑‡∏≠ ```)
    fenced = re.search(r"```(?:json)?\s*(\[.*?\])\s*```", text, re.DOTALL | re.IGNORECASE)
    if fenced:
        json_str = fenced.group(1)
    else:
        # 2. ‡∏´‡∏≤ balanced [] array
        start = text.find("[")
        if start == -1:
            return []
        depth = 0
        for i in range(start, len(text)):
            if text[i] == "[": depth += 1
            elif text[i] == "]":
                depth -= 1
                if depth == 0:
                    json_str = text[start:i+1]
                    break
        else:
            return []

    # 3. Parse ‡∏î‡πâ‡∏ß‡∏¢ json ‚Üí json5 fallback
    try:
        data = json.loads(json_str)
    except:
        try:
            data = json5.loads(json_str)
        except:
            logger.error(f"ActionPlan JSON parse failed: {json_str[:200]}")
            return []

    if not isinstance(data, list):
        return []
    return [item for item in data if isinstance(item, dict)]


def create_structured_action_plan(
    failed_statements: List[Dict[str, Any]],
    sub_id: str,
    target_level: int,
    llm_executor: Any,
    max_retries: int = 3
) -> List[Dict[str, Any]]:
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á Action Plan ‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÄ‡∏ó‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ
    ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏∏‡∏Å‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô Production
    """

    # ------------------------------------------------------------------
    # 1. ‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ú‡πà‡∏≤‡∏ô ‚Üí ‡πÅ‡∏ú‡∏ô‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏£‡∏∞‡∏î‡∏±‡∏ö (Sustain / Optimize)
    # ------------------------------------------------------------------
    if not failed_statements:
        if target_level >= 5:
            return [{
                "Phase": "Level 5 - Optimizing",
                "Goal": f"‡∏£‡∏±‡∏Å‡∏©‡∏≤‡πÅ‡∏•‡∏∞‡∏¢‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏•‡∏¥‡∏®‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {sub_id}",
                "Actions": [{
                    "Statement_ID": "OPT-L5",
                    "Recommendation": "‡πÄ‡∏ô‡πâ‡∏ô‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏° ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏ ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ä‡∏¥‡∏á‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á"
                }]
            }]
        else:
            return [{
                "Phase": f"Level {target_level} - Sustaining",
                "Goal": f"‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô Level {target_level} ‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡∏π‡πà Level {target_level + 1}",
                "Actions": [{
                    "Statement_ID": f"SUSTAIN-L{target_level}",
                    "Recommendation": f"‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏Å‡∏≤‡∏£‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏ï‡∏≤‡∏°‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á Level {target_level} ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏°‡πà‡∏≥‡πÄ‡∏™‡∏°‡∏≠ ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡∏π‡πà‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ñ‡∏±‡∏î‡πÑ‡∏õ"
                }]
            }]

    # ------------------------------------------------------------------
    # 2. LLM ‡πÑ‡∏°‡πà‡∏°‡∏µ ‚Üí Fallback ‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°
    # ------------------------------------------------------------------
    if llm_executor is None:
        logger.error("create_structured_action_plan: llm_executor is None ‚Üí ‡πÉ‡∏ä‡πâ fallback")
        actions = []
        for s in failed_statements[:10]:
            sid = s.get("sub_id") or s.get("statement_id") or "UNKNOWN"
            stmt = (s.get("statement") or "").strip()[:200]
            reason = (s.get("reason") or "").strip()[:300]
            actions.append({
                "Statement_ID": sid,
                "Recommendation": f"[{sid}] {stmt} | ‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏: {reason}"
            })
        return [{
            "Phase": f"Level {target_level}",
            "Goal": f"‡∏¢‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ Level {target_level} ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {sub_id}",
            "Actions": actions or [{"Statement_ID": "NO-LLM", "Recommendation": "‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ó‡∏µ‡πà‡∏Ç‡∏≤‡∏î‡∏´‡∏≤‡∏¢"}]
        }]

    # ------------------------------------------------------------------
    # 3. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Prompt + Schema
    # ------------------------------------------------------------------
    try:
        schema_json = json.dumps(ActionPlanActions.model_json_schema(), ensure_ascii=False, indent=2)
    except:
        schema_json = '{"Phase":"string","Goal":"string","Actions":[{"Statement_ID":"string","Recommendation":"string"}]}'

    system_prompt = (
        SYSTEM_ACTION_PLAN_PROMPT
        + "\n\n--- JSON SCHEMA (‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô ARRAY ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô) ---\n"
        + schema_json
        + "\n\nIMPORTANT:\n"
          "- ‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡∏î‡πâ‡∏ß‡∏¢ JSON ARRAY ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÄ‡∏ä‡πà‡∏ô: [ { ... }, { ... } ]\n"
          "- ‡∏´‡πâ‡∏≤‡∏°‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏≠‡∏Å JSON ‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≤‡∏î\n"
          "- ‡∏ó‡∏∏‡∏Å field ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢\n"
          "- Actions ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 1 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ï‡πà‡∏≠ Phase"
    )

    # ‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏° Statement ‡πÉ‡∏´‡πâ‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢
    stmt_blocks = []
    for i, s in enumerate(failed_statements, 1):
        sid = s.get("sub_id") or s.get("statement_id") or f"STMT-{i}"
        level = s.get("level", "?")
        text = str(s.get("statement") or "").strip()
        reason = str(s.get("reason") or "").strip()
        stmt_blocks.append(
            f"‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà {i}\n"
            f"Statement ID: {sid} (Level {level})\n"
            f"‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°: {text}\n"
            f"‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô: {reason}\n"
        )

    human_prompt = ACTION_PLAN_PROMPT.format(
        sub_id=sub_id,
        target_level=target_level,
        failed_statements_list="\n\n".join(stmt_blocks)
    )

    # ------------------------------------------------------------------
    # 4. ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å LLM + Extract (‡πÅ‡∏Ç‡πá‡∏á‡πÅ‡∏Å‡∏£‡πà‡∏á‡∏™‡∏∏‡∏î)
    # ------------------------------------------------------------------
    for attempt in range(max_retries):
        try:
            raw = _fetch_llm_response(
                system_prompt=system_prompt,
                user_prompt=human_prompt,
                max_retries=1,
                llm_executor=llm_executor
            )

            items = _extract_json_array_for_action_plan(raw)
            if not items:
                logger.warning(f"ActionPlan attempt {attempt+1}: ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ JSON array ‚Üí ‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà")
                time.sleep(1)
                continue

            # ‡πÄ‡∏ï‡∏¥‡∏° default + ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î
            result = []
            for item in items:
                phase = str(item.get("Phase") or f"Level {target_level}").strip()
                goal = str(item.get("Goal") or f"‡∏¢‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ Level {target_level}").strip()
                actions = item.get("Actions") or []

                if not isinstance(actions, list):
                    actions = [actions] if isinstance(actions, dict) else []

                clean_actions = []
                for act in actions:
                    if not isinstance(act, dict): continue
                    rec = str(act.get("Recommendation") or "").strip()
                    sid = str(act.get("Statement_ID") or "UNKNOWN").strip()
                    if rec:
                        clean_actions.append({"Statement_ID": sid, "Recommendation": rec})

                if not clean_actions:
                    clean_actions.append({
                        "Statement_ID": "FALLBACK",
                        "Recommendation": "‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏ï‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏ö‡∏Å‡∏û‡∏£‡πà‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏‡πÉ‡∏ô‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô"
                    })

                result.append({"Phase": phase, "Goal": goal, "Actions": clean_actions})

            if result:
                logger.info(f"Action Plan ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ‚Üí {len(result)} phase(s)")
                return result

        except Exception as e:
            logger.warning(f"ActionPlan attempt {attempt+1} ‡πÄ‡∏Å‡∏¥‡∏î error: {e}")

    # ------------------------------------------------------------------
    # 5. Final Fallback (‡πÑ‡∏°‡πà‡πÄ‡∏Ñ‡∏¢‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡πÉ‡∏ô Production ‡πÑ‡∏î‡πâ)
    # ------------------------------------------------------------------
    logger.error("ActionPlan: ‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß ‚Üí ‡πÉ‡∏ä‡πâ Hardcoded Template")
    actions = []
    for i, s in enumerate(failed_statements[:8], 1):
        sid = s.get("sub_id") or f"STMT-{i}"
        text = str(s.get("statement") or "").strip()[:150]
        actions.append({"Statement_ID": sid, "Recommendation": f"‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î: {text}"})

    return [{
        "Phase": f"Level {target_level} - ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏î‡πà‡∏ß‡∏ô",
        "Goal": f"‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ç‡πâ‡∏≠‡∏ö‡∏Å‡∏û‡∏£‡πà‡∏≠‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ Level {target_level}",
        "Actions": actions or [{"Statement_ID": "URGENT", "Recommendation": "‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≤‡∏°‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÇ‡∏î‡∏¢‡∏î‡πà‡∏ß‡∏ô"}]
    }]