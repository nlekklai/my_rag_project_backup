"""
llm_data_utils.py
Robust LLM + RAG utilities for SEAM assessment (CLEAN FINAL VERSION)
"""

import logging
import time
import json
import hashlib
import uuid
import re
from datetime import datetime
from typing import List, Dict, Any, Optional, Union, Callable, TypeVar, Set
import json5
from utils.enabler_keyword_map import ENABLER_KEYWORD_MAP, DEFAULT_KEYWORDS
from langchain.retrievers import EnsembleRetriever
from langchain_core.documents import Document
from langchain_community.retrievers import BM25Retriever # FIX: Import BM25 ‡∏à‡∏≤‡∏Å community
from core.action_plan_schema import get_clean_action_plan_schema


# Optional: regex ‡πÅ‡∏ó‡∏ô re (‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤) ‚Äî ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡πá‡πÉ‡∏ä‡πâ re ‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤
try:
    import regex as re  # type: ignore
except ImportError:
    pass  # ‡πÉ‡∏ä‡πâ re ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ

logger = logging.getLogger(__name__)
if not logger.handlers:
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# ===================================================================
# 1. Core Configuration (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô)
# ===================================================================
from config.global_vars import (
    DEFAULT_ENABLER,
    INITIAL_TOP_K,
    FINAL_K_RERANKED,
    MAX_EVAL_CONTEXT_LENGTH,
    USE_HYBRID_SEARCH, 
    HYBRID_VECTOR_WEIGHT, 
    HYBRID_BM25_WEIGHT,
    MAX_ACTION_PLAN_PHASES,
    MAX_STEPS_PER_ACTION,
    ACTION_PLAN_STEP_MAX_WORDS,
    ACTION_PLAN_LANGUAGE
)

# ===================================================================
# 2. Critical Utilities (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏à‡∏£‡∏¥‡∏á ‚Äî ‡πÑ‡∏°‡πà‡∏°‡∏µ fallback)
# ===================================================================
# üéØ FIX 1: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô Import ‡∏à‡∏≤‡∏Å _get_collection_name ‡πÑ‡∏õ‡πÄ‡∏õ‡πá‡∏ô get_doc_type_collection_key
from core.vectorstore import get_hf_embeddings
from utils.path_utils import get_doc_type_collection_key # <--- ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà‡∏à‡∏≤‡∏Å utils/path_utils
from core.json_extractor import (
    _robust_extract_json,
    _normalize_keys,
    _safe_int_parse,
    _extract_normalized_dict
)

# ===================================================================
# 3. Project Modules (‡∏ñ‡πâ‡∏≤‡∏´‡∏≤‡∏¢ ‚Üí ‡∏õ‡∏•‡πà‡∏≠‡∏¢‡πÉ‡∏´‡πâ error ‡∏ä‡∏±‡∏î ‡πÜ ‡πÑ‡∏õ‡πÄ‡∏•‡∏¢ ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏á‡∏µ‡∏¢‡∏ö)
# ===================================================================
from core.seam_prompts import (
    SYSTEM_ASSESSMENT_PROMPT,
    USER_ASSESSMENT_PROMPT,
    SYSTEM_ACTION_PLAN_PROMPT,
    ACTION_PLAN_PROMPT,
    SYSTEM_EVIDENCE_DESCRIPTION_PROMPT,
    EVIDENCE_DESCRIPTION_PROMPT,
    SYSTEM_LOW_LEVEL_PROMPT,
    USER_LOW_LEVEL_PROMPT,
    USER_LOW_LEVEL_PROMPT_TEMPLATE
)

from core.vectorstore import VectorStoreManager, get_global_reranker, ChromaRetriever
from core.assessment_schema import CombinedAssessment, EvidenceSummary
from core.action_plan_schema import ActionPlanActions

try:
    from core.assessment_schema import StatementAssessment
except ImportError:
    from pydantic import BaseModel
    class StatementAssessment(BaseModel):
        score: int = 0
        reason: str = ""

from langchain_core.documents import Document as LcDocument

# ===================================================================
# 4. Constants
# ===================================================================
LOW_LEVEL_K: int = 3
_MOCK_FLAG = False
_MAX_LLM_RETRIES = 3

def set_mock_control_mode(enable: bool):
    global _MOCK_FLAG
    _MOCK_FLAG = bool(enable)
    logger.info(f"Mock control mode: {_MOCK_FLAG}")

# Helper: ‡∏™‡∏£‡πâ‡∏≤‡∏á Chroma where filter
def _create_where_filter(stable_doc_ids: Optional[Set[str]] = None, 
                         subject: Optional[str] = None,
                         sub_topic: Optional[str] = None) -> Dict[str, Any]:
    """‡∏™‡∏£‡πâ‡∏≤‡∏á where filter ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ChromaDB ‡∏ó‡∏µ‡πà‡πÅ‡∏Ç‡πá‡∏á‡πÅ‡∏Å‡∏£‡πà‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î"""
    filters = []
    
    if stable_doc_ids:
        filters.append({"stable_doc_uuid": {"$in": list(stable_doc_ids)}})
    
    if subject:
        cleaned = subject.strip()
        if cleaned:
            filters.append({"subject": cleaned})
    
    if sub_topic:
        filters.append({"sub_topic": {"$eq": sub_topic}})
    
    if len(filters) > 1:
        return {"$and": filters}
    elif filters:
        return filters[0]
    else:
        return {}

def retrieve_context_for_endpoint(
    vectorstore_manager,
    query: str = "",
    tenant: Optional[str] = None,
    year: Optional[Union[int, str]] = None,
    stable_doc_ids: Optional[Set[str]] = None,
    doc_type: Optional[str] = None,
    enabler: Optional[str] = None,
    subject: Optional[str] = None,
    sub_topic: Optional[str] = None,  # ‡πÉ‡∏´‡∏°‡πà: ‡πÄ‡∏ä‡πà‡∏ô "KM-4.1"
    k_to_retrieve: int = INITIAL_TOP_K,
    k_to_rerank: int = FINAL_K_RERANKED,
) -> Dict[str, Any]:
    """
    ‡∏î‡∏∂‡∏á context ‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß (stable_doc_ids) ‡∏´‡∏£‡∏∑‡∏≠ filter ‡πÅ‡∏°‡πà‡∏ô ‡πÜ
    ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö sub_topic ‡πÄ‡∏ä‡πà‡∏ô "KM-4.1" ‚Üí ‡πÅ‡∏°‡πà‡∏ô‡∏™‡∏∏‡∏î
    """
    start_time = time.time()
    vsm = vectorstore_manager

    # 1. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î collection ‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤ doc_type ‡πÄ‡∏õ‡πá‡∏ô List/String Literal
    
    clean_doc_type = doc_type or 'seam'
    
    # üí° FIX A: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÅ‡∏õ‡∏•‡∏á String Literal ‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏à‡∏≤‡∏Å curl ‡πÄ‡∏ä‡πà‡∏ô '["seam"]'
    if isinstance(clean_doc_type, str) and clean_doc_type.strip().startswith('['):
        try:
            # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏õ‡πá‡∏ô JSON Array
            parsed_list = json.loads(clean_doc_type.strip())
            
            if isinstance(parsed_list, (list, tuple)) and parsed_list:
                # ‡∏ñ‡πâ‡∏≤‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏î‡πâ‡πÄ‡∏õ‡πá‡∏ô List/Tuple ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å
                clean_doc_type = parsed_list[0]
            elif isinstance(parsed_list, str):
                # ‡∏ñ‡πâ‡∏≤‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏î‡πâ‡πÄ‡∏õ‡πá‡∏ô String (‡∏≠‡∏≤‡∏à‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏î‡πâ)
                clean_doc_type = parsed_list
                
        except json.JSONDecodeError:
            # ‡∏ñ‡πâ‡∏≤‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏õ‡πÉ‡∏ä‡πâ String ‡πÄ‡∏î‡∏¥‡∏°
            logger.debug(f"Could not parse doc_type string literal: {clean_doc_type}")
            pass

    # üí° FIX B: ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏±‡∏ö List/Tuple ‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡∏õ‡∏Å‡∏ï‡∏¥ (‡∏Å‡∏£‡∏ì‡∏µ Router ‡∏™‡πà‡∏á‡∏°‡∏≤‡∏ñ‡∏π‡∏Å ‡∏´‡∏£‡∏∑‡∏≠‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£ Parse JSON)
    if isinstance(clean_doc_type, (list, tuple)):
        # ‡πÉ‡∏ä‡πâ element ‡πÅ‡∏£‡∏Å‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
        clean_doc_type = str(clean_doc_type[0]) if clean_doc_type else 'seam'
    elif not isinstance(clean_doc_type, str):
        # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô String
        clean_doc_type = str(clean_doc_type)

    # üí° FIX C: ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î Quote ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏ï‡∏¥‡∏î‡∏°‡∏≤ (‡πÄ‡∏ä‡πà‡∏ô 'seam' ‡∏´‡∏£‡∏∑‡∏≠ "seam")
    clean_doc_type = str(clean_doc_type).strip().strip("'\"")

    # üéØ ‡πÉ‡∏ä‡πâ get_doc_type_collection_key ‡∏à‡∏≤‡∏Å utils/path_utils.py
    collection_name = get_doc_type_collection_key(
        doc_type=clean_doc_type, 
        enabler=enabler
    )

    chroma = vsm._load_chroma_instance(collection_name)
    if not chroma:
        # üìå NOTE: ‡πÉ‡∏ä‡πâ clean_doc_type ‡πÉ‡∏ô Log ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
        logger.error(f"Collection {collection_name} (Doc Type: {clean_doc_type}) not found!")
        return {"top_evidences": [], "aggregated_context": "", "retrieval_time": 0, "used_chunk_uuids": []}

    # 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á filter ‡∏ó‡∏µ‡πà‡πÅ‡∏Ç‡πá‡∏á‡πÅ‡∏Å‡∏£‡πà‡∏á
    where_filter = _create_where_filter(stable_doc_ids, subject, sub_topic)
    logger.info(f"Retrieval ‚Üí Collection: {collection_name} | Filter: {where_filter} | Query: {query[:80]}...")

    # 3. Embed query
    try:
        emb = get_hf_embeddings()
        # BGE-M3 ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ prefix
        query_emb = emb.embed_query(f"query: {query}") 
    except Exception as e:
        logger.error(f"Embedding failed: {e}")
        return {"top_evidences": [], "aggregated_context": "", "retrieval_time": 0, "used_chunk_uuids": []}

    # 4. Query Chroma
    try:
        results = chroma._collection.query(
            query_embeddings=[query_emb],
            n_results=k_to_retrieve,
            where=where_filter if where_filter else None,
            include=["documents", "metadatas", "distances"]
        )
    except Exception as e:
        logger.error(f"Chroma query failed: {e}")
        return {"top_evidences": [], "aggregated_context": "", "retrieval_time": 0, "used_chunk_uuids": []}

    # 5. ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô LcDocument
    raw_chunks: List[LcDocument] = []
    for doc, meta, dist in zip(
        results["documents"][0],
        results["metadatas"][0],
        results["distances"][0]
    ):
        meta["retrieval_distance"] = float(dist)
        raw_chunks.append(LcDocument(page_content=doc, metadata=meta))

    logger.info(f"Raw retrieval: {len(raw_chunks)} chunks")

    # 6. Rerank (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å!)
    final_chunks = raw_chunks
    reranker = get_global_reranker()
    if reranker and len(raw_chunks) > k_to_rerank:
        try:
            reranked = reranker.compress_documents(
                documents=raw_chunks,
                query=query,
                top_n=k_to_rerank
            )
            # ‡∏î‡∏∂‡∏á Document ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤
            final_chunks = [getattr(r, "document", r) for r in reranked]
            logger.info(f"Reranked ‚Üí {len(final_chunks)} chunks")
        except Exception as e:
            logger.warning(f"Reranker failed: {e}")

    # 7. ‡∏™‡∏£‡πâ‡∏≤‡∏á output
    top_evidences = []
    aggregated_parts = []
    used_chunk_uuids = []

    for doc in final_chunks[:k_to_rerank]:
        md = doc.metadata or {}
        text = str(doc.page_content or "").strip()
        if not text:
            continue

        chunk_uuid = md.get("chunk_uuid") or md.get("dedup_chunk_uuid")
        if not chunk_uuid or len(chunk_uuid) < 32:
            continue  # ‡∏Å‡∏£‡∏≠‡∏á TEMP ID

        used_chunk_uuids.append(chunk_uuid)

        top_evidences.append({
            "doc_id": md.get("stable_doc_uuid"),
            "chunk_uuid": chunk_uuid,
            "source": md.get("source") or md.get("filename") or "Unknown",
            "text": text,
            "pdca_tag": md.get("pdca_tag", "Other"),
            "retrieval_distance": md.get("retrieval_distance", 1.0),
            "sub_topic": md.get("sub_topic"),
        })
        aggregated_parts.append(f"[SOURCE: {md.get('source', 'Unknown')}] {text}")

    result = {
        "top_evidences": top_evidences,
        "aggregated_context": "\n\n---\n\n".join(aggregated_parts),
        "retrieval_time": round(time.time() - start_time, 3),
        "used_chunk_uuids": used_chunk_uuids
    }
    logger.info(f"Final retrieval: {len(top_evidences)} chunks | Sub-topic: {sub_topic}")
    return result

# ========================
# 2. retrieve_context_by_doc_ids (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö hydration ‡πÉ‡∏ô router)
# ========================
def retrieve_context_by_doc_ids(
    doc_uuids: List[str],
    doc_type: str,
    enabler: Optional[str] = None,
    vectorstore_manager = None,
    limit: int = 50,
    tenant: Optional[str] = None, # <-- ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ
    year: Optional[Union[int, str]] = None, # <-- ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ (‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î)
) -> Dict[str, Any]:
    """
    ‡∏î‡∏∂‡∏á chunks ‡∏à‡∏≤‡∏Å stable_doc_uuid ‡∏´‡∏•‡∏≤‡∏¢‡∏ï‡∏±‡∏ß (‡πÉ‡∏ä‡πâ‡∏ï‡∏≠‡∏ô hydration sources)
    """
    start_time = time.time()
    vsm = vectorstore_manager or VectorStoreManager()
    
    # üéØ FIX: ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á collection_name ‡∏î‡πâ‡∏ß‡∏¢‡∏ï‡∏ô‡πÄ‡∏≠‡∏á
    # collection_name = f"{doc_type}"
    # if enabler and enabler != DEFAULT_ENABLER:
    #     collection_name = f"{doc_type}_{enabler.lower()}"
    
    # üü¢ ‡πÉ‡∏ä‡πâ get_doc_type_collection_key ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡πà‡∏≥‡πÄ‡∏™‡∏°‡∏≠‡πÅ‡∏•‡∏∞‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
    collection_name = get_doc_type_collection_key(doc_type=doc_type, enabler=enabler)

    chroma = vsm._load_chroma_instance(collection_name)
    if not chroma:
        logger.error(f"Collection {collection_name} not found for hydration")
        return {"top_evidences": []}

    if not doc_uuids:
        return {"top_evidences": []}

    logger.info(f"Hydration ‚Üí {len(doc_uuids)} doc IDs from {collection_name}")

    try:
        # ‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡πâ‡∏ß‡∏¢ stable_doc_uuid ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö hydration ‡∏ô‡∏±‡πâ‡∏ô‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß
        results = chroma._collection.get(
            where={"stable_doc_uuid": {"$in": doc_uuids}},
            limit=limit,
            include=["documents", "metadatas"]
        )
    except Exception as e:
        logger.error(f"Hydration query failed: {e}")
        return {"top_evidences": []}

    evidences = []
    for doc, meta in zip(results["documents"], results["metadatas"]):
        if not doc.strip():
            continue
        evidences.append({
            "doc_id": meta.get("stable_doc_uuid"),
            "chunk_uuid": meta.get("chunk_uuid"),
            "source": meta.get("source") or meta.get("filename") or "Unknown",
            "text": doc,
            "pdca_tag": meta.get("pdca_tag", "Other"),
        })

    logger.info(f"Hydration success: {len(evidences)} chunks from {len(doc_uuids)} docs")
    return {"top_evidences": evidences}

# ------------------------
# Retrieval: retrieve_context_with_filter (‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û + Logger Fix)
# ------------------------
def retrieve_context_with_filter(
    query: Union[str, List[str]],
    doc_type: str,
    tenant: Optional[str] = None,
    year: Optional[Union[int, str]] = None,
    enabler: Optional[str] = None,
    subject: Optional[str] = None,
    # ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô Instance ‡∏Ç‡∏≠‡∏á Manager ‡∏ó‡∏µ‡πà‡∏°‡∏µ create_hybrid_retriever
    vectorstore_manager: Optional['VectorStoreManager'] = None, 
    mapped_uuids: Optional[List[str]] = None,
    stable_doc_ids: Optional[List[str]] = None,
    priority_docs_input: Optional[List[Any]] = None,
    sequential_chunk_uuids: Optional[List[str]] = None,
    sub_id: Optional[str] = None,
    level: Optional[int] = None,
    get_previous_level_docs: Optional[Callable[[int, str], List[Any]]] = None,
) -> Dict[str, Any]:
    """
    ‡∏î‡∏∂‡∏á context ‡∏î‡πâ‡∏ß‡∏¢ semantic search + priority + fallback + rerank
    ‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏•‡πâ‡∏ß: ‡πÉ‡∏ä‡πâ Hybrid Search (BM25 + Vector) ‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞ Cache ‡πÑ‡∏ß‡πâ‡πÉ‡∏ô Manager
    """
    start_time = time.time()
    all_retrieved_chunks: List[Any] = []
    used_chunk_uuids: List[str] = []

    # 1. ‡πÉ‡∏ä‡πâ VectorStoreManager ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤ VectorStoreManager() ‡∏°‡∏µ Logic ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Initialise Chroma Client (self._client)
    manager = vectorstore_manager or VectorStoreManager() 
    if manager is None or not hasattr(manager, '_client') or manager._client is None:
        logger.error("VectorStoreManager not initialized or _client is missing!")
        return {"top_evidences": [], "aggregated_context": "", "retrieval_time": 0.0, "used_chunk_uuids": []}

    # üü¢ NEW FIX: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏Å‡∏≥‡∏´‡∏ô‡∏î Logger ‡πÉ‡∏´‡πâ‡∏Å‡∏±‡∏ö VSM Instance
    # ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏°‡∏ò‡∏≠‡∏î‡∏†‡∏≤‡∏¢‡πÉ‡∏ô (‡πÄ‡∏ä‡πà‡∏ô create_hybrid_retriever) ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ self.logger ‡πÑ‡∏î‡πâ
    if not hasattr(manager, 'logger') or manager.logger is None:
        try:
            manager.logger = logger # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î logger ‡∏Ç‡∏≠‡∏á module ‡∏ô‡∏µ‡πâ‡πÉ‡∏´‡πâ VSM
            logger.info("Assigned module logger to VectorStoreManager instance (Worker/Fallback Fix).")
        except NameError:
            # ‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà logger ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å import ‡πÉ‡∏ô module ‡∏ô‡∏µ‡πâ (‡∏ã‡∏∂‡πà‡∏á‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô)
            pass

    queries_to_run = [query] if isinstance(query, str) else list(query or [])
    if not queries_to_run:
        queries_to_run = [""]

    # ‡∏£‡∏ß‡∏° chunk ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÇ‡∏ú‡∏•‡πà (sequential)
    if sequential_chunk_uuids:
        mapped_uuids = (mapped_uuids or []) + sequential_chunk_uuids

    # 2. Fallback ‡∏à‡∏≤‡∏Å level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Level 3)
    fallback_chunks = []
    if level == 3 and callable(get_previous_level_docs):
        try:
            fallback_chunks = get_previous_level_docs(level - 1, sub_id) or []
            logger.info(f"Fallback from previous level: {len(fallback_chunks)} chunks")
        except Exception as e:
            logger.warning(f"Fallback failed: {e}")

    # 3. Priority chunks (‡∏à‡∏≤‡∏Å evidence mapping)
    guaranteed_priority_chunks = []
    if priority_docs_input:
        for doc in priority_docs_input:
            if doc is None:
                continue
            if isinstance(doc, dict):
                pc = doc.get('page_content') or doc.get('text') or ''
                meta = doc.get('metadata') or {}
                if 'chunk_uuid' in doc:
                    meta['chunk_uuid'] = doc['chunk_uuid']
                if 'doc_id' in doc:
                    meta['stable_doc_uuid'] = doc['doc_id']
                if 'pdca_tag' in doc:
                    meta['pdca_tag'] = doc['pdca_tag']
                if pc.strip():
                    guaranteed_priority_chunks.append(LcDocument(page_content=pc, metadata=meta))
            elif hasattr(doc, 'page_content'):
                guaranteed_priority_chunks.append(doc)

    # 4. Collection name
    collection_name = get_doc_type_collection_key(doc_type, enabler or "KM")
    logger.info(f"Requesting retriever ‚Üí collection='{collection_name}' (doc_type={doc_type}, enabler={enabler})")

    # 5. ‡∏™‡∏£‡πâ‡∏≤‡∏á Filter
    where_filter: Dict[str, Any] = {}
    if stable_doc_ids:
        logger.info(f"Applying Stable Doc ID filter: {len(stable_doc_ids)} IDs")
        where_filter = {"stable_doc_uuid": {"$in": stable_doc_ids}}

    if subject:
        subject_filter = {"subject": {"$eq": subject}}
        if where_filter:
            where_filter = {"$and": [where_filter, subject_filter]}
            logger.info(f"Adding Subject filter (AND logic): {subject}")
        else:
            where_filter = subject_filter

    # === HYBRID SEARCH MODE (BM25 + Vector) ===
    hybrid_retriever = None

    if USE_HYBRID_SEARCH:
        try:
            # üéØ FIX: ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ Manager ‡∏ó‡∏µ‡πà Cache Hybrid Retriever ‡πÑ‡∏ß‡πâ‡πÅ‡∏•‡πâ‡∏ß
            logger.info(f"Requesting Hybrid Retriever from Manager for {collection_name} (Cached)...")
            hybrid_retriever = manager.create_hybrid_retriever(collection_name=collection_name)
            logger.info(f"HYBRID mode activated: Vector 70% + BM25 30% for {collection_name} (Cached)")

        except Exception as e:
            # Fallback ‡∏´‡∏≤‡∏Å Manager ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á Hybrid Retriever ‡πÑ‡∏î‡πâ (‡πÄ‡∏ä‡πà‡∏ô ‡πÑ‡∏°‡πà‡∏°‡∏µ BM25 Index)
            # üö® BUG: ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Error ‡∏à‡∏≤‡∏Å VSM ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ Logger (‡πÅ‡∏ï‡πà‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏•‡πâ‡∏ß)
            logger.warning(f"Hybrid mode failed (Error calling manager.create_hybrid_retriever: {e}), falling back to vector only")
            use_hybrid = False

    # 6. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Retriever ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ
    if USE_HYBRID_SEARCH and hybrid_retriever:
        retriever = hybrid_retriever
    else:
        # ‡πÉ‡∏ä‡πâ Vector Retriever ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
        retriever = manager.get_retriever(collection_name)
        logger.info("Using VECTOR ONLY mode.")

    # 7. ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å VectorStore
    retrieved_chunks = []
    if retriever:
        for q in queries_to_run:
            q_log = q[:120] + "..." if len(q) > 120 else q
            logger.critical(f"[QUERY] Running: '{q_log}' ‚Üí collection='{collection_name}'")

            try:
                search_kwargs = {"k": INITIAL_TOP_K}  # INITIAL_TOP_K
                if where_filter:
                    search_kwargs["where"] = where_filter
                
                # üéØ FIX: ‡πÉ‡∏ä‡πâ get_relevant_documents ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ö **search_kwargs ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡∏à‡∏∞‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡∏Å‡∏ß‡πà‡∏≤
                if hasattr(retriever, "get_relevant_documents"):
                    # EnsembleRetriever ‡πÅ‡∏•‡∏∞ ChromaRetriever ‡∏°‡∏±‡∏Å‡∏à‡∏∞‡∏£‡∏±‡∏ö kwargs ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
                    docs = retriever.get_relevant_documents(q, **search_kwargs)
                elif hasattr(retriever, "invoke"):
                    # Fallback ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LangChain Runnable API 
                    docs = retriever.invoke(q, config={"configurable": {"search_kwargs": search_kwargs}})
                else:
                    docs = []
                    
                retrieved_chunks.extend(docs or [])
            except Exception as e:
                logger.error(f"Retriever invoke failed: {e}", exc_info=True)
    else:
        logger.error(f"Retriever NOT FOUND for collection: {collection_name}")

    logger.critical(f"[RETRIEVAL] Raw chunks from ChromaDB: {len(retrieved_chunks)} documents")

    # 8. ‡∏£‡∏ß‡∏° + deduplicate
    all_chunks = retrieved_chunks + fallback_chunks + guaranteed_priority_chunks
    unique_map: Dict[str, LcDocument] = {}

    for doc in all_chunks:
        if not doc or not hasattr(doc, "page_content"):
            continue
        md = getattr(doc, "metadata", {}) or {}
        pc = str(getattr(doc, "page_content", "") or "").strip()
        if not pc:
            continue

        if level == 3:
            pc = pc[:500]
            doc.page_content = pc

        chunk_uuid = md.get("chunk_uuid") or md.get("stable_doc_uuid") or f"TEMP-{uuid.uuid4().hex[:12]}"
        if chunk_uuid not in unique_map:
            md["dedup_chunk_uuid"] = chunk_uuid
            unique_map[chunk_uuid] = doc

    dedup_chunks = list(unique_map.values())
    logger.info(f"After dedup: {len(dedup_chunks)} chunks")

    # 9. Rerank
    final_docs = list(guaranteed_priority_chunks)
    slots_left = max(0, 12 - len(final_docs))  # FINAL_K_RERANKED
    candidates = [d for d in dedup_chunks if d not in final_docs]

    # Patch metadata ‡∏ó‡∏µ‡πà‡∏´‡∏≤‡∏¢‡πÑ‡∏õ‡∏´‡∏•‡∏±‡∏á rerank
    candidate_metadata_map = {
        doc.page_content: getattr(doc, 'metadata', {})
        for doc in candidates if hasattr(doc, 'page_content') and doc.page_content.strip()
    }

    if slots_left > 0 and candidates:
        reranker = get_global_reranker()
        if reranker and hasattr(reranker, "compress_documents"):
            try:
                reranked_results = reranker.compress_documents(
                    documents=candidates,
                    query=queries_to_run[0],
                    top_n=slots_left
                )
                for result in reranked_results:
                    doc_to_add = getattr(result, 'document', result)
                    if doc_to_add and hasattr(doc_to_add, 'page_content') and doc_to_add.page_content.strip():
                        current_md = getattr(doc_to_add, 'metadata', {})
                        if not current_md.get("chunk_uuid") and doc_to_add.page_content in candidate_metadata_map:
                            doc_to_add.metadata = candidate_metadata_map[doc_to_add.page_content]
                        final_docs.append(doc_to_add)
                logger.info(f"Reranker returned {len(reranked_results)} docs")
            except Exception as e:
                logger.warning(f"Reranker failed ({e}), using raw candidates")
                final_docs.extend(candidates[:slots_left])
        else:
            final_docs.extend(candidates[:slots_left])
    else:
        logger.info("No slots left or no candidates ‚Üí priority only")

    # 10. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
    top_evidences = []
    aggregated_parts = []
    used_chunk_uuids = []

    VALID_CHUNK_ID = re.compile(r"^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$|^[0-9a-f]{64}(-[0-9]+)?$", re.IGNORECASE)
    VALID_STABLE_ID = re.compile(r"^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$|^[0-9a-f]{64}$", re.IGNORECASE)

    for doc in final_docs[:12]:
        md = getattr(doc, "metadata", {}) or {}
        pc = str(getattr(doc, "page_content", "") or "").strip()
        if not pc:
            continue

        chunk_uuid = md.get("chunk_uuid") or md.get("dedup_chunk_uuid") or md.get("id")
        stable_doc_uuid = md.get("stable_doc_uuid") or md.get("source_doc_id")

        primary_id = None
        if stable_doc_uuid and VALID_STABLE_ID.match(str(stable_doc_uuid)):
            primary_id = stable_doc_uuid
        elif chunk_uuid and VALID_CHUNK_ID.match(str(chunk_uuid)):
            primary_id = chunk_uuid
        else:
            logger.warning(f"Chunk has no valid ID! Stable: {stable_doc_uuid}, Chunk: {chunk_uuid}")
            primary_id = f"TEMP-{uuid.uuid4().hex[:8]}"

        if not str(primary_id).startswith("TEMP-"):
            used_chunk_uuids.append(str(primary_id))

        source = md.get("source_filename") or md.get("source") or md.get("filename") or "Unknown File"
        pdca = md.get("pdca_tag", "Other")
        rerank_score = float(md.get("_rerank_score_force") or md.get("relevance_score") or 0.0)

        top_evidences.append({
            "doc_id": stable_doc_uuid or primary_id,
            "chunk_uuid": chunk_uuid or primary_id,
            "stable_doc_uuid": stable_doc_uuid,
            "source": source,
            "source_filename": source,
            "text": pc,
            "pdca_tag": pdca,
            "rerank_score": rerank_score,
        })
        aggregated_parts.append(f"[{pdca}] [SOURCE: {source}] {pc}")

    result = {
        "top_evidences": top_evidences,
        "aggregated_context": "\n\n---\n\n".join(aggregated_parts) if aggregated_parts else "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á",
        "retrieval_time": round(time.time() - start_time, 3),
        "used_chunk_uuids": used_chunk_uuids
    }

    logger.info(f"Final retrieval L{level or '?'} {sub_id or ''}: {len(top_evidences)} chunks in {result['retrieval_time']:.2f}s")
    return result

def retrieve_context_for_low_levels(query: str, doc_type: str, enabler: Optional[str]=None,
                                 vectorstore_manager: Optional['VectorStoreManager']=None,
                                 top_k: int=LOW_LEVEL_K, initial_k: int=INITIAL_TOP_K,
                                 # üü¢ NEW: ‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠ arguments
                                 mapped_uuids: Optional[List[str]]=None,
                                 priority_docs_input: Optional[List[Any]] = None,
                                 sequential_chunk_uuids: Optional[List[str]] = None, 
                                 sub_id: Optional[str]=None, level: Optional[int]=None) -> Dict[str, Any]:
    """
    Retrieves a small, focused context for low levels (L1, L2) using a reduced k (LOW_LEVEL_K).
    """
    # ‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å ‡πÅ‡∏ï‡πà‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏ä‡πâ k ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
    return retrieve_context_with_filter(
        query=query,
        doc_type=doc_type,
        enabler=enabler,
        vectorstore_manager=vectorstore_manager,
        top_k=LOW_LEVEL_K,
        initial_k=initial_k,
        mapped_uuids=mapped_uuids,
        priority_docs_input=priority_docs_input,
        sequential_chunk_uuids=sequential_chunk_uuids, 
        sub_id=sub_id,
        level=level
    )

# ----------------------------------------------------
# Helper function: Summarize evidence list (minimal stub)
# ----------------------------------------------------
def _summarize_evidence_list_short(evidences: list, max_sentences: int = 3) -> str:
    """
    Provides a concise summary of evidence items.
    """
    if not evidences:
        return ""
    
    parts = []
    for ev in evidences[:max(1, min(len(evidences), max_sentences))]:
        if isinstance(ev, dict):
            fn = ev.get("source_filename") or ev.get("source") or ev.get("doc_id", "unknown")
            txt = ev.get("text") or ev.get("content") or ""
        else:
            fn = str(ev)
            txt = str(ev)
        txt_short = txt[:120].replace("\n", " ").strip()
        if txt_short:
            parts.append(f"‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå `{fn}`: {txt_short}...")
        else:
            parts.append(f"‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå `{fn}`")
    return " | ".join(parts)


# ULTIMATE FINAL VERSION: build_multichannel_context_for_level (OPTIMIZED)
# ‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó‡πÉ‡∏´‡∏°‡πà: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏Ñ‡πà BASELINE ‡πÅ‡∏•‡∏∞ AUXILIARY summaries ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
# ----------------------------------------------------
def build_multichannel_context_for_level(
    level: int,
    top_evidences: List[Dict[str, Any]],
    previous_levels_map: Optional[Dict[str, Any]] = None,
    previous_levels_evidence: Optional[List[Dict[str, Any]]] = None, # List ‡∏Ç‡∏≠‡∏á Chunks ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏à‡∏≤‡∏Å Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤
    max_main_context_tokens: int = 3000, 
    max_summary_sentences: int = 4,
    max_context_length: Optional[int] = None, 
    **kwargs
) -> Dict[str, Any]:
    """
    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏±‡∏Å‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Context Summary ‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏°‡∏≤
    ‡πÇ‡∏î‡∏¢‡πÄ‡∏ô‡πâ‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Baseline Summary (‡∏à‡∏≤‡∏Å Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤) ‡πÅ‡∏•‡∏∞ Auxiliary Summary (‡∏à‡∏≤‡∏Å Level ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô)
    """
    logger = logging.getLogger(__name__)
    K_MAIN = 5
    MIN_RELEVANCE_FOR_AUX = 0.4  # ‡∏Å‡∏£‡∏≠‡∏á aux ‡∏ó‡∏µ‡πà‡∏ï‡πà‡∏≥‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ

    # --- 1) Baseline Summary ---
    # ‡πÉ‡∏ä‡πâ List ‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏°‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß (previous_levels_evidence_list) ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å
    baseline_evidence = previous_levels_evidence or [] 

    summarizable_baseline = [
        item for item in baseline_evidence
        if isinstance(item, dict) and (item.get("text") or item.get("content"))
    ]
    
    # üü¢ FIX: ‡∏õ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô baseline
    if not summarizable_baseline:
        baseline_summary = "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏à‡∏≤‡∏Å Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤"
    else:
        baseline_summary = _summarize_evidence_list_short(
            summarizable_baseline,
            max_sentences=max_summary_sentences
        )

    # --- 2) Auxiliary Summary ---
    direct, aux_candidates = [], []

    for ev in top_evidences:
        if not isinstance(ev, dict):
            # ‡∏≠‡∏≤‡∏à‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå
            continue # ‡∏Ç‡πâ‡∏≤‡∏° chunks ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô dict ‡πÑ‡∏õ‡πÄ‡∏•‡∏¢

        # NEW: ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö tag ‡∏ó‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö‡πÄ‡∏ï‡πá‡∏°‡πÅ‡∏•‡∏∞‡∏¢‡πà‡∏≠
        tag = (ev.get("pdca_tag") or ev.get("PDCA") or "Other").upper()
        relevance = ev.get("rerank_score") or ev.get("score", 0.0)

        # PDCA Chunks ‡∏ñ‡∏π‡∏Å‡∏™‡πà‡∏á‡πÑ‡∏õ‡πÄ‡∏õ‡πá‡∏ô Direct Context (‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏ô Engine)
        # ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ô‡∏µ‡πâ‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡πÅ‡∏Ñ‡πà‡πÅ‡∏¢‡∏Å‡πÅ‡∏¢‡∏∞ ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Direct Context
        if tag in {"P", "PLAN", "D", "DO", "C", "CHECK", "A", "ACT"}:
            direct.append(ev)
        elif relevance >= MIN_RELEVANCE_FOR_AUX:  # ‡∏Å‡∏£‡∏≠‡∏á aux ‡∏ó‡∏µ‡πà‡∏ï‡πà‡∏≥‡πÄ‡∏Å‡∏¥‡∏ô
            aux_candidates.append(ev)

    # Logic ‡∏Å‡∏≤‡∏£‡∏¢‡πâ‡∏≤‡∏¢‡∏à‡∏≤‡∏Å aux ‡πÑ‡∏õ direct (K_MAIN) ‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡∏≠‡∏¢‡∏π‡πà (‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏±‡∏î/Debug ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏™‡πà‡∏á Direct ‡∏≠‡∏≠‡∏Å‡πÑ‡∏õ)
    if len(direct) < K_MAIN:
        need = K_MAIN - len(direct)
        direct.extend(aux_candidates[:need])
        aux_candidates = aux_candidates[need:]
        
    if len(direct) < K_MAIN:
        logger.warning(f"L{level}: Direct PDCA chunks ‡∏¢‡∏±‡∏á‡∏ô‡πâ‡∏≠‡∏¢ ({len(direct)}) ‡∏´‡∏•‡∏±‡∏á‡∏¢‡πâ‡∏≤‡∏¢‡∏à‡∏≤‡∏Å aux")

    aux_summary = _summarize_evidence_list_short(aux_candidates, max_sentences=3) if aux_candidates else "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏£‡∏≠‡∏á"

    # --- 3) Return ---
    debug_meta = {
        "level": level,
        "direct_count": len(direct),
        "aux_count": len(aux_candidates),
        # üü¢ FIX: ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏™‡∏£‡∏∏‡∏õ‡∏à‡∏£‡∏¥‡∏á
        "baseline_count": len(summarizable_baseline), 
        "max_context_length_received": max_context_length 
    }
    logger.info(f"Context L{level} ‚Üí Direct:{len(direct)} | Aux:{len(aux_candidates)} | Baseline:{len(summarizable_baseline)}")

    return {
        "baseline_summary": baseline_summary,
        "direct_context": "",  
        "aux_summary": aux_summary,
        "debug_meta": debug_meta,
    }

# ------------------------
# LLM fetcher
# ------------------------
def _fetch_llm_response(
    system_prompt: str, 
    user_prompt: str, 
    max_retries: int = 3,
    llm_executor: Any = None 
) -> str:
    """
    ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å LLM ‡∏ú‡πà‡∏≤‡∏ô LangChain/Ollama ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Format ‡∏ú‡∏¥‡∏î‡πÄ‡∏û‡∏µ‡πâ‡∏¢‡∏ô:
    - ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö JSON output ‡∏î‡πâ‡∏ß‡∏¢ Strict English Prompt
    - ‡πÉ‡∏ä‡πâ Regex Extraction ‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡πà‡∏ß‡∏ô { ... } ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡∏≠‡∏≠‡∏Å
    - Log raw response ‡πÄ‡∏ï‡πá‡∏°‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Debug
    - Retry ‡∏û‡∏£‡πâ‡∏≠‡∏° Exponential Backoff ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏Å‡∏¥‡∏î Error
    """
    global _MOCK_FLAG

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ LLM Instance ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    if llm_executor is None and not _MOCK_FLAG: 
        raise ConnectionError("LLM instance not initialized (Missing llm_executor).")

    # 1. üõ†Ô∏è ENFORCED PROMPT (‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡∏°‡∏±‡∏Å‡∏Ñ‡∏∏‡∏° Format ‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏•‡πá‡∏Å)
    enforced_system_prompt = system_prompt.strip() + (
        "\n\n"
        "### STRICT OUTPUT RULES ###\n"
        "1. ANSWER IN VALID JSON OBJECT ONLY.\n"
        "2. NO EXPLANATIONS, NO PREFACE, NO CONVERSATION.\n"
        "3. START WITH '{' AND END WITH '}'.\n"
        "4. DO NOT USE MARKDOWN CODE BLOCKS (```json).\n"
        "5. IF NO EVIDENCE FOUND, RETURN: {\"score\": 0, \"reason\": \"No evidence\", \"is_passed\": false}"
    )

    messages = [
        {"role": "system", "content": enforced_system_prompt},
        {"role": "user",   "content": user_prompt}
    ]

    for attempt in range(1, max_retries + 1):
        try:
            # --- MOCK MODE CASE ---
            if _MOCK_FLAG:
                mock_json = '{"score": 1, "reason": "Mock mode active", "is_passed": true}'
                logger.critical(f"LLM RAW RESPONSE (DEBUG MOCK): {mock_json}")
                return mock_json

            # --- ACTUAL LLM CALL (OLLAMA / LANGCHAIN) ---
            # ‡πÉ‡∏ä‡πâ temperature=0.0 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
            response = llm_executor.invoke(messages, config={"temperature": 0.0})
            
            # ‡∏î‡∏∂‡∏á Text ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏à‡∏≤‡∏Å Response Object
            raw_text = ""
            if hasattr(response, "content"):
                raw_text = str(response.content)
            elif isinstance(response, str):
                raw_text = response
            else:
                raw_text = str(response)

            # üîç ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏´‡πá‡∏ô‡πÉ‡∏ô Log ‡∏Ñ‡∏∑‡∏≠‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ (Log ‡∏Å‡πà‡∏≠‡∏ô Clean ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°‡πÇ‡∏°‡πÄ‡∏î‡∏•)
            logger.critical(f"LLM RAW RESPONSE (DEBUG): {raw_text[:1000]}{'...' if len(raw_text) > 1000 else ''}")

            # 2. üßπ CLEANING LOGIC (Regex Extraction)
            # ‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ LLM ‡∏ï‡∏≠‡∏ö "Based on the text... { ... }"
            raw_text_stripped = raw_text.strip()
            
            # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏õ‡∏µ‡∏Å‡∏Å‡∏≤‡∏Ñ‡∏π‡πà‡πÅ‡∏£‡∏Å { ... }
            json_match = re.search(r'(\{.*\})', raw_text_stripped, re.DOTALL)
            
            if json_match:
                extracted_json = json_match.group(1)
                try:
                    # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô JSON ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                    json.loads(extracted_json) 
                    return extracted_json
                except json.JSONDecodeError:
                    logger.warning(f"Extracted string is not valid JSON: {extracted_json[:100]}")
            
            # 3. üõ°Ô∏è FALLBACK: ‡∏ñ‡πâ‡∏≤ Regex ‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ ‡∏´‡∏£‡∏∑‡∏≠ Parse ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏î‡∏π‡∏ß‡πà‡∏≤ raw_text (‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏î‡∏´‡∏±‡∏ß‡∏ó‡πâ‡∏≤‡∏¢) ‡∏û‡∏≠‡∏•‡∏∏‡πâ‡∏ô‡πÄ‡∏õ‡πá‡∏ô JSON ‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°
            return raw_text_stripped

        except Exception as e:
            logger.error(f"LLM call failed (attempt {attempt}/{max_retries}): {e}")
            if attempt < max_retries:
                # Exponential backoff: 2s, 4s, 8s...
                time.sleep(2 ** attempt)  
            else:
                logger.critical("All LLM attempts failed ‚Äì returning safe fallback JSON")
                return '{"score": 0, "reason": "LLM_TIMEOUT_OR_FAILURE", "is_passed": false}'

    return '{"score": 0, "reason": "Unknown execution error"}'

# # ------------------------------------------------------------------
# # ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ï‡∏±‡∏ß‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡∏°‡πà: ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÅ‡∏•‡∏∞‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤ String/Dict ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å Response
# # ------------------------------------------------------------------
# def _clean_llm_response_content(resp: Any) -> str:
#     """
#     ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏î‡∏∂‡∏á content ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö string ‡∏ó‡∏µ‡πà‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
#     ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏´‡πà‡∏≠‡∏´‡∏∏‡πâ‡∏°‡πÅ‡∏ö‡∏ö Tuple/List ‡∏ó‡∏µ‡πà‡∏°‡∏µ Dict/String ‡∏≠‡∏¢‡∏π‡πà‡∏†‡∏≤‡∏¢‡πÉ‡∏ô ‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ Regex Cleanup 
#     ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ JSON Object
#     """
    
#     # --- 1. ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô (Existing Logic) ---
#     cleaned_resp_str: str = ""

#     # 1.1 ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏≤‡∏£‡∏´‡πà‡∏≠‡∏´‡∏∏‡πâ‡∏° (Handle Tuple/List wrapper)
#     if isinstance(resp, (list, tuple)) and resp:
#         resp = resp[0]
#         logger.debug(f"LLM Response was wrapped in {type(resp).__name__}, extracted first element.")

#     # 1.2 ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Response Object/Dict ‡∏ó‡∏µ‡πà‡∏°‡∏µ 'content' field
#     if hasattr(resp, "content"): 
#         cleaned_resp_str = str(resp.content).strip()
#     elif isinstance(resp, dict) and "content" in resp: 
#         cleaned_resp_str = str(resp["content"]).strip()
#     elif isinstance(resp, str): 
#         cleaned_resp_str = resp.strip()
#     else: 
#         # 1.3 Fallback: ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô String
#         cleaned_resp_str = str(resp).strip()
    
#     # --- 2. ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î Regex (The CRITICAL Fix for Malform) ---
    
#     # 2.1 ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏õ‡∏µ‡∏Å‡∏Å‡∏≤ { ... }
#     # re.DOTALL: ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ . ‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà‡πÑ‡∏î‡πâ‡πÅ‡∏°‡πâ‡∏Å‡∏£‡∏∞‡∏ó‡∏±‡πà‡∏á‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏Ç‡∏∂‡πâ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÉ‡∏´‡∏°‡πà
#     match = re.search(r'\{.*\}', cleaned_resp_str, re.DOTALL)
    
#     if match:
#         json_string_only = match.group(0)
#         logger.debug("Regex Cleanup performed: Extracted pure JSON string.")
#         return json_string_only
    
#     # 2.2 ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏û‡∏ö JSON Object: ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ String ‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô‡πÑ‡∏õ
#     logger.warning("Regex Cleanup failed: Could not find JSON object. Returning original cleaned string.")
#     return cleaned_resp_str

# ------------------------
# Evaluation
# ------------------------
T = TypeVar("T", bound=BaseModel)

def _check_and_handle_empty_context(context: str, sub_id: str, level: int) -> Optional[Dict[str, Any]]:
    """
    Returns Failure result if context is empty or contains known error strings.
    Auto-fail with PDCA keys all set to 0.
    """
    if not context or "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á" in context or "ERROR:" in context.upper():
        logger.warning(f"Auto-FAIL L{level} for {sub_id}: Empty or Error Context detected from RAG.")
        context_preview = context.strip()[:100].replace("\n", " ") if context else "Empty Context"
        return {
            "score": 0,
            "reason": f"‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏î‡πâ‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á (Context: {context_preview}).",
            "is_passed": False,
            "P_Plan_Score": 0,
            "D_Do_Score": 0,
            "C_Check_Score": 0,
            "A_Act_Score": 0,
        }
    return None

def _get_context_for_level(context: str, level: int) -> str:
    """Return context string with appropriate length limit for each level."""
    if not context:
        return ""
    # L1-L2 ‡πÉ‡∏ä‡πâ context ‡∏¢‡∏≤‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô
    if level <= 2:
        return context[:6000]  
    # L3-L5 ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÉ‡∏ô global_vars ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î Latency
    return context[:MAX_EVAL_CONTEXT_LENGTH]  

# =========================
# Main Evaluation Function
# =========================

def evaluate_with_llm(
    context: str, 
    sub_criteria_name: str, 
    level: int, 
    statement_text: str, 
    sub_id: str, 
    llm_executor: Any = None, 
    pdca_phase: str = "",
    level_constraint: str = "",
    must_include_keywords: str = "",
    avoid_keywords: str = "",
    max_rerank_score: float = 0.0,
    max_evidence_strength: float = 10.0,
    **kwargs
) -> Dict[str, Any]:
    """Standard Evaluation for L3+ with robust handling."""
    
    # üéØ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: ‡πÉ‡∏ä‡πâ logic ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î Context ‡∏ï‡∏≤‡∏° Level
    context_to_send_eval = _get_context_for_level(context, level)
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö context ‡∏ß‡πà‡∏≤‡∏á
    failure_result = _check_and_handle_empty_context(context, sub_id, level)
    if failure_result:
        return failure_result

    # ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å kwargs (‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÉ‡∏ä‡πâ)
    baseline_summary = kwargs.get("baseline_summary", "")
    aux_summary = kwargs.get("aux_summary", "")

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á User Prompt
    try:
        user_prompt = USER_ASSESSMENT_PROMPT.format(
            sub_criteria_name=sub_criteria_name,
            sub_id=sub_id,
            level=level,
            pdca_phase=pdca_phase,
            statement_text=statement_text,
            context=context_to_send_eval, # ‡πÉ‡∏ä‡πâ Context ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡∏±‡∏î‡πÅ‡∏•‡πâ‡∏ß
            level_constraint=level_constraint,
            must_include_keywords=must_include_keywords or "‡πÑ‡∏°‡πà‡∏°‡∏µ",
            avoid_keywords=avoid_keywords or "‡πÑ‡∏°‡πà‡∏°‡∏µ",
            max_rerank_score=max_rerank_score,
            max_evidence_strength=max_evidence_strength
        )
    except KeyError as e:
        logger.error(f"Missing placeholder in prompt template: {e}")
        user_prompt = f"‡πÄ‡∏Å‡∏ì‡∏ë‡πå: {sub_criteria_name} L{level}\n‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {statement_text}\n‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô: {context_to_send_eval}"

    # ‡πÄ‡∏û‡∏¥‡πà‡∏° summary ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
    if baseline_summary:
        user_prompt += f"\n\n--- Baseline summary (‡∏à‡∏≤‡∏Å Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤): ---\n{baseline_summary}"
    if aux_summary:
        user_prompt += f"\n\n--- Auxiliary evidence summary: ---\n{aux_summary}"

    # System Prompt (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ placeholder)
    try:
        # üéØ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠ key ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö SYSTEM_ASSESSMENT_PROMPT (‡∏Ñ‡∏∑‡∏≠ max_evidence_strength)
        system_prompt = SYSTEM_ASSESSMENT_PROMPT.format(
            max_evidence_strength=max_evidence_strength 
        )
    except KeyError:
        system_prompt = SYSTEM_ASSESSMENT_PROMPT  # fallback ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ

    # ‡πÄ‡∏û‡∏¥‡πà‡∏° schema
    try:
        schema_json = json.dumps(CombinedAssessment.model_json_schema(), ensure_ascii=False, indent=2)
    except Exception:
        schema_json = '{"score":0,"reason":"string"}'

    system_prompt += "\n\n--- JSON SCHEMA ---\n" + schema_json + "\nIMPORTANT: Respond only with valid JSON."

    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å LLM
    try:
        raw = _fetch_llm_response(system_prompt, user_prompt, _MAX_LLM_RETRIES, llm_executor=llm_executor)
        parsed = _robust_extract_json(raw)
        
        if not isinstance(parsed, dict):
            logger.error(f"Parsed result is not dict: {type(parsed)}")
            parsed = {}

        return {
            "score": int(parsed.get("score", 0)),
            "reason": parsed.get("reason", "No reason provided."),
            "is_passed": parsed.get("is_passed", False),
            "P_Plan_Score": int(parsed.get("P_Plan_Score", 0)),
            "D_Do_Score": int(parsed.get("D_Do_Score", 0)),
            "C_Check_Score": int(parsed.get("C_Check_Score", 0)),
            "A_Act_Score": int(parsed.get("A_Act_Score", 0)),
        }

    except Exception as e:
        logger.exception(f"evaluate_with_llm failed for {sub_id} L{level}: {e}")
        return {
            "score": 0,
            "reason": f"LLM error: {str(e)}",
            "is_passed": False,
            "P_Plan_Score": 0,
            "D_Do_Score": 0,
            "C_Check_Score": 0,
            "A_Act_Score": 0,
        }
    
# =========================
# Patch for L1-L2 evaluation
# =========================

# 1Ô∏è‚É£ ‡πÄ‡∏û‡∏¥‡πà‡∏° context limit ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö L1/L2
def _get_context_for_level(context: str, level: int) -> str:
    """Return context string with appropriate length limit for each level."""
    if not context:
        return ""
    if level <= 2:
        return context[:6000]  # L1-L2 ‡πÉ‡∏ä‡πâ context ‡∏¢‡∏≤‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô
    return context[:MAX_EVAL_CONTEXT_LENGTH]  # L3-L5

def _extract_combined_assessment(parsed: Dict[str, Any], score_default_key: str = "score") -> Dict[str, Any]:
    """Helper to safely extract combined assessment results."""
    # üü¢ NEW: Extract all scores needed by seam_assessment.py (Action #1 logic)
    score = int(parsed.get(score_default_key, 0))
    is_passed = parsed.get("is_passed", score >= 1) # ‡πÉ‡∏ä‡πâ score >= 1 ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤ default ‡∏ñ‡πâ‡∏≤ LLM ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏™‡πà‡∏á is_passed

    result = {
        "score": score,
        "reason": parsed.get("reason", "No reason provided by LLM."),
        "is_passed": is_passed,
        "P_Plan_Score": int(parsed.get("P_Plan_Score", 0)),
        "D_Do_Score": int(parsed.get("D_Do_Score", 0)),
        "C_Check_Score": int(parsed.get("C_Check_Score", 0)),
        "A_Act_Score": int(parsed.get("A_Act_Score", 0)),
    }
    return result


# =================================================================
# ULTIMATE PRODUCTION: evaluate_with_llm_low_level (L1/L2 Multi-Enabler)
# =================================================================
def evaluate_with_llm_low_level(
    context: str,
    sub_criteria_name: str,
    level: int,
    statement_text: str,
    sub_id: str,
    llm_executor: Any = None,
    pdca_phase: str = "",
    level_constraint: str = "",
    must_include_keywords: str = "",
    avoid_keywords: str = "",
    max_rerank_score: float = 0.0,
    max_evidence_strength: float = 10.0,
    contextual_rules_map: Optional[Dict[str, Any]] = None,
    enabler_id: str = "KM",
    **kwargs
) -> Dict[str, Any]:
    """
    Standard Evaluation for L1/L2 using LOW_LEVEL_PROMPT (Dynamic Multi-Enabler)
    - ‡∏î‡∏∂‡∏á planning_keywords ‡∏à‡∏≤‡∏Å contextual_rules_map (‡∏à‡∏≤‡∏Å pea_km_contextual_rules.json)
    - ‡πÑ‡∏°‡πà hardcode ‡∏≠‡∏µ‡∏Å‡∏ï‡πà‡∏≠‡πÑ‡∏õ
    - ‡∏™‡πà‡∏á‡∏Ñ‡πà‡∏≤ P/D/C/A ‡∏î‡∏¥‡∏ö‡∏à‡∏≤‡∏Å LLM ‚Üí ‡πÉ‡∏´‡πâ _run_single_assessment ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏Å‡∏é L1/L2 ‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
    """
    
    # -------------------- 1. Setup & Context Check --------------------
    context_to_send_eval = context[:MAX_EVAL_CONTEXT_LENGTH] if context else "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á"
    
    failure_result = _check_and_handle_empty_context(context, sub_id, level)
    if failure_result:
        return failure_result

    # -------------------- 2. ‡∏î‡∏∂‡∏á planning_keywords ‡∏à‡∏≤‡∏Å pea_km_contextual_rules.json --------------------
    planning_keywords = "‡∏ß‡∏¥‡∏™‡∏±‡∏¢‡∏ó‡∏±‡∏®‡∏ô‡πå, ‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢, ‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á, ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢"  # fallback ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô

    if contextual_rules_map:
        # 2.1 ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å sub-criteria ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Å‡πà‡∏≠‡∏ô (‡πÄ‡∏ä‡πà‡∏ô 1.1 ‚Üí L1)
        sub_rules = contextual_rules_map.get(sub_id, {})
        l1_rules = sub_rules.get("L1", {})
        if l1_rules and "planning_keywords" in l1_rules:
            planning_keywords = l1_rules["planning_keywords"]
        else:
            # 2.2 Fallback ‡πÑ‡∏õ‡πÉ‡∏ä‡πâ _enabler_defaults (‡πÄ‡∏ä‡πà‡∏ô KM, DX)
            default_rules = contextual_rules_map.get("_enabler_defaults", {})
            if "planning_keywords" in default_rules:
                planning_keywords = default_rules["planning_keywords"]

    logger.debug(f"[L{level}] Using planning_keywords: {planning_keywords}")

    # -------------------- 3. Prompt Building --------------------
    try:
        # System Prompt: ‡πÉ‡∏™‡πà planning_keywords ‡∏î‡πâ‡∏ß‡∏¢ .format()
        system_prompt = SYSTEM_LOW_LEVEL_PROMPT.format(planning_keywords=planning_keywords)
        system_prompt += "\n\nIMPORTANT: Respond only with valid JSON."

        # User Prompt: ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏™‡πà‡∏á planning_keywords (‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô system ‡πÅ‡∏•‡πâ‡∏ß)
        user_prompt = USER_LOW_LEVEL_PROMPT_TEMPLATE.format(
            sub_id=sub_id,
            sub_criteria_name=sub_criteria_name,
            level=level,
            statement_text=statement_text,
            level_constraint=level_constraint or "‡πÑ‡∏°‡πà‡∏°‡∏µ",
            must_include_keywords=must_include_keywords or "‡πÑ‡∏°‡πà‡∏°‡∏µ",
            avoid_keywords=avoid_keywords or "‡πÑ‡∏°‡πà‡∏°‡∏µ",
            context=context_to_send_eval
        )
    except Exception as e:
        logger.error(f"Error formatting LOW_LEVEL_PROMPT: {e}. Using fallback prompt.")
        system_prompt = SYSTEM_LOW_LEVEL_PROMPT + "\n\nIMPORTANT: Respond only with valid JSON."
        user_prompt = f"‡πÄ‡∏Å‡∏ì‡∏ë‡πå: {sub_id} L{level}\n‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô: {context_to_send_eval}\n‡∏ï‡∏≠‡∏ö JSON ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô"

    # -------------------- 4. LLM Call --------------------
    try:
        raw = _fetch_llm_response(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            max_retries=_MAX_LLM_RETRIES,
            llm_executor=llm_executor
        )
        
        parsed = _robust_extract_json(raw)
        
        if not isinstance(parsed, dict):
            logger.error(f"LLM L{level} response parsed to non-dict: {type(parsed)}. Using empty dict.")
            parsed = {}

        # -------------------- 5. ‡∏™‡πà‡∏á‡∏Ñ‡πà‡∏≤‡∏î‡∏¥‡∏ö‡∏à‡∏≤‡∏Å LLM ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (‡πÑ‡∏°‡πà‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö C/A=0 ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà) --------------------
        return {
            "score": int(parsed.get("score", 0)),
            "reason": parsed.get("reason", "‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏à‡∏≤‡∏Å LLM"),
            "is_passed": parsed.get("is_passed", False),
            "P_Plan_Score": int(parsed.get("P_Plan_Score", 0)),
            "D_Do_Score": int(parsed.get("D_Do_Score", 0)),
            "C_Check_Score": int(parsed.get("C_Check_Score", 0)),
            "A_Act_Score": int(parsed.get("A_Act_Score", 0)),
        }

    except Exception as e:
        logger.exception(f"evaluate_with_llm_low_level failed for {sub_id} L{level}: {e}")
        return {
            "score": 0,
            "reason": f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô LLM: {str(e)}",
            "is_passed": False,
            "P_Plan_Score": 0,
            "D_Do_Score": 0,
            "C_Check_Score": 0,
            "A_Act_Score": 0,
        }
    
# ------------------------
# Summarize (FULL VERSION)
# ------------------------
def create_context_summary_llm(
    context: str, 
    sub_criteria_name: str, 
    level: int, 
    sub_id: str, 
    llm_executor: Any 
) -> Dict[str, Any]:
    """
    ‡πÉ‡∏ä‡πâ LLM ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏£‡∏≤‡∏¢ Level
    ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö String ‡πÅ‡∏•‡∏∞ Object (LLMResult/AIMessage)
    """
    logger = logging.getLogger("AssessmentApp")

    # 0. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ç‡∏≠‡∏á LLM
    if llm_executor is None: 
        logger.error("LLM instance is None. Cannot summarize context.")
        return {
            "summary": "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡∏∏‡∏õ‡πÑ‡∏î‡πâ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏ö‡∏ö LLM ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô",
            "suggestion_for_next_level": "‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ LLM"
        }

    # 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Context ‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏£‡∏ì‡∏µ context ‡πÄ‡∏õ‡πá‡∏ô None
    context_safe = context or ""
    context_limited = context_safe.strip()
    
    if not context_limited or len(context_limited) < 50:
        logger.info(f"Context too short for summarization L{level} {sub_id}. Skipping LLM call.")
        return {
            "summary": "‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏î‡πâ‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡πâ‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡∏µ‡πâ",
            "suggestion_for_next_level": "‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• KM"
        }

    # Cap context ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡πÄ‡∏Å‡∏¥‡∏ô Token Limit (‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 4000 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£)
    context_to_send = context_limited[:4000] 
    next_level = min(level + 1, 5)

    # 2. ‡∏î‡∏∂‡∏á Prompt Template
    from seam_prompts import USER_EVIDENCE_DESCRIPTION_TEMPLATE, SYSTEM_EVIDENCE_DESCRIPTION_PROMPT
    
    try:
        human_prompt = USER_EVIDENCE_DESCRIPTION_TEMPLATE.format(
            sub_id=f"{sub_id} - {sub_criteria_name}",
            level=level,
            next_level=next_level,
            context=context_to_send
        )
    except Exception as e:
        logger.error(f"Error formatting prompt template: {e}")
        return {"summary": "Error formatting prompt", "suggestion_for_next_level": "Check template variables"}

    system_instruction = SYSTEM_EVIDENCE_DESCRIPTION_PROMPT + "\nIMPORTANT: ‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡∏´‡πâ‡∏≤‡∏°‡∏°‡∏µ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏≠‡∏∑‡πà‡∏ô‡∏ô‡∏≠‡∏Å JSON."

    # 3. ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ LLM ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Retries ‡πÅ‡∏•‡∏∞ Object Parsing
    max_retries = 2
    for attempt in range(1, max_retries + 1):
        try:
            logger.info(f"Generating Thai Summary for {sub_id} L{level} (Attempt {attempt})")
            
            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ LLM
            raw_response_obj = llm_executor.generate(
                system=system_instruction, 
                prompts=[human_prompt]
            )

            # --- CRITICAL FIX START: ‡∏î‡∏∂‡∏á String ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å Object ---
            raw_response_str = ""
            if hasattr(raw_response_obj, 'generations'): # LLMResult
                raw_response_str = raw_response_obj.generations[0][0].text
            elif hasattr(raw_response_obj, 'content'):   # AIMessage
                raw_response_str = raw_response_obj.content
            else:
                raw_response_str = str(raw_response_obj)
            # --- CRITICAL FIX END ---

            # 4. Extract ‡πÅ‡∏•‡∏∞ Normalize JSON
            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ _extract_normalized_dict ‡∏à‡∏≤‡∏Å core/json_extractor.py
            parsed = _extract_normalized_dict(raw_response_str)
            
            if parsed and isinstance(parsed, dict) and "summary" in parsed:
                # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• String ‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
                summary_val = str(parsed.get("summary", "")).strip()
                suggestion_val = str(parsed.get("suggestion_for_next_level", "")).strip()
                
                return {
                    "summary": summary_val if summary_val else "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏£‡∏∏‡∏õ",
                    "suggestion_for_next_level": suggestion_val if suggestion_val else "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥"
                }
            
            logger.warning(f"Attempt {attempt}: LLM returned invalid summary format.")
            
        except Exception as e:
            logger.error(f"Attempt {attempt}: create_context_summary_llm failed: {str(e)}")
            time.sleep(1)

    # 5. Fallback ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏´‡∏≤‡∏Å‡∏£‡∏±‡∏ô‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
    return {
        "summary": f"‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö {level} ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ (LLM Parse Error)",
        "suggestion_for_next_level": f"‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á Level {next_level} ‡πÉ‡∏ô‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠ SE-AM"
    }


# =================================================================
# 1. JSON Extractor (‡∏ó‡∏ô‡∏ó‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î)
# =================================================================
def _extract_json_array_for_action_plan(text: Any, logger: logging.Logger) -> List[Dict[str, Any]]:
    """
    ‡∏™‡∏Å‡∏±‡∏î JSON Array ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å Text ‡πÇ‡∏î‡∏¢‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ã‡πà‡∏≠‡∏°‡πÅ‡∏ã‡∏°‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á (Auto-Repair)
    ‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Delimiter Error/Control Characters
    """
    try:
        if not isinstance(text, str):
            text = str(text) if text is not None else ""

        if not text.strip():
            return []

        # 1. ‡∏•‡∏ö Markdown Block (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
        clean_text = re.sub(r'```(?:json)?\s*([\s\S]*?)\s*```', r'\1', text, flags=re.IGNORECASE).strip()

        # 2. ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï‡∏ó‡∏µ‡πà‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á [ ] ‡∏´‡∏£‡∏∑‡∏≠ { }
        start_idx = clean_text.find('[')
        end_idx = clean_text.rfind(']')

        if start_idx == -1:
            # ‡∏Å‡∏£‡∏ì‡∏µ LLM ‡∏™‡πà‡∏á‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô Object ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (Single Phase)
            start_idx = clean_text.find('{')
            end_idx = clean_text.rfind('}')
            if start_idx == -1: return []
            json_candidate = clean_text[start_idx:end_idx + 1]
        else:
            json_candidate = clean_text[start_idx:end_idx + 1]

        # 3. ‡∏•‡πâ‡∏≤‡∏á‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏° (Control Characters) ‡∏ó‡∏µ‡πà‡∏°‡∏±‡∏Å‡∏ó‡∏≥‡πÉ‡∏´‡πâ JSON Parse ‡∏û‡∏±‡∏á
        # ‡∏•‡∏ö ASCII 0-31 ‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô newline, tab, carriage return
        json_candidate = "".join(char for char in json_candidate if ord(char) >= 32 or char in "\n\r\t")

        # 4. ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏¢‡πà‡∏≠‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏° Parse
        def try_parse(content):
            try:
                # json5 ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Trailing Comma ‡πÅ‡∏•‡∏∞ Single Quote
                data = json5.loads(content)
                return data if isinstance(data, list) else [data]
            except Exception:
                return None

        # --- ‡∏•‡∏≠‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà 1: Parse ‡∏õ‡∏Å‡∏ï‡∏¥ ---
        result = try_parse(json_candidate)
        if result: return result

        # --- ‡∏•‡∏≠‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà 2: ‡∏ã‡πà‡∏≠‡∏°‡πÅ‡∏ã‡∏°‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏Ñ‡∏≥‡∏û‡∏π‡∏î (Smart Quotes) ---
        repaired_quotes = json_candidate.replace('‚Äú', '"').replace('‚Äù', '"').replace("'", '"')
        result = try_parse(repaired_quotes)
        if result: return result

        # --- ‡∏•‡∏≠‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà 3: ‡∏Å‡∏£‡∏ì‡∏µ JSON ‡∏ï‡∏±‡∏î‡∏à‡∏ö (Truncated Repair) ---
        # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏õ‡∏¥‡∏î Bracket ‡∏ó‡∏µ‡πà LLM ‡πÄ‡∏à‡∏ô‡πÑ‡∏°‡πà‡∏à‡∏ö
        logger.warning("JSON truncated or malformed, attempting brute-force closure...")
        for suffix in ["]", "}", "}]", "}]}]", "}\n]"]:
            result = try_parse(json_candidate + suffix)
            if result:
                logger.info(f"‚úÖ Auto-repaired JSON success with suffix: {suffix}")
                return result

        # --- ‡∏•‡∏≠‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà 4: ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ ‡πÉ‡∏ä‡πâ Regex ‡∏î‡∏∂‡∏á Object ‡∏ó‡∏µ‡∏•‡∏∞‡∏ï‡∏±‡∏ß (Fallback) ---
        logger.warning("Falling back to Regex Object Extraction...")
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ pattern { ... } ‡∏ó‡∏µ‡πà‡∏î‡∏π‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô object
        objects = re.findall(r'\{(?:[^{}]|(?R))*\}', json_candidate) # ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ regex module ‡∏û‡∏¥‡πÄ‡∏®‡∏© ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢:
        if not objects:
            objects = re.findall(r'\{[\s\S]*?\}', json_candidate)
            
        fallback_results = []
        for obj_str in objects:
            try:
                obj_data = json5.loads(obj_str)
                if isinstance(obj_data, dict):
                    fallback_results.append(obj_data)
            except:
                continue
        
        if fallback_results:
            logger.info(f"‚úÖ Recovered {len(fallback_results)} objects via regex")
            return fallback_results

        logger.error(f"Failed to parse JSON. Snippet: {json_candidate[:200]}...")
        return []

    except Exception as e:
        logger.error(f"Extraction logic failed: {str(e)}", exc_info=True)
        return []

# =================================================================
# 2. Key Normalizer (‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö schema ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î)
# =================================================================

def action_plan_normalize_keys(obj: Any) -> Any:
    """
    ‡πÅ‡∏õ‡∏•‡∏á key ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö schema ‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Data Type Enforcement)
    ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞ failed_level ‡πÅ‡∏•‡∏∞ Step ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô Integer 100%
    """
    if isinstance(obj, list):
        return [action_plan_normalize_keys(i) for i in obj]
    
    if isinstance(obj, dict):
        field_mapping = {
            # Phase & Action level
            'phase': 'phase', 'Phase': 'phase',
            'goal': 'goal', 'Goal': 'goal',
            'actions': 'actions', 'Actions': 'actions',
            
            'statement_id': 'statement_id', 'Statement_ID': 'statement_id',
            'statement id': 'statement_id', 'title': 'statement_id', 'id': 'statement_id',
            
            'failed_level': 'failed_level', 'Failed_Level': 'failed_level',
            'failed level': 'failed_level', 'level': 'failed_level',
            
            'recommendation': 'recommendation', 'Recommendation': 'recommendation',
            'recommend': 'recommendation',
            
            'target_evidence_type': 'target_evidence_type', 'Target_Evidence_Type': 'target_evidence_type',
            'evidence_type': 'target_evidence_type', 'evidence': 'target_evidence_type',
            
            'key_metric': 'key_metric', 'Key_Metric': 'key_metric',
            'metric': 'key_metric',
            
            'steps': 'steps', 'Steps': 'steps',
            
            # StepDetail (Capitalized per schema)
            'step': 'Step', 'Step': 'Step',
            'description': 'Description', 'Description': 'Description', 'desc': 'Description',
            'responsible': 'Responsible', 'Responsible': 'Responsible', 'owner': 'Responsible',
            'tools_templates': 'Tools_Templates', 'Tools_Templates': 'Tools_Templates', 'tools': 'Tools_Templates',
            'verification_outcome': 'Verification_Outcome', 'Verification_Outcome': 'Verification_Outcome', 'outcome': 'Verification_Outcome',
        }
        
        new_obj = {}
        for k, v in obj.items():
            # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î Key
            k_clean = k.lower().replace('_', ' ').replace('-', ' ').strip()
            k_no_space = k_clean.replace(' ', '')
            target_key = field_mapping.get(k_clean) or field_mapping.get(k_no_space) or k
            
            # --- [CRITICAL FIX] ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç ---
            if target_key in ['failed_level', 'Step']:
                try:
                    if isinstance(v, str):
                        # ‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠‡πÉ‡∏ô string ‡πÄ‡∏ä‡πà‡∏ô "Level 3" -> 3
                        nums = re.findall(r'\d+', v)
                        v = int(nums[0]) if nums else 0
                    else:
                        v = int(v) if v is not None else 0
                except (ValueError, IndexError):
                    v = 0 # Fallback default
            
            new_obj[target_key] = action_plan_normalize_keys(v)
        
        return new_obj
    
    return obj


# =================================================================
# 3. Main Function: create_structured_action_plan
# =================================================================
def create_structured_action_plan(
    recommendation_statements: List[Dict[str, Any]],
    sub_id: str,
    sub_criteria_name: str,
    target_level: int,
    llm_executor: Any,
    logger: logging.Logger,
    max_retries: int = 3
) -> List[Dict[str, Any]]:
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á Action Plan ‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô Pydantic validation 100%
    ‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏à‡∏≤‡∏Å config.global_vars ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ï‡πá‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö:
    - ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Phase ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
    - ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Steps ‡∏ï‡πà‡∏≠ Action
    - ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß Step (‡∏Ñ‡∏≥)
    - ‡∏†‡∏≤‡∏©‡∏≤
    """
    from config import global_vars as gv

    # --- Sustain Mode (‡πÑ‡∏°‡πà‡∏°‡∏µ Gap) ---
    if not recommendation_statements:
        logger.info(f"[Sustain Mode] No gaps found ‚Üí Level {target_level}")
        return [{
            "phase": f"Level {target_level} Sustain & Innovation",
            "goal": f"‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏¢‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö {sub_criteria_name} ‡∏™‡∏π‡πà‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏•‡∏¥‡∏®‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á",
            "actions": [{
                "statement_id": f"SUSTAIN_L{target_level}",
                "failed_level": target_level,
                "recommendation": "‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ó‡∏≥ Benchmarking ‡∏Å‡∏±‡∏ö Best Practice ‡∏™‡∏≤‡∏Å‡∏•",
                "target_evidence_type": "Internal Audit Report / External Benchmarking Report",
                "key_metric": f"Maintain Maturity ‚â• Level {target_level}",
                "steps": [{
                    "Step": "1",
                    "Description": "‡∏ó‡∏ö‡∏ó‡∏ß‡∏ô‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏•‡∏∞ KPI ‡∏£‡∏≤‡∏¢‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™ ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏ï‡∏≤‡∏° PDCA",
                    "Responsible": "KM Committee / Top Management",
                    "Tools_Templates": "PDCA Dashboard / Quarterly Review Template",
                    "Verification_Outcome": "Quarterly KM Review Report"
                }, {
                    "Step": "2",
                    "Description": "‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏Ñ‡πâ‡∏ô‡∏Ñ‡∏ß‡πâ‡∏≤ Best Practices ‡∏à‡∏≤‡∏Å‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£‡∏ä‡∏±‡πâ‡∏ô‡∏ô‡∏≥‡∏ó‡∏±‡πâ‡∏á‡πÉ‡∏ô‡πÅ‡∏•‡∏∞‡∏ï‡πà‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®",
                    "Responsible": "KM Team",
                    "Tools_Templates": "Benchmarking Framework",
                    "Verification_Outcome": "Benchmarking Study Report"
                }]
            }]
        }]

    # --- ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Gap ‡πÅ‡∏•‡∏∞‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Phase ‡∏ï‡∏≤‡∏°‡∏£‡∏∞‡∏î‡∏±‡∏ö ---
    max_failed_level = max([s.get('level', 0) for s in recommendation_statements] or [1])

    if max_failed_level >= 5:
        advice_focus = "Innovation, External Benchmarking, Digital Transformation ‡πÅ‡∏•‡∏∞ Continuous Improvement"
    elif max_failed_level >= 3:
        advice_focus = "Standardization, KPI Monitoring, PDCA Cycle ‡πÅ‡∏•‡∏∞ Evidence Strengthening"
    else:
        advice_focus = "Policy Establishment, Resource Allocation, Communication ‡πÅ‡∏•‡∏∞ Basic Training"

    stmt_blocks = [
        f"- [Level {s.get('level')}] {s.get('statement')} (Gap: {s.get('reason')})"
        for s in recommendation_statements
    ]

    # --- ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° JSON Schema ---
    try:
        from core.action_plan_schema import get_clean_action_plan_schema
        schema_json = json.dumps(get_clean_action_plan_schema(), ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"Schema load failed: {e}")
        return []

    # --- ‡∏™‡∏£‡πâ‡∏≤‡∏á Prompt ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡πà‡∏á config ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö LLM ---
    human_prompt = ACTION_PLAN_PROMPT.format(
        sub_id=sub_id,
        sub_criteria_name=sub_criteria_name,
        target_level=target_level,
        advice_focus=advice_focus,
        recommendation_statements_list="\n".join(stmt_blocks),
        json_schema=schema_json,
        max_phases=gv.MAX_ACTION_PLAN_PHASES,
        max_steps=gv.MAX_STEPS_PER_ACTION,
        max_words_per_step=gv.ACTION_PLAN_STEP_MAX_WORDS,
        language="‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢" if gv.ACTION_PLAN_LANGUAGE == "th" else "English"
    )

    # --- Retry Loop ---
    for attempt in range(1, max_retries + 1):
        try:
            logger.info(f"Action Plan Generation | Attempt {attempt}/{gv.OLLAMA_MAX_RETRIES}")
            response = llm_executor.generate(
                system=SYSTEM_ACTION_PLAN_PROMPT,
                prompts=[human_prompt],
                temperature=gv.LLM_TEMPERATURE,
                max_tokens=3000
            )

            raw_text = ""
            if hasattr(response, 'generations') and response.generations:
                raw_text = response.generations[0][0].text
            elif hasattr(response, 'text'):
                raw_text = response.text
            else:
                raw_text = str(response)

            if attempt == 1:
                logger.debug(f"Raw Response (first 800 chars):\n{raw_text[:800]}")

            items = _extract_json_array_for_action_plan(raw_text, logger)
            if not items:
                logger.warning(f"Attempt {attempt}: No JSON extracted")
                continue

            validated_output = []
            for idx, entry in enumerate(items):
                try:
                    clean_entry = action_plan_normalize_keys(entry)
                    validated = ActionPlanActions.model_validate(clean_entry)
                    validated_output.append(validated.model_dump(by_alias=False))
                except Exception as ve:
                    logger.error(f"Entry {idx} validation failed: {ve}")
                    if idx < 3:
                        logger.debug(f"Failed Entry:\n{json.dumps(clean_entry, ensure_ascii=False, indent=2)[:1500]}")

            if validated_output:
                logger.info(f"‚úÖ Success: {len(validated_output)} valid phase(s) on attempt {attempt}")
                return validated_output

        except Exception as e:
            logger.error(f"Attempt {attempt} error: {e}", exc_info=True)

    # --- Emergency Fallback ---
    logger.warning("All attempts failed ‚Üí returning emergency fallback plan")
    return [{
        "phase": "Phase 1: Immediate Action Required",
        "goal": f"‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏´‡∏•‡∏±‡∏Å‡πÉ‡∏ô {sub_criteria_name} ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏£‡πà‡∏á‡∏î‡πà‡∏ß‡∏ô",
        "actions": [{
            "statement_id": f"GAP_L{max_failed_level}",
            "failed_level": max_failed_level,
            "recommendation": f"‡πÅ‡∏ï‡πà‡∏á‡∏ï‡∏±‡πâ‡∏á‡∏ó‡∏µ‡∏°‡∏á‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏ó‡∏≥‡πÅ‡∏ú‡∏ô‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡∏ö‡∏ö ‡πÇ‡∏î‡∏¢‡πÄ‡∏ô‡πâ‡∏ô {advice_focus}",
            "target_evidence_type": "‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÅ‡∏ï‡πà‡∏á‡∏ï‡∏±‡πâ‡∏á / ‡πÅ‡∏ú‡∏ô‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£",
            "key_metric": "‡∏à‡∏±‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏ó‡∏µ‡∏°‡πÅ‡∏•‡∏∞‡∏≠‡∏ô‡∏∏‡∏°‡∏±‡∏ï‡∏¥‡πÅ‡∏ú‡∏ô‡∏†‡∏≤‡∏¢‡πÉ‡∏ô 3 ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô",
            "steps": [
                {"Step": "1", "Description": "‡πÅ‡∏ï‡πà‡∏á‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡∏ì‡∏∞‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏û‡∏±‡∏í‡∏ô‡∏≤ KM ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏ô‡∏µ‡πâ", "Responsible": "‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î", "Tools_Templates": "‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÅ‡∏ï‡πà‡∏á‡∏ï‡∏±‡πâ‡∏á", "Verification_Outcome": "‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÅ‡∏ï‡πà‡∏á‡∏ï‡∏±‡πâ‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£"},
                {"Step": "2", "Description": "‡∏à‡∏±‡∏î‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏° Kick-off ‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Gap ‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô", "Responsible": "‡∏´‡∏±‡∏ß‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡∏° KM", "Tools_Templates": "Gap Analysis Template", "Verification_Outcome": "‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°"}
            ]
        }]
    }]