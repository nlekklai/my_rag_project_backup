"""
llm_data_utils.py
Robust LLM + RAG utilities for SEAM assessment (CLEAN FINAL VERSION)
"""

import logging

logger = logging.getLogger(__name__)
if not logger.handlers:
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")


import time
import json
import hashlib
import uuid
import re
from datetime import datetime
from typing import List, Dict, Any, Optional, Union, Callable, TypeVar, Set
import json5
from utils.enabler_keyword_map import ENABLER_KEYWORD_MAP, DEFAULT_KEYWORDS
from langchain.retrievers import EnsembleRetriever
from langchain_core.documents import Document
from langchain_community.retrievers import BM25Retriever # FIX: Import BM25 ‡∏à‡∏≤‡∏Å community
import os
from pydantic import ValidationError

# --- ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° JSON Schema ---
try:
    from core.action_plan_schema import get_clean_action_plan_schema
    schema_json = json.dumps(get_clean_action_plan_schema(), ensure_ascii=False, indent=2)
except Exception as e:
    logger.error(f"Schema load failed: {e}")


# Optional: regex ‡πÅ‡∏ó‡∏ô re (‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤) ‚Äî ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡πá‡πÉ‡∏ä‡πâ re ‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤
try:
    import regex as re  # type: ignore
except ImportError:
    pass  # ‡πÉ‡∏ä‡πâ re ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ

# ===================================================================
# 1. Core Configuration (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô)
# ===================================================================
from config.global_vars import (
    INITIAL_TOP_K,
    MAX_EVAL_CONTEXT_LENGTH,
    DEFAULT_EMBED_BATCH_SIZE,
    RERANK_THRESHOLD,
    ANALYSIS_FINAL_K
)

# ===================================================================
# 2. Critical Utilities (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏à‡∏£‡∏¥‡∏á ‚Äî ‡πÑ‡∏°‡πà‡∏°‡∏µ fallback)
# ===================================================================
# üéØ FIX 1: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô Import ‡∏à‡∏≤‡∏Å _get_collection_name ‡πÑ‡∏õ‡πÄ‡∏õ‡πá‡∏ô get_doc_type_collection_key
from core.vectorstore import get_hf_embeddings
from utils.path_utils import (
    get_doc_type_collection_key, 
    _n, get_mapping_file_path, # <--- ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà‡∏à‡∏≤‡∏Å utils/path_utils
    get_vectorstore_collection_path,
    get_vectorstore_tenant_root_path,
    get_rubric_file_path
)
from core.json_extractor import (
    _robust_extract_json,
    _normalize_keys,
    _safe_int_parse,
    _extract_normalized_dict
)

# ===================================================================
# 3. Project Modules (‡∏ñ‡πâ‡∏≤‡∏´‡∏≤‡∏¢ ‚Üí ‡∏õ‡∏•‡πà‡∏≠‡∏¢‡πÉ‡∏´‡πâ error ‡∏ä‡∏±‡∏î ‡πÜ ‡πÑ‡∏õ‡πÄ‡∏•‡∏¢ ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏á‡∏µ‡∏¢‡∏ö)
# ===================================================================
from core.seam_prompts import (
    SYSTEM_ASSESSMENT_PROMPT,
    USER_ASSESSMENT_PROMPT,
    SYSTEM_ACTION_PLAN_PROMPT,
    ACTION_PLAN_PROMPT,
    SYSTEM_EVIDENCE_DESCRIPTION_PROMPT,
    EVIDENCE_DESCRIPTION_PROMPT,
    SYSTEM_LOW_LEVEL_PROMPT,
    USER_LOW_LEVEL_PROMPT,
    USER_LOW_LEVEL_PROMPT_TEMPLATE,
    USER_EVIDENCE_DESCRIPTION_TEMPLATE,
    EXCELLENCE_ADVICE_PROMPT, 
    SYSTEM_EXCELLENCE_PROMPT,
    SYSTEM_QUALITY_PROMPT,
    QUALITY_REFINEMENT_PROMPT
)

from core.vectorstore import VectorStoreManager, get_global_reranker, ChromaRetriever
from core.assessment_schema import CombinedAssessment, EvidenceSummary
from core.action_plan_schema import ActionPlanActions, ActionPlanResult

try:
    from core.assessment_schema import StatementAssessment
except ImportError:
    from pydantic import BaseModel
    class StatementAssessment(BaseModel):
        score: int = 0
        reason: str = ""

from langchain_core.documents import Document as LcDocument
# ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î Import ‡πÉ‡∏ô routers/llm_router.py

# ===================================================================
# 4. Constants
# ===================================================================
LOW_LEVEL_K: int = 3
_MOCK_FLAG = False
_MAX_LLM_RETRIES = 3

def set_mock_control_mode(enable: bool):
    global _MOCK_FLAG
    _MOCK_FLAG = bool(enable)
    logger.info(f"Mock control mode: {_MOCK_FLAG}")

def _create_where_filter(
    stable_doc_ids: Optional[Union[Set[str], List[str]]] = None,
    subject: Optional[str] = None,
    sub_topic: Optional[str] = None,
    year: Optional[Union[int, str]] = None,
    enabler: Optional[str] = None,
    tenant: Optional[str] = None,
    **kwargs
) -> Dict[str, Any]:
    """
    [PRODUCTION VERSION] ‡∏™‡∏£‡πâ‡∏≤‡∏á Filter ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ChromaDB ‡∏ó‡∏µ‡πà‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥
    ‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á Data Type Mismatch (Int/Str) ‡πÅ‡∏•‡∏∞‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Multi-tenant
    """
    filters: List[Dict[str, Any]] = []

    # --- 1. ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Stable Doc IDs (‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î) ---
    if stable_doc_ids:
        ids_list = [str(i).strip() for i in (stable_doc_ids if isinstance(stable_doc_ids, (list, set)) else [stable_doc_ids]) if i]
        if ids_list:
            if len(ids_list) == 1:
                # ‡∏Å‡∏£‡∏ì‡∏µ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ $and
                return {"stable_doc_uuid": ids_list[0]}
            else:
                return {"stable_doc_uuid": {"$in": ids_list}}

    # --- 2. ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Year (‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ Local Mac ‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠) ---
    if year is not None:
        year_str = str(year).strip()
        if year_str and year_str.lower() != "none":
            # üéØ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: ‡∏™‡πà‡∏á‡∏ó‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö Int ‡πÅ‡∏•‡∏∞ Str ‡∏´‡∏£‡∏∑‡∏≠‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô Int ‡∏ï‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà Peek ‡πÄ‡∏à‡∏≠‡πÉ‡∏ô Mac
            try:
                # ‡∏ñ‡πâ‡∏≤‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ‡∏™‡πà‡∏á‡πÅ‡∏ö‡∏ö Integer (ChromaDB Local ‡∏°‡∏±‡∏Å‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏õ‡πá‡∏ô Int)
                val_year = int(year_str)
                filters.append({"year": val_year})
            except (ValueError, TypeError):
                # ‡∏ñ‡πâ‡∏≤‡πÅ‡∏õ‡∏•‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á‡πÜ ‡πÉ‡∏´‡πâ‡∏™‡πà‡∏á‡πÅ‡∏ö‡∏ö String
                filters.append({"year": year_str})
    
    # --- 3. ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Tenant (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ç‡πâ‡∏≤‡∏°‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó) ---
    effective_tenant = tenant or kwargs.get("tenant")
    if effective_tenant and str(effective_tenant).strip():
        filters.append({"tenant": str(effective_tenant).strip()})

    # --- 4. ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Enabler (KM, IM, etc.) ---
    if enabler and str(enabler).strip():
        filters.append({"enabler": enabler.strip().upper()})

    # --- 5. ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Subject (‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÄ‡∏Å‡∏ì‡∏ë‡πå) ---
    if subject and str(subject).strip():
        filters.append({"subject": str(subject).strip()})

    # --- 6. ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Sub Topic ---
    if sub_topic and str(sub_topic).strip():
        filters.append({"sub_topic": str(sub_topic).strip()})

    # --- ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Filter ---
    if not filters:
        return {}

    # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢‡∏≠‡∏±‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏î‡πâ‡∏ß‡∏¢ $and
    return filters[0] if len(filters) == 1 else {"$and": filters}


def retrieve_context_for_endpoint(
    vectorstore_manager,
    query: str = "",
    tenant: Optional[str] = None,
    year: Optional[Union[int, str]] = None,
    stable_doc_ids: Optional[Set[str]] = None,
    doc_type: Optional[str] = None,
    enabler: Optional[str] = None,
    subject: Optional[str] = None,
    sub_topic: Optional[str] = None,
    k_to_retrieve: int = 150, 
    k_to_rerank: int = 30,    
    strict_filter: bool = False,
    **kwargs
) -> Dict[str, Any]:
    """
    [ULTIMATE REVISED] Retrieval for Search Endpoint
    - ‡∏¢‡∏∂‡∏î Metadata ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢ (No Pydantic Error)
    - ‡πÉ‡∏ä‡πâ Deterministic MD5 Hash ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Deduplication
    - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Anchor Chunks ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á Context ‡∏ó‡∏µ‡πà‡∏°‡∏±‡πà‡∏ô‡∏Ñ‡∏á
    """
    start_time = time.time()
    vsm = vectorstore_manager

    # 1. Resolve collection
    clean_doc_type = str(doc_type or "document").strip().lower()
    collection_name = get_doc_type_collection_key(doc_type=clean_doc_type, enabler=enabler)
    
    chroma = vsm._load_chroma_instance(collection_name)
    if not chroma:
        logger.error(f"‚ùå Collection {collection_name} not found.")
        return {"top_evidences": [], "aggregated_context": "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", "retrieval_time": 0}

    # 2. Create where_filter
    where_filter = _create_where_filter(
        stable_doc_ids=list(stable_doc_ids) if stable_doc_ids else None, 
        subject=subject, 
        sub_topic=sub_topic, 
        year=year
    )

    # Dictionary ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Deduplication (‡πÉ‡∏ä‡πâ MD5 ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏õ‡πá‡∏ô Key)
    unique_map: Dict[str, LcDocument] = {}

    # =====================================================
    # ‚öì 2.1 ANCHOR RETRIEVAL (‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå)
    # =====================================================
    if stable_doc_ids:
        # ‡∏î‡∏∂‡∏á‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏£‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        anchors = chroma.get(where=where_filter, limit=10) 
        if anchors and anchors.get('documents'):
            for i in range(len(anchors['documents'])):
                content = anchors['documents'][i]
                md = anchors['metadatas'][i]
                
                # Deterministic MD5 Hash
                c_hash = hashlib.md5(content.encode()).hexdigest()
                uid = md.get("chunk_uuid") or f"anchor-{c_hash}"
                
                if uid not in unique_map:
                    # ‡∏â‡∏µ‡∏î‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÉ‡∏´‡πâ Anchor ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ï‡∏¥‡∏î‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡∏ï‡πâ‡∏ô‡πÜ ‡∏´‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á
                    md["score"] = 0.5 
                    md["is_anchor"] = True
                    unique_map[uid] = LcDocument(page_content=content, metadata=md)

    # =====================================================
    # üîç 2.2 SEMANTIC SEARCH
    # =====================================================
    search_query = query if (query and query != "*" and len(query) > 2) else ""
    
    # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö‡∏à‡∏≤‡∏Å Vector DB
    if search_query:
        docs = chroma.similarity_search(search_query, k=k_to_retrieve, filter=where_filter)
    else:
        # Fallback ‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏°‡πà‡∏°‡∏µ Query ‡πÉ‡∏´‡πâ‡∏î‡∏∂‡∏á‡πÅ‡∏ö‡∏ö‡∏Å‡∏ß‡∏≤‡∏î
        docs = chroma.similarity_search("*", k=k_to_retrieve, filter=where_filter)

    for d in docs:
        c_hash = hashlib.md5(d.page_content.encode()).hexdigest()
        md = d.metadata or {}
        uid = md.get("chunk_uuid") or c_hash
        if uid not in unique_map:
            unique_map[uid] = d

    candidates = list(unique_map.values())

    # =====================================================
    # üöÄ 3. BATCH RERANKING
    # =====================================================
    final_scored_docs = []
    reranker = getattr(vsm, "reranker", None)
    
    if reranker and candidates and search_query:
        try:
            batch_size = 100 
            logger.info(f"üöÄ Reranking {len(candidates)} candidates in batches...")
            for i in range(0, len(candidates), batch_size):
                batch = candidates[i : i + batch_size]
                # ‡πÉ‡∏ä‡πâ Reranker ‡∏Ç‡∏≠‡∏á LangChain (Compressor)
                scored_batch = reranker.compress_documents(documents=batch, query=search_query)
                final_scored_docs.extend(scored_batch)
        except Exception as e:
            logger.error(f"‚ö†Ô∏è Rerank failed: {e}")
            final_scored_docs = candidates
    else:
        final_scored_docs = candidates

    # =====================================================
    # 4. SORTING & SCORE INJECTION (‡∏´‡∏±‡∏ß‡πÉ‡∏à‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ 0.0000)
    # =====================================================
    def get_score(d) -> float:
        m = d.metadata or {}
        # ‡∏î‡∏∂‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏à‡∏≤‡∏Å‡∏ó‡∏∂‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ
        s = m.get("relevance_score") or m.get("score") or m.get("rerank_score") or 0.0
        try: return float(s)
        except: return 0.0

    final_scored_docs.sort(key=get_score, reverse=True)

    # =====================================================
    # 5. RESPONSE BUILD
    # =====================================================
    top_evidences = []
    aggregated_parts = []
    
    for doc in final_scored_docs[:k_to_rerank]:
        md = doc.metadata or {}
        text = doc.page_content.strip()
        score = get_score(doc)
        
        # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏•‡∏∞‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏≤‡πÉ‡∏´‡πâ Robust
        p_val = md.get("page") or md.get("page_label") or md.get("page_number") or "N/A"
        source_name = md.get('source') or md.get('source_filename') or md.get('file_name') or 'Unknown'
        s_uuid = md.get("stable_doc_uuid") or md.get("doc_id") or ""
        
        # SYNC SCORE ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤ Metadata ‡∏ó‡∏∏‡∏Å‡∏ï‡∏±‡∏ß
        md["score"] = score
        md["relevance_score"] = score
        
        evidence_item = {
            "doc_id": str(s_uuid),
            "chunk_uuid": str(md.get("chunk_uuid") or ""),
            "source": source_name,
            "text": text,
            "page": str(p_val),
            "score": score,
            "pdca_tag": md.get("pdca_tag", "Other"),
            "metadata": md
        }
        
        top_evidences.append(evidence_item)
        aggregated_parts.append(f"[‡πÑ‡∏ü‡∏•‡πå: {source_name}, ‡∏´‡∏ô‡πâ‡∏≤: {p_val}] {text}")

    retrieval_time = round(time.time() - start_time, 3)
    logger.info(f"üèÅ Finished: {len(top_evidences)} chunks in {retrieval_time}s")

    return {
        "top_evidences": top_evidences,
        "aggregated_context": "\n\n---\n\n".join(aggregated_parts) if aggregated_parts else "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•",
        "retrieval_time": retrieval_time,
        "total_candidates": len(candidates)
    }

# ------------------------
# Retrieval: retrieve_context_with_filter (Revised)
# ------------------------
def retrieve_context_with_filter(
    query: Union[str, List[str]],
    doc_type: str,
    tenant: Optional[str] = None,
    year: Optional[Union[int, str]] = None,
    enabler: Optional[str] = None,
    subject: Optional[str] = None,
    vectorstore_manager: Optional[Any] = None,
    mapped_uuids: Optional[List[str]] = None,
    stable_doc_ids: Optional[List[str]] = None,
    priority_docs_input: Optional[List[Any]] = None,
    sequential_chunk_uuids: Optional[List[str]] = None,
    sub_id: Optional[str] = None,
    level: Optional[int] = None,
    get_previous_level_docs: Optional[Callable[[int, str], List[Any]]] = None,
    top_k: int = 150, 
) -> Dict[str, Any]:
    """
    [FIXED VERSION] ‡∏¢‡∏∂‡∏î‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ (No Error) 
    ‡πÅ‡∏ï‡πà‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏¢‡∏±‡∏î‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏•‡∏á Metadata ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Rerank 0.0000
    """
    start_time = time.time()
    manager = vectorstore_manager
    queries_to_run = [query] if isinstance(query, str) else list(query or [""])
    
    # 1. Resolve Collection & Filter
    # ‡πÉ‡∏ä‡πâ helper ‡∏à‡∏≤‡∏Å utils ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Ç‡∏≠‡∏á‡∏ä‡∏∑‡πà‡∏≠ collection
    collection_name = get_doc_type_collection_key(doc_type, enabler or "KM")
    
    target_ids = set()
    if stable_doc_ids: target_ids.update([str(i) for i in stable_doc_ids])
    if mapped_uuids: target_ids.update([str(i) for i in mapped_uuids])
    if sequential_chunk_uuids: target_ids.update([str(i) for i in sequential_chunk_uuids])
    
    where_filter = _create_where_filter(
        stable_doc_ids=list(target_ids) if target_ids else None,
        subject=subject,
        year=year
    )

    # 2. Collect Chunks
    all_source_chunks = []

    # 2.1 Priority Docs
    if priority_docs_input:
        for doc in priority_docs_input:
            if not doc: continue
            if isinstance(doc, dict):
                pc = doc.get('page_content') or doc.get('text') or ''
                meta = doc.get('metadata') or {}
                if pc.strip():
                    all_source_chunks.append(LcDocument(page_content=pc, metadata=meta))
            elif hasattr(doc, 'page_content'):
                all_source_chunks.append(doc)

    # 2.2 Vector Retrieval
    try:
        full_retriever = manager.get_retriever(collection_name=collection_name)
        base_retriever = getattr(full_retriever, "base_retriever", full_retriever)
        
        search_kwargs = {"k": top_k}
        if where_filter: 
            search_kwargs["where"] = where_filter

        for q in queries_to_run:
            if not q: continue
            docs = base_retriever.invoke(q, config={"configurable": {"search_kwargs": search_kwargs}})
            if docs: all_source_chunks.extend(docs)
    except Exception as e:
        logger.error(f"Retrieval error: {e}")

    # 3. Deduplicate (‡πÉ‡∏ä‡πâ Hash ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥)
    unique_map: Dict[str, LcDocument] = {}
    for doc in all_source_chunks:
        if not doc or not doc.page_content.strip(): continue
        md = doc.metadata or {}
        # ‡πÉ‡∏ä‡πâ hashlib ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ ID ‡∏ó‡∏µ‡πà‡∏Ñ‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô‡∏ó‡∏∏‡∏Å‡∏£‡∏≠‡∏ö
        c_hash = hashlib.md5(doc.page_content.encode()).hexdigest()
        uid = str(md.get("chunk_uuid") or f"{md.get('stable_doc_uuid', 'unknown')}-{c_hash}")
        
        if uid not in unique_map:
            unique_map[uid] = doc

    candidates = list(unique_map.values())

    # 4. Batch Reranking
    final_scored_docs = []
    batch_size = 100 
    reranker = getattr(manager, "reranker", None)

    if reranker and candidates:
        main_query = queries_to_run[0]
        for i in range(0, len(candidates), batch_size):
            batch = candidates[i : i + batch_size]
            try:
                # üìå ‡∏Å‡∏∏‡∏ç‡πÅ‡∏à‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: Reranker ‡∏à‡∏∞‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÉ‡∏ô metadata
                scored_batch = reranker.compress_documents(batch, main_query)
                final_scored_docs.extend(scored_batch)
            except Exception as e:
                logger.error(f"Rerank Error: {e}")
                final_scored_docs.extend(batch)
    else:
        final_scored_docs = candidates

    # 5. Sorting & Score Injection (‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ 0.0000)
    def get_score(d) -> float:
        m = d.metadata or {}
        # ‡πÑ‡∏•‡πà‡∏´‡∏≤‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ
        s = m.get("relevance_score") or m.get("score") or m.get("rerank_score") or 0.0
        try: return float(s)
        except: return 0.0

    final_scored_docs.sort(key=get_score, reverse=True)

    # 6. Final Formatting (‡∏Ñ‡∏±‡∏î‡πÄ‡∏•‡∏∑‡∏≠‡∏Å K ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å)
    top_evidences = []
    aggregated_parts = []
    final_k = ANALYSIS_FINAL_K # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å config

    for doc in final_scored_docs[:final_k]:
        score = get_score(doc)
        # ‡∏Å‡∏£‡∏≠‡∏á Threshold ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡∏±‡πâ‡∏á‡πÑ‡∏ß‡πâ‡πÉ‡∏ô .env
        if score < RERANK_THRESHOLD and RERANK_THRESHOLD > 0:
            continue

        md = doc.metadata or {}
        text = doc.page_content.strip()
        
        # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö ingest.py
        page = str(md.get("page_label") or md.get("page_number") or md.get("page") or "N/A")
        source = md.get("source") or md.get("source_filename") or "Unknown"
        pdca = md.get("pdca_tag", "Other")

        # üéØ SYNC SCORE ‡πÄ‡∏Ç‡πâ‡∏≤ Metadata (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Loop ‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡πÅ‡∏•‡πâ‡∏ß‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ 0)
        md["score"] = score
        md["relevance_score"] = score
        md["rerank_score"] = score

        top_evidences.append({
            "doc_id": str(md.get("stable_doc_uuid") or md.get("doc_id") or ""),
            "chunk_uuid": str(md.get("chunk_uuid") or ""),
            "source": source,
            "text": text,
            "page": page,
            "pdca_tag": pdca,
            "score": score,
            "metadata": md
        })
        aggregated_parts.append(f"[{pdca}] [‡πÑ‡∏ü‡∏•‡πå: {source} ‡∏´‡∏ô‡πâ‡∏≤: {page}] {text}")

    return {
        "top_evidences": top_evidences,
        "aggregated_context": "\n\n---\n\n".join(aggregated_parts) if aggregated_parts else "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô",
        "retrieval_time": round(time.time() - start_time, 3),
        "total_candidates": len(candidates)
    }

# =====================================================================
# üõ† Helper: check_rubric_readiness (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏á‡∏µ‡∏¢‡∏ö‡∏•‡∏á)
# =====================================================================
def is_rubric_ready(tenant: str) -> bool:
    """ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏Ç‡∏≠‡∏á seam collection ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏û‡πà‡∏ô Warning ‡∏Å‡∏ß‡∏ô‡πÉ‡∏à """
    if not tenant:
        return False
    
    tenant_vs_root = get_vectorstore_tenant_root_path(tenant)
    chroma_path = os.path.join(tenant_vs_root, "seam")
    
    # ‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ True/False ‡πÄ‡∏á‡∏µ‡∏¢‡∏ö‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏î‡∏∂‡∏á Rubric ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    return os.path.exists(chroma_path)


# =====================================================================
# üöÄ Ultimate Version: retrieve_context_with_rubric (FIXED & REVISED)
# =====================================================================
def retrieve_context_with_rubric(
    vectorstore_manager,
    query: str,
    doc_type: str,
    enabler: Optional[str] = None,
    stable_doc_ids: Optional[Set[str]] = None,
    tenant: Optional[str] = None,
    year: Optional[Union[int, str]] = None,
    subject: Optional[str] = None,
    rubric_vectorstore_name: str = "seam", 
    top_k: int = 150,         # üöÄ ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏î‡∏∂‡∏á‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Reranker ‡∏°‡∏µ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô
    rubric_top_k: int = 15,  
    strict_filter: bool = True,
    k_to_rerank: int = 30    
) -> Dict[str, Any]:
    """
    [REVISED VERSION] ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Rubric + Evidence Retrieval ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏£‡∏∞‡∏ö‡∏ö Batch Reranking 
    ‡πÅ‡∏•‡∏∞ Content-Based Deduplication ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô Level 5 ‡∏´‡∏≤‡∏¢
    """
    start_time = time.time()
    vsm = vectorstore_manager

    # --- 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏™‡∏•‡∏±‡∏ö Collection ---
    if hasattr(vsm, 'doc_type') and vsm.doc_type != doc_type:
        logger.info(f"üîÑ Switching VSM doc_type to: {doc_type}")
        vsm.close()
        vsm.__init__(tenant=tenant, year=year, doc_type=doc_type, enabler=enabler)

    evidence_collection = get_doc_type_collection_key(doc_type, enabler or "KM")
    
    rubric_results = []
    # ‡πÉ‡∏ä‡πâ Dict ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥ Deduplication ‡∏î‡πâ‡∏ß‡∏¢ Content Hash ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ã‡πâ‡∏≥‡πÅ‡∏ï‡πà ID ‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô
    unique_evidence_map: Dict[str, LcDocument] = {}

    # --- 2. ‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á Rubrics (‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô) ---
    try:
        rubric_chroma = vsm._load_chroma_instance(rubric_vectorstore_name)
        if rubric_chroma:
            rubric_query = f"‡πÄ‡∏Å‡∏ì‡∏ë‡πå SE-AM {enabler} {subject or ''}: {query}"
            r_docs = rubric_chroma.similarity_search(rubric_query, k=rubric_top_k)
            for rd in r_docs:
                rubric_results.append({
                    "text": rd.page_content, 
                    "metadata": rd.metadata, 
                    "is_rubric": True
                })
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Rubric Retrieval Error: {e}")

    # --- 3. ‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á Evidence (‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô) ---
    try:
        evidence_chroma = vsm._load_chroma_instance(evidence_collection)
        if not evidence_chroma:
            return {"top_evidences": [], "rubric_context": rubric_results, "retrieval_time": 0}

        where_filter = None
        if stable_doc_ids:
            ids_list = [str(i).strip().lower() for i in stable_doc_ids if i]
            where_filter = {"stable_doc_uuid": ids_list[0]} if len(ids_list) == 1 else {"stable_doc_uuid": {"$in": ids_list}}
            
            # ‚öì 3.1 Fetch Anchor Chunks (‡∏™‡πà‡∏ß‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå ‡πÄ‡∏ä‡πà‡∏ô ‡∏´‡∏ô‡πâ‡∏≤ 1-5)
            anchors = evidence_chroma.get(where=where_filter, limit=10)
            if anchors and anchors.get('documents'):
                for i in range(len(anchors['documents'])):
                    content = anchors['documents'][i]
                    md = dict(anchors['metadatas'][i] or {}) # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô metadata ‡πÄ‡∏õ‡πá‡∏ô None
                    
                    # üéØ ‡πÅ‡∏Å‡πâ‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà 1: ‡πÉ‡∏ä‡πâ MD5 ‡πÅ‡∏ó‡∏ô hash() ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ ID ‡∏Ñ‡∏á‡∏ó‡∏µ‡πà (Deterministic)
                    content_hash = hashlib.md5(content.encode()).hexdigest()
                    uid = str(md.get("chunk_uuid") or f"anchor-{content_hash}")
                    
                    if uid not in unique_evidence_map:
                        # üéØ ‡πÅ‡∏Å‡πâ‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà 2: ‡∏â‡∏µ‡∏î‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏•‡∏á‡πÉ‡∏ô metadata ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á (Safe Injection)
                        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÉ‡∏´‡πâ Anchor ‡∏°‡∏µ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á (0.95) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏´‡∏•‡∏±‡∏Å
                        md["score"] = 0.95
                        md["relevance_score"] = 0.95
                        md["is_anchor"] = True
                        
                        unique_evidence_map[uid] = LcDocument(
                            page_content=content,
                            metadata=md
                        )

        # üîç 3.2 Semantic Search (‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢)
        search_results = evidence_chroma.similarity_search(query, k=top_k, filter=where_filter)
        for d in search_results:
            content_hash = str(hash(d.page_content))
            uid = d.metadata.get("chunk_uuid") or content_hash
            if uid not in unique_evidence_map:
                unique_evidence_map[uid] = d

        candidates = list(unique_evidence_map.values())

        # --- 4. BATCH RERANKING (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô OOM ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥) ---
        evidence_results = []
        reranker = get_global_reranker()
        
        if reranker and candidates and query:
            try:
                batch_size = 100 # üöÄ ‡πÅ‡∏ö‡πà‡∏á Batch ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏Ç‡∏≠‡∏á VRAM
                scored_candidates = []
                
                logger.info(f"üöÄ Batch Reranking {len(candidates)} chunks...")
                for i in range(0, len(candidates), batch_size):
                    batch = candidates[i : i + batch_size]
                    reranked_batch = reranker.compress_documents(documents=batch, query=query)
                    scored_candidates.extend(reranked_batch)
                
                # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å Batch
                scored_candidates = sorted(
                    scored_candidates, 
                    key=lambda x: getattr(x, "relevance_score", 0), 
                    reverse=True
                )
                
                for r in scored_candidates[:k_to_rerank]:
                    doc = r.document if hasattr(r, "document") else r
                    m = doc.metadata or {}
                    score = getattr(r, "relevance_score", 0.0)
                    
                    # üéØ Sync Score ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤ Metadata ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ
                    m["rerank_score"] = score
                    m["score"] = score
                    
                    evidence_results.append({
                        "text": doc.page_content,
                        "source_filename": m.get("source_filename") or m.get("source") or "Evidence",
                        "page_label": str(m.get("page_label") or m.get("page_number") or m.get("page") or "N/A"),
                        "doc_id": m.get("stable_doc_uuid") or m.get("doc_id"),
                        "chunk_uuid": m.get("chunk_uuid") or str(uuid.uuid4()),
                        "pdca_tag": m.get("pdca_tag") or "Content",
                        "rerank_score": score,
                        "is_evidence": True,
                        "metadata": m
                    })
            except Exception as e:
                logger.error(f"‚ö†Ô∏è Rerank failed: {e}")
                candidates = candidates[:k_to_rerank] # Fallback
        
        # ‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏°‡πà‡∏°‡∏µ Reranker ‡∏´‡∏£‡∏∑‡∏≠ Error ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏õ‡∏Å‡∏ï‡∏¥
        if not evidence_results:
            for d in candidates[:k_to_rerank]:
                m = d.metadata or {}
                evidence_results.append({
                    "text": d.page_content,
                    "source_filename": m.get("source_filename") or m.get("source") or "Evidence",
                    "page_label": str(m.get("page_label") or m.get("page_number") or m.get("page") or "N/A"),
                    "doc_id": m.get("stable_doc_uuid") or m.get("doc_id"),
                    "chunk_uuid": m.get("chunk_uuid") or str(uuid.uuid4()),
                    "pdca_tag": m.get("pdca_tag") or "Content",
                    "rerank_score": 0.0,
                    "is_evidence": True,
                    "metadata": m
                })

    except Exception as e:
        logger.error(f"‚ùå Evidence Retrieval Error: {e}", exc_info=True)

    retrieval_time = round(time.time() - start_time, 3)
    logger.info(f"‚úÖ Success: Retrieved {len(evidence_results)} evidence chunks in {retrieval_time}s")

    return {
        "top_evidences": evidence_results,
        "rubric_context": rubric_results,
        "retrieval_time": retrieval_time,
        "used_chunk_uuids": [e["chunk_uuid"] for e in evidence_results if e.get("chunk_uuid")]
    }

# ========================
#  retrieve_context_by_doc_ids (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö hydration ‡πÉ‡∏ô router)
# ========================
def retrieve_context_by_doc_ids(
    doc_uuids: List[str],
    doc_type: str,
    enabler: Optional[str] = None,
    vectorstore_manager = None,
    limit: int = 100, # üöÄ ‡πÄ‡∏û‡∏¥‡πà‡∏° limit ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô
    tenant: Optional[str] = None,
    year: Optional[Union[int, str]] = None,
) -> Dict[str, Any]:
    """
    [REVISED] ‡∏î‡∏∂‡∏á chunks ‡∏à‡∏≤‡∏Å stable_doc_uuid ‡∏´‡∏•‡∏≤‡∏¢‡∏ï‡∏±‡∏ß (‡πÉ‡∏ä‡πâ‡∏ï‡∏≠‡∏ô hydration sources)
    ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏∏‡∏õ‡∏µ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤ Collection ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏Å‡∏©‡∏≤ Metadata ‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô
    """
    start_time = time.time()
    vsm = vectorstore_manager or VectorStoreManager(tenant=tenant, year=year)
    
    # Resolve collection name ‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏õ‡∏µ‡πÅ‡∏•‡∏∞ enabler
    collection_name = get_doc_type_collection_key(doc_type=doc_type, enabler=enabler)

    chroma = vsm._load_chroma_instance(collection_name)
    if not chroma:
        logger.error(f"‚ùå Collection {collection_name} not found for hydration")
        return {"top_evidences": []}

    if not doc_uuids:
        return {"top_evidences": []}

    logger.info(f"üíß Hydration ‚Üí {len(doc_uuids)} doc IDs from {collection_name}")

    try:
        # ‡πÉ‡∏ä‡πâ Metadata filter ‡∏î‡∏∂‡∏á chunks ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏•‡∏¥‡∏™‡∏ï‡πå ID ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
        # ‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô list comprehension ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏ß‡∏£‡πå‡∏Ç‡∏≠‡∏á Type
        ids_to_query = [str(u) for u in doc_uuids if u]
        
        results = chroma._collection.get(
            where={"stable_doc_uuid": {"$in": ids_to_query}},
            limit=limit,
            include=["documents", "metadatas"]
        )
    except Exception as e:
        logger.error(f"‚ö†Ô∏è Hydration query failed: {e}")
        return {"top_evidences": []}

    evidences = []
    # üéØ ‡πÉ‡∏ä‡πâ Content-based Deduplication ‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô‡∏Å‡∏±‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ã‡πâ‡∏≥
    seen_contents = set()

    for doc_content, meta in zip(results.get("documents", []), results.get("metadatas", [])):
        if not doc_content or not doc_content.strip():
            continue
            
        content_hash = str(hash(doc_content))
        if content_hash in seen_contents:
            continue
        seen_contents.add(content_hash)

        p_val = meta.get("page_label") or meta.get("page_number") or meta.get("page") or "N/A"
        
        # üéØ Sync Score ‡∏´‡∏•‡∏≠‡∏Å (‡πÄ‡∏û‡∏£‡∏≤‡∏∞ Hydration ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ú‡πà‡∏≤‡∏ô Rerank ‡πÅ‡∏ï‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Logic ‡∏≠‡∏∑‡πà‡∏ô‡πÑ‡∏°‡πà‡∏û‡∏±‡∏á)
        score = meta.get("score") or meta.get("rerank_score") or 0.85

        evidences.append({
            "doc_id": meta.get("stable_doc_uuid") or meta.get("doc_id"),
            "chunk_uuid": meta.get("chunk_uuid"),
            "source": meta.get("source") or meta.get("source_filename") or "Unknown",
            "page": str(p_val),
            "text": doc_content.strip(),
            "pdca_tag": meta.get("pdca_tag", "Other"),
            "score": score,
            "metadata": meta # ‡πÅ‡∏ô‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏ï‡πá‡∏°‡πÑ‡∏ß‡πâ‡πÄ‡∏™‡∏°‡∏≠
        })

    logger.info(f"‚úÖ Hydration success: {len(evidences)} chunks from {len(doc_uuids)} docs")
    return {
        "top_evidences": evidences,
        "retrieval_time": round(time.time() - start_time, 3)
    }


def retrieve_context_for_low_levels(query: str, doc_type: str, enabler: Optional[str]=None,
                                 vectorstore_manager: Optional['VectorStoreManager']=None,
                                 top_k: int=LOW_LEVEL_K, initial_k: int=INITIAL_TOP_K,
                                 # üü¢ NEW: ‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠ arguments
                                 mapped_uuids: Optional[List[str]]=None,
                                 priority_docs_input: Optional[List[Any]] = None,
                                 sequential_chunk_uuids: Optional[List[str]] = None, 
                                 sub_id: Optional[str]=None, level: Optional[int]=None) -> Dict[str, Any]:
    """
    Retrieves a small, focused context for low levels (L1, L2) using a reduced k (LOW_LEVEL_K).
    """
    # ‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å ‡πÅ‡∏ï‡πà‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏ä‡πâ k ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
    return retrieve_context_with_filter(
        query=query,
        doc_type=doc_type,
        enabler=enabler,
        vectorstore_manager=vectorstore_manager,
        top_k=LOW_LEVEL_K,
        initial_k=initial_k,
        mapped_uuids=mapped_uuids,
        priority_docs_input=priority_docs_input,
        sequential_chunk_uuids=sequential_chunk_uuids, 
        sub_id=sub_id,
        level=level
    )

# ----------------------------------------------------
# Helper function: Summarize evidence list (minimal stub)
# ----------------------------------------------------
def _summarize_evidence_list_short(evidences: list, max_sentences: int = 3) -> str:
    """
    Provides a concise summary of evidence items.
    """
    if not evidences:
        return ""
    
    parts = []
    for ev in evidences[:max(1, min(len(evidences), max_sentences))]:
        if isinstance(ev, dict):
            fn = ev.get("source_filename") or ev.get("source") or ev.get("doc_id", "unknown")
            txt = ev.get("text") or ev.get("content") or ""
        else:
            fn = str(ev)
            txt = str(ev)
        txt_short = txt[:120].replace("\n", " ").strip()
        if txt_short:
            parts.append(f"‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå `{fn}`: {txt_short}...")
        else:
            parts.append(f"‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå `{fn}`")
    return " | ".join(parts)

def build_multichannel_context_for_level(
    level: int,
    top_evidences: List[Dict[str, Any]],
    previous_levels_map: Optional[Dict[str, Any]] = None,
    previous_levels_evidence: Optional[List[Dict[str, Any]]] = None,
    max_main_context_tokens: int = 3000, 
    max_summary_sentences: int = 4,
    max_context_length: Optional[int] = None, 
    **kwargs
) -> Dict[str, Any]:
    """
    [ULTIMATE OPTIMIZED v2026.8 - ULTRA L1 FALLBACK & MAX DEBUG]
    - ‡∏™‡∏£‡πâ‡∏≤‡∏á baseline_summary + aux_summary ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (direct ‡∏ß‡πà‡∏≤‡∏á‡πÉ‡∏´‡πâ engine ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£)
    - ‡∏•‡∏î threshold ‡∏´‡∏ô‡∏±‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö L1 (0.15) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
    - Ultra fallback ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö L1: ‡∏ñ‡πâ‡∏≤ direct ‡∏¢‡∏±‡∏á 0 ‚Üí force top chunks ‡πÄ‡∏Ç‡πâ‡∏≤ direct + pdca_groups (P/D)
    - Debug log ‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏™‡∏∏‡∏î: tag, relevance, text preview, force detail
    - Robust handling: ‡∏ß‡πà‡∏≤‡∏á ‚Üí ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° fallback ‡∏ó‡∏µ‡πà LLM ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏ß‡πà‡∏≤ "‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô"
    """
    logger = logging.getLogger(__name__)
    K_MAIN = 5
    MIN_RELEVANCE_FOR_AUX = 0.15 if level == 1 else 0.4  # ‡∏•‡∏î‡∏´‡∏ô‡∏±‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö L1

    # 1. Baseline Summary (‡∏à‡∏≤‡∏Å previous)
    baseline_evidence = previous_levels_evidence or []
    if previous_levels_map:
        for lvl_ev in previous_levels_map.values():
            baseline_evidence.extend(lvl_ev)

    summarizable_baseline = [
        item for item in baseline_evidence[:40]  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏õ‡πá‡∏ô 40 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô
        if isinstance(item, dict) and (item.get("text") or item.get("content", "")).strip()
    ]

    if not summarizable_baseline:
        baseline_summary = "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ (‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏ó‡∏µ‡πà Level 1)"
    else:
        baseline_summary = _summarize_evidence_list_short(
            summarizable_baseline,
            max_sentences=max_summary_sentences
        )

    # 2. Direct + Aux Separation + Ultra Debug
    direct, aux_candidates = [], []

    for idx, ev in enumerate(top_evidences[:40], 1):  # ‡∏à‡∏≥‡∏Å‡∏±‡∏î 40 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß
        if not isinstance(ev, dict):
            continue

        tag = (ev.get("pdca_tag") or ev.get("PDCA") or "Other").upper()
        relevance = ev.get("rerank_score") or ev.get("score", 0.0)
        text_preview = (ev.get('text', '')[:80] + "...") if ev.get('text') else "[No text]"

        # Debug ‡∏ó‡∏∏‡∏Å chunk (‡∏ä‡πà‡∏ß‡∏¢‡∏´‡∏≤‡∏ß‡πà‡∏≤‡∏ó‡∏≥‡πÑ‡∏°‡πÑ‡∏°‡πà‡πÄ‡∏Ç‡πâ‡∏≤ direct)
        logger.debug(f"[TAG-CHECK L{level} #{idx}] Rel: {relevance:.3f} | Tag: {tag} | Preview: {text_preview}")

        if tag in {"P", "PLAN", "D", "DO", "C", "CHECK", "A", "ACT"}:
            direct.append(ev)
        elif relevance >= MIN_RELEVANCE_FOR_AUX:
            aux_candidates.append(ev)

    # 3. ‡∏¢‡πâ‡∏≤‡∏¢ aux ‡πÑ‡∏õ‡πÄ‡∏ï‡∏¥‡∏° direct ‡∏ñ‡πâ‡∏≤‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏Å‡∏¥‡∏ô
    if len(direct) < K_MAIN:
        need = K_MAIN - len(direct)
        moved = aux_candidates[:need]
        direct.extend(moved)
        aux_candidates = aux_candidates[need:]
        logger.info(f"[DIRECT-FILL] Moved {need} aux chunks to direct (total direct now: {len(direct)})")

    # 4. L1 ULTRA FALLBACK: ‡∏ñ‡πâ‡∏≤ direct ‡∏¢‡∏±‡∏á 0 ‚Üí force top chunks ‡πÄ‡∏Ç‡πâ‡∏≤ direct + log ‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
    if level == 1 and len(direct) == 0 and top_evidences:
        need = min(K_MAIN, len(top_evidences))
        forced_chunks = sorted(top_evidences, key=lambda e: e.get("rerank_score", 0) or e.get("score", 0), reverse=True)[:need]
        direct.extend(forced_chunks)
        logger.warning(f"[L1-ULTRA-FALLBACK] No PDCA tag at all ‚Üí Forced top {need} chunks to direct (ignore tag)")
        for i, ev in enumerate(forced_chunks, 1):
            rel = ev.get("rerank_score", ev.get("score", 'N/A'))
            text = ev.get('text', '')[:80] + "..." if ev.get('text') else "[No text]"
            logger.info(f"[FORCED-DIRECT {i}] Rel: {rel} | Text: {text}")

    # Warning ‡∏ñ‡πâ‡∏≤ direct ‡∏¢‡∏±‡∏á‡∏ô‡πâ‡∏≠‡∏¢‡∏°‡∏≤‡∏Å (‡πÅ‡∏ó‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏´‡∏•‡∏±‡∏á ultra fallback)
    if len(direct) < 1:
        logger.critical(f"L{level}: CRITICAL - Direct PDCA chunks = 0 ‡∏´‡∏•‡∏±‡∏á ultra fallback ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î!")

    # 5. ‡∏™‡∏£‡πâ‡∏≤‡∏á aux_summary
    aux_summary = _summarize_evidence_list_short(aux_candidates, max_sentences=3) if aux_candidates else \
        "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏£‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠ (‡∏≠‡∏≤‡∏à‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£)"

    # 6. Return ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    debug_meta = {
        "level": level,
        "direct_count": len(direct),
        "aux_count": len(aux_candidates),
        "baseline_count": len(summarizable_baseline),
        "min_relevance_used": MIN_RELEVANCE_FOR_AUX,
        "top_relevance": max((ev.get("rerank_score", 0) or ev.get("score", 0) for ev in top_evidences), default=0)
    }
    logger.info(f"Context L{level} ‚Üí Direct:{len(direct)} | Aux:{len(aux_candidates)} | Baseline:{len(summarizable_baseline)}")

    return {
        "baseline_summary": baseline_summary,
        "direct_context": "",  # ‡∏ß‡πà‡∏≤‡∏á‡πÉ‡∏´‡πâ engine ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏≠‡∏á
        "aux_summary": aux_summary,
        "debug_meta": debug_meta,
    }


# ------------------------
# LLM fetcher
# ------------------------
def _fetch_llm_response(
    system_prompt: str, 
    user_prompt: str, 
    max_retries: int = 3,
    llm_executor: Any = None 
) -> str:
    """
    ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å LLM ‡∏ú‡πà‡∏≤‡∏ô LangChain/Ollama ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Format ‡∏ú‡∏¥‡∏î‡πÄ‡∏û‡∏µ‡πâ‡∏¢‡∏ô:
    - ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö JSON output ‡∏î‡πâ‡∏ß‡∏¢ Strict English Prompt
    - ‡πÉ‡∏ä‡πâ Regex Extraction ‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡πà‡∏ß‡∏ô { ... } ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡∏≠‡∏≠‡∏Å
    - Log raw response ‡πÄ‡∏ï‡πá‡∏°‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Debug
    - Retry ‡∏û‡∏£‡πâ‡∏≠‡∏° Exponential Backoff ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏Å‡∏¥‡∏î Error
    """
    global _MOCK_FLAG

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ LLM Instance ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    if llm_executor is None and not _MOCK_FLAG: 
        raise ConnectionError("LLM instance not initialized (Missing llm_executor).")

    # 1. üõ†Ô∏è ENFORCED PROMPT (‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡∏°‡∏±‡∏Å‡∏Ñ‡∏∏‡∏° Format ‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏•‡πá‡∏Å)
    enforced_system_prompt = system_prompt.strip() + (
        "\n\n"
        "### STRICT OUTPUT RULES ###\n"
        "1. ANSWER IN VALID JSON OBJECT ONLY.\n"
        "2. NO EXPLANATIONS, NO PREFACE, NO CONVERSATION.\n"
        "3. START WITH '{' AND END WITH '}'.\n"
        "4. DO NOT USE MARKDOWN CODE BLOCKS (```json).\n"
        "5. IF NO EVIDENCE FOUND, RETURN: {\"score\": 0, \"reason\": \"No evidence\", \"is_passed\": false}"
    )

    messages = [
        {"role": "system", "content": enforced_system_prompt},
        {"role": "user",   "content": user_prompt}
    ]

    for attempt in range(1, max_retries + 1):
        try:
            # --- MOCK MODE CASE ---
            if _MOCK_FLAG:
                mock_json = '{"score": 1, "reason": "Mock mode active", "is_passed": true}'
                logger.critical(f"LLM RAW RESPONSE (DEBUG MOCK): {mock_json}")
                return mock_json

            # --- ACTUAL LLM CALL (OLLAMA / LANGCHAIN) ---
            # ‡πÉ‡∏ä‡πâ temperature=0.0 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
            response = llm_executor.invoke(messages, config={"temperature": 0.0})
            
            # ‡∏î‡∏∂‡∏á Text ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏à‡∏≤‡∏Å Response Object
            raw_text = ""
            if hasattr(response, "content"):
                raw_text = str(response.content)
            elif isinstance(response, str):
                raw_text = response
            else:
                raw_text = str(response)

            # üîç ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏´‡πá‡∏ô‡πÉ‡∏ô Log ‡∏Ñ‡∏∑‡∏≠‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ (Log ‡∏Å‡πà‡∏≠‡∏ô Clean ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°‡πÇ‡∏°‡πÄ‡∏î‡∏•)
            logger.critical(f"LLM RAW RESPONSE (DEBUG): {raw_text[:1000]}{'...' if len(raw_text) > 1000 else ''}")

            # 2. üßπ CLEANING LOGIC (Regex Extraction)
            # ‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ LLM ‡∏ï‡∏≠‡∏ö "Based on the text... { ... }"
            raw_text_stripped = raw_text.strip()
            
            # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏õ‡∏µ‡∏Å‡∏Å‡∏≤‡∏Ñ‡∏π‡πà‡πÅ‡∏£‡∏Å { ... }
            json_match = re.search(r'(\{.*\})', raw_text_stripped, re.DOTALL)
            
            if json_match:
                extracted_json = json_match.group(1)
                try:
                    # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô JSON ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                    json.loads(extracted_json) 
                    return extracted_json
                except json.JSONDecodeError:
                    logger.warning(f"Extracted string is not valid JSON: {extracted_json[:100]}")
            
            # 3. üõ°Ô∏è FALLBACK: ‡∏ñ‡πâ‡∏≤ Regex ‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ ‡∏´‡∏£‡∏∑‡∏≠ Parse ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏î‡∏π‡∏ß‡πà‡∏≤ raw_text (‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏î‡∏´‡∏±‡∏ß‡∏ó‡πâ‡∏≤‡∏¢) ‡∏û‡∏≠‡∏•‡∏∏‡πâ‡∏ô‡πÄ‡∏õ‡πá‡∏ô JSON ‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°
            return raw_text_stripped

        except Exception as e:
            logger.error(f"LLM call failed (attempt {attempt}/{max_retries}): {e}")
            if attempt < max_retries:
                # Exponential backoff: 2s, 4s, 8s...
                time.sleep(2 ** attempt)  
            else:
                logger.critical("All LLM attempts failed ‚Äì returning safe fallback JSON")
                return '{"score": 0, "reason": "LLM_TIMEOUT_OR_FAILURE", "is_passed": false}'

    return '{"score": 0, "reason": "Unknown execution error"}'

# ------------------------
# Evaluation
# ------------------------
T = TypeVar("T", bound=BaseModel)

def _check_and_handle_empty_context(context: str, sub_id: str, level: int) -> Optional[Dict[str, Any]]:
    """
    Returns Failure result if context is empty or contains known error strings.
    Auto-fail with PDCA keys all set to 0.
    """
    if not context or "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á" in context or "ERROR:" in context.upper():
        logger.warning(f"Auto-FAIL L{level} for {sub_id}: Empty or Error Context detected from RAG.")
        context_preview = context.strip()[:100].replace("\n", " ") if context else "Empty Context"
        return {
            "score": 0,
            "reason": f"‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏î‡πâ‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á (Context: {context_preview}).",
            "is_passed": False,
            "P_Plan_Score": 0,
            "D_Do_Score": 0,
            "C_Check_Score": 0,
            "A_Act_Score": 0,
        }
    return None


def _get_context_for_level(context: str, level: int) -> str:
    """Return context string with appropriate length limit for each level."""
    if not context:
        return ""
    # L1-L2: ‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà
    if level <= 2:
        return context[:6000]  
    # L3-L5: ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏á‡∏ó‡∏µ‡πà‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÑ‡∏ß‡πâ (‡∏•‡∏î Latency ‡∏ö‡∏ô Server)
    return context[:MAX_EVAL_CONTEXT_LENGTH]  

def evaluate_with_llm(
    context: str, 
    sub_criteria_name: str, 
    level: int, 
    statement_text: str, 
    sub_id: str, 
    llm_executor: Any = None, 
    require_phase: List[str] = None,
    specific_contextual_rule: str = "‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô",
    ai_confidence: str = "MEDIUM",
    confidence_reason: str = "N/A",
    **kwargs
) -> Dict[str, Any]:
    """[SENIOR-AUDIT v36.4] ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏∞‡∏î‡∏±‡∏ö Maturity L3-L5"""
    context_to_send_eval = _get_context_for_level(context, level)
    phases_str = ", ".join(require_phase) if require_phase else "P, D, C, A"
    baseline_summary = kwargs.get("baseline_summary", "").strip()

    try:
        # ‡πÅ‡∏°‡∏õ‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö USER_ASSESSMENT_PROMPT ‡πÉ‡∏ô seam_prompts.py
        system_prompt = SYSTEM_ASSESSMENT_PROMPT.format(
            level=level,
            ai_confidence=ai_confidence,
            confidence_reason=confidence_reason
        )

        user_prompt = USER_ASSESSMENT_PROMPT.format(
            sub_criteria_name=sub_criteria_name,
            sub_id=sub_id,
            level=level,
            statement_text=statement_text,
            context=context_to_send_eval[:32000],
            required_phases=phases_str,
            specific_contextual_rule=specific_contextual_rule,
            ai_confidence=ai_confidence,
            confidence_reason=confidence_reason
        )

        if baseline_summary:
            user_prompt += f"\n\n--- BASELINE DATA ---\n{baseline_summary}"

        raw_response = _fetch_llm_response(system_prompt, user_prompt, llm_executor=llm_executor)
        parsed = _robust_extract_json(raw_response)

        return _build_audit_result_object(
            parsed, raw_response, context_to_send_eval, ai_confidence, 
            level=level, sub_id=sub_id
        )

    except Exception as e:
        return _create_fallback_error(sub_id, level, e, context_to_send_eval)

def evaluate_with_llm_low_level(
    context: str,
    sub_criteria_name: str,
    level: int,
    statement_text: str,
    sub_id: str,
    llm_executor: Any = None,
    require_phase: List[str] = None,
    ai_confidence: str = "MEDIUM",
    confidence_reason: str = "N/A",
    **kwargs
) -> Dict[str, Any]:
    """[FOUNDATION-AUDIT v36.4] ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏∞‡∏î‡∏±‡∏ö L1/L2"""
    context_to_send_eval = _get_context_for_level(context, level)
    phases_str = ", ".join(require_phase) if require_phase else "P, D"
    plan_keywords = kwargs.get("plan_keywords", "‡∏ß‡∏¥‡∏™‡∏±‡∏¢‡∏ó‡∏±‡∏®‡∏ô‡πå, ‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢, ‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô, ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á")

    try:
        system_prompt = SYSTEM_LOW_LEVEL_PROMPT.format(
            ai_confidence=ai_confidence,
            confidence_reason=confidence_reason
        )

        user_prompt = USER_LOW_LEVEL_PROMPT.format(
            sub_id=sub_id,
            sub_criteria_name=sub_criteria_name,
            level=level,
            statement_text=statement_text,
            context=context_to_send_eval[:32000],
            required_phases=phases_str,
            plan_keywords=plan_keywords,
            ai_confidence=ai_confidence,
            confidence_reason=confidence_reason
        )

        raw_response = _fetch_llm_response(system_prompt, user_prompt, llm_executor=llm_executor)
        parsed = _robust_extract_json(raw_response)

        return _build_audit_result_object(
            parsed, raw_response, context_to_send_eval, ai_confidence, 
            level=level, sub_id=sub_id
        )

    except Exception as e:
        return _create_fallback_error(sub_id, level, e, context_to_send_eval)

def _build_audit_result_object(parsed: Dict, raw_response: str, context: str, confidence: str, **kwargs):
    """‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏£‡πà‡∏≤‡∏á JSON Object ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏≤‡∏¢"""
    level = kwargs.get('level', 1)
    sub_id = kwargs.get('sub_id', 'Unknown')

    def clean_score(val, default=0.0):
        try:
            return float(val)
        except:
            return default

    score = clean_score(parsed.get("score"))
    is_passed = parsed.get("is_passed")
    
    # ‡∏ñ‡πâ‡∏≤ AI ‡∏ï‡∏≠‡∏ö‡πÑ‡∏°‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô (Logic ‡∏Ç‡∏≠‡∏á New Branch)
    if is_passed is None:
        is_passed = score >= 0.7 if level <= 2 else score >= 1.0

    return {
        "sub_id": str(sub_id),
        "level": int(level),
        "score": score,
        "is_passed": bool(is_passed),
        "reason": parsed.get("reason", "‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏à‡∏≤‡∏Å LLM"),
        "P_Plan_Score": clean_score(parsed.get("P_Plan_Score"), score if is_passed else 0.0),
        "D_Do_Score": clean_score(parsed.get("D_Do_Score"), score if is_passed else 0.0),
        "C_Check_Score": clean_score(parsed.get("C_Check_Score")),
        "A_Act_Score": clean_score(parsed.get("A_Act_Score")),
        "Extraction_P": parsed.get("Extraction_P", "-"),
        "Extraction_D": parsed.get("Extraction_D", "-"),
        "Extraction_C": parsed.get("Extraction_C", "-"),
        "Extraction_A": parsed.get("Extraction_A", "-"),
        "final_llm_context": context,
        "raw_llm_response": raw_response,
        "ai_confidence_at_eval": confidence,
        "consistency_check": parsed.get("consistency_check", True)
    }

def _create_fallback_error(sub_id: str, level: int, error: Exception, context: str) -> Dict[str, Any]:
    """[SAFETY NET] ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏ì‡∏µ LLM ‡∏´‡∏£‡∏∑‡∏≠ Prompt ‡∏û‡∏±‡∏á"""
    logger.error(f"üõë Critical Audit Failure {sub_id} L{level}: {str(error)}")
    
    return {
        "sub_id": sub_id,
        "level": level,
        "score": 0.0,
        "reason": f"Audit Engine Error: {str(error)}",
        "is_passed": False,
        "consistency_check": False,
        "P_Plan_Score": 0.0, "D_Do_Score": 0.0, "C_Check_Score": 0.0, "A_Act_Score": 0.0,
        "Extraction_P": "-", "Extraction_D": "-", "Extraction_C": "-", "Extraction_A": "-",
        "final_llm_context": context,
        "raw_llm_response": "",
        "ai_confidence_at_eval": "ERROR"
    }

# ------------------------
# Summarize (FULL VERSION - v2026.4 Ultra-Robust & Zero-Error)
# ------------------------
def create_context_summary_llm(
    context: str,
    sub_criteria_name: str,
    level: int,
    sub_id: str,
    statement_text: str = "",           # Default ‡∏ß‡πà‡∏≤‡∏á ‚Üí ‡πÑ‡∏°‡πà error ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏™‡πà‡∏á
    next_level: int = None,             # Default ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏≠‡∏á
    llm_executor: Any = None
) -> Dict[str, Any]:
    """
    [SUMMARIZER v2026.4 ‚Äî Ultra-Robust & Zero-Error]
    - ‡πÄ‡∏û‡∏¥‡πà‡∏° default ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö statement_text ‡πÅ‡∏•‡∏∞ next_level ‚Üí ‡πÑ‡∏°‡πà error ‡∏ñ‡πâ‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö
    - Fallback ‡πÉ‡∏ô prompt ‡∏ñ‡πâ‡∏≤ statement_text ‡∏ß‡πà‡∏≤‡∏á (‡∏™‡∏£‡∏∏‡∏õ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ)
    - Log ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ fallback + clean ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á
    - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö retry 4 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á + hint ‡πÉ‡∏ô prompt ‡∏£‡∏≠‡∏ö retry
    """
    logger = logging.getLogger("AssessmentApp")

    # 1. Validation ‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô
    if llm_executor is None:
        logger.warning("‚ö†Ô∏è LLM executor is None - returning fallback")
        return {
            "summary": "‡∏£‡∏∞‡∏ö‡∏ö LLM ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô",
            "suggestion_for_next_level": "‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ LLM",
            "compliance_note": "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏î‡πâ",
            "evidence_integrity_score": 0.0
        }

    context_safe = (context or "").strip()
    if len(context_safe) < 30:
        return {
            "summary": "‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏Å‡∏¥‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏™‡∏£‡∏∏‡∏õ‡πÑ‡∏î‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô",
            "suggestion_for_next_level": "‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á",
            "compliance_note": "‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô",
            "evidence_integrity_score": 0.1
        }

    # Fallback next_level ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏™‡πà‡∏á
    if next_level is None:
        next_level = min(level + 1, 5)
        logger.debug(f"[SUMMARY] next_level fallback to {next_level} for {sub_id} L{level}")

    # 2. Clean context ‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á (‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏à‡∏≤‡∏Å‡πÄ‡∏î‡∏¥‡∏°)
    context_to_send = re.sub(r'[\x00-\x1F\x7F-\x9F]', ' ', context_safe)[:6500]

    # 3. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Prompt
    try:
        fallback_statement = statement_text or "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏ statement (‡∏™‡∏£‡∏∏‡∏õ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°)"
        human_prompt = USER_EVIDENCE_DESCRIPTION_TEMPLATE.format(
            sub_id=f"{sub_id} - {sub_criteria_name}",
            sub_criteria_name=sub_criteria_name,
            level=level,
            statement_text=fallback_statement,
            next_level=next_level,
            context=context_to_send
        )
    except Exception as e:
        logger.error(f"‚ùå Formatting Error in Summary Prompt: {e}")
        return {
            "summary": "‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö prompt",
            "suggestion_for_next_level": "N/A",
            "compliance_note": "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏î‡πâ",
            "evidence_integrity_score": 0.0
        }

    # System Instruction ‡πÄ‡∏Ç‡πâ‡∏°‡∏á‡∏ß‡∏î + ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö fallback
    system_instruction = (
        f"{SYSTEM_EVIDENCE_DESCRIPTION_PROMPT}\n"
        "### STRICT RULES (‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏ï‡∏≤‡∏°‡∏ó‡∏∏‡∏Å‡∏Ç‡πâ‡∏≠) ###\n"
        "1. RETURN ONLY VALID JSON OBJECT. ‡∏´‡πâ‡∏≤‡∏°‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏î ‡πÜ ‡∏ô‡∏≠‡∏Å JSON\n"
        "2. ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏ä‡πâ ```json ‡∏´‡∏£‡∏∑‡∏≠ markdown block\n"
        "3. ‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏•‡πâ‡∏ß‡∏ô‡πÉ‡∏ô‡∏ó‡∏∏‡∏Å value\n"
        "4. ‡∏´‡πâ‡∏≤‡∏°‡∏°‡πÇ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô context\n"
        "5. ‡∏ñ‡πâ‡∏≤ statement_text ‡∏ß‡πà‡∏≤‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡πá‡∏ô '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏' ‡πÉ‡∏´‡πâ‡∏™‡∏£‡∏∏‡∏õ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô compliance\n"
        "6. ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ statement_text ‡πÉ‡∏´‡πâ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô compliance ‡∏Å‡∏±‡∏ö statement ‡∏à‡∏£‡∏¥‡∏á ‡πÜ"
    )

    # 4. Execution Loop with Advanced Parsing + Retry
    max_retries = 4  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏õ‡πá‡∏ô 4 ‡∏£‡∏≠‡∏ö
    for attempt in range(1, max_retries + 1):
        try:
            logger.info(f"üîÑ Generating Summary {sub_id} L{level} (Attempt {attempt})")

            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å LLM (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á LangChain & Ollama-style)
            if hasattr(llm_executor, 'generate'):
                raw_response = llm_executor.generate(system=system_instruction, prompts=[human_prompt])
            elif hasattr(llm_executor, 'invoke'):
                raw_response = llm_executor.invoke(human_prompt)
            else:
                raw_response = llm_executor(system_instruction + "\n" + human_prompt)

            # Robust Text Extraction
            res_text = ""
            if hasattr(raw_response, 'generations'):
                res_text = raw_response.generations[0][0].text.strip()
            elif hasattr(raw_response, 'content'):
                res_text = str(raw_response.content).strip()
            else:
                res_text = str(raw_response).strip()

            # 5. ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (Thai-safe)
            if res_text:
                res_text = res_text.replace('\xa0', ' ').replace('\u200b', '')
                res_text = "".join(c for c in res_text if ord(c) >= 32 or c in "\n\r\t")
                res_text = re.sub(r'```(?:json)?\s*|\s*```', '', res_text).strip()
                res_text = re.sub(r'^[^{\[]+', '', res_text).strip()

            # 6. Robust JSON Extraction
            parsed = _extract_normalized_dict(res_text)

            if parsed and isinstance(parsed, dict):
                summary_val = parsed.get("summary") or parsed.get("‡∏™‡∏£‡∏∏‡∏õ") or ""
                suggestion = parsed.get("suggestion_for_next_level") or parsed.get("‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ñ‡∏±‡∏î‡πÑ‡∏õ") or ""
                compliance = parsed.get("compliance_note") or parsed.get("‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á") or "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏"
                score = float(parsed.get("evidence_integrity_score", 0.5))

                if summary_val.strip():
                    logger.info(f"‚úÖ Summary Generated Successfully (Attempt {attempt})")
                    return {
                        "summary": str(summary_val).strip(),
                        "suggestion_for_next_level": str(suggestion).strip() or "‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ñ‡∏±‡∏î‡πÑ‡∏õ",
                        "compliance_note": str(compliance).strip(),
                        "evidence_integrity_score": max(0.0, min(1.0, score))
                    }

            logger.warning(f"‚ö†Ô∏è Attempt {attempt}: Invalid/empty JSON. Retrying...")
            human_prompt += "\n\n(‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å: ‡∏ï‡∏≠‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞ JSON ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ { ‡πÅ‡∏•‡∏∞‡∏à‡∏ö‡∏î‡πâ‡∏ß‡∏¢ } ‡∏´‡πâ‡∏≤‡∏°‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏∑‡πà‡∏ô‡πÉ‡∏î)"

            time.sleep(0.8)  # ‡∏û‡∏±‡∏Å‡∏ô‡∏≤‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô‡∏ô‡∏¥‡∏î‡∏´‡∏ô‡πà‡∏≠‡∏¢

        except Exception as e:
            logger.error(f"‚ùå Attempt {attempt} Error: {str(e)}")
            time.sleep(1.2)

    # 7. Ultimate Fallback
    logger.error(f"‚ùå All attempts failed for {sub_id} L{level}")
    return {
        "summary": f"‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö {level} ‡πÅ‡∏ï‡πà‡∏£‡∏∞‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡∏∏‡∏õ‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå",
        "suggestion_for_next_level": f"‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏£‡∏∞‡∏î‡∏±‡∏ö {next_level or level+1}",
        "compliance_note": "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡πÑ‡∏î‡πâ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î",
        "evidence_integrity_score": 0.3
    }

def create_structured_action_plan(
    recommendation_statements: List[Dict[str, Any]],
    sub_id: str,
    sub_criteria_name: str,
    enabler: str = "KM",
    target_level: int = 5,
    llm_executor: Any = None,
    logger: logging.Logger = None,
    max_retries: int = 3,
    enabler_rules: Dict[str, Any] = {}
) -> List[Dict[str, Any]]:
    """
    [STRATEGIC ROADMAP ENGINE v2026.6.15]
    - ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô‡πÄ‡∏ä‡∏¥‡∏á‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡πÅ‡∏ö‡∏ö Phase-based
    - ‡∏ö‡∏π‡∏£‡∏ì‡∏≤‡∏Å‡∏≤‡∏£ Gaps ‡∏£‡∏≤‡∏¢‡πÄ‡∏•‡πÄ‡∏ß‡∏•‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ö Coaching Insights
    - ‡∏°‡∏µ‡∏£‡∏∞‡∏ö‡∏ö Safety Net (Emergency Fallback) ‡∏ó‡∏µ‡πà‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥
    """
    if logger is None:
        logger = logging.getLogger(__name__)

    logger.info(f"üöÄ [ROADMAP START] Generating plan for {enabler} | {sub_id} (Target L{target_level})")

    # --- 1. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ (Condition Mode) ---
    is_sustain_mode = not recommendation_statements
    is_quality_refinement = False
    
    avg_score = 0.0
    if recommendation_statements:
        scores = [float(s.get('score', 0.0)) for s in recommendation_statements]
        avg_score = sum(scores) / len(scores)
        types = [str(s.get('recommendation_type', '')) for s in recommendation_statements]
        
        # ‡∏ñ‡πâ‡∏≤‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡πÅ‡∏ï‡πà‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠‡πÑ‡∏°‡πà‡πÄ‡∏ï‡πá‡∏° (‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ < 80%)
        if all(t not in ['FAILED_REMEDIATION'] for t in types) and avg_score < 0.8:
            is_quality_refinement = True

    # --- 2. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Advice Focus (‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ô‡∏≥ AI) ---
    specific_rule = enabler_rules.get(enabler, enabler_rules.get("DEFAULT", ""))
    
    if is_sustain_mode:
        advice_focus = f"‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏•‡∏¥‡∏® (Best Practice) ‡πÅ‡∏•‡∏∞‡∏Ç‡∏¢‡∏≤‡∏¢‡∏ú‡∏•‡∏™‡∏π‡πà‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {sub_criteria_name}"
        dynamic_max_phases, max_steps = 1, 4
    elif is_quality_refinement:
        advice_focus = f"‡∏¢‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÄ‡∏ä‡∏¥‡∏á‡∏õ‡∏£‡∏∞‡∏à‡∏±‡∏Å‡∏©‡πå (Evidence Quality) ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ß‡∏±‡∏î‡∏ú‡∏•‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (KPI) ‡πÉ‡∏´‡πâ‡∏Ñ‡∏°‡∏ä‡∏±‡∏î‡∏Ç‡∏∂‡πâ‡∏ô"
        dynamic_max_phases, max_steps = 1, 3
    else:
        # üéØ CORE LOGIC: ‡∏ï‡πâ‡∏≠‡∏á‡∏ã‡πà‡∏≠‡∏°‡∏ê‡∏≤‡∏ô (Level ‡∏ï‡πà‡∏≥) ‡∏Å‡πà‡∏≠‡∏ô‡∏õ‡∏µ‡∏ô Level ‡∏™‡∏π‡∏á
        advice_focus = (f"‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏¥‡∏î‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á (Gap Remediation) ‡∏ï‡∏≤‡∏°‡∏ß‡∏á‡∏à‡∏£ PDCA "
                        f"‡πÇ‡∏î‡∏¢‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢ (P) ‡∏™‡∏π‡πà‡∏Å‡∏≤‡∏£‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏à‡∏£‡∏¥‡∏á (D) ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ß‡∏±‡∏î‡∏ú‡∏• (C/A)")
        dynamic_max_phases = 3 if target_level >= 4 else 2
        max_steps = 3

    if specific_rule:
        advice_focus += f" (‡∏Å‡∏é‡πÄ‡∏â‡∏û‡∏≤‡∏∞: {specific_rule})"

    # --- 3. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Gaps & Insights (‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö Level) ---
    if is_sustain_mode:
        stmt_content = f"‡∏ö‡∏£‡∏£‡∏•‡∏∏‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏£‡∏∞‡∏î‡∏±‡∏ö {target_level} ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡πÅ‡∏ö‡∏ö ‡πÄ‡∏ô‡πâ‡∏ô‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏±‡πà‡∏á‡∏¢‡∏∑‡∏ô‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô Role Model"
    else:
        # ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡πÅ‡∏•‡∏∞‡∏Å‡∏≥‡∏à‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô ‡πÇ‡∏î‡∏¢‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Å‡∏±‡∏ö‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
        unique_gaps = {}
        for s in recommendation_statements:
            lvl = s.get('level', 0)
            reason = (s.get('context') or s.get('reason') or "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á").strip()
            # ‡∏î‡∏∂‡∏á Missing PDCA Phases ‡∏°‡∏≤‡πÇ‡∏ä‡∏ß‡πå‡πÉ‡∏ô Roadmap ‡∏î‡πâ‡∏ß‡∏¢
            missing = s.get('missing_phases', [])
            pdca_suffix = f" [Missing: {','.join(missing)}]" if missing else ""
            
            unique_gaps[lvl] = f"{reason}{pdca_suffix}"
        
        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏à‡∏≤‡∏Å L1 -> L5 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ AI ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÅ‡∏ú‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô
        stmt_content = "\n".join([f"- Level {l}: {unique_gaps[l]}" for l in sorted(unique_gaps.keys())])

    # --- 4. ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö Prompt ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö AI Engine ---
    # ‡πÉ‡∏ä‡πâ Global Variable ACTION_PLAN_PROMPT ‡∏ó‡∏µ‡πà‡∏û‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÑ‡∏ß‡πâ
    human_prompt = ACTION_PLAN_PROMPT.format(
        enabler=enabler,
        sub_id=sub_id,
        sub_criteria_name=sub_criteria_name,
        target_level=target_level,
        recommendation_statements_list=stmt_content,
        advice_focus=advice_focus,
        max_phases=dynamic_max_phases,
        max_steps=max_steps,
        max_words_per_step=120,
        language="‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢"
    )

    # --- 5. Execution Loop (Retry & Recovery) ---
    for attempt in range(1, max_retries + 1):
        try:
            response = llm_executor.generate(
                system=SYSTEM_ACTION_PLAN_PROMPT,
                prompts=[human_prompt],
                temperature=0.0 # ‡πÉ‡∏ä‡πâ 0 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏¥‡πà‡∏á‡∏Ç‡∏≠‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á JSON
            )

            # Extract text safely
            raw_text = getattr(response, 'content', str(response))
            if hasattr(response, 'generations'):
                raw_text = response.generations[0][0].text

            # ‡∏Å‡∏π‡πâ‡∏Ñ‡∏∑‡∏ô JSON Array
            items = _extract_json_array_for_action_plan(raw_text, logger)
            if not items: continue

            # Normalize Keys & Validate
            clean_items = action_plan_normalize_keys(items)
            
            try:
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏±‡∏ö Pydantic Schema
                validated = ActionPlanResult.validate_flexible(clean_items)
                logger.info(f"‚úÖ [SUCCESS] Roadmap Built for {sub_id} (Attempt {attempt})")
                
                # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô List ‡∏Ç‡∏≠‡∏á Dict
                if hasattr(validated, 'root'):
                    return [i.model_dump() for i in validated.root]
                return [v.model_dump() for v in validated]

            except Exception as ve:
                logger.warning(f"‚ö†Ô∏è Schema mismatch (Attempt {attempt}): {ve}")
                continue

        except Exception as e:
            logger.error(f"üí• Attempt {attempt} failed: {str(e)}")
            time.sleep(0.3)

    # --- 6. Emergency Fallback ---
    logger.warning(f"üÜò Using Predefined Emergency Roadmap for {sub_id}")
    return _get_emergency_fallback_plan(
        sub_id, sub_criteria_name, target_level, is_sustain_mode, is_quality_refinement, enabler
    )

# =================================================================
# 2. Key Normalizer: ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤ LLM ‡∏û‡πà‡∏ô Key ‡πÑ‡∏°‡πà‡∏ô‡∏¥‡πà‡∏á
# =================================================================
def action_plan_normalize_keys(obj: Any) -> Any:
    if isinstance(obj, list): return [action_plan_normalize_keys(i) for i in obj]
    if isinstance(obj, dict):
        field_mapping = {
            'phase': 'phase', 'goal': 'goal', 'actions': 'actions',
            'statementid': 'statement_id', 'statement_id': 'statement_id',
            'failedlevel': 'failed_level', 'failed_level': 'failed_level',
            'recommendation': 'recommendation',
            'targetevidencetype': 'target_evidence_type', 'target_evidence_type': 'target_evidence_type',
            'keymetric': 'key_metric', 'key_metric': 'key_metric',
            'steps': 'steps', 'step': 'step', 
            'description': 'description', 'responsible': 'responsible',
            'toolstemplates': 'tools_templates', 'tools_templates': 'tools_templates',
            'verificationoutcome': 'verification_outcome', 'verification_outcome': 'verification_outcome'
        }
        
        new_obj = {}
        for k, v in obj.items():
            # ‡∏Å‡∏ß‡∏≤‡∏î‡∏•‡πâ‡∏≤‡∏á Key ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÅ‡∏ï‡πà‡∏ï‡∏±‡∏ß‡∏û‡∏¥‡∏°‡∏û‡πå‡πÄ‡∏•‡πá‡∏Å‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö
            k_raw = str(k).lower().replace(' ', '').replace('_', '').strip()
            k_raw = re.sub(r'[^a-z0-9]', '', k_raw)
            
            target_key = field_mapping.get(k_raw) or k_raw
            
            # Enforcement: ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö Integer ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Level ‡πÅ‡∏•‡∏∞ Step
            if target_key in ['failed_level', 'step']:
                try:
                    if isinstance(v, (int, float)): v = int(v)
                    else:
                        nums = re.findall(r'\d+', str(v))
                        v = int(nums[0]) if nums else 0
                except: v = 0
            
            new_obj[target_key] = action_plan_normalize_keys(v)
        return new_obj
    return obj

# =================================================================
# 3. JSON Extractor: ‡∏£‡∏∞‡∏ö‡∏ö‡∏Å‡∏π‡πâ‡∏Ñ‡∏∑‡∏ô JSON ‡∏ó‡∏µ‡πà‡∏û‡∏±‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏à‡∏ô‡πÑ‡∏°‡πà‡∏à‡∏ö
# =================================================================
def _get_emergency_fallback_plan(
    sub_id: str, 
    sub_criteria_name: str, 
    target_level: int, 
    is_sustain_mode: bool, 
    is_quality_refinement: bool, 
    enabler: str = "KM"
) -> List[Dict[str, Any]]:
    """
    [SAFEGUARD] ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏Å‡∏£‡∏ì‡∏µ LLM ‡∏û‡∏•‡∏≤‡∏î ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡πÑ‡∏°‡πà Error
    """
    responsible_party = f"‡∏Ñ‡∏ì‡∏∞‡∏ó‡∏≥‡∏á‡∏≤‡∏ô {enabler} / ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô‡πÄ‡∏à‡πâ‡∏≤‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á"
    
    title = "‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏¥‡∏î‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á (Gap Remediation)"
    rec = f"‡πÄ‡∏£‡πà‡∏á‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå {sub_criteria_name} ‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡∏ï‡∏≤‡∏°‡∏ß‡∏á‡∏à‡∏£ PDCA"
    
    if is_sustain_mode:
        title = "‡πÅ‡∏ú‡∏ô‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô (Sustainability Plan)"
        rec = f"‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏•‡∏¥‡∏®‡πÉ‡∏ô‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠ {sub_criteria_name} ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡πâ‡∏ô‡πÅ‡∏ö‡∏ö‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°"
    elif is_quality_refinement:
        title = "‡πÅ‡∏ú‡∏ô‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏Ç‡πá‡∏á‡πÅ‡∏Å‡∏£‡πà‡∏á (Quality Enhancement)"
        rec = f"‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÄ‡∏ä‡∏¥‡∏á‡∏õ‡∏£‡∏∞‡∏à‡∏±‡∏Å‡∏©‡πå‡∏Ç‡∏≠‡∏á {sub_criteria_name} ‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏¢‡∏¥‡πà‡∏á‡∏Ç‡∏∂‡πâ‡∏ô"
    
    return [{
        "phase": f"Phase 1: {title}",
        "goal": f"‡∏¢‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô {sub_criteria_name} ({enabler}) ‡∏™‡∏π‡πà‡∏£‡∏∞‡∏î‡∏±‡∏ö {target_level}",
        "actions": [{
            "statement_id": sub_id, 
            "failed_level": target_level,
            "recommendation": rec, 
            "target_evidence_type": "Evidence Package / ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•",
            "key_metric": "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏£‡∏∞‡∏î‡∏±‡∏ö‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ 100%",
            "steps": [
                {
                    "step": 1, 
                    "description": f"‡∏ó‡∏ö‡∏ó‡∏ß‡∏ô‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô ‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏ó‡∏≥‡πÅ‡∏ú‡∏ô‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏Å‡πá‡∏ö‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô", 
                    "responsible": responsible_party, 
                    "tools_templates": "SE-AM Gap Analysis Template", 
                    "verification_outcome": "remediation_plan_report.pdf"
                },
                {
                    "step": 2, 
                    "description": "‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏´‡πâ‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡∏Ç‡∏≤‡∏î‡∏´‡∏≤‡∏¢ (PDCA)", 
                    "responsible": responsible_party, 
                    "tools_templates": "Checklist ‡πÄ‡∏Å‡∏ì‡∏ë‡πå SE-AM", 
                    "verification_outcome": "evidence_package_final.pdf"
                }
            ]
        }]
    }]

def _extract_json_array_for_action_plan(text: Any, logger: logging.Logger) -> List[Dict[str, Any]]:
    """
    ‡∏£‡∏∞‡∏ö‡∏ö‡∏Å‡∏π‡πâ‡∏Ñ‡∏∑‡∏ô JSON ‡∏ó‡∏µ‡πà‡∏û‡∏±‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏à‡∏ô‡πÑ‡∏°‡πà‡∏à‡∏ö (Robust JSON Extractor)
    """
    try:
        if not isinstance(text, str): text = str(text) if text is not None else ""
        if not text.strip(): return []

        # 1. ‡∏•‡∏ö Markdown tags
        clean_text = re.sub(r'```(?:json)?\s*([\s\S]*?)\s*```', r'\1', text, flags=re.IGNORECASE).strip()

        # 2. ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï JSON
        start_idx = clean_text.find('[')
        end_idx = clean_text.rfind(']')

        if start_idx == -1:
            # ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡πÅ‡∏ö‡∏ö Object ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß {} ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡∏£‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢ []
            start_idx = clean_text.find('{')
            end_idx = clean_text.rfind('}')
            if start_idx == -1: return []
            json_candidate = f"[{clean_text[start_idx:end_idx + 1]}]"
        else:
            json_candidate = clean_text[start_idx:end_idx + 1]

        # 3. ‡∏•‡πâ‡∏≤‡∏á Control characters ‡πÅ‡∏•‡∏∞ Trailing commas
        json_candidate = "".join(char for char in json_candidate if ord(char) >= 32 or char in "\n\r\t")
        json_candidate = re.sub(r',\s*([\]}])', r'\1', json_candidate) # ‡∏•‡∏ö comma ‡∏ï‡∏±‡∏ß‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢

        def try_parse(content):
            try:
                data = json5.loads(content)
                return data if isinstance(data, list) else [data]
            except: return None

        # 4. ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏° Parse ‡πÅ‡∏•‡∏∞‡∏ã‡πà‡∏≠‡∏°‡πÅ‡∏ã‡∏°‡∏Å‡∏£‡∏ì‡∏µ‡∏ï‡∏±‡∏î‡∏à‡∏ö
        result = try_parse(json_candidate)
        if not result:
            # ‡∏Å‡∏£‡∏ì‡∏µ LLM ‡∏û‡πà‡∏ô‡∏°‡∏≤‡πÑ‡∏°‡πà‡∏à‡∏ö ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏ß‡∏á‡πÄ‡∏•‡πá‡∏ö‡∏õ‡∏¥‡∏î‡πÉ‡∏´‡πâ
            for suffix in ["]", "}", "}]", "}\n]"]:
                result = try_parse(json_candidate + suffix)
                if result: break

        return result or []
    except Exception as e:
        logger.error(f"JSON Extraction failed: {str(e)}")
        return []
