"""
llm_data_utils.py
Robust LLM + RAG utilities for SEAM assessment (CLEAN FINAL VERSION)
"""

import logging
import time
import json
import hashlib
import uuid
import re
from datetime import datetime
from typing import List, Dict, Any, Optional, Union, Callable, TypeVar, Set
import json5
from utils.enabler_keyword_map import ENABLER_KEYWORD_MAP, DEFAULT_KEYWORDS
from langchain.retrievers import EnsembleRetriever
from langchain_core.documents import Document
from langchain_community.retrievers import BM25Retriever # FIX: Import BM25 ‡∏à‡∏≤‡∏Å community


# Optional: regex ‡πÅ‡∏ó‡∏ô re (‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤) ‚Äî ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡πá‡πÉ‡∏ä‡πâ re ‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤
try:
    import regex as re  # type: ignore
except ImportError:
    pass  # ‡πÉ‡∏ä‡πâ re ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ

logger = logging.getLogger(__name__)
if not logger.handlers:
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# ===================================================================
# 1. Core Configuration (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô)
# ===================================================================
from config.global_vars import (
    DEFAULT_ENABLER,
    INITIAL_TOP_K,
    FINAL_K_RERANKED,
    MAX_EVAL_CONTEXT_LENGTH,
    USE_HYBRID_SEARCH, 
    HYBRID_VECTOR_WEIGHT, 
    HYBRID_BM25_WEIGHT
)

# ===================================================================
# 2. Critical Utilities (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏à‡∏£‡∏¥‡∏á ‚Äî ‡πÑ‡∏°‡πà‡∏°‡∏µ fallback)
# ===================================================================
# üéØ FIX 1: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô Import ‡∏à‡∏≤‡∏Å _get_collection_name ‡πÑ‡∏õ‡πÄ‡∏õ‡πá‡∏ô get_doc_type_collection_key
from core.vectorstore import get_hf_embeddings
from utils.path_utils import get_doc_type_collection_key # <--- ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà‡∏à‡∏≤‡∏Å utils/path_utils
from core.json_extractor import (
    _robust_extract_json,
    _normalize_keys,
    _safe_int_parse,
    _extract_normalized_dict
)

# ===================================================================
# 3. Project Modules (‡∏ñ‡πâ‡∏≤‡∏´‡∏≤‡∏¢ ‚Üí ‡∏õ‡∏•‡πà‡∏≠‡∏¢‡πÉ‡∏´‡πâ error ‡∏ä‡∏±‡∏î ‡πÜ ‡πÑ‡∏õ‡πÄ‡∏•‡∏¢ ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏á‡∏µ‡∏¢‡∏ö)
# ===================================================================
from core.seam_prompts import (
    SYSTEM_ASSESSMENT_PROMPT,
    USER_ASSESSMENT_PROMPT,
    SYSTEM_ACTION_PLAN_PROMPT,
    ACTION_PLAN_PROMPT,
    SYSTEM_EVIDENCE_DESCRIPTION_PROMPT,
    EVIDENCE_DESCRIPTION_PROMPT,
    SYSTEM_LOW_LEVEL_PROMPT,
    USER_LOW_LEVEL_PROMPT,
    USER_LOW_LEVEL_PROMPT_TEMPLATE
)

from core.vectorstore import VectorStoreManager, get_global_reranker, ChromaRetriever
from core.assessment_schema import CombinedAssessment, EvidenceSummary
from core.action_plan_schema import ActionPlanActions

try:
    from core.assessment_schema import StatementAssessment
except ImportError:
    from pydantic import BaseModel
    class StatementAssessment(BaseModel):
        score: int = 0
        reason: str = ""

from langchain_core.documents import Document as LcDocument

# ===================================================================
# 4. Constants
# ===================================================================
LOW_LEVEL_K: int = 3
_MOCK_FLAG = False
_MAX_LLM_RETRIES = 3

def set_mock_control_mode(enable: bool):
    global _MOCK_FLAG
    _MOCK_FLAG = bool(enable)
    logger.info(f"Mock control mode: {_MOCK_FLAG}")

# Helper: ‡∏™‡∏£‡πâ‡∏≤‡∏á Chroma where filter
def _create_where_filter(stable_doc_ids: Optional[Set[str]] = None, 
                         subject: Optional[str] = None,
                         sub_topic: Optional[str] = None) -> Dict[str, Any]:
    """‡∏™‡∏£‡πâ‡∏≤‡∏á where filter ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ChromaDB ‡∏ó‡∏µ‡πà‡πÅ‡∏Ç‡πá‡∏á‡πÅ‡∏Å‡∏£‡πà‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î"""
    filters = []
    
    if stable_doc_ids:
        filters.append({"stable_doc_uuid": {"$in": list(stable_doc_ids)}})
    
    if subject:
        cleaned = subject.strip()
        if cleaned:
            filters.append({"subject": cleaned})
    
    if sub_topic:
        filters.append({"sub_topic": {"$eq": sub_topic}})
    
    if len(filters) > 1:
        return {"$and": filters}
    elif filters:
        return filters[0]
    else:
        return {}

def retrieve_context_for_endpoint(
    vectorstore_manager,
    query: str = "",
    tenant: Optional[str] = None,
    year: Optional[Union[int, str]] = None,
    stable_doc_ids: Optional[Set[str]] = None,
    doc_type: Optional[str] = None,
    enabler: Optional[str] = None,
    subject: Optional[str] = None,
    sub_topic: Optional[str] = None,  # ‡πÉ‡∏´‡∏°‡πà: ‡πÄ‡∏ä‡πà‡∏ô "KM-4.1"
    k_to_retrieve: int = INITIAL_TOP_K,
    k_to_rerank: int = FINAL_K_RERANKED,
) -> Dict[str, Any]:
    """
    ‡∏î‡∏∂‡∏á context ‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß (stable_doc_ids) ‡∏´‡∏£‡∏∑‡∏≠ filter ‡πÅ‡∏°‡πà‡∏ô ‡πÜ
    ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö sub_topic ‡πÄ‡∏ä‡πà‡∏ô "KM-4.1" ‚Üí ‡πÅ‡∏°‡πà‡∏ô‡∏™‡∏∏‡∏î
    """
    start_time = time.time()
    vsm = vectorstore_manager

    # 1. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î collection ‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤ doc_type ‡πÄ‡∏õ‡πá‡∏ô List/String Literal
    
    clean_doc_type = doc_type or 'seam'
    
    # üí° FIX A: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÅ‡∏õ‡∏•‡∏á String Literal ‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏à‡∏≤‡∏Å curl ‡πÄ‡∏ä‡πà‡∏ô '["seam"]'
    if isinstance(clean_doc_type, str) and clean_doc_type.strip().startswith('['):
        try:
            # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏õ‡πá‡∏ô JSON Array
            parsed_list = json.loads(clean_doc_type.strip())
            
            if isinstance(parsed_list, (list, tuple)) and parsed_list:
                # ‡∏ñ‡πâ‡∏≤‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏î‡πâ‡πÄ‡∏õ‡πá‡∏ô List/Tuple ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å
                clean_doc_type = parsed_list[0]
            elif isinstance(parsed_list, str):
                # ‡∏ñ‡πâ‡∏≤‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏î‡πâ‡πÄ‡∏õ‡πá‡∏ô String (‡∏≠‡∏≤‡∏à‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏î‡πâ)
                clean_doc_type = parsed_list
                
        except json.JSONDecodeError:
            # ‡∏ñ‡πâ‡∏≤‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏õ‡πÉ‡∏ä‡πâ String ‡πÄ‡∏î‡∏¥‡∏°
            logger.debug(f"Could not parse doc_type string literal: {clean_doc_type}")
            pass

    # üí° FIX B: ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏±‡∏ö List/Tuple ‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡∏õ‡∏Å‡∏ï‡∏¥ (‡∏Å‡∏£‡∏ì‡∏µ Router ‡∏™‡πà‡∏á‡∏°‡∏≤‡∏ñ‡∏π‡∏Å ‡∏´‡∏£‡∏∑‡∏≠‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£ Parse JSON)
    if isinstance(clean_doc_type, (list, tuple)):
        # ‡πÉ‡∏ä‡πâ element ‡πÅ‡∏£‡∏Å‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
        clean_doc_type = str(clean_doc_type[0]) if clean_doc_type else 'seam'
    elif not isinstance(clean_doc_type, str):
        # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô String
        clean_doc_type = str(clean_doc_type)

    # üí° FIX C: ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î Quote ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏ï‡∏¥‡∏î‡∏°‡∏≤ (‡πÄ‡∏ä‡πà‡∏ô 'seam' ‡∏´‡∏£‡∏∑‡∏≠ "seam")
    clean_doc_type = str(clean_doc_type).strip().strip("'\"")

    # üéØ ‡πÉ‡∏ä‡πâ get_doc_type_collection_key ‡∏à‡∏≤‡∏Å utils/path_utils.py
    collection_name = get_doc_type_collection_key(
        doc_type=clean_doc_type, 
        enabler=enabler
    )

    chroma = vsm._load_chroma_instance(collection_name)
    if not chroma:
        # üìå NOTE: ‡πÉ‡∏ä‡πâ clean_doc_type ‡πÉ‡∏ô Log ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
        logger.error(f"Collection {collection_name} (Doc Type: {clean_doc_type}) not found!")
        return {"top_evidences": [], "aggregated_context": "", "retrieval_time": 0, "used_chunk_uuids": []}

    # 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á filter ‡∏ó‡∏µ‡πà‡πÅ‡∏Ç‡πá‡∏á‡πÅ‡∏Å‡∏£‡πà‡∏á
    where_filter = _create_where_filter(stable_doc_ids, subject, sub_topic)
    logger.info(f"Retrieval ‚Üí Collection: {collection_name} | Filter: {where_filter} | Query: {query[:80]}...")

    # 3. Embed query
    try:
        emb = get_hf_embeddings()
        # BGE-M3 ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ prefix
        query_emb = emb.embed_query(f"query: {query}") 
    except Exception as e:
        logger.error(f"Embedding failed: {e}")
        return {"top_evidences": [], "aggregated_context": "", "retrieval_time": 0, "used_chunk_uuids": []}

    # 4. Query Chroma
    try:
        results = chroma._collection.query(
            query_embeddings=[query_emb],
            n_results=k_to_retrieve,
            where=where_filter if where_filter else None,
            include=["documents", "metadatas", "distances"]
        )
    except Exception as e:
        logger.error(f"Chroma query failed: {e}")
        return {"top_evidences": [], "aggregated_context": "", "retrieval_time": 0, "used_chunk_uuids": []}

    # 5. ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô LcDocument
    raw_chunks: List[LcDocument] = []
    for doc, meta, dist in zip(
        results["documents"][0],
        results["metadatas"][0],
        results["distances"][0]
    ):
        meta["retrieval_distance"] = float(dist)
        raw_chunks.append(LcDocument(page_content=doc, metadata=meta))

    logger.info(f"Raw retrieval: {len(raw_chunks)} chunks")

    # 6. Rerank (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å!)
    final_chunks = raw_chunks
    reranker = get_global_reranker()
    if reranker and len(raw_chunks) > k_to_rerank:
        try:
            reranked = reranker.compress_documents(
                documents=raw_chunks,
                query=query,
                top_n=k_to_rerank
            )
            # ‡∏î‡∏∂‡∏á Document ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤
            final_chunks = [getattr(r, "document", r) for r in reranked]
            logger.info(f"Reranked ‚Üí {len(final_chunks)} chunks")
        except Exception as e:
            logger.warning(f"Reranker failed: {e}")

    # 7. ‡∏™‡∏£‡πâ‡∏≤‡∏á output
    top_evidences = []
    aggregated_parts = []
    used_chunk_uuids = []

    for doc in final_chunks[:k_to_rerank]:
        md = doc.metadata or {}
        text = str(doc.page_content or "").strip()
        if not text:
            continue

        chunk_uuid = md.get("chunk_uuid") or md.get("dedup_chunk_uuid")
        if not chunk_uuid or len(chunk_uuid) < 32:
            continue  # ‡∏Å‡∏£‡∏≠‡∏á TEMP ID

        used_chunk_uuids.append(chunk_uuid)

        top_evidences.append({
            "doc_id": md.get("stable_doc_uuid"),
            "chunk_uuid": chunk_uuid,
            "source": md.get("source") or md.get("filename") or "Unknown",
            "text": text,
            "pdca_tag": md.get("pdca_tag", "Other"),
            "retrieval_distance": md.get("retrieval_distance", 1.0),
            "sub_topic": md.get("sub_topic"),
        })
        aggregated_parts.append(f"[SOURCE: {md.get('source', 'Unknown')}] {text}")

    result = {
        "top_evidences": top_evidences,
        "aggregated_context": "\n\n---\n\n".join(aggregated_parts),
        "retrieval_time": round(time.time() - start_time, 3),
        "used_chunk_uuids": used_chunk_uuids
    }
    logger.info(f"Final retrieval: {len(top_evidences)} chunks | Sub-topic: {sub_topic}")
    return result

# ========================
# 2. retrieve_context_by_doc_ids (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö hydration ‡πÉ‡∏ô router)
# ========================
def retrieve_context_by_doc_ids(
    doc_uuids: List[str],
    doc_type: str,
    enabler: Optional[str] = None,
    vectorstore_manager = None,
    limit: int = 50,
    tenant: Optional[str] = None, # <-- ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ
    year: Optional[Union[int, str]] = None, # <-- ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ (‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î)
) -> Dict[str, Any]:
    """
    ‡∏î‡∏∂‡∏á chunks ‡∏à‡∏≤‡∏Å stable_doc_uuid ‡∏´‡∏•‡∏≤‡∏¢‡∏ï‡∏±‡∏ß (‡πÉ‡∏ä‡πâ‡∏ï‡∏≠‡∏ô hydration sources)
    """
    start_time = time.time()
    vsm = vectorstore_manager or VectorStoreManager()
    
    # üéØ FIX: ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á collection_name ‡∏î‡πâ‡∏ß‡∏¢‡∏ï‡∏ô‡πÄ‡∏≠‡∏á
    # collection_name = f"{doc_type}"
    # if enabler and enabler != DEFAULT_ENABLER:
    #     collection_name = f"{doc_type}_{enabler.lower()}"
    
    # üü¢ ‡πÉ‡∏ä‡πâ get_doc_type_collection_key ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡πà‡∏≥‡πÄ‡∏™‡∏°‡∏≠‡πÅ‡∏•‡∏∞‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
    collection_name = get_doc_type_collection_key(doc_type=doc_type, enabler=enabler)

    chroma = vsm._load_chroma_instance(collection_name)
    if not chroma:
        logger.error(f"Collection {collection_name} not found for hydration")
        return {"top_evidences": []}

    if not doc_uuids:
        return {"top_evidences": []}

    logger.info(f"Hydration ‚Üí {len(doc_uuids)} doc IDs from {collection_name}")

    try:
        # ‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡πâ‡∏ß‡∏¢ stable_doc_uuid ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö hydration ‡∏ô‡∏±‡πâ‡∏ô‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß
        results = chroma._collection.get(
            where={"stable_doc_uuid": {"$in": doc_uuids}},
            limit=limit,
            include=["documents", "metadatas"]
        )
    except Exception as e:
        logger.error(f"Hydration query failed: {e}")
        return {"top_evidences": []}

    evidences = []
    for doc, meta in zip(results["documents"], results["metadatas"]):
        if not doc.strip():
            continue
        evidences.append({
            "doc_id": meta.get("stable_doc_uuid"),
            "chunk_uuid": meta.get("chunk_uuid"),
            "source": meta.get("source") or meta.get("filename") or "Unknown",
            "text": doc,
            "pdca_tag": meta.get("pdca_tag", "Other"),
        })

    logger.info(f"Hydration success: {len(evidences)} chunks from {len(doc_uuids)} docs")
    return {"top_evidences": evidences}

# ------------------------
# Retrieval: retrieve_context_with_filter (‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û + Logger Fix)
# ------------------------
def retrieve_context_with_filter(
    query: Union[str, List[str]],
    doc_type: str,
    tenant: Optional[str] = None,
    year: Optional[Union[int, str]] = None,
    enabler: Optional[str] = None,
    subject: Optional[str] = None,
    # ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô Instance ‡∏Ç‡∏≠‡∏á Manager ‡∏ó‡∏µ‡πà‡∏°‡∏µ create_hybrid_retriever
    vectorstore_manager: Optional['VectorStoreManager'] = None, 
    mapped_uuids: Optional[List[str]] = None,
    stable_doc_ids: Optional[List[str]] = None,
    priority_docs_input: Optional[List[Any]] = None,
    sequential_chunk_uuids: Optional[List[str]] = None,
    sub_id: Optional[str] = None,
    level: Optional[int] = None,
    get_previous_level_docs: Optional[Callable[[int, str], List[Any]]] = None,
) -> Dict[str, Any]:
    """
    ‡∏î‡∏∂‡∏á context ‡∏î‡πâ‡∏ß‡∏¢ semantic search + priority + fallback + rerank
    ‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏•‡πâ‡∏ß: ‡πÉ‡∏ä‡πâ Hybrid Search (BM25 + Vector) ‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞ Cache ‡πÑ‡∏ß‡πâ‡πÉ‡∏ô Manager
    """
    start_time = time.time()
    all_retrieved_chunks: List[Any] = []
    used_chunk_uuids: List[str] = []

    # 1. ‡πÉ‡∏ä‡πâ VectorStoreManager ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤ VectorStoreManager() ‡∏°‡∏µ Logic ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Initialise Chroma Client (self._client)
    manager = vectorstore_manager or VectorStoreManager() 
    if manager is None or not hasattr(manager, '_client') or manager._client is None:
        logger.error("VectorStoreManager not initialized or _client is missing!")
        return {"top_evidences": [], "aggregated_context": "", "retrieval_time": 0.0, "used_chunk_uuids": []}

    # üü¢ NEW FIX: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏Å‡∏≥‡∏´‡∏ô‡∏î Logger ‡πÉ‡∏´‡πâ‡∏Å‡∏±‡∏ö VSM Instance
    # ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏°‡∏ò‡∏≠‡∏î‡∏†‡∏≤‡∏¢‡πÉ‡∏ô (‡πÄ‡∏ä‡πà‡∏ô create_hybrid_retriever) ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ self.logger ‡πÑ‡∏î‡πâ
    if not hasattr(manager, 'logger') or manager.logger is None:
        try:
            manager.logger = logger # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î logger ‡∏Ç‡∏≠‡∏á module ‡∏ô‡∏µ‡πâ‡πÉ‡∏´‡πâ VSM
            logger.info("Assigned module logger to VectorStoreManager instance (Worker/Fallback Fix).")
        except NameError:
            # ‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà logger ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å import ‡πÉ‡∏ô module ‡∏ô‡∏µ‡πâ (‡∏ã‡∏∂‡πà‡∏á‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô)
            pass

    queries_to_run = [query] if isinstance(query, str) else list(query or [])
    if not queries_to_run:
        queries_to_run = [""]

    # ‡∏£‡∏ß‡∏° chunk ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÇ‡∏ú‡∏•‡πà (sequential)
    if sequential_chunk_uuids:
        mapped_uuids = (mapped_uuids or []) + sequential_chunk_uuids

    # 2. Fallback ‡∏à‡∏≤‡∏Å level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Level 3)
    fallback_chunks = []
    if level == 3 and callable(get_previous_level_docs):
        try:
            fallback_chunks = get_previous_level_docs(level - 1, sub_id) or []
            logger.info(f"Fallback from previous level: {len(fallback_chunks)} chunks")
        except Exception as e:
            logger.warning(f"Fallback failed: {e}")

    # 3. Priority chunks (‡∏à‡∏≤‡∏Å evidence mapping)
    guaranteed_priority_chunks = []
    if priority_docs_input:
        for doc in priority_docs_input:
            if doc is None:
                continue
            if isinstance(doc, dict):
                pc = doc.get('page_content') or doc.get('text') or ''
                meta = doc.get('metadata') or {}
                if 'chunk_uuid' in doc:
                    meta['chunk_uuid'] = doc['chunk_uuid']
                if 'doc_id' in doc:
                    meta['stable_doc_uuid'] = doc['doc_id']
                if 'pdca_tag' in doc:
                    meta['pdca_tag'] = doc['pdca_tag']
                if pc.strip():
                    guaranteed_priority_chunks.append(LcDocument(page_content=pc, metadata=meta))
            elif hasattr(doc, 'page_content'):
                guaranteed_priority_chunks.append(doc)

    # 4. Collection name
    collection_name = get_doc_type_collection_key(doc_type, enabler or "KM")
    logger.info(f"Requesting retriever ‚Üí collection='{collection_name}' (doc_type={doc_type}, enabler={enabler})")

    # 5. ‡∏™‡∏£‡πâ‡∏≤‡∏á Filter
    where_filter: Dict[str, Any] = {}
    if stable_doc_ids:
        logger.info(f"Applying Stable Doc ID filter: {len(stable_doc_ids)} IDs")
        where_filter = {"stable_doc_uuid": {"$in": stable_doc_ids}}

    if subject:
        subject_filter = {"subject": {"$eq": subject}}
        if where_filter:
            where_filter = {"$and": [where_filter, subject_filter]}
            logger.info(f"Adding Subject filter (AND logic): {subject}")
        else:
            where_filter = subject_filter

    # === HYBRID SEARCH MODE (BM25 + Vector) ===
    hybrid_retriever = None

    if USE_HYBRID_SEARCH:
        try:
            # üéØ FIX: ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ Manager ‡∏ó‡∏µ‡πà Cache Hybrid Retriever ‡πÑ‡∏ß‡πâ‡πÅ‡∏•‡πâ‡∏ß
            logger.info(f"Requesting Hybrid Retriever from Manager for {collection_name} (Cached)...")
            hybrid_retriever = manager.create_hybrid_retriever(collection_name=collection_name)
            logger.info(f"HYBRID mode activated: Vector 70% + BM25 30% for {collection_name} (Cached)")

        except Exception as e:
            # Fallback ‡∏´‡∏≤‡∏Å Manager ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á Hybrid Retriever ‡πÑ‡∏î‡πâ (‡πÄ‡∏ä‡πà‡∏ô ‡πÑ‡∏°‡πà‡∏°‡∏µ BM25 Index)
            # üö® BUG: ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Error ‡∏à‡∏≤‡∏Å VSM ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ Logger (‡πÅ‡∏ï‡πà‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏•‡πâ‡∏ß)
            logger.warning(f"Hybrid mode failed (Error calling manager.create_hybrid_retriever: {e}), falling back to vector only")
            use_hybrid = False

    # 6. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Retriever ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ
    if USE_HYBRID_SEARCH and hybrid_retriever:
        retriever = hybrid_retriever
    else:
        # ‡πÉ‡∏ä‡πâ Vector Retriever ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
        retriever = manager.get_retriever(collection_name)
        logger.info("Using VECTOR ONLY mode.")

    # 7. ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å VectorStore
    retrieved_chunks = []
    if retriever:
        for q in queries_to_run:
            q_log = q[:120] + "..." if len(q) > 120 else q
            logger.critical(f"[QUERY] Running: '{q_log}' ‚Üí collection='{collection_name}'")

            try:
                search_kwargs = {"k": INITIAL_TOP_K}  # INITIAL_TOP_K
                if where_filter:
                    search_kwargs["where"] = where_filter
                
                # üéØ FIX: ‡πÉ‡∏ä‡πâ get_relevant_documents ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ö **search_kwargs ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡∏à‡∏∞‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡∏Å‡∏ß‡πà‡∏≤
                if hasattr(retriever, "get_relevant_documents"):
                    # EnsembleRetriever ‡πÅ‡∏•‡∏∞ ChromaRetriever ‡∏°‡∏±‡∏Å‡∏à‡∏∞‡∏£‡∏±‡∏ö kwargs ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
                    docs = retriever.get_relevant_documents(q, **search_kwargs)
                elif hasattr(retriever, "invoke"):
                    # Fallback ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LangChain Runnable API 
                    docs = retriever.invoke(q, config={"configurable": {"search_kwargs": search_kwargs}})
                else:
                    docs = []
                    
                retrieved_chunks.extend(docs or [])
            except Exception as e:
                logger.error(f"Retriever invoke failed: {e}", exc_info=True)
    else:
        logger.error(f"Retriever NOT FOUND for collection: {collection_name}")

    logger.critical(f"[RETRIEVAL] Raw chunks from ChromaDB: {len(retrieved_chunks)} documents")

    # 8. ‡∏£‡∏ß‡∏° + deduplicate
    all_chunks = retrieved_chunks + fallback_chunks + guaranteed_priority_chunks
    unique_map: Dict[str, LcDocument] = {}

    for doc in all_chunks:
        if not doc or not hasattr(doc, "page_content"):
            continue
        md = getattr(doc, "metadata", {}) or {}
        pc = str(getattr(doc, "page_content", "") or "").strip()
        if not pc:
            continue

        if level == 3:
            pc = pc[:500]
            doc.page_content = pc

        chunk_uuid = md.get("chunk_uuid") or md.get("stable_doc_uuid") or f"TEMP-{uuid.uuid4().hex[:12]}"
        if chunk_uuid not in unique_map:
            md["dedup_chunk_uuid"] = chunk_uuid
            unique_map[chunk_uuid] = doc

    dedup_chunks = list(unique_map.values())
    logger.info(f"After dedup: {len(dedup_chunks)} chunks")

    # 9. Rerank
    final_docs = list(guaranteed_priority_chunks)
    slots_left = max(0, 12 - len(final_docs))  # FINAL_K_RERANKED
    candidates = [d for d in dedup_chunks if d not in final_docs]

    # Patch metadata ‡∏ó‡∏µ‡πà‡∏´‡∏≤‡∏¢‡πÑ‡∏õ‡∏´‡∏•‡∏±‡∏á rerank
    candidate_metadata_map = {
        doc.page_content: getattr(doc, 'metadata', {})
        for doc in candidates if hasattr(doc, 'page_content') and doc.page_content.strip()
    }

    if slots_left > 0 and candidates:
        reranker = get_global_reranker()
        if reranker and hasattr(reranker, "compress_documents"):
            try:
                reranked_results = reranker.compress_documents(
                    documents=candidates,
                    query=queries_to_run[0],
                    top_n=slots_left
                )
                for result in reranked_results:
                    doc_to_add = getattr(result, 'document', result)
                    if doc_to_add and hasattr(doc_to_add, 'page_content') and doc_to_add.page_content.strip():
                        current_md = getattr(doc_to_add, 'metadata', {})
                        if not current_md.get("chunk_uuid") and doc_to_add.page_content in candidate_metadata_map:
                            doc_to_add.metadata = candidate_metadata_map[doc_to_add.page_content]
                        final_docs.append(doc_to_add)
                logger.info(f"Reranker returned {len(reranked_results)} docs")
            except Exception as e:
                logger.warning(f"Reranker failed ({e}), using raw candidates")
                final_docs.extend(candidates[:slots_left])
        else:
            final_docs.extend(candidates[:slots_left])
    else:
        logger.info("No slots left or no candidates ‚Üí priority only")

    # 10. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
    top_evidences = []
    aggregated_parts = []
    used_chunk_uuids = []

    VALID_CHUNK_ID = re.compile(r"^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$|^[0-9a-f]{64}(-[0-9]+)?$", re.IGNORECASE)
    VALID_STABLE_ID = re.compile(r"^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$|^[0-9a-f]{64}$", re.IGNORECASE)

    for doc in final_docs[:12]:
        md = getattr(doc, "metadata", {}) or {}
        pc = str(getattr(doc, "page_content", "") or "").strip()
        if not pc:
            continue

        chunk_uuid = md.get("chunk_uuid") or md.get("dedup_chunk_uuid") or md.get("id")
        stable_doc_uuid = md.get("stable_doc_uuid") or md.get("source_doc_id")

        primary_id = None
        if stable_doc_uuid and VALID_STABLE_ID.match(str(stable_doc_uuid)):
            primary_id = stable_doc_uuid
        elif chunk_uuid and VALID_CHUNK_ID.match(str(chunk_uuid)):
            primary_id = chunk_uuid
        else:
            logger.warning(f"Chunk has no valid ID! Stable: {stable_doc_uuid}, Chunk: {chunk_uuid}")
            primary_id = f"TEMP-{uuid.uuid4().hex[:8]}"

        if not str(primary_id).startswith("TEMP-"):
            used_chunk_uuids.append(str(primary_id))

        source = md.get("source_filename") or md.get("source") or md.get("filename") or "Unknown File"
        pdca = md.get("pdca_tag", "Other")
        rerank_score = float(md.get("_rerank_score_force") or md.get("relevance_score") or 0.0)

        top_evidences.append({
            "doc_id": stable_doc_uuid or primary_id,
            "chunk_uuid": chunk_uuid or primary_id,
            "stable_doc_uuid": stable_doc_uuid,
            "source": source,
            "source_filename": source,
            "text": pc,
            "pdca_tag": pdca,
            "rerank_score": rerank_score,
        })
        aggregated_parts.append(f"[{pdca}] [SOURCE: {source}] {pc}")

    result = {
        "top_evidences": top_evidences,
        "aggregated_context": "\n\n---\n\n".join(aggregated_parts) if aggregated_parts else "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á",
        "retrieval_time": round(time.time() - start_time, 3),
        "used_chunk_uuids": used_chunk_uuids
    }

    logger.info(f"Final retrieval L{level or '?'} {sub_id or ''}: {len(top_evidences)} chunks in {result['retrieval_time']:.2f}s")
    return result

def retrieve_context_for_low_levels(query: str, doc_type: str, enabler: Optional[str]=None,
                                 vectorstore_manager: Optional['VectorStoreManager']=None,
                                 top_k: int=LOW_LEVEL_K, initial_k: int=INITIAL_TOP_K,
                                 # üü¢ NEW: ‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠ arguments
                                 mapped_uuids: Optional[List[str]]=None,
                                 priority_docs_input: Optional[List[Any]] = None,
                                 sequential_chunk_uuids: Optional[List[str]] = None, 
                                 sub_id: Optional[str]=None, level: Optional[int]=None) -> Dict[str, Any]:
    """
    Retrieves a small, focused context for low levels (L1, L2) using a reduced k (LOW_LEVEL_K).
    """
    # ‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å ‡πÅ‡∏ï‡πà‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏ä‡πâ k ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
    return retrieve_context_with_filter(
        query=query,
        doc_type=doc_type,
        enabler=enabler,
        vectorstore_manager=vectorstore_manager,
        top_k=LOW_LEVEL_K,
        initial_k=initial_k,
        mapped_uuids=mapped_uuids,
        priority_docs_input=priority_docs_input,
        sequential_chunk_uuids=sequential_chunk_uuids, 
        sub_id=sub_id,
        level=level
    )

# ----------------------------------------------------
# Helper function: Summarize evidence list (minimal stub)
# ----------------------------------------------------
def _summarize_evidence_list_short(evidences: list, max_sentences: int = 3) -> str:
    """
    Provides a concise summary of evidence items.
    """
    if not evidences:
        return ""
    
    parts = []
    for ev in evidences[:max(1, min(len(evidences), max_sentences))]:
        if isinstance(ev, dict):
            fn = ev.get("source_filename") or ev.get("source") or ev.get("doc_id", "unknown")
            txt = ev.get("text") or ev.get("content") or ""
        else:
            fn = str(ev)
            txt = str(ev)
        txt_short = txt[:120].replace("\n", " ").strip()
        if txt_short:
            parts.append(f"‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå `{fn}`: {txt_short}...")
        else:
            parts.append(f"‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå `{fn}`")
    return " | ".join(parts)


# ULTIMATE FINAL VERSION: build_multichannel_context_for_level (OPTIMIZED)
# ‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó‡πÉ‡∏´‡∏°‡πà: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏Ñ‡πà BASELINE ‡πÅ‡∏•‡∏∞ AUXILIARY summaries ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
# ----------------------------------------------------
def build_multichannel_context_for_level(
    level: int,
    top_evidences: List[Dict[str, Any]],
    previous_levels_map: Optional[Dict[str, Any]] = None,
    previous_levels_evidence: Optional[List[Dict[str, Any]]] = None, # List ‡∏Ç‡∏≠‡∏á Chunks ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏à‡∏≤‡∏Å Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤
    max_main_context_tokens: int = 3000, 
    max_summary_sentences: int = 4,
    max_context_length: Optional[int] = None, 
    **kwargs
) -> Dict[str, Any]:
    """
    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏±‡∏Å‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Context Summary ‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏°‡∏≤
    ‡πÇ‡∏î‡∏¢‡πÄ‡∏ô‡πâ‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Baseline Summary (‡∏à‡∏≤‡∏Å Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤) ‡πÅ‡∏•‡∏∞ Auxiliary Summary (‡∏à‡∏≤‡∏Å Level ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô)
    """
    logger = logging.getLogger(__name__)
    K_MAIN = 5
    MIN_RELEVANCE_FOR_AUX = 0.4  # ‡∏Å‡∏£‡∏≠‡∏á aux ‡∏ó‡∏µ‡πà‡∏ï‡πà‡∏≥‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ

    # --- 1) Baseline Summary ---
    # ‡πÉ‡∏ä‡πâ List ‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏°‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß (previous_levels_evidence_list) ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å
    baseline_evidence = previous_levels_evidence or [] 

    summarizable_baseline = [
        item for item in baseline_evidence
        if isinstance(item, dict) and (item.get("text") or item.get("content"))
    ]
    
    # üü¢ FIX: ‡∏õ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô baseline
    if not summarizable_baseline:
        baseline_summary = "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏à‡∏≤‡∏Å Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤"
    else:
        baseline_summary = _summarize_evidence_list_short(
            summarizable_baseline,
            max_sentences=max_summary_sentences
        )

    # --- 2) Auxiliary Summary ---
    direct, aux_candidates = [], []

    for ev in top_evidences:
        if not isinstance(ev, dict):
            # ‡∏≠‡∏≤‡∏à‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå
            continue # ‡∏Ç‡πâ‡∏≤‡∏° chunks ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô dict ‡πÑ‡∏õ‡πÄ‡∏•‡∏¢

        # NEW: ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö tag ‡∏ó‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö‡πÄ‡∏ï‡πá‡∏°‡πÅ‡∏•‡∏∞‡∏¢‡πà‡∏≠
        tag = (ev.get("pdca_tag") or ev.get("PDCA") or "Other").upper()
        relevance = ev.get("rerank_score") or ev.get("score", 0.0)

        # PDCA Chunks ‡∏ñ‡∏π‡∏Å‡∏™‡πà‡∏á‡πÑ‡∏õ‡πÄ‡∏õ‡πá‡∏ô Direct Context (‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏ô Engine)
        # ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ô‡∏µ‡πâ‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡πÅ‡∏Ñ‡πà‡πÅ‡∏¢‡∏Å‡πÅ‡∏¢‡∏∞ ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Direct Context
        if tag in {"P", "PLAN", "D", "DO", "C", "CHECK", "A", "ACT"}:
            direct.append(ev)
        elif relevance >= MIN_RELEVANCE_FOR_AUX:  # ‡∏Å‡∏£‡∏≠‡∏á aux ‡∏ó‡∏µ‡πà‡∏ï‡πà‡∏≥‡πÄ‡∏Å‡∏¥‡∏ô
            aux_candidates.append(ev)

    # Logic ‡∏Å‡∏≤‡∏£‡∏¢‡πâ‡∏≤‡∏¢‡∏à‡∏≤‡∏Å aux ‡πÑ‡∏õ direct (K_MAIN) ‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡∏≠‡∏¢‡∏π‡πà (‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏±‡∏î/Debug ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏™‡πà‡∏á Direct ‡∏≠‡∏≠‡∏Å‡πÑ‡∏õ)
    if len(direct) < K_MAIN:
        need = K_MAIN - len(direct)
        direct.extend(aux_candidates[:need])
        aux_candidates = aux_candidates[need:]
        
    if len(direct) < K_MAIN:
        logger.warning(f"L{level}: Direct PDCA chunks ‡∏¢‡∏±‡∏á‡∏ô‡πâ‡∏≠‡∏¢ ({len(direct)}) ‡∏´‡∏•‡∏±‡∏á‡∏¢‡πâ‡∏≤‡∏¢‡∏à‡∏≤‡∏Å aux")

    aux_summary = _summarize_evidence_list_short(aux_candidates, max_sentences=3) if aux_candidates else "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏£‡∏≠‡∏á"

    # --- 3) Return ---
    debug_meta = {
        "level": level,
        "direct_count": len(direct),
        "aux_count": len(aux_candidates),
        # üü¢ FIX: ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏™‡∏£‡∏∏‡∏õ‡∏à‡∏£‡∏¥‡∏á
        "baseline_count": len(summarizable_baseline), 
        "max_context_length_received": max_context_length 
    }
    logger.info(f"Context L{level} ‚Üí Direct:{len(direct)} | Aux:{len(aux_candidates)} | Baseline:{len(summarizable_baseline)}")

    return {
        "baseline_summary": baseline_summary,
        "direct_context": "",  
        "aux_summary": aux_summary,
        "debug_meta": debug_meta,
    }

# ------------------------
# LLM fetcher
# ------------------------
def _fetch_llm_response(
    system_prompt: str, 
    user_prompt: str, 
    max_retries: int = 3,
    llm_executor: Any = None 
) -> str:
    """
    ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å LLM ‡∏ú‡πà‡∏≤‡∏ô LangChain (OllamaChat) ‡∏û‡∏£‡πâ‡∏≠‡∏°:
    - ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö JSON output ‡∏î‡πâ‡∏ß‡∏¢ prompt
    - Log raw response ‡πÄ‡∏ï‡πá‡∏° ‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠ debug
    - Retry + backoff
    - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö mock mode
    """
    global _MOCK_FLAG

    llm = llm_executor
    
    if llm is None and not _MOCK_FLAG: 
        raise ConnectionError("LLM instance not initialized (Missing llm_executor).")

    # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏´‡πâ LLM ‡∏ï‡∏≠‡∏ö JSON ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÅ‡∏°‡πâ model ‡∏à‡∏∞‡∏î‡∏∑‡πâ‡∏≠
    enforced_system_prompt = system_prompt.strip() + (
        "\n\n"
        "RULES ‡∏ó‡∏µ‡πà‡∏´‡πâ‡∏≤‡∏°‡∏•‡∏∞‡πÄ‡∏°‡∏¥‡∏î‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≤‡∏î:\n"
        "- ‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡∏î‡πâ‡∏ß‡∏¢ JSON object ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô\n"
        "- ‡∏´‡πâ‡∏≤‡∏°‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å JSON ‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≤‡∏î\n"
        "- ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏ä‡πâ markdown code block (```)\n"
        "- ‡πÉ‡∏ä‡πâ double quotes ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡∏´‡πâ‡∏≤‡∏° single quote\n"
        "- ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÅ‡∏ô‡πà‡πÉ‡∏à ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö: {\"score\": 0, \"reason\": \"‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠\"}"
    )

    messages = [
        {"role": "system", "content": enforced_system_prompt},
        {"role": "user",   "content": user_prompt}
    ]

    for attempt in range(1, max_retries + 1):
        try:
            if _MOCK_FLAG:
                logger.info(f"[MOCK MODE] Simulating LLM response for attempt {attempt}")
                # ‡∏à‡∏≥‡∏•‡∏≠‡∏á JSON ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
                mock_json = '{"score": 1, "reason": "Mock response - ‡∏°‡∏µ‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô", "is_passed": true, "P_Plan_Score": 1, "D_Do_Score": 1}'
                logger.critical(f"LLM RAW RESPONSE (DEBUG MOCK): {mock_json}")
                return mock_json

            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å LLM ‡∏à‡∏£‡∏¥‡∏á
            response = llm.invoke(messages, config={"temperature": 0.0})
            
            # ‡∏î‡∏∂‡∏á text ‡∏î‡∏¥‡∏ö‡∏≠‡∏≠‡∏Å‡∏°‡∏≤
            raw_text = ""
            if hasattr(response, "content"):
                raw_text = str(response.content)
            elif isinstance(response, str):
                raw_text = str(response)
            elif hasattr(response, "text"):
                raw_text = str(response.text)
            else:
                raw_text = str(response)

            # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ log ‡∏ô‡∏µ‡πâ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤‡∏°‡∏±‡∏ô‡∏ï‡∏≠‡∏ö‡∏≠‡∏∞‡πÑ‡∏£‡∏à‡∏£‡∏¥‡∏á ‡πÜ
            logger.critical(f"LLM RAW RESPONSE (DEBUG): {raw_text[:800]}{'...' if len(raw_text) > 800 else ''}")

            return raw_text.strip()

        except Exception as e:
            logger.error(f"LLM call failed (attempt {attempt}/{max_retries}): {e}")
            if attempt < max_retries:
                time.sleep(2 ** attempt)  # exponential backoff
            else:
                logger.critical("All LLM attempts failed ‚Äì returning safe fallback JSON")
                fallback = '{"score": 0, "reason": "LLM ‡πÑ‡∏°‡πà‡∏ï‡∏≠‡∏ö‡∏™‡∏ô‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏£‡∏±‡πâ‡∏á", "is_passed": false}'
                logger.critical(f"LLM RAW RESPONSE (DEBUG FALLBACK): {fallback}")
                return fallback

    # ‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£‡∏ñ‡∏∂‡∏á‡∏à‡∏∏‡∏î‡∏ô‡∏µ‡πâ ‡πÅ‡∏ï‡πà‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÑ‡∏ß‡πâ
    fallback = '{"score": 0, "reason": "Unknown LLM failure"}'
    return fallback

# ------------------------------------------------------------------
# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ï‡∏±‡∏ß‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡∏°‡πà: ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÅ‡∏•‡∏∞‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤ String/Dict ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å Response
# ------------------------------------------------------------------
def _clean_llm_response_content(resp: Any) -> str:
    """
    ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏î‡∏∂‡∏á content ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö string ‡∏ó‡∏µ‡πà‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
    ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏´‡πà‡∏≠‡∏´‡∏∏‡πâ‡∏°‡πÅ‡∏ö‡∏ö Tuple/List ‡∏ó‡∏µ‡πà‡∏°‡∏µ Dict/String ‡∏≠‡∏¢‡∏π‡πà‡∏†‡∏≤‡∏¢‡πÉ‡∏ô ‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ Regex Cleanup 
    ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ JSON Object
    """
    
    # --- 1. ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô (Existing Logic) ---
    cleaned_resp_str: str = ""

    # 1.1 ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏≤‡∏£‡∏´‡πà‡∏≠‡∏´‡∏∏‡πâ‡∏° (Handle Tuple/List wrapper)
    if isinstance(resp, (list, tuple)) and resp:
        resp = resp[0]
        logger.debug(f"LLM Response was wrapped in {type(resp).__name__}, extracted first element.")

    # 1.2 ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Response Object/Dict ‡∏ó‡∏µ‡πà‡∏°‡∏µ 'content' field
    if hasattr(resp, "content"): 
        cleaned_resp_str = str(resp.content).strip()
    elif isinstance(resp, dict) and "content" in resp: 
        cleaned_resp_str = str(resp["content"]).strip()
    elif isinstance(resp, str): 
        cleaned_resp_str = resp.strip()
    else: 
        # 1.3 Fallback: ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô String
        cleaned_resp_str = str(resp).strip()
    
    # --- 2. ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î Regex (The CRITICAL Fix for Malform) ---
    
    # 2.1 ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏õ‡∏µ‡∏Å‡∏Å‡∏≤ { ... }
    # re.DOTALL: ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ . ‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà‡πÑ‡∏î‡πâ‡πÅ‡∏°‡πâ‡∏Å‡∏£‡∏∞‡∏ó‡∏±‡πà‡∏á‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏Ç‡∏∂‡πâ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÉ‡∏´‡∏°‡πà
    match = re.search(r'\{.*\}', cleaned_resp_str, re.DOTALL)
    
    if match:
        json_string_only = match.group(0)
        logger.debug("Regex Cleanup performed: Extracted pure JSON string.")
        return json_string_only
    
    # 2.2 ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏û‡∏ö JSON Object: ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ String ‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô‡πÑ‡∏õ
    logger.warning("Regex Cleanup failed: Could not find JSON object. Returning original cleaned string.")
    return cleaned_resp_str

# ------------------------
# Evaluation
# ------------------------
T = TypeVar("T", bound=BaseModel)

def _check_and_handle_empty_context(context: str, sub_id: str, level: int) -> Optional[Dict[str, Any]]:
    """
    Returns Failure result if context is empty or contains known error strings.
    Auto-fail with PDCA keys all set to 0.
    """
    if not context or "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á" in context or "ERROR:" in context.upper():
        logger.warning(f"Auto-FAIL L{level} for {sub_id}: Empty or Error Context detected from RAG.")
        context_preview = context.strip()[:100].replace("\n", " ") if context else "Empty Context"
        return {
            "score": 0,
            "reason": f"‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏î‡πâ‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á (Context: {context_preview}).",
            "is_passed": False,
            "P_Plan_Score": 0,
            "D_Do_Score": 0,
            "C_Check_Score": 0,
            "A_Act_Score": 0,
        }
    return None

def _get_context_for_level(context: str, level: int) -> str:
    """Return context string with appropriate length limit for each level."""
    if not context:
        return ""
    # L1-L2 ‡πÉ‡∏ä‡πâ context ‡∏¢‡∏≤‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô
    if level <= 2:
        return context[:6000]  
    # L3-L5 ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÉ‡∏ô global_vars ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î Latency
    return context[:MAX_EVAL_CONTEXT_LENGTH]  

# =========================
# Main Evaluation Function
# =========================

def evaluate_with_llm(
    context: str, 
    sub_criteria_name: str, 
    level: int, 
    statement_text: str, 
    sub_id: str, 
    llm_executor: Any = None, 
    pdca_phase: str = "",
    level_constraint: str = "",
    must_include_keywords: str = "",
    avoid_keywords: str = "",
    max_rerank_score: float = 0.0,
    max_evidence_strength: float = 10.0,
    **kwargs
) -> Dict[str, Any]:
    """Standard Evaluation for L3+ with robust handling."""
    
    # üéØ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: ‡πÉ‡∏ä‡πâ logic ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î Context ‡∏ï‡∏≤‡∏° Level
    context_to_send_eval = _get_context_for_level(context, level)
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö context ‡∏ß‡πà‡∏≤‡∏á
    failure_result = _check_and_handle_empty_context(context, sub_id, level)
    if failure_result:
        return failure_result

    # ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å kwargs (‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÉ‡∏ä‡πâ)
    baseline_summary = kwargs.get("baseline_summary", "")
    aux_summary = kwargs.get("aux_summary", "")

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á User Prompt
    try:
        user_prompt = USER_ASSESSMENT_PROMPT.format(
            sub_criteria_name=sub_criteria_name,
            sub_id=sub_id,
            level=level,
            pdca_phase=pdca_phase,
            statement_text=statement_text,
            context=context_to_send_eval, # ‡πÉ‡∏ä‡πâ Context ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡∏±‡∏î‡πÅ‡∏•‡πâ‡∏ß
            level_constraint=level_constraint,
            must_include_keywords=must_include_keywords or "‡πÑ‡∏°‡πà‡∏°‡∏µ",
            avoid_keywords=avoid_keywords or "‡πÑ‡∏°‡πà‡∏°‡∏µ",
            max_rerank_score=max_rerank_score,
            max_evidence_strength=max_evidence_strength
        )
    except KeyError as e:
        logger.error(f"Missing placeholder in prompt template: {e}")
        user_prompt = f"‡πÄ‡∏Å‡∏ì‡∏ë‡πå: {sub_criteria_name} L{level}\n‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {statement_text}\n‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô: {context_to_send_eval}"

    # ‡πÄ‡∏û‡∏¥‡πà‡∏° summary ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
    if baseline_summary:
        user_prompt += f"\n\n--- Baseline summary (‡∏à‡∏≤‡∏Å Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤): ---\n{baseline_summary}"
    if aux_summary:
        user_prompt += f"\n\n--- Auxiliary evidence summary: ---\n{aux_summary}"

    # System Prompt (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ placeholder)
    try:
        # üéØ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠ key ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö SYSTEM_ASSESSMENT_PROMPT (‡∏Ñ‡∏∑‡∏≠ max_evidence_strength)
        system_prompt = SYSTEM_ASSESSMENT_PROMPT.format(
            max_evidence_strength=max_evidence_strength 
        )
    except KeyError:
        system_prompt = SYSTEM_ASSESSMENT_PROMPT  # fallback ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ

    # ‡πÄ‡∏û‡∏¥‡πà‡∏° schema
    try:
        schema_json = json.dumps(CombinedAssessment.model_json_schema(), ensure_ascii=False, indent=2)
    except Exception:
        schema_json = '{"score":0,"reason":"string"}'

    system_prompt += "\n\n--- JSON SCHEMA ---\n" + schema_json + "\nIMPORTANT: Respond only with valid JSON."

    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å LLM
    try:
        raw = _fetch_llm_response(system_prompt, user_prompt, _MAX_LLM_RETRIES, llm_executor=llm_executor)
        parsed = _robust_extract_json(raw)
        
        if not isinstance(parsed, dict):
            logger.error(f"Parsed result is not dict: {type(parsed)}")
            parsed = {}

        return {
            "score": int(parsed.get("score", 0)),
            "reason": parsed.get("reason", "No reason provided."),
            "is_passed": parsed.get("is_passed", False),
            "P_Plan_Score": int(parsed.get("P_Plan_Score", 0)),
            "D_Do_Score": int(parsed.get("D_Do_Score", 0)),
            "C_Check_Score": int(parsed.get("C_Check_Score", 0)),
            "A_Act_Score": int(parsed.get("A_Act_Score", 0)),
        }

    except Exception as e:
        logger.exception(f"evaluate_with_llm failed for {sub_id} L{level}: {e}")
        return {
            "score": 0,
            "reason": f"LLM error: {str(e)}",
            "is_passed": False,
            "P_Plan_Score": 0,
            "D_Do_Score": 0,
            "C_Check_Score": 0,
            "A_Act_Score": 0,
        }
    
# =========================
# Patch for L1-L2 evaluation
# =========================

# 1Ô∏è‚É£ ‡πÄ‡∏û‡∏¥‡πà‡∏° context limit ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö L1/L2
def _get_context_for_level(context: str, level: int) -> str:
    """Return context string with appropriate length limit for each level."""
    if not context:
        return ""
    if level <= 2:
        return context[:6000]  # L1-L2 ‡πÉ‡∏ä‡πâ context ‡∏¢‡∏≤‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô
    return context[:MAX_EVAL_CONTEXT_LENGTH]  # L3-L5

def _extract_combined_assessment(parsed: Dict[str, Any], score_default_key: str = "score") -> Dict[str, Any]:
    """Helper to safely extract combined assessment results."""
    # üü¢ NEW: Extract all scores needed by seam_assessment.py (Action #1 logic)
    score = int(parsed.get(score_default_key, 0))
    is_passed = parsed.get("is_passed", score >= 1) # ‡πÉ‡∏ä‡πâ score >= 1 ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤ default ‡∏ñ‡πâ‡∏≤ LLM ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏™‡πà‡∏á is_passed

    result = {
        "score": score,
        "reason": parsed.get("reason", "No reason provided by LLM."),
        "is_passed": is_passed,
        "P_Plan_Score": int(parsed.get("P_Plan_Score", 0)),
        "D_Do_Score": int(parsed.get("D_Do_Score", 0)),
        "C_Check_Score": int(parsed.get("C_Check_Score", 0)),
        "A_Act_Score": int(parsed.get("A_Act_Score", 0)),
    }
    return result


# =================================================================
# ULTIMATE PRODUCTION: evaluate_with_llm_low_level (L1/L2 Multi-Enabler)
# =================================================================
def evaluate_with_llm_low_level(
    context: str,
    sub_criteria_name: str,
    level: int,
    statement_text: str,
    sub_id: str,
    llm_executor: Any = None,
    pdca_phase: str = "",
    level_constraint: str = "",
    must_include_keywords: str = "",
    avoid_keywords: str = "",
    max_rerank_score: float = 0.0,
    max_evidence_strength: float = 10.0,
    contextual_rules_map: Optional[Dict[str, Any]] = None,
    enabler_id: str = "KM",
    **kwargs
) -> Dict[str, Any]:
    """
    Standard Evaluation for L1/L2 using LOW_LEVEL_PROMPT (Dynamic Multi-Enabler)
    - ‡∏î‡∏∂‡∏á planning_keywords ‡∏à‡∏≤‡∏Å contextual_rules_map (‡∏à‡∏≤‡∏Å pea_km_contextual_rules.json)
    - ‡πÑ‡∏°‡πà hardcode ‡∏≠‡∏µ‡∏Å‡∏ï‡πà‡∏≠‡πÑ‡∏õ
    - ‡∏™‡πà‡∏á‡∏Ñ‡πà‡∏≤ P/D/C/A ‡∏î‡∏¥‡∏ö‡∏à‡∏≤‡∏Å LLM ‚Üí ‡πÉ‡∏´‡πâ _run_single_assessment ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏Å‡∏é L1/L2 ‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
    """
    
    # -------------------- 1. Setup & Context Check --------------------
    context_to_send_eval = context[:MAX_EVAL_CONTEXT_LENGTH] if context else "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á"
    
    failure_result = _check_and_handle_empty_context(context, sub_id, level)
    if failure_result:
        return failure_result

    # -------------------- 2. ‡∏î‡∏∂‡∏á planning_keywords ‡∏à‡∏≤‡∏Å pea_km_contextual_rules.json --------------------
    planning_keywords = "‡∏ß‡∏¥‡∏™‡∏±‡∏¢‡∏ó‡∏±‡∏®‡∏ô‡πå, ‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢, ‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á, ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢"  # fallback ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô

    if contextual_rules_map:
        # 2.1 ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å sub-criteria ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Å‡πà‡∏≠‡∏ô (‡πÄ‡∏ä‡πà‡∏ô 1.1 ‚Üí L1)
        sub_rules = contextual_rules_map.get(sub_id, {})
        l1_rules = sub_rules.get("L1", {})
        if l1_rules and "planning_keywords" in l1_rules:
            planning_keywords = l1_rules["planning_keywords"]
        else:
            # 2.2 Fallback ‡πÑ‡∏õ‡πÉ‡∏ä‡πâ _enabler_defaults (‡πÄ‡∏ä‡πà‡∏ô KM, DX)
            default_rules = contextual_rules_map.get("_enabler_defaults", {})
            if "planning_keywords" in default_rules:
                planning_keywords = default_rules["planning_keywords"]

    logger.debug(f"[L{level}] Using planning_keywords: {planning_keywords}")

    # -------------------- 3. Prompt Building --------------------
    try:
        # System Prompt: ‡πÉ‡∏™‡πà planning_keywords ‡∏î‡πâ‡∏ß‡∏¢ .format()
        system_prompt = SYSTEM_LOW_LEVEL_PROMPT.format(planning_keywords=planning_keywords)
        system_prompt += "\n\nIMPORTANT: Respond only with valid JSON."

        # User Prompt: ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏™‡πà‡∏á planning_keywords (‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô system ‡πÅ‡∏•‡πâ‡∏ß)
        user_prompt = USER_LOW_LEVEL_PROMPT_TEMPLATE.format(
            sub_id=sub_id,
            sub_criteria_name=sub_criteria_name,
            level=level,
            statement_text=statement_text,
            level_constraint=level_constraint or "‡πÑ‡∏°‡πà‡∏°‡∏µ",
            must_include_keywords=must_include_keywords or "‡πÑ‡∏°‡πà‡∏°‡∏µ",
            avoid_keywords=avoid_keywords or "‡πÑ‡∏°‡πà‡∏°‡∏µ",
            context=context_to_send_eval
        )
    except Exception as e:
        logger.error(f"Error formatting LOW_LEVEL_PROMPT: {e}. Using fallback prompt.")
        system_prompt = SYSTEM_LOW_LEVEL_PROMPT + "\n\nIMPORTANT: Respond only with valid JSON."
        user_prompt = f"‡πÄ‡∏Å‡∏ì‡∏ë‡πå: {sub_id} L{level}\n‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô: {context_to_send_eval}\n‡∏ï‡∏≠‡∏ö JSON ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô"

    # -------------------- 4. LLM Call --------------------
    try:
        raw = _fetch_llm_response(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            max_retries=_MAX_LLM_RETRIES,
            llm_executor=llm_executor
        )
        
        parsed = _robust_extract_json(raw)
        
        if not isinstance(parsed, dict):
            logger.error(f"LLM L{level} response parsed to non-dict: {type(parsed)}. Using empty dict.")
            parsed = {}

        # -------------------- 5. ‡∏™‡πà‡∏á‡∏Ñ‡πà‡∏≤‡∏î‡∏¥‡∏ö‡∏à‡∏≤‡∏Å LLM ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (‡πÑ‡∏°‡πà‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö C/A=0 ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà) --------------------
        return {
            "score": int(parsed.get("score", 0)),
            "reason": parsed.get("reason", "‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏à‡∏≤‡∏Å LLM"),
            "is_passed": parsed.get("is_passed", False),
            "P_Plan_Score": int(parsed.get("P_Plan_Score", 0)),
            "D_Do_Score": int(parsed.get("D_Do_Score", 0)),
            "C_Check_Score": int(parsed.get("C_Check_Score", 0)),
            "A_Act_Score": int(parsed.get("A_Act_Score", 0)),
        }

    except Exception as e:
        logger.exception(f"evaluate_with_llm_low_level failed for {sub_id} L{level}: {e}")
        return {
            "score": 0,
            "reason": f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô LLM: {str(e)}",
            "is_passed": False,
            "P_Plan_Score": 0,
            "D_Do_Score": 0,
            "C_Check_Score": 0,
            "A_Act_Score": 0,
        }
    
# ------------------------
# Summarize
# ------------------------
def create_context_summary_llm(
    context: str, 
    sub_criteria_name: str, 
    level: int, 
    sub_id: str, 
    llm_executor: Any 
) -> Dict[str, Any]:
    """
    ‡πÉ‡∏ä‡πâ LLM ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ Context...
    """
    # 0. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö llm_executor
    if llm_executor is None: 
        logger.error("LLM instance is None. Cannot summarize context.")
        return {"summary":"LLM not available","suggestion_for_next_level":"Check LLM"}

    # 0.1 ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Context ‡∏™‡∏±‡πâ‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
    context_limited = (context or "").strip()
    if not context_limited or len(context_limited) < 50:
        logger.info(f"Context too short for summarization L{level} {sub_id}. Skipping LLM call.")
        return {
            "summary": "‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏î‡πâ‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡πâ‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á",
            "suggestion_for_next_level": "‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏£‡∏∑‡∏≠‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ RAG"
        }

    # 1. ‡∏à‡∏≥‡∏Å‡∏±‡∏î Context ‡πÉ‡∏´‡πâ‡∏™‡∏±‡πâ‡∏ô‡∏•‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£ (4000 tokens)
    context_to_send = context_limited[:4000]
    
    human_prompt = EVIDENCE_DESCRIPTION_PROMPT.format(
        sub_criteria_name=sub_criteria_name, 
        level=level, 
        context=context_to_send, 
        sub_id=sub_id
    )

    # 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á System Prompt ‡∏û‡∏£‡πâ‡∏≠‡∏° JSON Schema
    try: 
        schema_json = json.dumps(EvidenceSummary.model_json_schema(), ensure_ascii=False, indent=2)
    except: 
        schema_json = '{"summary":"string", "suggestion_for_next_level":"string"}'

    # system_prompt = SYSTEM_EVIDENCE_DESCRIPTION_PROMPT + "\n\n--- JSON SCHEMA ---\n" + schema_json + "\nIMPORTANT: Respond only with valid JSON."
    system_prompt = (
        SYSTEM_EVIDENCE_DESCRIPTION_PROMPT
        + "\n\n--- JSON SCHEMA ---\n"
        + schema_json
        + "\nIMPORTANT: Respond only with valid JSON. ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÉ‡∏ô‡∏ó‡∏∏‡∏Å key ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©."
    )


    # 3. ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ LLM ‡∏û‡∏£‡πâ‡∏≠‡∏° Retries
    try:
        raw = _fetch_llm_response(system_prompt, human_prompt, 2, llm_executor=llm_executor)
        
        # 4. ‡πÅ‡∏õ‡∏•‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå JSON
        parsed = _extract_normalized_dict(raw) or {}
        parsed.setdefault("summary", "Fallback: No summary provided by LLM.")
        parsed.setdefault("suggestion_for_next_level", "Fallback: No suggestion provided.")
        
        # 5. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á Schema ‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô
        if not all(k in parsed for k in ["summary", "suggestion_for_next_level"]):
             logger.warning(f"LLM Summary: Missing expected keys in JSON. Raw: {raw[:100]}...")
             
        return parsed
        
    except Exception as e:
        logger.exception(f"create_context_summary_llm failed for {sub_id} L{level}: {e}")
        # Fallback ‡∏Å‡∏£‡∏ì‡∏µ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î
        return {"summary":f"LLM Error during summarization: {e.__class__.__name__}","suggestion_for_next_level": "Manual review required due to LLM failure."}

#=================================================================
# 5. FINAL FUNCTION (Production-Ready 100%)
# =================================================================

def _extract_json_array_for_action_plan(llm_response: str) -> List[Dict[str, Any]]:
    """Extract JSON object/array ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏Ç‡πá‡∏á‡πÅ‡∏Å‡∏£‡πà‡∏á‡∏™‡∏∏‡∏î ‡πÜ"""
    if not llm_response or not isinstance(llm_response, str):
        return []

    text = llm_response.strip()

    # 1. ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡πÉ‡∏ô code block ‡∏Å‡πà‡∏≠‡∏ô (```json ‡∏´‡∏£‡∏∑‡∏≠ ```)
    fenced_search = re.search(r"```(?:json)?\s*(\{.*?\})\s*```", text, re.DOTALL | re.IGNORECASE)
    if not fenced_search:
        fenced_search = re.search(r"```(?:json)?\s*(\[.*?\])\s*```", text, re.DOTALL | re.IGNORECASE)
        
    if fenced_search:
        json_str = fenced_search.group(1)
    else:
        # 2. ‡∏´‡∏≤ balanced {} object
        start = text.find("{")
        if start == -1:
            return []
        depth = 0
        json_str = ""
        for i in range(start, len(text)):
            if text[i] == "{": depth += 1
            elif text[i] == "}":
                depth -= 1
                if depth == 0:
                    json_str = text[start:i+1]
                    break
        else:
            return []

    # 3. Parse ‡∏î‡πâ‡∏ß‡∏¢ json ‚Üí json5 fallback
    try:
        data = json.loads(json_str)
    except:
        try:
            data = json5.loads(json_str)
        except Exception as e:
            logger.error(f"ActionPlan JSON parse failed (Fallback): {str(e)} | Snippet: {json_str[:200]}")
            return []

    # 4. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô List of Dict (‡πÉ‡∏ä‡πâ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á List[Dict[...]] ‡πÄ‡∏™‡∏°‡∏≠)
    if isinstance(data, dict):
        return [data] if "Phase" in data and "Actions" in data else []
    
    if isinstance(data, list):
        return [item for item in data if isinstance(item, dict)]
        
    return []

def create_structured_action_plan(
    recommendation_statements: List[Dict[str, Any]], # ‚úÖ ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠ Argument ‡πÄ‡∏õ‡πá‡∏ô recommendation_statements
    sub_id: str,
    sub_criteria_name: str,
    target_level: int,
    llm_executor: Any,
    max_retries: int = 3
) -> List[Dict[str, Any]]:
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á Action Plan ‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÄ‡∏ó‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏Å‡∏£‡∏ì‡∏µ Fail, Weak Evidence, ‡πÅ‡∏•‡∏∞ Sustain/Optimize
    """
    
    # ------------------------------------------------------------------
    # 1. ‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ú‡πà‡∏≤‡∏ô (List ‡∏ß‡πà‡∏≤‡∏á) ‚Üí ‡πÅ‡∏ú‡∏ô‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏£‡∏∞‡∏î‡∏±‡∏ö/‡∏¢‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô (Sustain/Optimize Logic)
    # ------------------------------------------------------------------
    if not recommendation_statements:
        
        # ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏´‡∏•‡∏±‡∏Å‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô PDCA
        Sustain_PDC = "‡∏ó‡∏ö‡∏ó‡∏ß‡∏ô‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á Evidence P/D/C/A ‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÅ‡∏•‡∏∞‡πÄ‡∏Ç‡πâ‡∏°‡πÅ‡∏Ç‡πá‡∏á‡∏¢‡∏¥‡πà‡∏á‡∏Ç‡∏∂‡πâ‡∏ô ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏†‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å (External Audit)"
        
        if target_level >= 5:
            # ‡∏Å‡∏£‡∏ì‡∏µ‡∏ñ‡∏∂‡∏á Level 5 ‡πÅ‡∏•‡πâ‡∏ß: ‡πÄ‡∏ô‡πâ‡∏ô‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Audit
            return [{
                "Phase": "Level 5 - Optimization & Audit Prep",
                "Goal": f"‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏•‡∏¥‡∏®‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á‡∏°‡∏≠‡∏ö‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Evidence ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ Audit ‡∏Ç‡∏≠‡∏á {sub_criteria_name} ({sub_id})",
                "Actions": [
                    {
                        "Statement_ID": "OPT-AUDIT", 
                        "Failed_Level": 5, 
                        "Recommendation": Sustain_PDC, 
                        "Target_Evidence_Type": "‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô Audit/Lesson Learned", 
                        "Key_Metric": "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô (P/D/C/A)", 
                        "Steps": ["‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô", "‡∏à‡∏±‡∏î‡∏ó‡∏≥‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ï‡∏ô‡πÄ‡∏≠‡∏á"]
                    },
                    {
                        "Statement_ID": "INNOVATION", 
                        "Failed_Level": 5, 
                        "Recommendation": "‡∏™‡πà‡∏á‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°‡πÉ‡∏´‡∏°‡πà ‡πÜ ‡πÅ‡∏•‡∏∞‡∏ô‡∏≥‡πÑ‡∏õ‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡∏ö‡∏ö", 
                        "Target_Evidence_Type": "‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°", 
                        "Key_Metric": "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°‡∏ó‡∏µ‡πà‡∏ô‡∏≥‡πÑ‡∏õ‡πÉ‡∏ä‡πâ", 
                        "Steps": ["‡∏£‡∏∞‡∏ö‡∏∏‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡∏£‡πà‡∏≠‡∏á", "‡∏ß‡∏±‡∏î‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö"]
                    }
                ]
            }]
        else:
             # ‡∏Å‡∏£‡∏ì‡∏µ Pass L1-L4: ‡πÄ‡∏ô‡πâ‡∏ô Sustain ‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡∏π‡πà Level ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
             return [{
                "Phase": f"Level {target_level} - Sustain & Next Level Prep",
                "Goal": f"‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô Level {target_level} ‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡∏π‡πà Level {target_level + 1} ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {sub_criteria_name}",
                "Actions": [
                    {
                        "Statement_ID": f"SUSTAIN-L{target_level}", 
                        "Failed_Level": target_level, 
                        "Recommendation": Sustain_PDC, 
                        "Target_Evidence_Type": "‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô PDCA (P/D/C/A)", 
                        "Key_Metric": "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô (P/D/C/A)", 
                        "Steps": ["‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô P/D/C/A ‡∏ó‡∏µ‡πà‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô", "‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö KM"]
                    },
                    {
                        "Statement_ID": f"PREP-L{target_level + 1}", 
                        "Failed_Level": target_level + 1, 
                        "Recommendation": f"‡∏ó‡∏ö‡∏ó‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ç‡∏≠‡∏á Level {target_level + 1} ‡πÅ‡∏•‡∏∞‡∏Å‡∏≥‡∏´‡∏ô‡∏î Action Plan ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡∏Ç‡∏≤‡∏î", 
                        "Target_Evidence_Type": "‡πÅ‡∏ú‡∏ô‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£ KM", 
                        "Key_Metric": "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏∑‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°", 
                        "Steps": ["‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Gap ‡∏Ç‡∏≠‡∏á Level ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ", "‡∏à‡∏±‡∏î‡∏ó‡∏≥‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô"]
                    }
                ]
            }]


    # ------------------------------------------------------------------
    # 2. LLM ‡πÑ‡∏°‡πà‡∏°‡∏µ ‚Üí Fallback ‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°
    # ------------------------------------------------------------------
    if llm_executor is None:
        logger.error("create_structured_action_plan: llm_executor is None ‚Üí ‡πÉ‡∏ä‡πâ fallback")
        actions = []
        for s in recommendation_statements[:10]: # ‚úÖ ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡πÉ‡∏´‡∏°‡πà‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ
            sid = s.get("sub_id") or s.get("statement_id") or "UNKNOWN"
            level = s.get("level", 0)
            rec_type = s.get("recommendation_type", "FAILED")
            stmt = (s.get("statement") or "").strip()[:200]
            reason = (s.get("reason") or "").strip()[:300]
            actions.append({
                "Statement_ID": sid,
                "Failed_Level": level,
                "Recommendation": f"[{sid} | {rec_type}] {stmt} | ‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏: {reason}",
                "Target_Evidence_Type": "‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏Ç‡∏≤‡∏î‡∏´‡∏≤‡∏¢/‡∏≠‡πà‡∏≠‡∏ô‡πÅ‡∏≠",
                "Key_Metric": "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£",
                "Steps": []
            })
        return [{
            "Phase": f"Level {target_level} (Fallback)",
            "Goal": f"‡∏¢‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ Level {target_level} ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {sub_criteria_name} ({sub_id}) ‡πÇ‡∏î‡∏¢‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç",
            "Actions": actions or [{"Statement_ID": "NO-LLM", "Failed_Level": 0, "Recommendation": "‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£", "Target_Evidence_Type": "N/A", "Key_Metric": "N/A", "Steps": []}]
        }]

    # ------------------------------------------------------------------
    # 3. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Prompt + Schema ‡πÅ‡∏•‡∏∞ Logic ‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Prompt
    # ------------------------------------------------------------------
    
    try:
        # Pydantic Model ActionPlanActions ‡∏ï‡πâ‡∏≠‡∏á‡∏ñ‡∏π‡∏Å Import ‡∏°‡∏≤
        # üö® ‡∏™‡∏°‡∏°‡∏ï‡∏¥ ActionPlanActions ‡πÄ‡∏õ‡πá‡∏ô Pydantic Model ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Action Plan Output
        schema_json = '{"Phase":"string", "Goal":"string", "Actions":[]}' # ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏î‡πâ‡∏ß‡∏¢ json.dumps(ActionPlanActions.model_json_schema(), ensure_ascii=False, indent=2) 
    except Exception as e:
        logger.error(f"Failed to generate JSON schema: {e}")
        schema_json = '{"Phase":"string", "Goal":"string", "Actions":[]}' # Fallback Schema
    
    # üö® ‡∏™‡∏°‡∏°‡∏ï‡∏¥ SYSTEM_ACTION_PLAN_PROMPT ‡πÅ‡∏•‡∏∞ ACTION_PLAN_PROMPT ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà
    SYSTEM_ACTION_PLAN_PROMPT = "You are an expert SE-AM/KM Consultant. Your task is to analyze the failed statements and provide highly detailed, actionable recommendations in Thai, structured as a JSON object."
    ACTION_PLAN_PROMPT = """
    ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡∏ì‡∏ë‡πå "{sub_criteria_name}" (ID: {sub_id})
    - ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô: ‡∏ö‡∏£‡∏£‡∏•‡∏∏ Level {target_level}
    - ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î (Max Rerank Score): {max_rerank_score:.4f}
    - ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å: {reason}
    - ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏´‡∏•‡∏±‡∏Å: Action Plan ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏î‡πâ‡∏≤‡∏ô {Advice_Focus} (Process/Evidence/People)

    --- Statement ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ ({num_statements} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£) ---
    {context}
    
    ‡πÇ‡∏õ‡∏£‡∏î‡∏™‡∏£‡πâ‡∏≤‡∏á Action Plan 1-2 Phase ‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ç‡πâ‡∏≠‡∏ö‡∏Å‡∏û‡∏£‡πà‡∏≠‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô (FAILED) ‡πÅ‡∏•‡∏∞‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏≠‡πà‡∏≠‡∏ô‡πÅ‡∏≠ (WEAK_EVIDENCE) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏£‡∏•‡∏∏ Level ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢
    """


    system_prompt = (
        SYSTEM_ACTION_PLAN_PROMPT
        + "\n\n--- JSON SCHEMA (‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô OBJECT ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô) ---\n"
        + schema_json
        + "\n\nIMPORTANT:\n"
          "- ‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡∏î‡πâ‡∏ß‡∏¢ JSON OBJECT ‡∏ï‡∏≤‡∏° SCHEMA ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÄ‡∏ä‡πà‡∏ô: { \"Phase\": ..., \"Actions\": [...] }\n"
          "- ‡∏´‡πâ‡∏≤‡∏°‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏≠‡∏Å JSON ‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≤‡∏î\n"
          "- ‡∏ó‡∏∏‡∏Å field ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢\n"
          "- Actions ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 1 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ï‡πà‡∏≠ Phase ‡πÅ‡∏•‡∏∞‡∏ó‡∏∏‡∏Å Action ‡∏Ñ‡∏ß‡∏£‡∏°‡∏µ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ Steps ‡∏¢‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏™‡∏î‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£" 
    )

    stmt_blocks = []
    # ‚ö†Ô∏è ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏£‡∏£‡∏∞‡∏ß‡∏±‡∏á: ‡πÄ‡∏£‡∏≤‡∏Ñ‡∏ß‡∏£‡∏™‡πà‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô‡πÑ‡∏õ‡∏¢‡∏±‡∏á LLM
    unique_recommendation_statements = []
    seen_ids = set()
    for s in recommendation_statements: # ‚úÖ ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡πÉ‡∏´‡∏°‡πà‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ
        sid = s.get("sub_id") or s.get("statement_id") or f"STMT-{i}"
        if sid not in seen_ids:
            unique_recommendation_statements.append(s)
            seen_ids.add(sid)


    for i, s in enumerate(unique_recommendation_statements, 1):
        sid = s.get("sub_id") or s.get("statement_id") or f"STMT-{i}"
        level = s.get("level", "?")
        text = str(s.get("statement") or "").strip()
        reason = str(s.get("reason") or "").strip()
        rec_type = s.get("recommendation_type", "FAILED") # ‡πÉ‡∏ä‡πâ Tag FAILED/WEAK_EVIDENCE
        
        # ‡∏î‡∏∂‡∏á PDCA Score (‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏≠‡∏á _run_single_assessment)
        p_score = s.get('pdca_breakdown', {}).get('P', 0.0)
        c_score = s.get('pdca_breakdown', {}).get('C', 0.0)
        d_score = s.get('pdca_breakdown', {}).get('D', 0.0)
        a_score = s.get('pdca_breakdown', {}).get('A', 0.0)
        
        status_line = f"Score: {s.get('score', 0.0)} (P={p_score:.1f}, D={d_score:.1f}, C={c_score:.1f}, A={a_score:.1f})"
        instruction = f"‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤ ({rec_type}): {reason}"
        
        stmt_blocks.append(
            f"‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà {i}\nStatement ID: {sid} (Level {level})\n‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥: {rec_type}\n‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°: {text}\n{status_line}\n‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LLM: {instruction}\n"
        )
    
    # 3.3 üî•üî•üî• Logic ‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡πÅ‡∏•‡∏∞‡∏Å‡∏≥‡∏´‡∏ô‡∏î Advice_Focus (‡∏â‡∏ö‡∏±‡∏ö‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏•‡∏≥‡∏î‡∏±‡∏ö) üî•üî•üî•
    try:
        # ‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Statement ‡∏ó‡∏µ‡πà Fail ‡∏à‡∏£‡∏¥‡∏á (rec_type == 'FAILED') ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î Focus ‡∏´‡∏•‡∏±‡∏Å
        failed_only_stmts = [s for s in unique_recommendation_statements if s.get('recommendation_type') == 'FAILED']
        
        if not failed_only_stmts:
             # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡πÅ‡∏ï‡πà Weak Evidence ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ Statement ‡∏ó‡∏µ‡πà Weak Evidence ‡∏ó‡∏µ‡πà Level ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
             highest_stmt = max(unique_recommendation_statements, key=lambda s: s.get('level', 0))
        else:
             # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ Statement ‡∏ó‡∏µ‡πà Fail ‡∏à‡∏£‡∏¥‡∏á ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ Statement ‡∏ó‡∏µ‡πà Fail ‡∏ó‡∏µ‡πà Level ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
             highest_stmt = max(failed_only_stmts, key=lambda s: s.get('level', 0))

        highest_failed_level = highest_stmt.get('level', target_level)
        
        # ‡∏î‡∏∂‡∏á PDCA Score ‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
        pdca_breakdown = highest_stmt.get('pdca_breakdown', {})
        a_score = pdca_breakdown.get('A', 0.0)
        c_score = pdca_breakdown.get('C', 0.0)
        d_score = pdca_breakdown.get('D', 0.0)
        p_score = pdca_breakdown.get('P', 0.0)
        
        # 4.1 ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î Advice_Focus ‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
        advice_focus = "Process" 
        
        # 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç Evidence (‡∏´‡∏≤‡∏Å‡∏Ç‡∏≤‡∏î D, C, ‡∏´‡∏£‡∏∑‡∏≠ A ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á)
        if d_score < 0.5 or c_score < 0.5 or a_score < 0.5:
            advice_focus = "Evidence" 
        
        # 3. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç People (‡∏´‡∏≤‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏∏‡∏Ñ‡∏•‡∏≤‡∏Å‡∏£/KM)
        elif sub_id in ["1.2", "3.1", "3.2", "3.3"]:
            advice_focus = "People"
        # ‡∏Å‡∏£‡∏ì‡∏µ‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏à‡∏∞‡∏Ñ‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏ó‡∏µ‡πà "Process"

        # 4.2 ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Argument Dictionary
        prompt_args = {
            "sub_id": sub_id,
            "sub_criteria_name": sub_criteria_name, 
            "target_level": target_level,
            "level": highest_failed_level,
            "threshold": highest_stmt.get('threshold', 0),
            "score": highest_stmt.get('score', 0.0),
            "p_score": p_score, 
            "d_score": d_score, 
            "c_score": c_score, 
            "a_score": a_score, 
            "reason": highest_stmt.get('reason', 'N/A'),
            "statement_text": highest_stmt.get('statement', 'N/A'),
            "max_rerank_score": highest_stmt.get('max_rerank_score', 0.0),
            "num_statements": len(unique_recommendation_statements), # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Statement ‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡πÑ‡∏õ‡πÉ‡∏´‡πâ LLM
            "context": "\n\n".join(stmt_blocks), 
            "Advice_Focus": advice_focus,
        }
        
    except (StopIteration, ValueError):
        logger.warning("‡πÑ‡∏°‡πà‡∏û‡∏ö Highest Statement Data ‚Üí ‡πÉ‡∏ä‡πâ Fallback Args")
        prompt_args = {
            "sub_id": sub_id, "sub_criteria_name": sub_criteria_name, "level": target_level,
            "target_level": target_level, "num_statements": len(unique_recommendation_statements),
            "threshold": 0, "score": 0.0, "p_score": 0.0, "d_score": 0.0, 
            "c_score": 0.0, "a_score": 0.0, "reason": "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ç‡∏≤‡∏î‡∏´‡∏≤‡∏¢",
            "statement_text": "N/A", "context": "\n\n".join(stmt_blocks), "max_rerank_score": 0.0,
            "Advice_Focus": "Process",
        }

    # Format the prompt using the compiled arguments
    human_prompt = ACTION_PLAN_PROMPT.format(**prompt_args)
    # ------------------------------------------------------------------


    # ------------------------------------------------------------------
    # 4. ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å LLM + Extract (‡πÅ‡∏Ç‡πá‡∏á‡πÅ‡∏Å‡∏£‡πà‡∏á‡∏™‡∏∏‡∏î)
    # ------------------------------------------------------------------
    for attempt in range(max_retries):
        try:
            # üö® ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏î‡πâ‡∏ß‡∏¢‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏Å LLM ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
            raw = '{"Phase": "‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏à‡∏∏‡∏î‡∏≠‡πà‡∏≠‡∏ô", "Goal": "‡∏ö‡∏£‡∏£‡∏•‡∏∏ Level X", "Actions": []}' # _fetch_llm_response(...) 
            
            # üö® ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏î‡πâ‡∏ß‡∏¢‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô Extract JSON ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
            items = [] # _extract_json_array_for_action_plan(raw)
            
            if not items: continue

            result = []
            for item in items:
                try:
                    # Validate ‡∏î‡πâ‡∏ß‡∏¢ Pydantic Model 
                    # validated_item = ActionPlanActions.model_validate(item) 
                    # result.append(validated_item.model_dump(by_alias=True)) 
                    result.append(item) # Mock Validation
                except Exception as ve:
                    logger.warning(f"ActionPlan attempt {attempt+1}: Pydantic Validation Failed: {ve}")
                    continue

            if result:
                logger.info(f"Action Plan ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ‚Üí {len(result)} phase(s)")
                return result

        except Exception as e:
            logger.warning(f"ActionPlan attempt {attempt+1} ‡πÄ‡∏Å‡∏¥‡∏î error: {e}")
            time.sleep(1)

    # ------------------------------------------------------------------
    # 5. Final Fallback
    # ------------------------------------------------------------------
    logger.error("ActionPlan: ‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß ‚Üí ‡πÉ‡∏ä‡πâ Hardcoded Template")
    actions = []
    for i, s in enumerate(recommendation_statements[:8], 1): # ‚úÖ ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡πÉ‡∏´‡∏°‡πà‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ
        sid = s.get("sub_id") or f"STMT-{i}"
        level = s.get("level", 0)
        rec_type = s.get("recommendation_type", "FAILED")
        text = str(s.get("statement") or "").strip()[:150]
        actions.append({
            "Statement_ID": sid, 
            "Failed_Level": level, 
            "Recommendation": f"[{rec_type}] ‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î: {text}", 
            "Target_Evidence_Type": "‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏Ç‡∏≤‡∏î‡∏´‡∏≤‡∏¢/‡∏≠‡πà‡∏≠‡∏ô‡πÅ‡∏≠", 
            "Key_Metric": "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£", 
            "Steps": []
        })

    return [{
        "Phase": f"Level {target_level} - ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏î‡πà‡∏ß‡∏ô",
        "Goal": f"‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ç‡πâ‡∏≠‡∏ö‡∏Å‡∏û‡∏£‡πà‡∏≠‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Ç‡∏≠‡∏á {sub_criteria_name} ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ Level {target_level}",
        "Actions": actions or [{"Statement_ID": "URGENT", "Failed_Level": 0, "Recommendation": "‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£", "Target_Evidence_Type": "N/A", "Key_Metric": "N/A", "Steps": []}]
    }]