"""
llm_data_utils.py
Robust LLM + RAG utilities for SEAM assessment (CLEAN FINAL VERSION)
"""

import logging

logger = logging.getLogger(__name__)
if not logger.handlers:
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")


import time
import json
import hashlib
import uuid
from datetime import datetime
from typing import List, Dict, Any, Optional, Union, Callable, TypeVar, Set
import json5
from langchain.retrievers import EnsembleRetriever
from langchain_core.documents import Document
import os
import unicodedata
# ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥: pip install json-repair (‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏π‡πâ JSON ‡∏û‡∏±‡∏á‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏°‡∏≤‡∏Å
try:
    from json_repair import repair_json
except ImportError:
    repair_json = None  # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ ‡∏à‡∏∞‡πÉ‡∏ä‡πâ manual repair ‡πÅ‡∏ó‡∏ô

# Optional: regex ‡πÅ‡∏ó‡∏ô re (‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤) ‚Äî ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡πá‡πÉ‡∏ä‡πâ re ‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤
try:
    import regex as re  # type: ignore
except ImportError:
    pass  # ‡πÉ‡∏ä‡πâ re ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ

# ===================================================================
# 1. Core Configuration (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô)
# ===================================================================
from config.global_vars import (
    INITIAL_TOP_K,
    MAX_EVAL_CONTEXT_LENGTH,
    DEFAULT_EMBED_BATCH_SIZE,
    RERANK_THRESHOLD,
    ANALYSIS_FINAL_K,
    ACTION_PLAN_STEP_MAX_WORDS,
    LLM_TEMPERATURE,
    MAX_ACTION_PLAN_TOKENS
)

# ===================================================================
# 2. Critical Utilities (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏à‡∏£‡∏¥‡∏á ‚Äî ‡πÑ‡∏°‡πà‡∏°‡∏µ fallback)
# ===================================================================
# üéØ FIX 1: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô Import ‡∏à‡∏≤‡∏Å _get_collection_name ‡πÑ‡∏õ‡πÄ‡∏õ‡πá‡∏ô get_doc_type_collection_key
from core.vectorstore import get_hf_embeddings
from utils.path_utils import (
    get_doc_type_collection_key, 
    _n, get_mapping_file_path, # <--- ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà‡∏à‡∏≤‡∏Å utils/path_utils
    get_vectorstore_collection_path,
    get_vectorstore_tenant_root_path,
    get_rubric_file_path
)
from core.json_extractor import (
    _robust_extract_json,
    _normalize_keys,
    _safe_int_parse,
    _extract_normalized_dict
)

# ===================================================================
# 3. Project Modules (‡∏ñ‡πâ‡∏≤‡∏´‡∏≤‡∏¢ ‚Üí ‡∏õ‡∏•‡πà‡∏≠‡∏¢‡πÉ‡∏´‡πâ error ‡∏ä‡∏±‡∏î ‡πÜ ‡πÑ‡∏õ‡πÄ‡∏•‡∏¢ ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏á‡∏µ‡∏¢‡∏ö)
# ===================================================================
from core.seam_prompts import (
    SYSTEM_ASSESSMENT_PROMPT,
    USER_ASSESSMENT_PROMPT,
    SYSTEM_EVIDENCE_DESCRIPTION_PROMPT,
    EVIDENCE_DESCRIPTION_PROMPT,
    USER_LOW_LEVEL_PROMPT,
    USER_EVIDENCE_DESCRIPTION_TEMPLATE,
)

from core.vectorstore import VectorStoreManager, get_global_reranker, ChromaRetriever
from core.assessment_schema import CombinedAssessment, EvidenceSummary

try:
    from core.assessment_schema import StatementAssessment
except ImportError:
    from pydantic import BaseModel
    class StatementAssessment(BaseModel):
        score: int = 0
        reason: str = ""

from langchain_core.documents import Document as LcDocument
# ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î Import ‡πÉ‡∏ô routers/llm_router.py

# ===================================================================
# 4. Constants
# ===================================================================
LOW_LEVEL_K: int = 3
_MOCK_FLAG = False
_MAX_LLM_RETRIES = 3

def set_mock_control_mode(enable: bool):
    global _MOCK_FLAG
    _MOCK_FLAG = bool(enable)
    logger.info(f"Mock control mode: {_MOCK_FLAG}")

def _create_where_filter(
    stable_doc_ids: Optional[Union[Set[str], List[str]]] = None,
    subject: Optional[str] = None,
    sub_topic: Optional[str] = None,
    year: Optional[Union[int, str]] = None,
    enabler: Optional[str] = None,
    tenant: Optional[str] = None,
    **kwargs
) -> Dict[str, Any]:
    """
    [PRODUCTION VERSION] ‡∏™‡∏£‡πâ‡∏≤‡∏á Filter ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ChromaDB ‡∏ó‡∏µ‡πà‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥
    ‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á Data Type Mismatch (Int/Str) ‡πÅ‡∏•‡∏∞‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Multi-tenant
    """
    filters: List[Dict[str, Any]] = []

    # --- 1. ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Stable Doc IDs (‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î) ---
    if stable_doc_ids:
        ids_list = [str(i).strip() for i in (stable_doc_ids if isinstance(stable_doc_ids, (list, set)) else [stable_doc_ids]) if i]
        if ids_list:
            if len(ids_list) == 1:
                # ‡∏Å‡∏£‡∏ì‡∏µ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ $and
                return {"stable_doc_uuid": ids_list[0]}
            else:
                return {"stable_doc_uuid": {"$in": ids_list}}

    # --- 2. ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Year (‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ Local Mac ‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠) ---
    if year is not None:
        year_str = str(year).strip()
        if year_str and year_str.lower() != "none":
            # üéØ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: ‡∏™‡πà‡∏á‡∏ó‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö Int ‡πÅ‡∏•‡∏∞ Str ‡∏´‡∏£‡∏∑‡∏≠‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô Int ‡∏ï‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà Peek ‡πÄ‡∏à‡∏≠‡πÉ‡∏ô Mac
            try:
                # ‡∏ñ‡πâ‡∏≤‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ‡∏™‡πà‡∏á‡πÅ‡∏ö‡∏ö Integer (ChromaDB Local ‡∏°‡∏±‡∏Å‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏õ‡πá‡∏ô Int)
                val_year = int(year_str)
                filters.append({"year": val_year})
            except (ValueError, TypeError):
                # ‡∏ñ‡πâ‡∏≤‡πÅ‡∏õ‡∏•‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á‡πÜ ‡πÉ‡∏´‡πâ‡∏™‡πà‡∏á‡πÅ‡∏ö‡∏ö String
                filters.append({"year": year_str})
    
    # --- 3. ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Tenant (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ç‡πâ‡∏≤‡∏°‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó) ---
    effective_tenant = tenant or kwargs.get("tenant")
    if effective_tenant and str(effective_tenant).strip():
        filters.append({"tenant": str(effective_tenant).strip()})

    # --- 4. ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Enabler (KM, IM, etc.) ---
    if enabler and str(enabler).strip():
        filters.append({"enabler": enabler.strip().upper()})

    # --- 5. ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Subject (‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÄ‡∏Å‡∏ì‡∏ë‡πå) ---
    if subject and str(subject).strip():
        filters.append({"subject": str(subject).strip()})

    # --- 6. ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Sub Topic ---
    if sub_topic and str(sub_topic).strip():
        filters.append({"sub_topic": str(sub_topic).strip()})

    # --- ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Filter ---
    if not filters:
        return {}

    # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢‡∏≠‡∏±‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏î‡πâ‡∏ß‡∏¢ $and
    return filters[0] if len(filters) == 1 else {"$and": filters}


def retrieve_context_for_endpoint(
    vectorstore_manager,
    query: str = "",
    tenant: Optional[str] = None,
    year: Optional[Union[int, str]] = None,
    stable_doc_ids: Optional[Set[str]] = None,
    doc_type: Optional[str] = None,
    enabler: Optional[str] = None,
    subject: Optional[str] = None,
    sub_topic: Optional[str] = None,
    k_to_retrieve: int = 150, 
    k_to_rerank: int = 30,    
    strict_filter: bool = False,
    **kwargs
) -> Dict[str, Any]:
    """
    [ULTIMATE REVISED] Retrieval for Search Endpoint
    - ‡∏¢‡∏∂‡∏î Metadata ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢ (No Pydantic Error)
    - ‡πÉ‡∏ä‡πâ Deterministic MD5 Hash ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Deduplication
    - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Anchor Chunks ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á Context ‡∏ó‡∏µ‡πà‡∏°‡∏±‡πà‡∏ô‡∏Ñ‡∏á
    """
    start_time = time.time()
    vsm = vectorstore_manager

    # 1. Resolve collection
    clean_doc_type = str(doc_type or "document").strip().lower()
    collection_name = get_doc_type_collection_key(doc_type=clean_doc_type, enabler=enabler)
    
    chroma = vsm._load_chroma_instance(collection_name)
    if not chroma:
        logger.error(f"‚ùå Collection {collection_name} not found.")
        return {"top_evidences": [], "aggregated_context": "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", "retrieval_time": 0}

    # 2. Create where_filter
    where_filter = _create_where_filter(
        stable_doc_ids=list(stable_doc_ids) if stable_doc_ids else None, 
        subject=subject, 
        sub_topic=sub_topic, 
        year=year
    )

    # Dictionary ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Deduplication (‡πÉ‡∏ä‡πâ MD5 ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏õ‡πá‡∏ô Key)
    unique_map: Dict[str, LcDocument] = {}

    # =====================================================
    # ‚öì 2.1 ANCHOR RETRIEVAL (‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå)
    # =====================================================
    if stable_doc_ids:
        # ‡∏î‡∏∂‡∏á‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏£‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        anchors = chroma.get(where=where_filter, limit=10) 
        if anchors and anchors.get('documents'):
            for i in range(len(anchors['documents'])):
                content = anchors['documents'][i]
                md = anchors['metadatas'][i]
                
                # Deterministic MD5 Hash
                c_hash = hashlib.md5(content.encode()).hexdigest()
                uid = md.get("chunk_uuid") or f"anchor-{c_hash}"
                
                if uid not in unique_map:
                    # ‡∏â‡∏µ‡∏î‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÉ‡∏´‡πâ Anchor ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ï‡∏¥‡∏î‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡∏ï‡πâ‡∏ô‡πÜ ‡∏´‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á
                    md["score"] = 0.5 
                    md["is_anchor"] = True
                    unique_map[uid] = LcDocument(page_content=content, metadata=md)

    # =====================================================
    # üîç 2.2 SEMANTIC SEARCH
    # =====================================================
    search_query = query if (query and query != "*" and len(query) > 2) else ""
    
    # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö‡∏à‡∏≤‡∏Å Vector DB
    if search_query:
        docs = chroma.similarity_search(search_query, k=k_to_retrieve, filter=where_filter)
    else:
        # Fallback ‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏°‡πà‡∏°‡∏µ Query ‡πÉ‡∏´‡πâ‡∏î‡∏∂‡∏á‡πÅ‡∏ö‡∏ö‡∏Å‡∏ß‡∏≤‡∏î
        docs = chroma.similarity_search("*", k=k_to_retrieve, filter=where_filter)

    for d in docs:
        c_hash = hashlib.md5(d.page_content.encode()).hexdigest()
        md = d.metadata or {}
        uid = md.get("chunk_uuid") or c_hash
        if uid not in unique_map:
            unique_map[uid] = d

    candidates = list(unique_map.values())

    # =====================================================
    # üöÄ 3. BATCH RERANKING
    # =====================================================
    final_scored_docs = []
    reranker = getattr(vsm, "reranker", None)
    
    if reranker and candidates and search_query:
        try:
            batch_size = 100 
            logger.info(f"üöÄ Reranking {len(candidates)} candidates in batches...")
            for i in range(0, len(candidates), batch_size):
                batch = candidates[i : i + batch_size]
                # ‡πÉ‡∏ä‡πâ Reranker ‡∏Ç‡∏≠‡∏á LangChain (Compressor)
                scored_batch = reranker.compress_documents(documents=batch, query=search_query)
                final_scored_docs.extend(scored_batch)
        except Exception as e:
            logger.error(f"‚ö†Ô∏è Rerank failed: {e}")
            final_scored_docs = candidates
    else:
        final_scored_docs = candidates

    # =====================================================
    # 4. SORTING & SCORE INJECTION (‡∏´‡∏±‡∏ß‡πÉ‡∏à‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ 0.0000)
    # =====================================================
    def get_score(d) -> float:
        m = d.metadata or {}
        # ‡∏î‡∏∂‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏à‡∏≤‡∏Å‡∏ó‡∏∂‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ
        s = m.get("relevance_score") or m.get("score") or m.get("rerank_score") or 0.0
        try: return float(s)
        except: return 0.0

    final_scored_docs.sort(key=get_score, reverse=True)

    # =====================================================
    # 5. RESPONSE BUILD
    # =====================================================
    top_evidences = []
    aggregated_parts = []
    
    for doc in final_scored_docs[:k_to_rerank]:
        md = doc.metadata or {}
        text = doc.page_content.strip()
        score = get_score(doc)
        
        # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏•‡∏∞‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏≤‡πÉ‡∏´‡πâ Robust
        p_val = md.get("page") or md.get("page_label") or md.get("page_number") or "N/A"
        source_name = md.get('source') or md.get('source_filename') or md.get('file_name') or 'Unknown'
        s_uuid = md.get("stable_doc_uuid") or md.get("doc_id") or ""
        
        # SYNC SCORE ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤ Metadata ‡∏ó‡∏∏‡∏Å‡∏ï‡∏±‡∏ß
        md["score"] = score
        md["relevance_score"] = score
        
        evidence_item = {
            "doc_id": str(s_uuid),
            "chunk_uuid": str(md.get("chunk_uuid") or ""),
            "source": source_name,
            "text": text,
            "page": str(p_val),
            "score": score,
            "pdca_tag": md.get("pdca_tag", "Other"),
            "metadata": md
        }
        
        top_evidences.append(evidence_item)
        aggregated_parts.append(f"[‡πÑ‡∏ü‡∏•‡πå: {source_name}, ‡∏´‡∏ô‡πâ‡∏≤: {p_val}] {text}")

    retrieval_time = round(time.time() - start_time, 3)
    logger.info(f"üèÅ Finished: {len(top_evidences)} chunks in {retrieval_time}s")

    return {
        "top_evidences": top_evidences,
        "aggregated_context": "\n\n---\n\n".join(aggregated_parts) if aggregated_parts else "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•",
        "retrieval_time": retrieval_time,
        "total_candidates": len(candidates)
    }


# ------------------------
# Retrieval: retrieve_context_with_filter (Revised & Optimized)
# ------------------------
def retrieve_context_with_filter(
    query: Union[str, List[str]],
    doc_type: str,
    tenant: Optional[str] = None,
    year: Optional[Union[int, str]] = None,
    enabler: Optional[str] = None,
    subject: Optional[str] = None,
    vectorstore_manager: Optional[Any] = None,
    mapped_uuids: Optional[List[str]] = None,
    stable_doc_ids: Optional[List[str]] = None,
    priority_docs_input: Optional[List[Any]] = None,
    sequential_chunk_uuids: Optional[List[str]] = None,
    sub_id: Optional[str] = None,
    level: Optional[int] = None,
    get_previous_level_docs: Optional[Callable[[int, str], List[Any]]] = None,
    top_k: int = 100, 
) -> Dict[str, Any]:
    """
    [ULTIMATE REVISED] 
    - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ Batch Reranking ‡∏ï‡∏≤‡∏° DEFAULT_EMBED_BATCH_SIZE (Mac 16 / CUDA 128)
    - ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏≤‡∏£ Sync ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÉ‡∏´‡πâ‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏•‡∏î‡∏•‡∏á‡πÄ‡∏Å‡∏¥‡∏ô‡∏à‡∏£‡∏¥‡∏á
    - ‡πÉ‡∏ä‡πâ Retriever Caching ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏†‡∏≤‡∏£‡∏∞‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î BM25 ‡∏ö‡∏ô Mac
    """
    start_time = time.time()
    manager = vectorstore_manager
    if not manager:
        logger.error("‚ùå VectorStoreManager is missing!")
        return {"top_evidences": [], "aggregated_context": "Missing VSM", "retrieval_time": 0}

    # 1. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Query ‡πÅ‡∏•‡∏∞ Filter
    queries_to_run = [query] if isinstance(query, str) else list(query or [""])
    collection_name = get_doc_type_collection_key(doc_type, enabler or "KM")
    
    target_ids = set()
    if stable_doc_ids: target_ids.update([str(i) for i in stable_doc_ids])
    if mapped_uuids: target_ids.update([str(i) for i in mapped_uuids])
    if sequential_chunk_uuids: target_ids.update([str(i) for i in sequential_chunk_uuids])
    
    where_filter = _create_where_filter(
        stable_doc_ids=list(target_ids) if target_ids else None,
        subject=subject,
        year=year,
        tenant=tenant
    )

    # 2. Hybrid Retrieval (Vector + BM25) ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏£‡∏∞‡∏ö‡∏ö Cache
    all_source_chunks = []

    # 2.1 ‡πÅ‡∏ó‡∏£‡∏Å Priority Docs (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
    if priority_docs_input:
        for doc in priority_docs_input:
            if not doc: continue
            if isinstance(doc, dict):
                pc = doc.get('page_content') or doc.get('text') or ''
                meta = doc.get('metadata') or {}
                if pc.strip(): all_source_chunks.append(LcDocument(page_content=pc, metadata=meta))
            elif hasattr(doc, 'page_content'):
                all_source_chunks.append(doc)

    # 2.2 ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏à‡∏£‡∏¥‡∏á (‡πÉ‡∏ä‡πâ Retriever Cache ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î RAM ‡∏ö‡∏ô Mac)
    try:
        if not hasattr(manager, '_retriever_cache'):
            manager._retriever_cache = {}
        
        if collection_name not in manager._retriever_cache:
            logger.info(f"üß¨ [CACHE-MISS] Initializing Hybrid Retriever for: {collection_name}")
            manager._retriever_cache[collection_name] = manager.get_retriever(collection_name=collection_name)
        
        full_retriever = manager._retriever_cache[collection_name]
        base_retriever = getattr(full_retriever, "base_retriever", full_retriever)
        
        search_kwargs = {"k": top_k}
        if where_filter: search_kwargs["where"] = where_filter

        for q in queries_to_run:
            if not q or len(q.strip()) < 2: continue
            docs = base_retriever.invoke(q, config={"configurable": {"search_kwargs": search_kwargs}})
            if docs: all_source_chunks.extend(docs)
    except Exception as e:
        logger.error(f"‚ùå Retrieval failure: {e}")

    # 3. Deduplication (Deterministic MD5)
    unique_map: Dict[str, LcDocument] = {}
    for doc in all_source_chunks:
        if not doc or not doc.page_content.strip(): continue
        md = doc.metadata or {}
        c_hash = hashlib.md5(doc.page_content.encode()).hexdigest()
        uid = str(md.get("chunk_uuid") or f"hash-{c_hash}")
        if uid not in unique_map:
            unique_map[uid] = doc

    candidates = list(unique_map.values())

    # 4. Batch Reranking (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Batch Size ‡∏ï‡∏≤‡∏° Device)
    final_scored_docs = []
    reranker = get_global_reranker()

    if reranker and candidates and queries_to_run:
        try:
            # ‡πÉ‡∏ä‡πâ Query ‡πÅ‡∏£‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏Å‡∏ô‡∏Å‡∏•‡∏≤‡∏á‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Rerank
            main_query = queries_to_run[0]
            
            # [CRITICAL] ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ Batch Size ‡∏à‡∏≤‡∏Å global_vars (16 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Mac) 
            # ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡∏ï‡∏Å‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏û‡∏µ‡πâ‡∏¢‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÄ‡∏¢‡∏≠‡∏∞‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ‡∏ó‡∏µ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
            final_scored_docs = reranker.compress_documents(
                documents=candidates, 
                query=main_query,
                batch_size=DEFAULT_EMBED_BATCH_SIZE
            )
        except Exception as e:
            logger.error(f"‚ö†Ô∏è Rerank Error: {e}")
            final_scored_docs = candidates
    else:
        final_scored_docs = candidates

    # 5. Ranking & Score Extraction (‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Ç‡∏∂‡πâ‡∏ô)
    def extract_score(d) -> float:
        m = d.metadata or {}
        # ‡∏î‡∏∂‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏à‡∏≤‡∏Å attribute ‡∏Ç‡∏≠‡∏á Reranker ‡∏Å‡πà‡∏≠‡∏ô ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≠‡∏¢‡πÑ‡∏õ‡∏î‡∏π‡πÉ‡∏ô Metadata
        if hasattr(d, "relevance_score"): return float(d.relevance_score)
        return float(m.get("relevance_score") or m.get("score") or m.get("rerank_score") or 0.0)

    final_scored_docs.sort(key=extract_score, reverse=True)

    # 6. ‡∏Ñ‡∏±‡∏î‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö
    top_evidences = []
    aggregated_parts = []
    final_limit = ANALYSIS_FINAL_K

    for doc in final_scored_docs:
        if len(top_evidences) >= final_limit:
            break
            
        score = extract_score(doc)
        
        # ‡∏Å‡∏£‡∏≠‡∏á‡∏ó‡∏¥‡πâ‡∏á‡∏´‡∏≤‡∏Å‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤ Threshold (0.20)
        if score < RERANK_THRESHOLD and RERANK_THRESHOLD > 0:
            continue

        md = doc.metadata or {}
        text = doc.page_content.strip()
        
        # ‡∏™‡∏Å‡∏±‡∏î Metadata (Page / Source / PDCA)
        page = str(md.get("page_label") or md.get("page_number") or md.get("page") or "N/A")
        source = md.get("source_filename") or md.get("source") or "Unknown"
        pdca = md.get("pdca_tag", "Other")

        # [IMPORTANT] Sync ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤ metadata ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Prompt Assessment ‡πÄ‡∏´‡πá‡∏ô‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏à‡∏£‡∏¥‡∏á
        md["score"] = score
        md["relevance_score"] = score

        top_evidences.append({
            "doc_id": str(md.get("stable_doc_uuid") or md.get("doc_id") or ""),
            "chunk_uuid": str(md.get("chunk_uuid") or str(uuid.uuid4())),
            "source": source,
            "text": text,
            "page": page,
            "pdca_tag": pdca,
            "score": score,
            "metadata": md
        })
        aggregated_parts.append(f"[{pdca}] [‡πÑ‡∏ü‡∏•‡πå: {source} ‡∏´‡∏ô‡πâ‡∏≤: {page}] {text}")

    total_time = round(time.time() - start_time, 3)
    max_score = extract_score(final_scored_docs[0]) if final_scored_docs else 0.0
    
    logger.info(f"üèÅ Retrieval Finished: {len(top_evidences)} chunks | Max Score: {max_score:.4f} | Time: {total_time}s")

    return {
        "top_evidences": top_evidences,
        "aggregated_context": "\n\n---\n\n".join(aggregated_parts) if aggregated_parts else "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô",
        "retrieval_time": total_time,
        "max_score": max_score
    }

# =====================================================================
# üõ† Helper: check_rubric_readiness (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏á‡∏µ‡∏¢‡∏ö‡∏•‡∏á)
# =====================================================================
def is_rubric_ready(tenant: str) -> bool:
    """ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏Ç‡∏≠‡∏á seam collection ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏û‡πà‡∏ô Warning ‡∏Å‡∏ß‡∏ô‡πÉ‡∏à """
    if not tenant:
        return False
    
    tenant_vs_root = get_vectorstore_tenant_root_path(tenant)
    chroma_path = os.path.join(tenant_vs_root, "seam")
    
    # ‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ True/False ‡πÄ‡∏á‡∏µ‡∏¢‡∏ö‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏î‡∏∂‡∏á Rubric ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    return os.path.exists(chroma_path)

def _format_evidence_item(doc: LcDocument, score: float) -> Dict[str, Any]:
    """ Helper ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Output ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô """
    m = doc.metadata or {}
    return {
        "text": doc.page_content,
        "source_filename": m.get("source_filename") or m.get("source") or "Evidence",
        "page_label": str(m.get("page_label") or m.get("page_number") or m.get("page") or "N/A"),
        "doc_id": str(m.get("stable_doc_uuid") or m.get("doc_id") or ""),
        "chunk_uuid": str(m.get("chunk_uuid") or str(uuid.uuid4())),
        "pdca_tag": m.get("pdca_tag") or "Content",
        "rerank_score": score,
        "is_evidence": True,
        "metadata": m
    }

# =====================================================================
# üöÄ Ultimate Version: retrieve_context_with_rubric (FULL REVISED)
# =====================================================================
def retrieve_context_with_rubric(
    vectorstore_manager,
    query: str,
    doc_type: str,
    enabler: Optional[str] = None,
    stable_doc_ids: Optional[Set[str]] = None,
    tenant: Optional[str] = None,
    year: Optional[Union[int, str]] = None,
    subject: Optional[str] = None,
    rubric_vectorstore_name: str = "seam", 
    top_k: int = 150,         
    rubric_top_k: int = 15,  
    k_to_rerank: int = 30    
) -> Dict[str, Any]:
    """
    [PRODUCTION REVISED] ‡∏£‡∏∞‡∏ö‡∏ö‡∏î‡∏∂‡∏á‡∏ö‡∏£‡∏¥‡∏ö‡∏ó (Context) ‡πÅ‡∏ö‡∏ö Hybrid: Rubric + Evidence
    - ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Content-Based MD5 Deduplication
    - ‡πÉ‡∏ä‡πâ Batch Reranking ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡∏ö‡∏ô Mac/Server
    - ‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏á‡∏ó‡∏µ‡πà‡∏Ç‡∏≠‡∏á ID ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô SE-AM
    """
    start_time = time.time()
    vsm = vectorstore_manager

    # --- 1. ‡∏Å‡∏≤‡∏£‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Collection (Auto-Switch) ---
    if hasattr(vsm, 'doc_type') and vsm.doc_type != doc_type:
        logger.info(f"üîÑ Switching VSM doc_type to: {doc_type}")
        vsm.close()
        vsm.__init__(tenant=tenant, year=year, doc_type=doc_type, enabler=enabler)

    evidence_collection = get_doc_type_collection_key(doc_type, enabler or "KM")
    
    rubric_results = []
    unique_evidence_map: Dict[str, LcDocument] = {}

    # --- 2. ‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á Rubrics (‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô SE-AM) ---
    # ‡πÉ‡∏ä‡πâ Helper check_rubric_ready ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏á‡∏µ‡∏¢‡∏ö (Silent Mode)
    if is_rubric_ready(tenant): 
        try:
            rubric_chroma = vsm._load_chroma_instance(rubric_vectorstore_name)
            if rubric_chroma:
                rubric_query = f"‡πÄ‡∏Å‡∏ì‡∏ë‡πå SE-AM {enabler} {subject or ''}: {query}"
                r_docs = rubric_chroma.similarity_search(rubric_query, k=rubric_top_k)
                for rd in r_docs:
                    rubric_results.append({
                        "text": rd.page_content, 
                        "metadata": rd.metadata, 
                        "is_rubric": True
                    })
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Rubric Retrieval Error: {e}")
    else:
        logger.info(f"‚ÑπÔ∏è Rubric skip: Collection 'seam' not found for tenant: {tenant}")

    # --- 3. ‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á Evidence (‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô) ---
    try:
        evidence_chroma = vsm._load_chroma_instance(evidence_collection)
        if not evidence_chroma:
            return {"top_evidences": [], "rubric_context": rubric_results, "retrieval_time": 0}

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Filter ‡πÉ‡∏´‡πâ‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö DataType (Int/Str)
        where_filter = None
        if stable_doc_ids:
            ids_list = [str(i).strip().lower() for i in stable_doc_ids if i]
            if len(ids_list) == 1:
                where_filter = {"stable_doc_uuid": ids_list[0]}
            else:
                where_filter = {"stable_doc_uuid": {"$in": ids_list}}
            
            # ‚öì 3.1 Anchor Chunks (‡∏™‡πà‡∏ß‡∏ô‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏•‡∏±‡∏Å ‡πÄ‡∏ä‡πà‡∏ô ‡∏´‡∏ô‡πâ‡∏≤ 1-5)
            # ‡∏î‡∏±‡∏ô‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÉ‡∏´‡πâ‡∏™‡∏π‡∏á (0.95) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà AI ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏´‡πá‡∏ô
            anchors = evidence_chroma.get(where=where_filter, limit=10)
            if anchors and anchors.get('documents'):
                for i in range(len(anchors['documents'])):
                    content = anchors['documents'][i]
                    md = dict(anchors['metadatas'][i] or {}) 
                    
                    # ‡πÉ‡∏ä‡πâ MD5 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ ID ‡∏ô‡∏¥‡πà‡∏á‡∏ï‡∏•‡∏≠‡∏î‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô (Deterministic)
                    content_hash = hashlib.md5(content.encode()).hexdigest()
                    uid = str(md.get("chunk_uuid") or f"anchor-{content_hash}")
                    
                    if uid not in unique_evidence_map:
                        md.update({
                            "score": 0.95,
                            "relevance_score": 0.95,
                            "is_anchor": True
                        })
                        unique_evidence_map[uid] = LcDocument(page_content=content, metadata=md)

        # üîç 3.2 Semantic Search (‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á Query)
        search_results = evidence_chroma.similarity_search(query, k=top_k, filter=where_filter)
        for d in search_results:
            # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å hash() ‡πÄ‡∏õ‡πá‡∏ô MD5 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Ç‡πâ‡∏≤‡∏° Environment (Mac/Server)
            c_hash = hashlib.md5(d.page_content.encode()).hexdigest()
            uid = d.metadata.get("chunk_uuid") or c_hash
            if uid not in unique_evidence_map:
                unique_evidence_map[uid] = d

        candidates = list(unique_evidence_map.values())

        # --- 4. BATCH RERANKING (‡∏£‡∏∞‡∏ö‡∏ö‡∏Ñ‡∏±‡∏î‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á) ---
        evidence_results = []
        reranker = get_global_reranker()
        
        if reranker and candidates and query:
            try:
                # ‡∏î‡∏∂‡∏á Batch Size ‡∏à‡∏≤‡∏Å ENV (Mac: 16 / Server: 128)
                from config.global_vars import DEFAULT_EMBED_BATCH_SIZE
                batch_size = DEFAULT_EMBED_BATCH_SIZE
                scored_candidates = []
                
                logger.info(f"üöÄ Batch Reranking {len(candidates)} chunks...")
                for i in range(0, len(candidates), batch_size):
                    batch = candidates[i : i + batch_size]
                    reranked_batch = reranker.compress_documents(documents=batch, query=query)
                    scored_candidates.extend(reranked_batch)
                
                # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô (Desc)
                scored_candidates = sorted(
                    scored_candidates, 
                    key=lambda x: getattr(x, "relevance_score", 0), 
                    reverse=True
                )
                
                for r in scored_candidates[:k_to_rerank]:
                    doc = r.document if hasattr(r, "document") else r
                    m = doc.metadata or {}
                    score = getattr(r, "relevance_score", 0.0)
                    
                    # Sync ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤ Metadata ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ô‡∏≥‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡∏ï‡πà‡∏≠
                    m["rerank_score"] = score
                    m["score"] = score
                    
                    evidence_results.append({
                        "text": doc.page_content,
                        "source_filename": m.get("source_filename") or m.get("source") or "Evidence",
                        "page_label": str(m.get("page_label") or m.get("page_number") or m.get("page") or "N/A"),
                        "doc_id": str(m.get("stable_doc_uuid") or m.get("doc_id") or ""),
                        "chunk_uuid": str(m.get("chunk_uuid") or str(uuid.uuid4())),
                        "pdca_tag": m.get("pdca_tag") or "Content",
                        "rerank_score": score,
                        "is_evidence": True,
                        "metadata": m
                    })
            except Exception as e:
                logger.error(f"‚ö†Ô∏è Rerank failed: {e}")
                # Fallback: ‡πÉ‡∏ä‡πâ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡πÅ‡∏ö‡∏ö‡∏õ‡∏Å‡∏ï‡∏¥‡∏´‡∏≤‡∏Å Reranker ‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤
                for d in candidates[:k_to_rerank]:
                    evidence_results.append(_format_evidence_item(d, 0.0))
        
        # ‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏°‡πà‡∏°‡∏µ Reranker ‡∏´‡∏£‡∏∑‡∏≠‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö
        elif candidates:
            for d in candidates[:k_to_rerank]:
                evidence_results.append(_format_evidence_item(d, 0.0))

    except Exception as e:
        logger.error(f"‚ùå Evidence Retrieval Error: {e}", exc_info=True)

    retrieval_time = round(time.time() - start_time, 3)
    logger.info(f"‚úÖ Success: Retrieved {len(evidence_results)} evidence chunks in {retrieval_time}s")

    return {
        "top_evidences": evidence_results,
        "rubric_context": rubric_results,
        "retrieval_time": retrieval_time,
        "used_chunk_uuids": [e["chunk_uuid"] for e in evidence_results if e.get("chunk_uuid")]
    }


# =====================================================================
# üöÄ Ultimate Version: retrieve_context_by_doc_ids (FULL REVISED)
# =====================================================================
def retrieve_context_by_doc_ids(
    doc_uuids: List[str],
    doc_type: str,
    enabler: Optional[str] = None,
    vectorstore_manager = None,
    limit: int = 200,          # üöÄ ‡πÄ‡∏û‡∏¥‡πà‡∏° limit ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏ç‡πà
    tenant: Optional[str] = None,
    year: Optional[Union[int, str]] = None,
) -> Dict[str, Any]:
    """
    [PRODUCTION REVISED] ‡∏î‡∏∂‡∏á Chunks ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏ (Hydration)
    - ‡πÉ‡∏ä‡πâ MD5 Hashing ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥ Deduplication ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ã‡πâ‡∏≥
    - ‡∏£‡∏±‡∏Å‡∏©‡∏≤‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Metadata ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ö‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô Search ‡∏´‡∏•‡∏±‡∏Å
    - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Multi-tenant ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å Collection ‡∏ï‡∏≤‡∏° Enabler/Year
    """
    start_time = time.time()
    
    # 1. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ VectorStore Manager
    vsm = vectorstore_manager
    if not vsm:
        from core.vectorstore import VectorStoreManager # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Circular Import
        vsm = VectorStoreManager(tenant=tenant, year=year)

    # 2. ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ä‡∏∑‡πà‡∏≠ Collection ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
    collection_name = get_doc_type_collection_key(doc_type=doc_type, enabler=enabler or "KM")

    chroma = vsm._load_chroma_instance(collection_name)
    if not chroma:
        logger.error(f"‚ùå Collection '{collection_name}' not found for hydration")
        return {"top_evidences": [], "retrieval_time": 0}

    if not doc_uuids:
        return {"top_evidences": [], "retrieval_time": 0}

    # ‡∏•‡πâ‡∏≤‡∏á‡∏Ñ‡πà‡∏≤ ID ‡πÉ‡∏´‡πâ‡∏™‡∏∞‡∏≠‡∏≤‡∏î (Trim & Lower)
    ids_to_query = [str(u).strip().lower() for u in doc_uuids if u]
    logger.info(f"üíß Hydrating Context: {len(ids_to_query)} docs from '{collection_name}'")

    try:
        # 3. ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å ChromaDB ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á (Direct Get)
        # ‡πÉ‡∏ä‡πâ Metadata Filter ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
        results = chroma._collection.get(
            where={"stable_doc_uuid": {"$in": ids_to_query}},
            limit=limit,
            include=["documents", "metadatas"]
        )
    except Exception as e:
        logger.error(f"‚ö†Ô∏è Hydration query failed: {e}")
        return {"top_evidences": [], "retrieval_time": 0}

    evidences = []
    seen_contents = set()

    # 4. ‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Formatting & Deduplication)
    documents = results.get("documents") or []
    metadatas = results.get("metadatas") or []

    for doc_content, meta in zip(documents, metadatas):
        if not doc_content or not doc_content.strip():
            continue
            
        # üéØ ‡πÉ‡∏ä‡πâ MD5 ‡πÅ‡∏ó‡∏ô hash() ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏á‡∏ó‡∏µ‡πà (Deterministic)
        content_hash = hashlib.md5(doc_content.encode()).hexdigest()
        if content_hash in seen_contents:
            continue
        seen_contents.add(content_hash)

        # ‡∏î‡∏∂‡∏á Metadata ‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç (Fallback ‡πÅ‡∏ö‡∏ö‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö Search)
        m = meta or {}
        p_val = str(m.get("page_label") or m.get("page_number") or m.get("page") or "N/A")
        
        # üéØ ‡∏õ‡∏£‡∏±‡∏ö Key ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö _format_evidence_item ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Router ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
        evidences.append({
            "text": doc_content.strip(),
            "source_filename": m.get("source_filename") or m.get("source") or "Evidence",
            "page_label": p_val,
            "doc_id": str(m.get("stable_doc_uuid") or m.get("doc_id") or ""),
            "chunk_uuid": str(m.get("chunk_uuid") or content_hash),
            "pdca_tag": m.get("pdca_tag") or "Content",
            "rerank_score": float(m.get("score") or m.get("rerank_score") or 0.85), # Hydration Score
            "is_evidence": True,
            "metadata": m
        })

    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏°‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ AI ‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤)
    try:
        evidences.sort(key=lambda x: int(x['page_label']) if x['page_label'].isdigit() else 999)
    except:
        pass

    retrieval_time = round(time.time() - start_time, 3)
    logger.info(f"‚úÖ Hydration success: {len(evidences)} chunks in {retrieval_time}s")

    return {
        "top_evidences": evidences,
        "retrieval_time": retrieval_time
    }


def _fetch_llm_response(
    system_prompt: str = "",
    user_prompt: str = "",
    max_retries: int = 3,
    llm_executor: Any = None
) -> str:
    """
    [IRONCLAD LLM FETCHER - FINAL POLISH v2026.1.26]
    - ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö LLM ‡πÉ‡∏´‡πâ‡∏™‡πà‡∏á VALID JSON ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (prompt ‡πÄ‡∏Ç‡πâ‡∏°‡∏á‡∏ß‡∏î + ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö)
    - Multi-stage clean-up + greedy extraction + json_repair (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
    - Retry ‡∏â‡∏•‡∏≤‡∏î + exponential backoff + prompt variation ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß
    - Log raw + cleaned ‡∏ó‡∏∏‡∏Å attempt ‡πÄ‡∏û‡∏∑‡πà‡∏≠ debug (‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ)
    - ‡∏Ñ‡∏∑‡∏ô JSON string ‡∏ó‡∏µ‡πà‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÄ‡∏™‡∏°‡∏≠ (fallback ‡∏ñ‡πâ‡∏≤‡∏û‡∏±‡∏á‡∏´‡∏°‡∏î)
    - ‡πÄ‡∏û‡∏¥‡πà‡∏°: Avoid Unicode escape sequences ‡πÉ‡∏ô Thai text
    """
    if llm_executor is None:
        raise ConnectionError("LLM instance not initialized.")

    # 1. System Prompt ‡πÄ‡∏Ç‡πâ‡∏°‡∏á‡∏ß‡∏î‡∏™‡∏∏‡∏î (‡∏£‡∏ß‡∏° Avoid Unicode escape + ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô)
    enforced_system = (
        (system_prompt or "").strip() + "\n\n"
        "### ABSOLUTE RULES - MUST FOLLOW OR FAIL ###\n"
        "1. Respond with **ONLY** valid JSON. NO text before or after.\n"
        "2. NO markdown (```json, ```), NO explanations, NO 'Here is...', NO apologies.\n"
        "3. Use double quotes for ALL keys and string values.\n"
        "4. If string contains double quote, escape it as \\\" or use single quote instead.\n"
        "5. All braces { } and brackets [ ] MUST be balanced.\n"
        "6. IMPORTANT: For Thai text, use normal Thai characters ONLY.\n"
        "   DO NOT use Unicode escape sequences (e.g. \\u0e23 \\u0e35 \\u0e07).\n"
        "   Output readable Thai directly (‡πÄ‡∏ä‡πà‡∏ô \"‡∏à‡∏±‡∏î‡∏ó‡∏≥‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á\" ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà \"\\u0e08\\u0e31\\u0e14\\u0e17\\u0e33\").\n"
        "7. Return ONLY array or object. Examples:\n"
        '   [{"action": "‡∏à‡∏±‡∏î‡∏ó‡∏≥‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÅ‡∏ï‡πà‡∏á‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡∏ì‡∏∞‡∏ó‡∏≥‡∏á‡∏≤‡∏ô KM", "target_evidence": "‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£ ‡∏â‡∏ö‡∏±‡∏ö‡∏ó‡∏µ‡πà..."}]\n'
        '   [{"score": 1.0, "is_passed": true, "reason": "‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô"}]\n'
        "FAILURE TO COMPLY = INVALID RESPONSE"
    )

    messages = [
        {"role": "system", "content": enforced_system},
        {"role": "user",   "content": (user_prompt or "").strip()}
    ]

    for attempt in range(1, max_retries + 1):
        try:
            # 2. LLM Call (temperature 0.0 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° deterministic ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î)
            response = llm_executor.invoke(messages, config={"temperature": 0.0})

            raw_text = ""
            if hasattr(response, "content"):
                raw_text = str(response.content).strip()
            elif hasattr(response, "text"):
                raw_text = str(response.text).strip()
            else:
                raw_text = str(response or "").strip()

            # Log raw response ‡πÄ‡∏ï‡πá‡∏° (‡∏à‡∏≥‡∏Å‡∏±‡∏î 1500 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£)
            logger.critical(f"[LLM-RAW attempt {attempt}] (len={len(raw_text)}):\n{raw_text[:1500]}{'...' if len(raw_text) > 1500 else ''}")

            # 3. Multi-stage Clean-up
            # Stage 1: ‡∏•‡∏ö markdown ‡πÅ‡∏•‡∏∞ code fences
            cleaned = re.sub(r'```(?:json)?\s*|\s*```', '', raw_text).strip()

            # Stage 2: ‡∏•‡∏ö whitespace ‡πÄ‡∏Å‡∏¥‡∏ô + trailing comma + unbalanced quotes
            cleaned = re.sub(r',\s*([}\]])', r'\1', cleaned)
            cleaned = re.sub(r'\s+', ' ', cleaned).strip()

            # Stage 3: Greedy ‡∏´‡∏≤ JSON block ‡πÉ‡∏´‡∏ç‡πà‡∏™‡∏∏‡∏î (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á object ‡πÅ‡∏•‡∏∞ array)
            json_match = re.search(r'(\{[\s\S]*?\}|\[[\s\S]*?\])', cleaned, re.DOTALL)
            if json_match:
                extracted = json_match.group(1)
            else:
                extracted = cleaned  # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î

            # Stage 4: ‡πÉ‡∏ä‡πâ json_repair ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ (‡∏î‡∏µ‡∏°‡∏≤‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LLM output ‡∏û‡∏±‡∏á)
            if repair_json:
                try:
                    repaired = repair_json(extracted)
                    json.loads(repaired)  # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö parse
                    logger.debug(f"[JSON-REPAIR-SUCCESS] attempt {attempt}")
                    return repaired
                except Exception as repair_err:
                    logger.debug(f"[JSON-REPAIR-FAIL] {str(repair_err)} ‚Üí fallback to manual")

            # Stage 5: Manual salvage (‡∏•‡∏ö control chars + unbalanced)
            extracted = re.sub(r'[\x00-\x1F\x7F]', '', extracted)  # ‡∏•‡∏ö control chars
            try:
                json.loads(extracted)
                logger.debug(f"[MANUAL-PARSE-SUCCESS] attempt {attempt}")
                return extracted
            except json.JSONDecodeError as je:
                logger.warning(f"[JSON-PARSE-FAIL attempt {attempt}]: {str(je)}")

            # ‡∏ñ‡πâ‡∏≤‡∏û‡∏±‡∏á‡∏´‡∏°‡∏î ‚Üí retry ‡∏î‡πâ‡∏ß‡∏¢ prompt variation
            if attempt < max_retries:
                messages[1]["content"] += (
                    f"\n\nPrevious attempt failed (invalid JSON). "
                    f"Fix it now and return **ONLY** valid JSON. No extra text."
                )
                time.sleep(1.5 ** attempt)  # exponential backoff
                continue

        except Exception as e:
            logger.error(f"[LLM-EXCEPTION attempt {attempt}]: {str(e)}")
            if attempt < max_retries:
                time.sleep(1.5 ** attempt)
            else:
                break

    # Ultimate Fallback (‡∏Ñ‡∏∑‡∏ô JSON ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡πÄ‡∏™‡∏°‡∏≠)
    logger.critical(f"[LLM-FINAL-FALLBACK] All {max_retries} attempts failed")
    return json.dumps({
        "score": 0.0,
        "is_passed": False,
        "reason": "Failed to generate valid JSON after retries (system fallback)",
        "fallback": True
    }, ensure_ascii=False)  # ensure_ascii=False ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÑ‡∏°‡πà escape

# ------------------------
# Evaluation
# ------------------------
T = TypeVar("T", bound=BaseModel)

def _check_and_handle_empty_context(context: str, sub_id: str, level: int) -> Optional[Dict[str, Any]]:
    """
    Returns Failure result if context is empty or contains known error strings.
    Auto-fail with PDCA keys all set to 0.
    """
    if not context or "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á" in context or "ERROR:" in context.upper():
        logger.warning(f"Auto-FAIL L{level} for {sub_id}: Empty or Error Context detected from RAG.")
        context_preview = context.strip()[:100].replace("\n", " ") if context else "Empty Context"
        return {
            "score": 0,
            "reason": f"‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏î‡πâ‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á (Context: {context_preview}).",
            "is_passed": False,
            "P_Plan_Score": 0,
            "D_Do_Score": 0,
            "C_Check_Score": 0,
            "A_Act_Score": 0,
        }
    return None

def _get_context_for_level(
    context: str,
    level: int,
    chunks: list = None,
    **kwargs
) -> str:
    """
    [DYNAMIC REVISED 2026] 
    ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á Context ‡∏ï‡∏≤‡∏° RAG_RUN_MODE ‡πÉ‡∏ô .env ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
    - Mac (LOCAL_OLLAMA): ‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î Context ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß (8B Model)
    - Server (PRODUCTION): ‡∏Ç‡∏¢‡∏≤‡∏¢ Context ‡πÄ‡∏ï‡πá‡∏°‡∏®‡∏±‡∏Å‡∏¢‡∏†‡∏≤‡∏û (70B Model)
    """
    if not context:
        return "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô"

    import os
    # 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÇ‡∏´‡∏°‡∏î‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô‡∏à‡∏≤‡∏Å .env ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß
    run_mode = os.getenv("RAG_RUN_MODE", "LOCAL_OLLAMA")
    is_server = run_mode == "PRODUCTION"

    # 2. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ç‡∏µ‡∏î‡∏à‡∏≥‡∏Å‡∏±‡∏î (Limits) ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏£‡∏á‡∏Ç‡∏≠‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á (Detect ‡∏à‡∏≤‡∏Å env ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á)
    if is_server:
        # ‡∏ù‡∏±‡πà‡∏á Server: ‡πÉ‡∏ä‡πâ ANALYSIS_FINAL_K ‡∏à‡∏≤‡∏Å .env ‡∏´‡∏£‡∏∑‡∏≠ Default 35
        max_chunks = int(os.getenv("ANALYSIS_FINAL_K", 35))
        max_chars = 20000 if level > 2 else 25000  # 70B ‡∏£‡∏±‡∏ö‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡πÑ‡∏î‡πâ‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡∏Å‡∏ß‡πà‡∏≤
    else:
        # ‡∏ù‡∏±‡πà‡∏á Mac: ‡∏ö‡∏µ‡∏ö‡πÉ‡∏´‡πâ‡πÄ‡∏•‡πá‡∏Å‡∏•‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∑‡πà‡∏ô‡πÑ‡∏´‡∏•‡∏Ç‡∏≠‡∏á 8B
        max_chunks = int(os.getenv("ANALYSIS_FINAL_K", 15))
        max_chars = 8000 if level > 2 else 10000

    # 3. ‡∏Å‡∏£‡∏ì‡∏µ‡∏°‡∏µ Chunks ( List of Dicts )
    if chunks:
        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ï‡∏≤‡∏° Score (‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÑ‡∏ß‡πâ‡∏ö‡∏ô‡∏™‡∏∏‡∏î)
        sorted_chunks = sorted(
            chunks,
            key=lambda c: float(c.get('rerank_score') or c.get('score') or 0.0),
            reverse=True
        )
        
        selected = sorted_chunks[:max_chunks]
        parts = []
        
        for i, c in enumerate(selected, 1):
            score = c.get('rerank_score') or c.get('score', 'N/A')
            source = c.get('source') or c.get('source_filename', 'Unknown')
            page = c.get('page') or c.get('page_label', 'N/A')
            text = c.get('text', '').strip()
            pdca = c.get('pdca_tag', 'N/A')
            
            if text:
                # ‡∏à‡∏±‡∏î Format ‡πÉ‡∏´‡πâ AI ‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ô‡∏≥‡πÑ‡∏õ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô Markdown Table ‡∏ï‡πà‡∏≠‡πÑ‡∏î‡πâ
                parts.append(
                    f"### ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ä‡∏¥‡πâ‡∏ô‡∏ó‡∏µ‡πà {i} | Tag: {pdca} | Score: {score} | ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏≤: {source} ‡∏´‡∏ô‡πâ‡∏≤ {page}\n"
                    f"{text}\n"
                    f"{'-'*40}"
                )

        final_text = "\n\n".join(parts)
        
        # Hard Cap ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Context Overload
        if len(final_text) > max_chars:
            final_text = final_text[:max_chars] + "\n... [‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ñ‡∏π‡∏Å‡∏ï‡∏±‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°]"
        return final_text

    # 4. Fallback ‡∏Å‡∏£‡∏ì‡∏µ‡∏™‡πà‡∏á‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô String ‡∏¢‡∏≤‡∏ß‡πÜ
    return context[:max_chars] + ("... [truncated]" if len(context) > max_chars else "")

# =================================================================
# 1. CORE LLM EVALUATION FUNCTIONS (Revised for New Prompts)
# =================================================================
def evaluate_with_llm(
    context: str, 
    sub_criteria_name: str, 
    level: int, 
    statement_text: str, 
    sub_id: str, 
    llm_executor: Any = None, 
    required_phases: List[str] = None,
    specific_contextual_rule: str = "‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô",
    ai_confidence: str = "MEDIUM",
    confidence_reason: str = "N/A", # ‚úÖ ‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏ó‡∏µ‡πà‡∏´‡∏±‡∏ß‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏ï‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
    pdca_context: str = "", 
    **kwargs
) -> Dict[str, Any]:
    """
    [EXPLICIT REVISED v2026.01.27] - ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á (L3-L5)
    - STRATEGY: ‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏® Argument ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡πÅ‡∏•‡∏∞‡∏•‡πâ‡∏≤‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô‡πÉ‡∏ô kwargs
    """
    logger = logging.getLogger(__name__)

    # 1. üõ°Ô∏è [SHIELDING] ‡∏•‡πâ‡∏≤‡∏á‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô‡πÉ‡∏ô kwargs ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô TypeError 
    # ‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å Argument ‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏Ç‡πâ‡∏≤‡∏á‡∏ö‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å ‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏ô kwargs ‡πÉ‡∏´‡πâ‡∏•‡∏ö‡∏ó‡∏¥‡πâ‡∏á‡πÑ‡∏õ
    kwargs.pop("confidence_reason", None)
    
    # ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤ Enabler (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Default)
    e_code = str(kwargs.pop("enabler", "UNK")).upper()
    e_name_th = str(kwargs.pop("enabler_name_th", f"‡∏î‡πâ‡∏≤‡∏ô {e_code}"))

    # 2. [PREPARING] ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
    ctx_raw = str(context or "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô")
    pdca_ctx = str(pdca_context or "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏¢‡∏Å‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà")
    context_to_send_eval = _get_context_for_level(ctx_raw, level) or ""
    phases_str = ", ".join(str(p).strip() for p in (required_phases or [])) if required_phases else "P, D, C, A"

    try:
        # 3. üéØ [FORMATTING] ‡∏â‡∏µ‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Ç‡πâ‡∏≤ Prompt
        full_prompt = USER_ASSESSMENT_PROMPT.format(
            sub_criteria_name=sub_criteria_name,
            sub_id=sub_id,
            level=level,
            statement_text=statement_text,
            context=context_to_send_eval[:25000], 
            pdca_context=pdca_ctx[:8000],         
            required_phases=phases_str,
            specific_contextual_rule=specific_contextual_rule,
            ai_confidence=ai_confidence,
            confidence_reason=confidence_reason, # ‚úÖ ‡πÉ‡∏ä‡πâ‡∏à‡∏≤‡∏Å Argument ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
            enabler=e_code,
            enabler_name_th=e_name_th,
            **kwargs # ‚úÖ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠ (focus_points, evidence_guidelines) ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏â‡∏µ‡∏î‡πÄ‡∏Ç‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà
        )
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏° Baseline Summary (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏™‡πà‡∏á‡∏°‡∏≤‡πÉ‡∏ô kwargs)
        if kwargs.get("baseline_summary"):
            full_prompt += f"\n\n--- BASELINE DATA ---\n{kwargs['baseline_summary']}"

        system_msg = f"Expert SE-AM Auditor for {e_name_th} ({e_code})"
        
        # 4. [EXECUTION] ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å LLM
        raw_response = _fetch_llm_response(
            system_prompt=system_msg,
            user_prompt=full_prompt,
            llm_executor=llm_executor
        )

        # 5. [PARSING] ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô Audit Object
        parsed = _robust_extract_json(raw_response)
        return _build_audit_result_object(
            parsed, raw_response, context_to_send_eval, ai_confidence, 
            level=level, sub_id=sub_id, enabler_full_name=e_name_th, enabler_code=e_code
        )

    except Exception as e:
        logger.error(f"üõë Evaluation Error Sub:{sub_id} L{level}: {str(e)}")
        return _create_fallback_error(sub_id, level, e, context_to_send_eval, e_name_th, e_code)


def evaluate_with_llm_low_level(
    context: str,
    sub_criteria_name: str,
    level: int,
    statement_text: str,
    sub_id: str,
    llm_executor: Any = None,
    required_phases: List[str] = None,
    specific_contextual_rule: str = "‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô",
    ai_confidence: str = "MEDIUM",
    confidence_reason: str = "N/A", # ‚úÖ ‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏ó‡∏µ‡πà‡∏´‡∏±‡∏ß‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
    pdca_context: str = "",
    **kwargs
) -> Dict[str, Any]:
    """
    [EXPLICIT REVISED v2026.01.27] - ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô (L1-L2)
    """
    logger = logging.getLogger(__name__)

    # 1. üõ°Ô∏è [SHIELDING] ‡∏•‡πâ‡∏≤‡∏á‡∏Ñ‡πà‡∏≤‡∏ã‡πâ‡∏≥‡πÉ‡∏ô kwargs
    kwargs.pop("confidence_reason", None)
    e_code = str(kwargs.pop("enabler", "UNK")).upper()
    e_name_th = str(kwargs.pop("enabler_name_th", f"‡∏î‡πâ‡∏≤‡∏ô {e_code}"))
    
    # ‡∏î‡∏∂‡∏á Keywords ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡∏≠‡∏á L1-L2
    plan_keywords = kwargs.pop("plan_keywords", "‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô, ‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢, ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á, ‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô")
    
    pdca_ctx = str(pdca_context or "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏¢‡∏Å‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà")
    phases_str = ", ".join(str(p) for p in (required_phases or [])) if required_phases else "P, D"

    try:
        # 2. üéØ [FORMATTING]
        full_prompt = USER_LOW_LEVEL_PROMPT.format(
            sub_id=sub_id,
            sub_criteria_name=sub_criteria_name,
            level=level,
            statement_text=statement_text,
            context=str(context)[:25000],
            pdca_context=pdca_ctx[:8000],
            required_phases=phases_str,
            specific_contextual_rule=specific_contextual_rule,
            ai_confidence=ai_confidence,
            confidence_reason=confidence_reason, # ‚úÖ ‡πÉ‡∏ä‡πâ‡∏à‡∏≤‡∏Å Argument ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
            plan_keywords=plan_keywords,
            enabler=e_code,
            enabler_name_th=e_name_th,
            **kwargs 
        )

        system_msg = f"Foundation Auditor for {e_name_th} ({e_code})"
        
        # 3. [EXECUTION]
        raw_response = _fetch_llm_response(
            system_prompt=system_msg,
            user_prompt=full_prompt,
            llm_executor=llm_executor
        )

        parsed = _robust_extract_json(raw_response)
        return _build_audit_result_object(
            parsed, raw_response, context, ai_confidence, 
            level=level, sub_id=sub_id, enabler_full_name=e_name_th, enabler_code=e_code
        )

    except Exception as e:
        logger.error(f"üõë Low-Level Eval Error Sub:{sub_id} L{level}: {str(e)}")
        return _create_fallback_error(sub_id, level, e, context, e_name_th, e_code)
    
def _build_audit_result_object(
    parsed: Dict, 
    raw_response: str, 
    context: str, 
    confidence: str, 
    **kwargs
) -> Dict[str, Any]:
    """
    [ULTIMATE-SYNC v2026.01.27] ‚Äî THE COMPLETE AUDITOR OBJECT
    - üëî Integrated 'executive_summary' as primary narrative output.
    - üìé Enhanced 'evidence_sources' mapping for UI linking.
    - üõ°Ô∏è PDCA Coercion & Safety Fallback for scoring.
    """
    from datetime import datetime
    
    # 1. [EXTRACT METADATA]
    level = int(kwargs.get('level', 1))
    sub_id = str(kwargs.get('sub_id', 'Unknown'))
    enabler_full_name = kwargs.get('enabler_full_name', 'Unknown Enabler')
    enabler_code = kwargs.get('enabler_code', 'UNK')

    def clean_score(val, default=0.0):
        if val is None: return default
        try:
            return round(float(val), 2)
        except (ValueError, TypeError):
            return default

    # ‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏Å‡∏£‡∏ì‡∏µ parsed ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà dict
    if not isinstance(parsed, dict):
        parsed = {}

    # 2. [SCORING & STATUS] üìä
    # ‡∏î‡∏∂‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏´‡∏•‡∏±‡∏Å ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì is_passed ‡∏´‡∏≤‡∏Å AI ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏™‡πà‡∏á‡∏°‡∏≤
    score = clean_score(parsed.get("score"))
    is_passed = parsed.get("is_passed")
    if is_passed is None:
        # Fallback ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏ú‡πà‡∏≤‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô: L1-L2 (0.7), L3-L5 (1.0)
        is_passed = score >= 0.7 if level <= 2 else score >= 1.0
    else:
        is_passed = bool(is_passed)

    # 3. [EVIDENCE SOURCES & SOURCES] üìé
    # ‡∏£‡∏∞‡∏ö‡∏ö‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏ä‡πâ 'evidence_sources' ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Object ‡πÄ‡∏ï‡πá‡∏° ‡πÅ‡∏•‡∏∞ 'sources' ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠ Doc ID
    # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å Key ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏ó‡∏∏‡∏Å Prompt Version
    evidence_sources = (
        parsed.get("evidence_sources") or 
        parsed.get("top_chunks_data") or 
        []
    )
    
    sources = (
        parsed.get("sources") or 
        parsed.get("evidence") or 
        parsed.get("doc_ids") or 
        parsed.get("reference_documents") or []
    )
    # Normalize 'sources' ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô List ‡∏Ç‡∏≠‡∏á String ‡πÄ‡∏™‡∏°‡∏≠
    if isinstance(sources, str):
        sources = [s.strip() for s in sources.split(',') if s.strip()]
    elif not isinstance(sources, list):
        sources = []

    # 4. [PDCA BREAKDOWN NORMALIZATION] üß©
    # ‡∏î‡∏∂‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏≤‡∏¢ Phase (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©)
    p_val = parsed.get("P_Plan_Score") or parsed.get("P_Score") or parsed.get("plan_score") or parsed.get("P", 0)
    d_val = parsed.get("D_Do_Score") or parsed.get("D_Score") or parsed.get("do_score") or parsed.get("D", 0)
    c_val = parsed.get("C_Check_Score") or parsed.get("C_Score") or parsed.get("check_score") or parsed.get("C", 0)
    a_val = parsed.get("A_Act_Score") or parsed.get("A_Score") or parsed.get("act_score") or parsed.get("A", 0)

    p_score = clean_score(p_val)
    d_score = clean_score(d_val)
    c_score = clean_score(c_val)
    a_score = clean_score(a_val)

    # ‡∏Å‡∏é‡πÄ‡∏´‡∏•‡πá‡∏Å: ‡∏ñ‡πâ‡∏≤‡∏ú‡πà‡∏≤‡∏ô L1-L2 ‡πÅ‡∏ï‡πà AI ‡∏•‡∏∑‡∏°‡πÉ‡∏™‡πà‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô P ‡πÉ‡∏´‡πâ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏°‡∏≤‡πÉ‡∏™‡πà
    if is_passed and level <= 2 and p_score == 0:
        p_score = score

    # 5. [TEXTUAL CONTENT] üìù
    # ‡∏î‡∏∂‡∏á‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏° Phase
    ext_p = str(parsed.get("Extraction_P") or parsed.get("‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô P") or "-").strip()
    ext_d = str(parsed.get("Extraction_D") or parsed.get("‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô D") or "-").strip()
    ext_c = str(parsed.get("Extraction_C") or parsed.get("‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô C") or "-").strip()
    ext_a = str(parsed.get("Extraction_A") or parsed.get("‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô A") or "-").strip()

    # 6. [EXECUTIVE & COACHING NARRATIVE] üëî
    # ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£
    executive_summary = str(
        parsed.get("executive_summary") or 
        parsed.get("summary_thai") or 
        parsed.get("‡∏ö‡∏ó‡∏™‡∏£‡∏∏‡∏õ") or ""
    ).strip()
    
    reason = str(parsed.get("reason") or parsed.get("‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•") or "‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏™‡∏£‡∏∏‡∏õ‡∏à‡∏≤‡∏Å AI").strip()
    coaching_insight = str(parsed.get("coaching_insight") or parsed.get("‡∏Ç‡πâ‡∏≠‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥") or "").strip()

    # 7. [FINAL ASSEMBLY] üèõÔ∏è
    return {
        "sub_id": sub_id,
        "level": level,
        "score": score,
        "is_passed": is_passed,
        "reason": reason,
        "executive_summary": executive_summary,
        "coaching_insight": coaching_insight,
        
        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Flat Report
        "P_Plan_Score": p_score,
        "D_Do_Score": d_score,
        "C_Check_Score": c_score,
        "A_Act_Score": a_score,

        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö UI/Dashboard Radar Chart
        "pdca_breakdown": {
            "P": p_score,
            "D": d_score,
            "C": c_score,
            "A": a_score
        },

        # ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Å‡∏≤‡∏£‡∏™‡∏Å‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        "Extraction_P": ext_p,
        "Extraction_D": ext_d,
        "Extraction_C": ext_c,
        "Extraction_A": ext_a,
        
        # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ Merge Mapping)
        "evidence_sources": evidence_sources, 
        "sources": sources, 
        
        # Metadata ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Audit Trail
        "ai_confidence_at_eval": str(confidence or "MEDIUM"),
        "enabler_at_eval": f"{enabler_full_name} ({enabler_code})",
        "generated_at": datetime.now().isoformat(),
        "is_safety_pass": parsed.get("is_safety_pass", True) # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Judicial Review
    }

def _create_fallback_error(sub_id: str, level: int, error: Exception, context: str, 
                          enabler_full_name: str = "Unknown", enabler_code: str = "UNK") -> Dict[str, Any]:
    """[SAFETY NET] ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏ì‡∏µ LLM ‡∏´‡∏£‡∏∑‡∏≠ Network ‡∏û‡∏±‡∏á‡πÅ‡∏ö‡∏ö 100%"""
    logger = logging.getLogger(__name__)
    logger.error(f"üõë Critical Audit Failure Enabler:{enabler_code} Sub:{sub_id} L{level}: {str(error)}")
    
    return {
        "sub_id": str(sub_id),
        "level": int(level),
        "score": 0.0,
        "is_passed": False,
        "reason": f"System Error: {str(error)}",
        "executive_summary": "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÑ‡∏î‡πâ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏ó‡∏≤‡∏á‡∏£‡∏∞‡∏ö‡∏ö", # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Word ‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        "coaching_insight": "‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ LLM ‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á", # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Word ‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        "consistency_check": False,
        "P_Plan_Score": 0.0, "D_Do_Score": 0.0, "C_Check_Score": 0.0, "A_Act_Score": 0.0,
        "Extraction_P": "ERR", "Extraction_D": "ERR", "Extraction_C": "ERR", "Extraction_A": "ERR",
        "final_llm_context": str(context or ""),
        "raw_llm_response": "SYSTEM_CRASH",
        "ai_confidence_at_eval": "ERROR",
        "enabler_at_eval": f"{enabler_full_name} ({enabler_code})"
    }

def _heuristic_fallback_parse(raw_text: str) -> Dict:
    """
    [ENHANCED v2026.1.23] Heuristic Fallback 
    - ‡∏Å‡∏π‡πâ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡∏¥‡∏ö‡πÄ‡∏°‡∏∑‡πà‡∏≠ JSON ‡∏û‡∏±‡∏á
    - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÉ‡∏ô Regex
    """
    parsed = {
        "score": 0.0,
        "is_passed": False,
        "reason": "JSON Parse Failed (Heuristic Applied)",
        "executive_summary": "‡∏™‡∏Å‡∏±‡∏î‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡∏¥‡∏ö",
        "coaching_insight": "‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÉ‡∏ô Raw Response",
        "P_Plan_Score": 0.0, "D_Do_Score": 0.0, "C_Check_Score": 0.0, "A_Act_Score": 0.0,
        "consistency_check": False
    }

    import re
    # üéØ Regex ‡∏ó‡∏µ‡πà‡πÅ‡∏Ç‡πá‡∏á‡πÅ‡∏Å‡∏£‡πà‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏´‡∏≤ Score ‡∏£‡∏ß‡∏°
    # ‡∏î‡∏±‡∏Å‡πÑ‡∏î‡πâ‡∏ó‡∏±‡πâ‡∏á: "Score: 1.5", "‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°: 2", "Total = 0.5"
    score_match = re.search(r"(?:score|‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô|total|‡∏ú‡∏•‡∏£‡∏ß‡∏°)\D*([\d\.]+)", raw_text, re.I)
    if score_match:
        try:
            val = float(score_match.group(1))
            parsed["score"] = min(val, 10.0) # ‡∏Å‡∏±‡∏ô‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô
            parsed["is_passed"] = parsed["score"] >= 0.7
        except: pass

    # üéØ Regex ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö PDCA ‡πÅ‡∏¢‡∏Å‡∏£‡∏≤‡∏¢‡∏ï‡∏±‡∏ß
    patterns = {
        "P_Plan_Score": r"[Pp](?:lan|score)?\D*([\d\.]+)",
        "D_Do_Score": r"[Dd](?:o|score)?\D*([\d\.]+)",
        "C_Check_Score": r"[Cc](?:heck|score)?\D*([\d\.]+)",
        "A_Act_Score": r"[Aa](?:ct|score)?\D*([\d\.]+)"
    }

    for key, pat in patterns.items():
        m = re.search(pat, raw_text)
        if m:
            try: parsed[key] = float(m.group(1))
            except: pass

    # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏î‡∏∂‡∏á "‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•" ‡∏à‡∏≤‡∏Å‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÅ‡∏£‡∏Å‡πÜ
    lines = [l.strip() for l in raw_text.split('\n') if len(l.strip()) > 10]
    if lines:
        parsed["executive_summary"] = lines[0][:200]
        
    return parsed

# ------------------------
# Summarize (FULL VERSION - v2026.4 Ultra-Robust & Zero-Error)
# ------------------------
def create_context_summary_llm(
    context: str,
    sub_criteria_name: str,
    level: int,
    sub_id: str,
    statement_text: str = "",           # Default ‡∏ß‡πà‡∏≤‡∏á ‚Üí ‡πÑ‡∏°‡πà error ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏™‡πà‡∏á
    next_level: int = None,             # Default ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏≠‡∏á
    llm_executor: Any = None
) -> Dict[str, Any]:
    """
    [SUMMARIZER v2026.4 ‚Äî Ultra-Robust & Zero-Error]
    - ‡πÄ‡∏û‡∏¥‡πà‡∏° default ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö statement_text ‡πÅ‡∏•‡∏∞ next_level ‚Üí ‡πÑ‡∏°‡πà error ‡∏ñ‡πâ‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö
    - Fallback ‡πÉ‡∏ô prompt ‡∏ñ‡πâ‡∏≤ statement_text ‡∏ß‡πà‡∏≤‡∏á (‡∏™‡∏£‡∏∏‡∏õ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ)
    - Log ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ fallback + clean ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á
    - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö retry 4 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á + hint ‡πÉ‡∏ô prompt ‡∏£‡∏≠‡∏ö retry
    """
    logger = logging.getLogger("AssessmentApp")

    # 1. Validation ‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô
    if llm_executor is None:
        logger.warning("‚ö†Ô∏è LLM executor is None - returning fallback")
        return {
            "summary": "‡∏£‡∏∞‡∏ö‡∏ö LLM ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô",
            "suggestion_for_next_level": "‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ LLM",
            "compliance_note": "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏î‡πâ",
            "evidence_integrity_score": 0.0
        }

    context_safe = (context or "").strip()
    if len(context_safe) < 30:
        return {
            "summary": "‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏Å‡∏¥‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏™‡∏£‡∏∏‡∏õ‡πÑ‡∏î‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô",
            "suggestion_for_next_level": "‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á",
            "compliance_note": "‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô",
            "evidence_integrity_score": 0.1
        }

    # Fallback next_level ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏™‡πà‡∏á
    if next_level is None:
        next_level = min(level + 1, 5)
        logger.debug(f"[SUMMARY] next_level fallback to {next_level} for {sub_id} L{level}")

    # 2. Clean context ‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á (‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏à‡∏≤‡∏Å‡πÄ‡∏î‡∏¥‡∏°)
    context_to_send = re.sub(r'[\x00-\x1F\x7F-\x9F]', ' ', context_safe)[:6500]

    # 3. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Prompt
    try:
        fallback_statement = statement_text or "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏ statement (‡∏™‡∏£‡∏∏‡∏õ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°)"
        human_prompt = USER_EVIDENCE_DESCRIPTION_TEMPLATE.format(
            sub_id=f"{sub_id} - {sub_criteria_name}",
            sub_criteria_name=sub_criteria_name,
            level=level,
            statement_text=fallback_statement,
            next_level=next_level,
            context=context_to_send
        )
    except Exception as e:
        logger.error(f"‚ùå Formatting Error in Summary Prompt: {e}")
        return {
            "summary": "‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö prompt",
            "suggestion_for_next_level": "N/A",
            "compliance_note": "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏î‡πâ",
            "evidence_integrity_score": 0.0
        }

    # System Instruction ‡πÄ‡∏Ç‡πâ‡∏°‡∏á‡∏ß‡∏î + ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö fallback
    system_instruction = (
        f"{SYSTEM_EVIDENCE_DESCRIPTION_PROMPT}\n"
        "### STRICT RULES (‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏ï‡∏≤‡∏°‡∏ó‡∏∏‡∏Å‡∏Ç‡πâ‡∏≠) ###\n"
        "1. RETURN ONLY VALID JSON OBJECT. ‡∏´‡πâ‡∏≤‡∏°‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏î ‡πÜ ‡∏ô‡∏≠‡∏Å JSON\n"
        "2. ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏ä‡πâ ```json ‡∏´‡∏£‡∏∑‡∏≠ markdown block\n"
        "3. ‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏•‡πâ‡∏ß‡∏ô‡πÉ‡∏ô‡∏ó‡∏∏‡∏Å value\n"
        "4. ‡∏´‡πâ‡∏≤‡∏°‡∏°‡πÇ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô context\n"
        "5. ‡∏ñ‡πâ‡∏≤ statement_text ‡∏ß‡πà‡∏≤‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡πá‡∏ô '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏' ‡πÉ‡∏´‡πâ‡∏™‡∏£‡∏∏‡∏õ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô compliance\n"
        "6. ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ statement_text ‡πÉ‡∏´‡πâ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô compliance ‡∏Å‡∏±‡∏ö statement ‡∏à‡∏£‡∏¥‡∏á ‡πÜ"
    )

    # 4. Execution Loop with Advanced Parsing + Retry
    max_retries = 4  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏õ‡πá‡∏ô 4 ‡∏£‡∏≠‡∏ö
    for attempt in range(1, max_retries + 1):
        try:
            logger.info(f"üîÑ Generating Summary {sub_id} L{level} (Attempt {attempt})")

            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å LLM (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á LangChain & Ollama-style)
            if hasattr(llm_executor, 'generate'):
                raw_response = llm_executor.generate(system=system_instruction, prompts=[human_prompt])
            elif hasattr(llm_executor, 'invoke'):
                raw_response = llm_executor.invoke(human_prompt)
            else:
                raw_response = llm_executor(system_instruction + "\n" + human_prompt)

            # Robust Text Extraction
            res_text = ""
            if hasattr(raw_response, 'generations'):
                res_text = raw_response.generations[0][0].text.strip()
            elif hasattr(raw_response, 'content'):
                res_text = str(raw_response.content).strip()
            else:
                res_text = str(raw_response).strip()

            # 5. ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (Thai-safe)
            if res_text:
                res_text = res_text.replace('\xa0', ' ').replace('\u200b', '')
                res_text = "".join(c for c in res_text if ord(c) >= 32 or c in "\n\r\t")
                res_text = re.sub(r'```(?:json)?\s*|\s*```', '', res_text).strip()
                res_text = re.sub(r'^[^{\[]+', '', res_text).strip()

            # 6. Robust JSON Extraction
            parsed = _extract_normalized_dict(res_text)

            if parsed and isinstance(parsed, dict):
                summary_val = parsed.get("summary") or parsed.get("‡∏™‡∏£‡∏∏‡∏õ") or ""
                suggestion = parsed.get("suggestion_for_next_level") or parsed.get("‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ñ‡∏±‡∏î‡πÑ‡∏õ") or ""
                compliance = parsed.get("compliance_note") or parsed.get("‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á") or "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏"
                score = float(parsed.get("evidence_integrity_score", 0.5))

                if summary_val.strip():
                    logger.info(f"‚úÖ Summary Generated Successfully (Attempt {attempt})")
                    return {
                        "summary": str(summary_val).strip(),
                        "suggestion_for_next_level": str(suggestion).strip() or "‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ñ‡∏±‡∏î‡πÑ‡∏õ",
                        "compliance_note": str(compliance).strip(),
                        "evidence_integrity_score": max(0.0, min(1.0, score))
                    }

            logger.warning(f"‚ö†Ô∏è Attempt {attempt}: Invalid/empty JSON. Retrying...")
            human_prompt += "\n\n(‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å: ‡∏ï‡∏≠‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞ JSON ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ { ‡πÅ‡∏•‡∏∞‡∏à‡∏ö‡∏î‡πâ‡∏ß‡∏¢ } ‡∏´‡πâ‡∏≤‡∏°‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏∑‡πà‡∏ô‡πÉ‡∏î)"

            time.sleep(0.8)  # ‡∏û‡∏±‡∏Å‡∏ô‡∏≤‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô‡∏ô‡∏¥‡∏î‡∏´‡∏ô‡πà‡∏≠‡∏¢

        except Exception as e:
            logger.error(f"‚ùå Attempt {attempt} Error: {str(e)}")
            time.sleep(1.2)

    # 7. Ultimate Fallback
    logger.error(f"‚ùå All attempts failed for {sub_id} L{level}")
    return {
        "summary": f"‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö {level} ‡πÅ‡∏ï‡πà‡∏£‡∏∞‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡∏∏‡∏õ‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå",
        "suggestion_for_next_level": f"‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏£‡∏∞‡∏î‡∏±‡∏ö {next_level or level+1}",
        "compliance_note": "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡πÑ‡∏î‡πâ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î",
        "evidence_integrity_score": 0.3
    }
    
# =================================================================
# 2. Key Normalizer: ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤ LLM ‡∏û‡πà‡∏ô Key ‡πÑ‡∏°‡πà‡∏ô‡∏¥‡πà‡∏á
# =================================================================
def action_plan_normalize_keys(obj: Any) -> Any:
    """
    [ULTIMATE NORMALIZER v2026.3.26]
    - ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Key ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà LLM ‡∏≠‡∏≤‡∏à‡πÄ‡∏ú‡∏•‡∏≠‡∏û‡πà‡∏ô‡∏≠‡∏≠‡∏Å‡∏°‡∏≤ (‡πÄ‡∏ä‡πà‡∏ô '‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô' -> 'steps')
    - ‡∏•‡πâ‡∏≤‡∏á‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏û‡∏¥‡πÄ‡∏®‡∏©‡πÅ‡∏•‡∏∞ Newline ‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ JSON ‡∏û‡∏±‡∏á
    - ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö Type ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡πÄ‡∏Å‡∏ì‡∏ë‡πå (Coercion) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏ö UI/Frontend
    """
    if isinstance(obj, list):
        return [action_plan_normalize_keys(i) for i in obj]

    if isinstance(obj, dict):
        # ‡πÅ‡∏ú‡∏ô‡∏ú‡∏±‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á Key ‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏≠‡∏ö‡∏à‡∏±‡∏Å‡∏£‡∏ß‡∏≤‡∏• (‡∏£‡∏ß‡∏°‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡∏¢‡πà‡∏≠)
        FIELD_MAPPING = {
            # Level 1: Phase
            "phase": "phase", "‡πÄ‡∏ü‡∏™": "phase", "‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏Å": "phase",
            "goal": "goal", "‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢": "goal", "‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå": "goal",
            "actions": "actions", "‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°": "actions", "‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç": "actions",

            # Level 2: Action Detail
            "statementid": "statement_id", "id": "statement_id",
            "failedlevel": "failed_level", "‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô": "failed_level",
            "recommendation": "recommendation", "‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥": "recommendation",
            "coachinginsight": "coaching_insight", "insight": "coaching_insight",
            "targetevidencetype": "target_evidence_type", "‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ": "target_evidence_type",
            "keymetric": "key_metric", "‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î": "key_metric",

            # Level 3: Steps
            "steps": "steps", "‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏¢‡πà‡∏≠‡∏¢": "steps",
            "step": "step", "‡∏•‡∏≥‡∏î‡∏±‡∏ö": "step",
            "description": "description", "‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î": "description",
            "responsible": "responsible", "‡∏ú‡∏π‡πâ‡∏£‡∏±‡∏ö‡∏ú‡∏¥‡∏î‡∏ä‡∏≠‡∏ö": "responsible",
            "verificationoutcome": "verification_outcome", "‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏´‡πá‡∏ô": "verification_outcome"
        }

        new_obj = {}
        for raw_key, raw_value in obj.items():
            # 1. Clean Key: ‡∏ï‡∏±‡∏î‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á, Newline ‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô Lowercase
            clean_k = str(raw_key).strip().lower().replace("_", "").replace(" ", "")
            
            # 2. Map Key: ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡πÉ‡∏ô Map ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ Key ‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏µ‡πà‡∏•‡πâ‡∏≤‡∏á‡πÅ‡∏•‡πâ‡∏ß
            target_key = FIELD_MAPPING.get(clean_k, clean_k)

            # 3. Value Normalization: ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏•‡∏Ç ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏•‡∏Ç‡∏à‡∏£‡∏¥‡∏á‡πÜ
            if target_key in ["failed_level", "step"]:
                try:
                    if isinstance(raw_value, str):
                        nums = re.findall(r"\d+", raw_value)
                        value = int(nums[0]) if nums else 0
                    else:
                        value = int(raw_value)
                except: value = 0
            else:
                value = raw_value

            # 4. Recursive Call: ‡∏ó‡∏≥‡∏ï‡πà‡∏≠‡πÉ‡∏ô‡∏ä‡∏±‡πâ‡∏ô‡∏•‡∏π‡∏Å
            new_obj[target_key] = action_plan_normalize_keys(value)
        return new_obj

    return obj
