"""
llm_data_utils.py
Robust LLM + RAG utilities for SEAM assessment (CLEAN FINAL VERSION)
"""

import logging

logger = logging.getLogger(__name__)
if not logger.handlers:
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")


import time
import json
import hashlib
import uuid
import re
from datetime import datetime
from typing import List, Dict, Any, Optional, Union, Callable, TypeVar, Set
import json5
from langchain.retrievers import EnsembleRetriever
from langchain_core.documents import Document
import os
import unicodedata


# Optional: regex ‡πÅ‡∏ó‡∏ô re (‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤) ‚Äî ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡πá‡πÉ‡∏ä‡πâ re ‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤
try:
    import regex as re  # type: ignore
except ImportError:
    pass  # ‡πÉ‡∏ä‡πâ re ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ

# ===================================================================
# 1. Core Configuration (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô)
# ===================================================================
from config.global_vars import (
    INITIAL_TOP_K,
    MAX_EVAL_CONTEXT_LENGTH,
    DEFAULT_EMBED_BATCH_SIZE,
    RERANK_THRESHOLD,
    ANALYSIS_FINAL_K,
    ACTION_PLAN_STEP_MAX_WORDS,
    LLM_TEMPERATURE,
    MAX_ACTION_PLAN_TOKENS
)

# ===================================================================
# 2. Critical Utilities (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏à‡∏£‡∏¥‡∏á ‚Äî ‡πÑ‡∏°‡πà‡∏°‡∏µ fallback)
# ===================================================================
# üéØ FIX 1: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô Import ‡∏à‡∏≤‡∏Å _get_collection_name ‡πÑ‡∏õ‡πÄ‡∏õ‡πá‡∏ô get_doc_type_collection_key
from core.vectorstore import get_hf_embeddings
from utils.path_utils import (
    get_doc_type_collection_key, 
    _n, get_mapping_file_path, # <--- ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà‡∏à‡∏≤‡∏Å utils/path_utils
    get_vectorstore_collection_path,
    get_vectorstore_tenant_root_path,
    get_rubric_file_path
)
from core.json_extractor import (
    _robust_extract_json,
    _normalize_keys,
    _safe_int_parse,
    _extract_normalized_dict
)

# ===================================================================
# 3. Project Modules (‡∏ñ‡πâ‡∏≤‡∏´‡∏≤‡∏¢ ‚Üí ‡∏õ‡∏•‡πà‡∏≠‡∏¢‡πÉ‡∏´‡πâ error ‡∏ä‡∏±‡∏î ‡πÜ ‡πÑ‡∏õ‡πÄ‡∏•‡∏¢ ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏á‡∏µ‡∏¢‡∏ö)
# ===================================================================
from core.seam_prompts import (
    SYSTEM_ASSESSMENT_PROMPT,
    USER_ASSESSMENT_PROMPT,
    SYSTEM_EVIDENCE_DESCRIPTION_PROMPT,
    EVIDENCE_DESCRIPTION_PROMPT,
    USER_LOW_LEVEL_PROMPT,
    USER_EVIDENCE_DESCRIPTION_TEMPLATE,
)

from core.vectorstore import VectorStoreManager, get_global_reranker, ChromaRetriever
from core.assessment_schema import CombinedAssessment, EvidenceSummary

try:
    from core.assessment_schema import StatementAssessment
except ImportError:
    from pydantic import BaseModel
    class StatementAssessment(BaseModel):
        score: int = 0
        reason: str = ""

from langchain_core.documents import Document as LcDocument
# ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î Import ‡πÉ‡∏ô routers/llm_router.py

# ===================================================================
# 4. Constants
# ===================================================================
LOW_LEVEL_K: int = 3
_MOCK_FLAG = False
_MAX_LLM_RETRIES = 3

def set_mock_control_mode(enable: bool):
    global _MOCK_FLAG
    _MOCK_FLAG = bool(enable)
    logger.info(f"Mock control mode: {_MOCK_FLAG}")

def _create_where_filter(
    stable_doc_ids: Optional[Union[Set[str], List[str]]] = None,
    subject: Optional[str] = None,
    sub_topic: Optional[str] = None,
    year: Optional[Union[int, str]] = None,
    enabler: Optional[str] = None,
    tenant: Optional[str] = None,
    **kwargs
) -> Dict[str, Any]:
    """
    [PRODUCTION VERSION] ‡∏™‡∏£‡πâ‡∏≤‡∏á Filter ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ChromaDB ‡∏ó‡∏µ‡πà‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥
    ‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á Data Type Mismatch (Int/Str) ‡πÅ‡∏•‡∏∞‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Multi-tenant
    """
    filters: List[Dict[str, Any]] = []

    # --- 1. ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Stable Doc IDs (‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î) ---
    if stable_doc_ids:
        ids_list = [str(i).strip() for i in (stable_doc_ids if isinstance(stable_doc_ids, (list, set)) else [stable_doc_ids]) if i]
        if ids_list:
            if len(ids_list) == 1:
                # ‡∏Å‡∏£‡∏ì‡∏µ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ $and
                return {"stable_doc_uuid": ids_list[0]}
            else:
                return {"stable_doc_uuid": {"$in": ids_list}}

    # --- 2. ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Year (‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ Local Mac ‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠) ---
    if year is not None:
        year_str = str(year).strip()
        if year_str and year_str.lower() != "none":
            # üéØ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: ‡∏™‡πà‡∏á‡∏ó‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö Int ‡πÅ‡∏•‡∏∞ Str ‡∏´‡∏£‡∏∑‡∏≠‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô Int ‡∏ï‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà Peek ‡πÄ‡∏à‡∏≠‡πÉ‡∏ô Mac
            try:
                # ‡∏ñ‡πâ‡∏≤‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ‡∏™‡πà‡∏á‡πÅ‡∏ö‡∏ö Integer (ChromaDB Local ‡∏°‡∏±‡∏Å‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏õ‡πá‡∏ô Int)
                val_year = int(year_str)
                filters.append({"year": val_year})
            except (ValueError, TypeError):
                # ‡∏ñ‡πâ‡∏≤‡πÅ‡∏õ‡∏•‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á‡πÜ ‡πÉ‡∏´‡πâ‡∏™‡πà‡∏á‡πÅ‡∏ö‡∏ö String
                filters.append({"year": year_str})
    
    # --- 3. ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Tenant (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ç‡πâ‡∏≤‡∏°‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó) ---
    effective_tenant = tenant or kwargs.get("tenant")
    if effective_tenant and str(effective_tenant).strip():
        filters.append({"tenant": str(effective_tenant).strip()})

    # --- 4. ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Enabler (KM, IM, etc.) ---
    if enabler and str(enabler).strip():
        filters.append({"enabler": enabler.strip().upper()})

    # --- 5. ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Subject (‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÄ‡∏Å‡∏ì‡∏ë‡πå) ---
    if subject and str(subject).strip():
        filters.append({"subject": str(subject).strip()})

    # --- 6. ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Sub Topic ---
    if sub_topic and str(sub_topic).strip():
        filters.append({"sub_topic": str(sub_topic).strip()})

    # --- ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Filter ---
    if not filters:
        return {}

    # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢‡∏≠‡∏±‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏î‡πâ‡∏ß‡∏¢ $and
    return filters[0] if len(filters) == 1 else {"$and": filters}


def retrieve_context_for_endpoint(
    vectorstore_manager,
    query: str = "",
    tenant: Optional[str] = None,
    year: Optional[Union[int, str]] = None,
    stable_doc_ids: Optional[Set[str]] = None,
    doc_type: Optional[str] = None,
    enabler: Optional[str] = None,
    subject: Optional[str] = None,
    sub_topic: Optional[str] = None,
    k_to_retrieve: int = 150, 
    k_to_rerank: int = 30,    
    strict_filter: bool = False,
    **kwargs
) -> Dict[str, Any]:
    """
    [ULTIMATE REVISED] Retrieval for Search Endpoint
    - ‡∏¢‡∏∂‡∏î Metadata ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢ (No Pydantic Error)
    - ‡πÉ‡∏ä‡πâ Deterministic MD5 Hash ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Deduplication
    - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Anchor Chunks ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á Context ‡∏ó‡∏µ‡πà‡∏°‡∏±‡πà‡∏ô‡∏Ñ‡∏á
    """
    start_time = time.time()
    vsm = vectorstore_manager

    # 1. Resolve collection
    clean_doc_type = str(doc_type or "document").strip().lower()
    collection_name = get_doc_type_collection_key(doc_type=clean_doc_type, enabler=enabler)
    
    chroma = vsm._load_chroma_instance(collection_name)
    if not chroma:
        logger.error(f"‚ùå Collection {collection_name} not found.")
        return {"top_evidences": [], "aggregated_context": "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", "retrieval_time": 0}

    # 2. Create where_filter
    where_filter = _create_where_filter(
        stable_doc_ids=list(stable_doc_ids) if stable_doc_ids else None, 
        subject=subject, 
        sub_topic=sub_topic, 
        year=year
    )

    # Dictionary ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Deduplication (‡πÉ‡∏ä‡πâ MD5 ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏õ‡πá‡∏ô Key)
    unique_map: Dict[str, LcDocument] = {}

    # =====================================================
    # ‚öì 2.1 ANCHOR RETRIEVAL (‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå)
    # =====================================================
    if stable_doc_ids:
        # ‡∏î‡∏∂‡∏á‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏£‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        anchors = chroma.get(where=where_filter, limit=10) 
        if anchors and anchors.get('documents'):
            for i in range(len(anchors['documents'])):
                content = anchors['documents'][i]
                md = anchors['metadatas'][i]
                
                # Deterministic MD5 Hash
                c_hash = hashlib.md5(content.encode()).hexdigest()
                uid = md.get("chunk_uuid") or f"anchor-{c_hash}"
                
                if uid not in unique_map:
                    # ‡∏â‡∏µ‡∏î‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÉ‡∏´‡πâ Anchor ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ï‡∏¥‡∏î‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡∏ï‡πâ‡∏ô‡πÜ ‡∏´‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á
                    md["score"] = 0.5 
                    md["is_anchor"] = True
                    unique_map[uid] = LcDocument(page_content=content, metadata=md)

    # =====================================================
    # üîç 2.2 SEMANTIC SEARCH
    # =====================================================
    search_query = query if (query and query != "*" and len(query) > 2) else ""
    
    # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö‡∏à‡∏≤‡∏Å Vector DB
    if search_query:
        docs = chroma.similarity_search(search_query, k=k_to_retrieve, filter=where_filter)
    else:
        # Fallback ‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏°‡πà‡∏°‡∏µ Query ‡πÉ‡∏´‡πâ‡∏î‡∏∂‡∏á‡πÅ‡∏ö‡∏ö‡∏Å‡∏ß‡∏≤‡∏î
        docs = chroma.similarity_search("*", k=k_to_retrieve, filter=where_filter)

    for d in docs:
        c_hash = hashlib.md5(d.page_content.encode()).hexdigest()
        md = d.metadata or {}
        uid = md.get("chunk_uuid") or c_hash
        if uid not in unique_map:
            unique_map[uid] = d

    candidates = list(unique_map.values())

    # =====================================================
    # üöÄ 3. BATCH RERANKING
    # =====================================================
    final_scored_docs = []
    reranker = getattr(vsm, "reranker", None)
    
    if reranker and candidates and search_query:
        try:
            batch_size = 100 
            logger.info(f"üöÄ Reranking {len(candidates)} candidates in batches...")
            for i in range(0, len(candidates), batch_size):
                batch = candidates[i : i + batch_size]
                # ‡πÉ‡∏ä‡πâ Reranker ‡∏Ç‡∏≠‡∏á LangChain (Compressor)
                scored_batch = reranker.compress_documents(documents=batch, query=search_query)
                final_scored_docs.extend(scored_batch)
        except Exception as e:
            logger.error(f"‚ö†Ô∏è Rerank failed: {e}")
            final_scored_docs = candidates
    else:
        final_scored_docs = candidates

    # =====================================================
    # 4. SORTING & SCORE INJECTION (‡∏´‡∏±‡∏ß‡πÉ‡∏à‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ 0.0000)
    # =====================================================
    def get_score(d) -> float:
        m = d.metadata or {}
        # ‡∏î‡∏∂‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏à‡∏≤‡∏Å‡∏ó‡∏∂‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ
        s = m.get("relevance_score") or m.get("score") or m.get("rerank_score") or 0.0
        try: return float(s)
        except: return 0.0

    final_scored_docs.sort(key=get_score, reverse=True)

    # =====================================================
    # 5. RESPONSE BUILD
    # =====================================================
    top_evidences = []
    aggregated_parts = []
    
    for doc in final_scored_docs[:k_to_rerank]:
        md = doc.metadata or {}
        text = doc.page_content.strip()
        score = get_score(doc)
        
        # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏•‡∏∞‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏≤‡πÉ‡∏´‡πâ Robust
        p_val = md.get("page") or md.get("page_label") or md.get("page_number") or "N/A"
        source_name = md.get('source') or md.get('source_filename') or md.get('file_name') or 'Unknown'
        s_uuid = md.get("stable_doc_uuid") or md.get("doc_id") or ""
        
        # SYNC SCORE ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤ Metadata ‡∏ó‡∏∏‡∏Å‡∏ï‡∏±‡∏ß
        md["score"] = score
        md["relevance_score"] = score
        
        evidence_item = {
            "doc_id": str(s_uuid),
            "chunk_uuid": str(md.get("chunk_uuid") or ""),
            "source": source_name,
            "text": text,
            "page": str(p_val),
            "score": score,
            "pdca_tag": md.get("pdca_tag", "Other"),
            "metadata": md
        }
        
        top_evidences.append(evidence_item)
        aggregated_parts.append(f"[‡πÑ‡∏ü‡∏•‡πå: {source_name}, ‡∏´‡∏ô‡πâ‡∏≤: {p_val}] {text}")

    retrieval_time = round(time.time() - start_time, 3)
    logger.info(f"üèÅ Finished: {len(top_evidences)} chunks in {retrieval_time}s")

    return {
        "top_evidences": top_evidences,
        "aggregated_context": "\n\n---\n\n".join(aggregated_parts) if aggregated_parts else "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•",
        "retrieval_time": retrieval_time,
        "total_candidates": len(candidates)
    }

# ------------------------
# Retrieval: retrieve_context_with_filter (Revised)
# ------------------------
def retrieve_context_with_filter(
    query: Union[str, List[str]],
    doc_type: str,
    tenant: Optional[str] = None,
    year: Optional[Union[int, str]] = None,
    enabler: Optional[str] = None,
    subject: Optional[str] = None,
    vectorstore_manager: Optional[Any] = None,
    mapped_uuids: Optional[List[str]] = None,
    stable_doc_ids: Optional[List[str]] = None,
    priority_docs_input: Optional[List[Any]] = None,
    sequential_chunk_uuids: Optional[List[str]] = None,
    sub_id: Optional[str] = None,
    level: Optional[int] = None,
    get_previous_level_docs: Optional[Callable[[int, str], List[Any]]] = None,
    top_k: int = 150, 
) -> Dict[str, Any]:
    """
    [FIXED VERSION] ‡∏¢‡∏∂‡∏î‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ (No Error) 
    ‡πÅ‡∏ï‡πà‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏¢‡∏±‡∏î‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏•‡∏á Metadata ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Rerank 0.0000
    """
    start_time = time.time()
    manager = vectorstore_manager
    queries_to_run = [query] if isinstance(query, str) else list(query or [""])
    
    # 1. Resolve Collection & Filter
    # ‡πÉ‡∏ä‡πâ helper ‡∏à‡∏≤‡∏Å utils ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Ç‡∏≠‡∏á‡∏ä‡∏∑‡πà‡∏≠ collection
    collection_name = get_doc_type_collection_key(doc_type, enabler or "KM")
    
    target_ids = set()
    if stable_doc_ids: target_ids.update([str(i) for i in stable_doc_ids])
    if mapped_uuids: target_ids.update([str(i) for i in mapped_uuids])
    if sequential_chunk_uuids: target_ids.update([str(i) for i in sequential_chunk_uuids])
    
    where_filter = _create_where_filter(
        stable_doc_ids=list(target_ids) if target_ids else None,
        subject=subject,
        year=year
    )

    # 2. Collect Chunks
    all_source_chunks = []

    # 2.1 Priority Docs
    if priority_docs_input:
        for doc in priority_docs_input:
            if not doc: continue
            if isinstance(doc, dict):
                pc = doc.get('page_content') or doc.get('text') or ''
                meta = doc.get('metadata') or {}
                if pc.strip():
                    all_source_chunks.append(LcDocument(page_content=pc, metadata=meta))
            elif hasattr(doc, 'page_content'):
                all_source_chunks.append(doc)

    # 2.2 Vector Retrieval
    try:
        full_retriever = manager.get_retriever(collection_name=collection_name)
        base_retriever = getattr(full_retriever, "base_retriever", full_retriever)
        
        search_kwargs = {"k": top_k}
        if where_filter: 
            search_kwargs["where"] = where_filter

        for q in queries_to_run:
            if not q: continue
            docs = base_retriever.invoke(q, config={"configurable": {"search_kwargs": search_kwargs}})
            if docs: all_source_chunks.extend(docs)
    except Exception as e:
        logger.error(f"Retrieval error: {e}")

    # 3. Deduplicate (‡πÉ‡∏ä‡πâ Hash ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥)
    unique_map: Dict[str, LcDocument] = {}
    for doc in all_source_chunks:
        if not doc or not doc.page_content.strip(): continue
        md = doc.metadata or {}
        # ‡πÉ‡∏ä‡πâ hashlib ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ ID ‡∏ó‡∏µ‡πà‡∏Ñ‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô‡∏ó‡∏∏‡∏Å‡∏£‡∏≠‡∏ö
        c_hash = hashlib.md5(doc.page_content.encode()).hexdigest()
        uid = str(md.get("chunk_uuid") or f"{md.get('stable_doc_uuid', 'unknown')}-{c_hash}")
        
        if uid not in unique_map:
            unique_map[uid] = doc

    candidates = list(unique_map.values())

    # 4. Batch Reranking
    final_scored_docs = []
    batch_size = 100 
    reranker = getattr(manager, "reranker", None)

    if reranker and candidates:
        main_query = queries_to_run[0]
        for i in range(0, len(candidates), batch_size):
            batch = candidates[i : i + batch_size]
            try:
                # üìå ‡∏Å‡∏∏‡∏ç‡πÅ‡∏à‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: Reranker ‡∏à‡∏∞‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÉ‡∏ô metadata
                scored_batch = reranker.compress_documents(batch, main_query)
                final_scored_docs.extend(scored_batch)
            except Exception as e:
                logger.error(f"Rerank Error: {e}")
                final_scored_docs.extend(batch)
    else:
        final_scored_docs = candidates

    # 5. Sorting & Score Injection (‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ 0.0000)
    def get_score(d) -> float:
        m = d.metadata or {}
        # ‡πÑ‡∏•‡πà‡∏´‡∏≤‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ
        s = m.get("relevance_score") or m.get("score") or m.get("rerank_score") or 0.0
        try: return float(s)
        except: return 0.0

    final_scored_docs.sort(key=get_score, reverse=True)

    # 6. Final Formatting (‡∏Ñ‡∏±‡∏î‡πÄ‡∏•‡∏∑‡∏≠‡∏Å K ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å)
    top_evidences = []
    aggregated_parts = []
    final_k = ANALYSIS_FINAL_K # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å config

    for doc in final_scored_docs[:final_k]:
        score = get_score(doc)
        # ‡∏Å‡∏£‡∏≠‡∏á Threshold ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡∏±‡πâ‡∏á‡πÑ‡∏ß‡πâ‡πÉ‡∏ô .env
        if score < RERANK_THRESHOLD and RERANK_THRESHOLD > 0:
            continue

        md = doc.metadata or {}
        text = doc.page_content.strip()
        
        # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö ingest.py
        page = str(md.get("page_label") or md.get("page_number") or md.get("page") or "N/A")
        source = md.get("source") or md.get("source_filename") or "Unknown"
        pdca = md.get("pdca_tag", "Other")

        # üéØ SYNC SCORE ‡πÄ‡∏Ç‡πâ‡∏≤ Metadata (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Loop ‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡πÅ‡∏•‡πâ‡∏ß‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ 0)
        md["score"] = score
        md["relevance_score"] = score
        md["rerank_score"] = score

        top_evidences.append({
            "doc_id": str(md.get("stable_doc_uuid") or md.get("doc_id") or ""),
            "chunk_uuid": str(md.get("chunk_uuid") or ""),
            "source": source,
            "text": text,
            "page": page,
            "pdca_tag": pdca,
            "score": score,
            "metadata": md
        })
        aggregated_parts.append(f"[{pdca}] [‡πÑ‡∏ü‡∏•‡πå: {source} ‡∏´‡∏ô‡πâ‡∏≤: {page}] {text}")

    return {
        "top_evidences": top_evidences,
        "aggregated_context": "\n\n---\n\n".join(aggregated_parts) if aggregated_parts else "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô",
        "retrieval_time": round(time.time() - start_time, 3),
        "total_candidates": len(candidates)
    }

# =====================================================================
# üõ† Helper: check_rubric_readiness (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏á‡∏µ‡∏¢‡∏ö‡∏•‡∏á)
# =====================================================================
def is_rubric_ready(tenant: str) -> bool:
    """ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏Ç‡∏≠‡∏á seam collection ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏û‡πà‡∏ô Warning ‡∏Å‡∏ß‡∏ô‡πÉ‡∏à """
    if not tenant:
        return False
    
    tenant_vs_root = get_vectorstore_tenant_root_path(tenant)
    chroma_path = os.path.join(tenant_vs_root, "seam")
    
    # ‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ True/False ‡πÄ‡∏á‡∏µ‡∏¢‡∏ö‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏î‡∏∂‡∏á Rubric ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    return os.path.exists(chroma_path)


# =====================================================================
# üöÄ Ultimate Version: retrieve_context_with_rubric (FIXED & REVISED)
# =====================================================================
def retrieve_context_with_rubric(
    vectorstore_manager,
    query: str,
    doc_type: str,
    enabler: Optional[str] = None,
    stable_doc_ids: Optional[Set[str]] = None,
    tenant: Optional[str] = None,
    year: Optional[Union[int, str]] = None,
    subject: Optional[str] = None,
    rubric_vectorstore_name: str = "seam", 
    top_k: int = 150,         # üöÄ ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏î‡∏∂‡∏á‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Reranker ‡∏°‡∏µ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô
    rubric_top_k: int = 15,  
    strict_filter: bool = True,
    k_to_rerank: int = 30    
) -> Dict[str, Any]:
    """
    [REVISED VERSION] ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Rubric + Evidence Retrieval ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏£‡∏∞‡∏ö‡∏ö Batch Reranking 
    ‡πÅ‡∏•‡∏∞ Content-Based Deduplication ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô Level 5 ‡∏´‡∏≤‡∏¢
    """
    start_time = time.time()
    vsm = vectorstore_manager

    # --- 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏™‡∏•‡∏±‡∏ö Collection ---
    if hasattr(vsm, 'doc_type') and vsm.doc_type != doc_type:
        logger.info(f"üîÑ Switching VSM doc_type to: {doc_type}")
        vsm.close()
        vsm.__init__(tenant=tenant, year=year, doc_type=doc_type, enabler=enabler)

    evidence_collection = get_doc_type_collection_key(doc_type, enabler or "KM")
    
    rubric_results = []
    # ‡πÉ‡∏ä‡πâ Dict ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥ Deduplication ‡∏î‡πâ‡∏ß‡∏¢ Content Hash ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ã‡πâ‡∏≥‡πÅ‡∏ï‡πà ID ‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô
    unique_evidence_map: Dict[str, LcDocument] = {}

    # --- 2. ‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á Rubrics (‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô) ---
    try:
        rubric_chroma = vsm._load_chroma_instance(rubric_vectorstore_name)
        if rubric_chroma:
            rubric_query = f"‡πÄ‡∏Å‡∏ì‡∏ë‡πå SE-AM {enabler} {subject or ''}: {query}"
            r_docs = rubric_chroma.similarity_search(rubric_query, k=rubric_top_k)
            for rd in r_docs:
                rubric_results.append({
                    "text": rd.page_content, 
                    "metadata": rd.metadata, 
                    "is_rubric": True
                })
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Rubric Retrieval Error: {e}")

    # --- 3. ‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á Evidence (‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô) ---
    try:
        evidence_chroma = vsm._load_chroma_instance(evidence_collection)
        if not evidence_chroma:
            return {"top_evidences": [], "rubric_context": rubric_results, "retrieval_time": 0}

        where_filter = None
        if stable_doc_ids:
            ids_list = [str(i).strip().lower() for i in stable_doc_ids if i]
            where_filter = {"stable_doc_uuid": ids_list[0]} if len(ids_list) == 1 else {"stable_doc_uuid": {"$in": ids_list}}
            
            # ‚öì 3.1 Fetch Anchor Chunks (‡∏™‡πà‡∏ß‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå ‡πÄ‡∏ä‡πà‡∏ô ‡∏´‡∏ô‡πâ‡∏≤ 1-5)
            anchors = evidence_chroma.get(where=where_filter, limit=10)
            if anchors and anchors.get('documents'):
                for i in range(len(anchors['documents'])):
                    content = anchors['documents'][i]
                    md = dict(anchors['metadatas'][i] or {}) # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô metadata ‡πÄ‡∏õ‡πá‡∏ô None
                    
                    # üéØ ‡πÅ‡∏Å‡πâ‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà 1: ‡πÉ‡∏ä‡πâ MD5 ‡πÅ‡∏ó‡∏ô hash() ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ ID ‡∏Ñ‡∏á‡∏ó‡∏µ‡πà (Deterministic)
                    content_hash = hashlib.md5(content.encode()).hexdigest()
                    uid = str(md.get("chunk_uuid") or f"anchor-{content_hash}")
                    
                    if uid not in unique_evidence_map:
                        # üéØ ‡πÅ‡∏Å‡πâ‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà 2: ‡∏â‡∏µ‡∏î‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏•‡∏á‡πÉ‡∏ô metadata ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á (Safe Injection)
                        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÉ‡∏´‡πâ Anchor ‡∏°‡∏µ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á (0.95) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏´‡∏•‡∏±‡∏Å
                        md["score"] = 0.95
                        md["relevance_score"] = 0.95
                        md["is_anchor"] = True
                        
                        unique_evidence_map[uid] = LcDocument(
                            page_content=content,
                            metadata=md
                        )

        # üîç 3.2 Semantic Search (‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢)
        search_results = evidence_chroma.similarity_search(query, k=top_k, filter=where_filter)
        for d in search_results:
            content_hash = str(hash(d.page_content))
            uid = d.metadata.get("chunk_uuid") or content_hash
            if uid not in unique_evidence_map:
                unique_evidence_map[uid] = d

        candidates = list(unique_evidence_map.values())

        # --- 4. BATCH RERANKING (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô OOM ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥) ---
        evidence_results = []
        reranker = get_global_reranker()
        
        if reranker and candidates and query:
            try:
                batch_size = 100 # üöÄ ‡πÅ‡∏ö‡πà‡∏á Batch ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏Ç‡∏≠‡∏á VRAM
                scored_candidates = []
                
                logger.info(f"üöÄ Batch Reranking {len(candidates)} chunks...")
                for i in range(0, len(candidates), batch_size):
                    batch = candidates[i : i + batch_size]
                    reranked_batch = reranker.compress_documents(documents=batch, query=query)
                    scored_candidates.extend(reranked_batch)
                
                # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å Batch
                scored_candidates = sorted(
                    scored_candidates, 
                    key=lambda x: getattr(x, "relevance_score", 0), 
                    reverse=True
                )
                
                for r in scored_candidates[:k_to_rerank]:
                    doc = r.document if hasattr(r, "document") else r
                    m = doc.metadata or {}
                    score = getattr(r, "relevance_score", 0.0)
                    
                    # üéØ Sync Score ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤ Metadata ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ
                    m["rerank_score"] = score
                    m["score"] = score
                    
                    evidence_results.append({
                        "text": doc.page_content,
                        "source_filename": m.get("source_filename") or m.get("source") or "Evidence",
                        "page_label": str(m.get("page_label") or m.get("page_number") or m.get("page") or "N/A"),
                        "doc_id": m.get("stable_doc_uuid") or m.get("doc_id"),
                        "chunk_uuid": m.get("chunk_uuid") or str(uuid.uuid4()),
                        "pdca_tag": m.get("pdca_tag") or "Content",
                        "rerank_score": score,
                        "is_evidence": True,
                        "metadata": m
                    })
            except Exception as e:
                logger.error(f"‚ö†Ô∏è Rerank failed: {e}")
                candidates = candidates[:k_to_rerank] # Fallback
        
        # ‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏°‡πà‡∏°‡∏µ Reranker ‡∏´‡∏£‡∏∑‡∏≠ Error ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏õ‡∏Å‡∏ï‡∏¥
        if not evidence_results:
            for d in candidates[:k_to_rerank]:
                m = d.metadata or {}
                evidence_results.append({
                    "text": d.page_content,
                    "source_filename": m.get("source_filename") or m.get("source") or "Evidence",
                    "page_label": str(m.get("page_label") or m.get("page_number") or m.get("page") or "N/A"),
                    "doc_id": m.get("stable_doc_uuid") or m.get("doc_id"),
                    "chunk_uuid": m.get("chunk_uuid") or str(uuid.uuid4()),
                    "pdca_tag": m.get("pdca_tag") or "Content",
                    "rerank_score": 0.0,
                    "is_evidence": True,
                    "metadata": m
                })

    except Exception as e:
        logger.error(f"‚ùå Evidence Retrieval Error: {e}", exc_info=True)

    retrieval_time = round(time.time() - start_time, 3)
    logger.info(f"‚úÖ Success: Retrieved {len(evidence_results)} evidence chunks in {retrieval_time}s")

    return {
        "top_evidences": evidence_results,
        "rubric_context": rubric_results,
        "retrieval_time": retrieval_time,
        "used_chunk_uuids": [e["chunk_uuid"] for e in evidence_results if e.get("chunk_uuid")]
    }

# ========================
#  retrieve_context_by_doc_ids (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö hydration ‡πÉ‡∏ô router)
# ========================
def retrieve_context_by_doc_ids(
    doc_uuids: List[str],
    doc_type: str,
    enabler: Optional[str] = None,
    vectorstore_manager = None,
    limit: int = 100, # üöÄ ‡πÄ‡∏û‡∏¥‡πà‡∏° limit ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô
    tenant: Optional[str] = None,
    year: Optional[Union[int, str]] = None,
) -> Dict[str, Any]:
    """
    [REVISED] ‡∏î‡∏∂‡∏á chunks ‡∏à‡∏≤‡∏Å stable_doc_uuid ‡∏´‡∏•‡∏≤‡∏¢‡∏ï‡∏±‡∏ß (‡πÉ‡∏ä‡πâ‡∏ï‡∏≠‡∏ô hydration sources)
    ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏∏‡∏õ‡∏µ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤ Collection ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏Å‡∏©‡∏≤ Metadata ‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô
    """
    start_time = time.time()
    vsm = vectorstore_manager or VectorStoreManager(tenant=tenant, year=year)
    
    # Resolve collection name ‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏õ‡∏µ‡πÅ‡∏•‡∏∞ enabler
    collection_name = get_doc_type_collection_key(doc_type=doc_type, enabler=enabler)

    chroma = vsm._load_chroma_instance(collection_name)
    if not chroma:
        logger.error(f"‚ùå Collection {collection_name} not found for hydration")
        return {"top_evidences": []}

    if not doc_uuids:
        return {"top_evidences": []}

    logger.info(f"üíß Hydration ‚Üí {len(doc_uuids)} doc IDs from {collection_name}")

    try:
        # ‡πÉ‡∏ä‡πâ Metadata filter ‡∏î‡∏∂‡∏á chunks ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏•‡∏¥‡∏™‡∏ï‡πå ID ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
        # ‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô list comprehension ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏ß‡∏£‡πå‡∏Ç‡∏≠‡∏á Type
        ids_to_query = [str(u) for u in doc_uuids if u]
        
        results = chroma._collection.get(
            where={"stable_doc_uuid": {"$in": ids_to_query}},
            limit=limit,
            include=["documents", "metadatas"]
        )
    except Exception as e:
        logger.error(f"‚ö†Ô∏è Hydration query failed: {e}")
        return {"top_evidences": []}

    evidences = []
    # üéØ ‡πÉ‡∏ä‡πâ Content-based Deduplication ‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô‡∏Å‡∏±‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ã‡πâ‡∏≥
    seen_contents = set()

    for doc_content, meta in zip(results.get("documents", []), results.get("metadatas", [])):
        if not doc_content or not doc_content.strip():
            continue
            
        content_hash = str(hash(doc_content))
        if content_hash in seen_contents:
            continue
        seen_contents.add(content_hash)

        p_val = meta.get("page_label") or meta.get("page_number") or meta.get("page") or "N/A"
        
        # üéØ Sync Score ‡∏´‡∏•‡∏≠‡∏Å (‡πÄ‡∏û‡∏£‡∏≤‡∏∞ Hydration ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ú‡πà‡∏≤‡∏ô Rerank ‡πÅ‡∏ï‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Logic ‡∏≠‡∏∑‡πà‡∏ô‡πÑ‡∏°‡πà‡∏û‡∏±‡∏á)
        score = meta.get("score") or meta.get("rerank_score") or 0.85

        evidences.append({
            "doc_id": meta.get("stable_doc_uuid") or meta.get("doc_id"),
            "chunk_uuid": meta.get("chunk_uuid"),
            "source": meta.get("source") or meta.get("source_filename") or "Unknown",
            "page": str(p_val),
            "text": doc_content.strip(),
            "pdca_tag": meta.get("pdca_tag", "Other"),
            "score": score,
            "metadata": meta # ‡πÅ‡∏ô‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏ï‡πá‡∏°‡πÑ‡∏ß‡πâ‡πÄ‡∏™‡∏°‡∏≠
        })

    logger.info(f"‚úÖ Hydration success: {len(evidences)} chunks from {len(doc_uuids)} docs")
    return {
        "top_evidences": evidences,
        "retrieval_time": round(time.time() - start_time, 3)
    }


def retrieve_context_for_low_levels(query: str, doc_type: str, enabler: Optional[str]=None,
                                 vectorstore_manager: Optional['VectorStoreManager']=None,
                                 top_k: int=LOW_LEVEL_K, initial_k: int=INITIAL_TOP_K,
                                 # üü¢ NEW: ‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠ arguments
                                 mapped_uuids: Optional[List[str]]=None,
                                 priority_docs_input: Optional[List[Any]] = None,
                                 sequential_chunk_uuids: Optional[List[str]] = None, 
                                 sub_id: Optional[str]=None, level: Optional[int]=None) -> Dict[str, Any]:
    """
    Retrieves a small, focused context for low levels (L1, L2) using a reduced k (LOW_LEVEL_K).
    """
    # ‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å ‡πÅ‡∏ï‡πà‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏ä‡πâ k ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
    return retrieve_context_with_filter(
        query=query,
        doc_type=doc_type,
        enabler=enabler,
        vectorstore_manager=vectorstore_manager,
        top_k=LOW_LEVEL_K,
        initial_k=initial_k,
        mapped_uuids=mapped_uuids,
        priority_docs_input=priority_docs_input,
        sequential_chunk_uuids=sequential_chunk_uuids, 
        sub_id=sub_id,
        level=level
    )

# ------------------------
# LLM fetcher
# ------------------------
def _fetch_llm_response(
    system_prompt: str, 
    user_prompt: str, 
    max_retries: int = 3,
    llm_executor: Any = None 
) -> str:
    """
    Ultimate Robust LLM Fetcher v2026.1.19-final
    - ‡∏Ñ‡∏∑‡∏ô STRING ‡πÄ‡∏™‡∏°‡∏≠ ‡πÑ‡∏°‡πà‡πÄ‡∏Ñ‡∏¢‡∏Ñ‡∏∑‡∏ô None
    - ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô NoneType.strip() ‡∏ó‡∏∏‡∏Å‡∏à‡∏∏‡∏î
    - Log ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠ debug
    """

    if llm_executor is None and not _MOCK_FLAG:
        raise ConnectionError("LLM instance not initialized.")

    # Enforced prompt (‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏µ‡∏Å‡∏ô‡∏¥‡∏î)
    enforced_system_prompt = (system_prompt or "").strip() + (
        "\n\n### STRICT OUTPUT RULES - FOLLOW EXACTLY ###\n"
        "1. Respond with ONLY valid JSON object. No other text.\n"
        "2. Start with '{' and end with '}'.\n"
        "3. No markdown, no explanations, no prefixes.\n"
        "4. If no evidence: {\"score\": 0, \"reason\": \"No evidence\", \"is_passed\": false}"
    )

    messages = [
        {"role": "system", "content": enforced_system_prompt},
        {"role": "user",   "content": (user_prompt or "").strip()}
    ]

    for attempt in range(1, max_retries + 1):
        try:
            if _MOCK_FLAG:
                mock = '{"score": 1, "reason": "Mock active", "is_passed": true}'
                logger.critical(f"LLM RAW (MOCK): {mock}")
                return mock

            # LLM CALL
            response = llm_executor.invoke(messages, config={"temperature": 0.0})

            # SAFE EXTRACTION - ‡∏£‡∏ß‡∏° patch ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
            raw_text = ""
            if response is None:
                logger.warning("Response object is None")
            elif hasattr(response, "content"):
                raw_text = str(response.content or "")
            elif hasattr(response, "text"):
                raw_text = str(response.text or "")
            elif isinstance(response, str):
                raw_text = response
            else:
                raw_text = str(response or "")

            # Log raw ‡∏Å‡πà‡∏≠‡∏ô clean
            preview = (raw_text[:1000] + "...") if len(raw_text) > 1000 else raw_text
            logger.critical(f"LLM RAW RESPONSE (attempt {attempt}): {preview}")

            # Clean - ‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô
            raw_text_stripped = (raw_text or "").strip()

            # ‡∏´‡∏≤ JSON block
            json_match = re.search(r'\{[\s\S]*?\}', raw_text_stripped, re.DOTALL)
            if json_match:
                extracted = json_match.group(0)
                try:
                    json.loads(extracted)  # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡πà‡∏≠‡∏ô
                    return extracted
                except json.JSONDecodeError:
                    logger.warning(f"Extracted not valid JSON: {extracted[:120]}...")

            # Fallback: ‡∏•‡∏≠‡∏á parse ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
            try:
                parsed = json.loads(raw_text_stripped)
                return json.dumps(parsed, ensure_ascii=False)
            except json.JSONDecodeError:
                pass

            # Ultimate safe return - ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö string ‡πÄ‡∏™‡∏°‡∏≠ (‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥)
            final_return = str(raw_text_stripped or "").strip()
            logger.debug(f"No valid JSON ‚Üí returning cleaned string (len={len(final_return)})")
            return final_return

        except Exception as e:
            logger.error(f"Attempt {attempt} failed: {str(e)}", exc_info=True)
            if attempt < max_retries:
                time.sleep(2 ** attempt)
            else:
                logger.critical("All retries failed")
                return '{"score": 0, "reason": "LLM failed after retries", "is_passed": false}'

    return '{"score": 0, "reason": "Unknown error", "is_passed": false}'

# ------------------------
# Evaluation
# ------------------------
T = TypeVar("T", bound=BaseModel)

def _check_and_handle_empty_context(context: str, sub_id: str, level: int) -> Optional[Dict[str, Any]]:
    """
    Returns Failure result if context is empty or contains known error strings.
    Auto-fail with PDCA keys all set to 0.
    """
    if not context or "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á" in context or "ERROR:" in context.upper():
        logger.warning(f"Auto-FAIL L{level} for {sub_id}: Empty or Error Context detected from RAG.")
        context_preview = context.strip()[:100].replace("\n", " ") if context else "Empty Context"
        return {
            "score": 0,
            "reason": f"‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏î‡πâ‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á (Context: {context_preview}).",
            "is_passed": False,
            "P_Plan_Score": 0,
            "D_Do_Score": 0,
            "C_Check_Score": 0,
            "A_Act_Score": 0,
        }
    return None

def _get_context_for_level(
    context: str,
    level: int,
    chunks: list = None,  # ‡∏™‡πà‡∏á chunks ‡∏°‡∏≤‡∏à‡∏≤‡∏Å _run_single_assessment
    max_chars_l1_l2: int = 15000,  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 6000
    max_chars_l3_up: int = 10000,
    max_chunks_l1_l2: int = 40,
    max_chunks_l3_up: int = 25
) -> str:
    if not context:
        return "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô"

    # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ chunks ‚Üí ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏° score ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å top
    if chunks:
        sorted_chunks = sorted(
            chunks,
            key=lambda c: float(c.get('rerank_score', 0) or c.get('score', 0)),
            reverse=True
        )
        max_chunks = max_chunks_l1_l2 if level <= 2 else max_chunks_l3_up
        selected = sorted_chunks[:max_chunks]

        parts = []
        for i, c in enumerate(selected, 1):
            score = c.get('rerank_score', 'N/A')
            source = c.get('source', '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏')
            text = c.get('text', '').strip()
            if text:
                parts.append(f"[Chunk {i} | Score: {score} | {source}]\n{text}\n{'-'*80}\n")

        final = "".join(parts)
        max_chars = max_chars_l1_l2 if level <= 2 else max_chars_l3_up
        if len(final) > max_chars:
            final = final[:max_chars] + "\n... (‡∏ï‡∏±‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°)"
        return final

    # fallback ‡πÄ‡∏î‡∏¥‡∏°
    max_chars = max_chars_l1_l2 if level <= 2 else max_chars_l3_up
    return context[:max_chars] + ("... [truncated]" if len(context) > max_chars else "")

def evaluate_with_llm(
    context: str, 
    sub_criteria_name: str, 
    level: int, 
    statement_text: str, 
    sub_id: str, 
    llm_executor: Any = None, 
    required_phases: List[str] = None,
    specific_contextual_rule: str = "‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô",
    ai_confidence: str = "MEDIUM",
    confidence_reason: str = "N/A",
    pdca_context: str = "", # <--- [ADD] ‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏¢‡∏Å‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà P-D-C-A
    **kwargs
) -> Dict[str, Any]:
    """
    [REVISED v2026.3.5 ‚Äî PDCA Block Enabled]
    - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏â‡∏µ‡∏î pdca_context ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏Å‡∏±‡∏î‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô
    - ‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏£‡∏∞‡∏ö‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Parse JSON ‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Multi-Enabler
    """
    logger = logging.getLogger(__name__)

    # 1. Safe casting + defaults
    ctx_raw = str(context or "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô")
    pdca_ctx = str(pdca_context or "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏¢‡∏Å‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà (‡πÇ‡∏õ‡∏£‡∏î‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏à‡∏≤‡∏Å Full Context)")
    s_name = str(sub_criteria_name or "N/A")
    sid = str(sub_id or "N/A")
    s_text = str(statement_text or "N/A")
    
    # Enabler info
    enabler_full_name = str(kwargs.get("enabler_full_name", "Unknown Enabler"))
    enabler_code = str(kwargs.get("enabler_code", "UNK"))
    
    logger.info(f"[EVAL START] Enabler: {enabler_full_name} ({enabler_code}) | Sub: {sid} | L{level}")

    # ‡∏î‡∏∂‡∏á Context ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏ï‡∏≤‡∏°‡∏£‡∏∞‡∏î‡∏±‡∏ö (Slice ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î)
    context_to_send_eval = _get_context_for_level(ctx_raw, level) or ""
    
    # 2. Safe phases
    phases_str = ", ".join(str(p).strip() for p in (required_phases or [])) if required_phases else "P, D, C, A"

    # 3. Baseline clean
    baseline_raw = kwargs.get("baseline_summary")
    baseline_summary = str(baseline_raw or "").strip()

    try:
        # Build prompt with PDCA Context support
        # ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡πÉ‡∏ô USER_ASSESSMENT_PROMPT ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ placeholder {pdca_context}
        full_prompt = USER_ASSESSMENT_PROMPT.format(
            sub_criteria_name=s_name,
            sub_id=sid,
            level=int(level),
            statement_text=s_text,
            context=context_to_send_eval[:28000], # ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ PDCA Block
            pdca_context=pdca_ctx[:8000],         # [CRITICAL] ‡∏â‡∏µ‡∏î‡∏ö‡∏•‡πá‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏¢‡∏Å‡∏´‡∏°‡∏ß‡∏î
            required_phases=phases_str,
            specific_contextual_rule=str(specific_contextual_rule or "‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå"),
            ai_confidence=str(ai_confidence or "MEDIUM"),
            confidence_reason=str(confidence_reason or "N/A"),
            enabler_full_name=enabler_full_name,
            enabler_code=enabler_code
        )

        if baseline_summary:
            full_prompt += f"\n\n--- BASELINE DATA (‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤) ---\n{baseline_summary}"

        # 4. LLM call with guard
        if llm_executor is None:
            raise ValueError("No LLM executor provided")

        raw_response = _fetch_llm_response(None, full_prompt, llm_executor=llm_executor)
        raw_response = str(raw_response or "").strip()

        # 5. Parse with fallback
        parsed = _robust_extract_json(raw_response)
        if not parsed or not isinstance(parsed, dict):
            logger.warning(f"[JSON PARSE FAIL] Sub {sid} L{level} - Using heuristic fallback")
            parsed = _heuristic_fallback_parse(raw_response)

        # 6. Build final result object
        result = _build_audit_result_object(
            parsed, raw_response, context_to_send_eval, ai_confidence, 
            level=level, sub_id=sid, enabler_full_name=enabler_full_name, enabler_code=enabler_code
        )
        return result

    except Exception as e:
        logger.error(f"üõë Evaluation Error Enabler:{enabler_code} Sub:{sid} L{level}: {str(e)}", exc_info=True)
        return _create_fallback_error(sid, level, e, context_to_send_eval, enabler_full_name, enabler_code)


def evaluate_with_llm_low_level(
    context: str,
    sub_criteria_name: str,
    level: int,
    statement_text: str,
    sub_id: str,
    llm_executor: Any = None,
    required_phases: List[str] = None,
    specific_contextual_rule: str = "‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô",
    ai_confidence: str = "MEDIUM",
    pdca_context: str = "", # <--- [ADD]
    **kwargs
) -> Dict[str, Any]:
    """
    [REVISED v2026.3.5 ‚Äî Low Level PDCA Enabled]
    - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö pdca_context ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏∞‡∏î‡∏±‡∏ö L1-L2
    - ‡πÄ‡∏ô‡πâ‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà '‡πÅ‡∏ú‡∏ô' ‡πÅ‡∏•‡∏∞ '‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°'
    """
    logger = logging.getLogger(__name__)

    # 1. Safe casting
    ctx = str(context or "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô")
    pdca_ctx = str(pdca_context or "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏¢‡∏Å‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà")
    s_name = str(sub_criteria_name or "N/A")
    s_text = str(statement_text or "N/A")
    sid = str(sub_id or "N/A")
    
    # Enabler info
    enabler_full_name = str(kwargs.get("enabler_full_name", "Unknown Enabler"))
    enabler_code = str(kwargs.get("enabler_code", "UNK"))
    
    logger.info(f"[LOW EVAL START] Enabler: {enabler_full_name} ({enabler_code}) | Sub: {sid} | L{level}")

    plan_kws = str(kwargs.get("plan_keywords") or "‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢, ‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô, ‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå")
    baseline_summary = str(kwargs.get("baseline_summary") or "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤").strip()
    conf_reason = str(kwargs.get("confidence_reason") or "‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ï‡∏≤‡∏°‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏á‡∏≤‡∏ô")
    
    phases_str = ", ".join(str(p) for p in (required_phases or [])) if required_phases else "P, D"

    try:
        full_prompt = USER_LOW_LEVEL_PROMPT.format(
            sub_id=sid,
            sub_criteria_name=s_name,
            level=int(level),
            statement_text=s_text,
            context=ctx[:28000],
            pdca_context=pdca_ctx[:8000], # [CRITICAL]
            required_phases=phases_str,
            plan_keywords=plan_kws,
            baseline_summary=baseline_summary,
            specific_contextual_rule=str(specific_contextual_rule or "‡∏ï‡∏£‡∏ß‡∏à‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå"),
            ai_confidence=str(ai_confidence or "MEDIUM"),
            confidence_reason=conf_reason,
            enabler_full_name=enabler_full_name,
            enabler_code=enabler_code
        )

        if llm_executor is None:
            raise ValueError("No LLM executor provided")

        raw_response = _fetch_llm_response(None, full_prompt, llm_executor=llm_executor)
        raw_response = str(raw_response or "").strip()

        parsed = _robust_extract_json(raw_response)
        if not parsed or not isinstance(parsed, dict):
            logger.warning(f"[LOW JSON PARSE FAIL] Sub {sid} L{level} - Using heuristic fallback")
            parsed = _heuristic_fallback_parse(raw_response)

        result = _build_audit_result_object(
            parsed, raw_response, ctx, ai_confidence, 
            level=level, sub_id=sid, enabler_full_name=enabler_full_name, enabler_code=enabler_code
        )
        return result

    except Exception as e:
        logger.error(f"üõë Low-Level Eval Error Enabler:{enabler_code} Sub:{sid} L{level}: {str(e)}", exc_info=True)
        return _create_fallback_error(sid, level, e, ctx, enabler_full_name, enabler_code)
    
def _build_audit_result_object(parsed: Dict, raw_response: str, context: str, confidence: str, **kwargs) -> Dict[str, Any]:
    """
    [ULTIMATE-SYNC v2026.1.22] ‚Äî Multi-Enabler + Zero-Error
    - ‡πÄ‡∏û‡∏¥‡πà‡∏° enabler_full_name & enabler_code ‡πÉ‡∏ô result
    - Robust string handling (str(val or "") ‡∏Å‡πà‡∏≠‡∏ô strip)
    - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö key ‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏à‡∏≤‡∏Å LLM
    """
    level = kwargs.get('level', 1)
    sub_id = kwargs.get('sub_id', 'Unknown')
    enabler_full_name = kwargs.get('enabler_full_name', 'Unknown Enabler')
    enabler_code = kwargs.get('enabler_code', 'UNK')

    def clean_score(val, default=0.0):
        if val is None: return default
        try:
            return float(val)
        except (ValueError, TypeError):
            return default

    if not isinstance(parsed, dict):
        parsed = {}

    score = clean_score(parsed.get("score"))
    is_passed = parsed.get("is_passed")
    if is_passed is None:
        is_passed = score >= 0.7 if level <= 2 else score >= 1.0

    # Robust extraction (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô NoneType.strip)
    ext_p = str(parsed.get("Extraction_P") or parsed.get("‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô P") or parsed.get("p_plan_extraction") or "-").strip()
    ext_d = str(parsed.get("Extraction_D") or parsed.get("‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô D") or parsed.get("d_do_extraction") or "-").strip()
    ext_c = str(parsed.get("Extraction_C") or parsed.get("‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô C") or parsed.get("c_check_extraction") or "-").strip()
    ext_a = str(parsed.get("Extraction_A") or parsed.get("‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô A") or parsed.get("a_act_extraction") or "-").strip()

    # PDCA scores ‚Äî ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö key ‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢
    p_plan_score = clean_score(parsed.get("P_Plan_Score") or parsed.get("P_Score") or parsed.get("plan_score") or parsed.get("P"))
    d_do_score = clean_score(parsed.get("D_Do_Score") or parsed.get("D_Score") or parsed.get("do_score") or parsed.get("D"))
    c_check_score = clean_score(parsed.get("C_Check_Score") or parsed.get("C_Score") or parsed.get("check_score") or parsed.get("C"))
    a_act_score = clean_score(parsed.get("A_Act_Score") or parsed.get("A_Score") or parsed.get("act_score") or parsed.get("A"))

    # Fallback ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö L1-L2
    if bool(is_passed) and level <= 2 and p_plan_score == 0:
        p_plan_score = score

    return {
        "sub_id": str(sub_id),
        "level": int(level),
        "score": score,
        "is_passed": bool(is_passed),
        "reason": str(parsed.get("reason") or parsed.get("‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•") or "‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏à‡∏≤‡∏Å LLM").strip(),
        "summary_thai": str(parsed.get("summary_thai") or parsed.get("‡∏ö‡∏ó‡∏™‡∏£‡∏∏‡∏õ") or "").strip(),
        "coaching_insight": str(parsed.get("coaching_insight") or parsed.get("‡∏Ç‡πâ‡∏≠‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥") or "").strip(),
        
        "P_Plan_Score": p_plan_score,
        "D_Do_Score": d_do_score,
        "C_Check_Score": c_check_score,
        "A_Act_Score": a_act_score,

        "Extraction_P": ext_p,
        "Extraction_D": ext_d,
        "Extraction_C": ext_c,
        "Extraction_A": ext_a,
        
        "final_llm_context": str(context or ""),
        "raw_llm_response": str(raw_response or ""),
        "ai_confidence_at_eval": str(confidence or "MEDIUM"),
        "consistency_check": bool(parsed.get("consistency_check", True)),
        
        # Multi-Enabler Traceability
        "enabler_at_eval": f"{enabler_full_name} ({enabler_code})"
    }


def _create_fallback_error(sub_id: str, level: int, error: Exception, context: str, 
                          enabler_full_name: str = "Unknown", enabler_code: str = "UNK") -> Dict[str, Any]:
    """[SAFETY NET] ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏ì‡∏µ LLM ‡∏´‡∏£‡∏∑‡∏≠ Prompt ‡∏û‡∏±‡∏á"""
    logger = logging.getLogger(__name__)
    logger.error(f"üõë Critical Audit Failure Enabler:{enabler_code} Sub:{sub_id} L{level}: {str(error)}")
    
    return {
        "sub_id": sub_id,
        "level": level,
        "score": 0.0,
        "reason": f"Audit Engine Error: {str(error)}",
        "is_passed": False,
        "consistency_check": False,
        "P_Plan_Score": 0.0, "D_Do_Score": 0.0, "C_Check_Score": 0.0, "A_Act_Score": 0.0,
        "Extraction_P": "-", "Extraction_D": "-", "Extraction_C": "-", "Extraction_A": "-",
        "final_llm_context": str(context or ""),
        "raw_llm_response": "",
        "ai_confidence_at_eval": "ERROR",
        "enabler_at_eval": f"{enabler_full_name} ({enabler_code})"
    }


def _heuristic_fallback_parse(raw_text: str) -> Dict:
    """
    [ENHANCED v2026.1.22] Fallback parse ‚Äî ‡∏´‡∏≤ score/PDCA ‡∏à‡∏≤‡∏Å raw text ‡∏î‡πâ‡∏ß‡∏¢ regex + keyword
    """
    parsed = {
        "score": 0.0,
        "is_passed": False,
        "reason": "JSON parse failed - fallback heuristic",
        "summary_thai": "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏¢‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå",
        "P_Plan_Score": 0.0,
        "D_Do_Score": 0.0,
        "C_Check_Score": 0.0,
        "A_Act_Score": 0.0,
        "consistency_check": False
    }

    import re

    # ‡∏´‡∏≤ score ‡∏´‡∏•‡∏±‡∏Å
    score_match = re.search(r"(?:score|‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô|total score)\D*([\d\.]+)", raw_text, re.IGNORECASE)
    if score_match:
        try:
            parsed["score"] = float(score_match.group(1))
            parsed["is_passed"] = parsed["score"] >= 0.7
        except:
            pass

    # ‡∏´‡∏≤ PDCA scores
    pdca_patterns = {
        "P_Plan_Score": r"(?:P_Plan|P|Plan|‡πÅ‡∏ú‡∏ô)\D*([\d\.]+)",
        "D_Do_Score": r"(?:D_Do|D|Do|‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥)\D*([\d\.]+)",
        "C_Check_Score": r"(?:C_Check|C|Check|‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö)\D*([\d\.]+)",
        "A_Act_Score": r"(?:A_Act|A|Act|‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á)\D*([\d\.]+)"
    }

    for key, pattern in pdca_patterns.items():
        match = re.search(pattern, raw_text, re.IGNORECASE)
        if match:
            try:
                parsed[key] = float(match.group(1))
            except:
                pass

    parsed["reason"] += f" | Raw snippet: {raw_text[:300]}..."
    return parsed

# ------------------------
# Summarize (FULL VERSION - v2026.4 Ultra-Robust & Zero-Error)
# ------------------------
def create_context_summary_llm(
    context: str,
    sub_criteria_name: str,
    level: int,
    sub_id: str,
    statement_text: str = "",           # Default ‡∏ß‡πà‡∏≤‡∏á ‚Üí ‡πÑ‡∏°‡πà error ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏™‡πà‡∏á
    next_level: int = None,             # Default ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏≠‡∏á
    llm_executor: Any = None
) -> Dict[str, Any]:
    """
    [SUMMARIZER v2026.4 ‚Äî Ultra-Robust & Zero-Error]
    - ‡πÄ‡∏û‡∏¥‡πà‡∏° default ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö statement_text ‡πÅ‡∏•‡∏∞ next_level ‚Üí ‡πÑ‡∏°‡πà error ‡∏ñ‡πâ‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö
    - Fallback ‡πÉ‡∏ô prompt ‡∏ñ‡πâ‡∏≤ statement_text ‡∏ß‡πà‡∏≤‡∏á (‡∏™‡∏£‡∏∏‡∏õ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ)
    - Log ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ fallback + clean ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á
    - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö retry 4 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á + hint ‡πÉ‡∏ô prompt ‡∏£‡∏≠‡∏ö retry
    """
    logger = logging.getLogger("AssessmentApp")

    # 1. Validation ‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô
    if llm_executor is None:
        logger.warning("‚ö†Ô∏è LLM executor is None - returning fallback")
        return {
            "summary": "‡∏£‡∏∞‡∏ö‡∏ö LLM ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô",
            "suggestion_for_next_level": "‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ LLM",
            "compliance_note": "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏î‡πâ",
            "evidence_integrity_score": 0.0
        }

    context_safe = (context or "").strip()
    if len(context_safe) < 30:
        return {
            "summary": "‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏Å‡∏¥‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏™‡∏£‡∏∏‡∏õ‡πÑ‡∏î‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô",
            "suggestion_for_next_level": "‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á",
            "compliance_note": "‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô",
            "evidence_integrity_score": 0.1
        }

    # Fallback next_level ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏™‡πà‡∏á
    if next_level is None:
        next_level = min(level + 1, 5)
        logger.debug(f"[SUMMARY] next_level fallback to {next_level} for {sub_id} L{level}")

    # 2. Clean context ‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á (‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏à‡∏≤‡∏Å‡πÄ‡∏î‡∏¥‡∏°)
    context_to_send = re.sub(r'[\x00-\x1F\x7F-\x9F]', ' ', context_safe)[:6500]

    # 3. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Prompt
    try:
        fallback_statement = statement_text or "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏ statement (‡∏™‡∏£‡∏∏‡∏õ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°)"
        human_prompt = USER_EVIDENCE_DESCRIPTION_TEMPLATE.format(
            sub_id=f"{sub_id} - {sub_criteria_name}",
            sub_criteria_name=sub_criteria_name,
            level=level,
            statement_text=fallback_statement,
            next_level=next_level,
            context=context_to_send
        )
    except Exception as e:
        logger.error(f"‚ùå Formatting Error in Summary Prompt: {e}")
        return {
            "summary": "‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö prompt",
            "suggestion_for_next_level": "N/A",
            "compliance_note": "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏î‡πâ",
            "evidence_integrity_score": 0.0
        }

    # System Instruction ‡πÄ‡∏Ç‡πâ‡∏°‡∏á‡∏ß‡∏î + ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö fallback
    system_instruction = (
        f"{SYSTEM_EVIDENCE_DESCRIPTION_PROMPT}\n"
        "### STRICT RULES (‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏ï‡∏≤‡∏°‡∏ó‡∏∏‡∏Å‡∏Ç‡πâ‡∏≠) ###\n"
        "1. RETURN ONLY VALID JSON OBJECT. ‡∏´‡πâ‡∏≤‡∏°‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏î ‡πÜ ‡∏ô‡∏≠‡∏Å JSON\n"
        "2. ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏ä‡πâ ```json ‡∏´‡∏£‡∏∑‡∏≠ markdown block\n"
        "3. ‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏•‡πâ‡∏ß‡∏ô‡πÉ‡∏ô‡∏ó‡∏∏‡∏Å value\n"
        "4. ‡∏´‡πâ‡∏≤‡∏°‡∏°‡πÇ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô context\n"
        "5. ‡∏ñ‡πâ‡∏≤ statement_text ‡∏ß‡πà‡∏≤‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡πá‡∏ô '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏' ‡πÉ‡∏´‡πâ‡∏™‡∏£‡∏∏‡∏õ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô compliance\n"
        "6. ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ statement_text ‡πÉ‡∏´‡πâ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô compliance ‡∏Å‡∏±‡∏ö statement ‡∏à‡∏£‡∏¥‡∏á ‡πÜ"
    )

    # 4. Execution Loop with Advanced Parsing + Retry
    max_retries = 4  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏õ‡πá‡∏ô 4 ‡∏£‡∏≠‡∏ö
    for attempt in range(1, max_retries + 1):
        try:
            logger.info(f"üîÑ Generating Summary {sub_id} L{level} (Attempt {attempt})")

            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å LLM (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á LangChain & Ollama-style)
            if hasattr(llm_executor, 'generate'):
                raw_response = llm_executor.generate(system=system_instruction, prompts=[human_prompt])
            elif hasattr(llm_executor, 'invoke'):
                raw_response = llm_executor.invoke(human_prompt)
            else:
                raw_response = llm_executor(system_instruction + "\n" + human_prompt)

            # Robust Text Extraction
            res_text = ""
            if hasattr(raw_response, 'generations'):
                res_text = raw_response.generations[0][0].text.strip()
            elif hasattr(raw_response, 'content'):
                res_text = str(raw_response.content).strip()
            else:
                res_text = str(raw_response).strip()

            # 5. ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (Thai-safe)
            if res_text:
                res_text = res_text.replace('\xa0', ' ').replace('\u200b', '')
                res_text = "".join(c for c in res_text if ord(c) >= 32 or c in "\n\r\t")
                res_text = re.sub(r'```(?:json)?\s*|\s*```', '', res_text).strip()
                res_text = re.sub(r'^[^{\[]+', '', res_text).strip()

            # 6. Robust JSON Extraction
            parsed = _extract_normalized_dict(res_text)

            if parsed and isinstance(parsed, dict):
                summary_val = parsed.get("summary") or parsed.get("‡∏™‡∏£‡∏∏‡∏õ") or ""
                suggestion = parsed.get("suggestion_for_next_level") or parsed.get("‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ñ‡∏±‡∏î‡πÑ‡∏õ") or ""
                compliance = parsed.get("compliance_note") or parsed.get("‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á") or "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏"
                score = float(parsed.get("evidence_integrity_score", 0.5))

                if summary_val.strip():
                    logger.info(f"‚úÖ Summary Generated Successfully (Attempt {attempt})")
                    return {
                        "summary": str(summary_val).strip(),
                        "suggestion_for_next_level": str(suggestion).strip() or "‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ñ‡∏±‡∏î‡πÑ‡∏õ",
                        "compliance_note": str(compliance).strip(),
                        "evidence_integrity_score": max(0.0, min(1.0, score))
                    }

            logger.warning(f"‚ö†Ô∏è Attempt {attempt}: Invalid/empty JSON. Retrying...")
            human_prompt += "\n\n(‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å: ‡∏ï‡∏≠‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞ JSON ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ { ‡πÅ‡∏•‡∏∞‡∏à‡∏ö‡∏î‡πâ‡∏ß‡∏¢ } ‡∏´‡πâ‡∏≤‡∏°‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏∑‡πà‡∏ô‡πÉ‡∏î)"

            time.sleep(0.8)  # ‡∏û‡∏±‡∏Å‡∏ô‡∏≤‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô‡∏ô‡∏¥‡∏î‡∏´‡∏ô‡πà‡∏≠‡∏¢

        except Exception as e:
            logger.error(f"‚ùå Attempt {attempt} Error: {str(e)}")
            time.sleep(1.2)

    # 7. Ultimate Fallback
    logger.error(f"‚ùå All attempts failed for {sub_id} L{level}")
    return {
        "summary": f"‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö {level} ‡πÅ‡∏ï‡πà‡∏£‡∏∞‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡∏∏‡∏õ‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå",
        "suggestion_for_next_level": f"‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏£‡∏∞‡∏î‡∏±‡∏ö {next_level or level+1}",
        "compliance_note": "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡πÑ‡∏î‡πâ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î",
        "evidence_integrity_score": 0.3
    }
    
# =================================================================
# 2. Key Normalizer: ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤ LLM ‡∏û‡πà‡∏ô Key ‡πÑ‡∏°‡πà‡∏ô‡∏¥‡πà‡∏á
# =================================================================
def action_plan_normalize_keys(obj: Any) -> Any:
    """
    [ULTIMATE NORMALIZER v2026.3.26]
    - ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Key ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà LLM ‡∏≠‡∏≤‡∏à‡πÄ‡∏ú‡∏•‡∏≠‡∏û‡πà‡∏ô‡∏≠‡∏≠‡∏Å‡∏°‡∏≤ (‡πÄ‡∏ä‡πà‡∏ô '‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô' -> 'steps')
    - ‡∏•‡πâ‡∏≤‡∏á‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏û‡∏¥‡πÄ‡∏®‡∏©‡πÅ‡∏•‡∏∞ Newline ‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ JSON ‡∏û‡∏±‡∏á
    - ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö Type ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡πÄ‡∏Å‡∏ì‡∏ë‡πå (Coercion) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏ö UI/Frontend
    """
    if isinstance(obj, list):
        return [action_plan_normalize_keys(i) for i in obj]

    if isinstance(obj, dict):
        # ‡πÅ‡∏ú‡∏ô‡∏ú‡∏±‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á Key ‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏≠‡∏ö‡∏à‡∏±‡∏Å‡∏£‡∏ß‡∏≤‡∏• (‡∏£‡∏ß‡∏°‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡∏¢‡πà‡∏≠)
        FIELD_MAPPING = {
            # Level 1: Phase
            "phase": "phase", "‡πÄ‡∏ü‡∏™": "phase", "‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏Å": "phase",
            "goal": "goal", "‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢": "goal", "‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå": "goal",
            "actions": "actions", "‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°": "actions", "‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç": "actions",

            # Level 2: Action Detail
            "statementid": "statement_id", "id": "statement_id",
            "failedlevel": "failed_level", "‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô": "failed_level",
            "recommendation": "recommendation", "‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥": "recommendation",
            "coachinginsight": "coaching_insight", "insight": "coaching_insight",
            "targetevidencetype": "target_evidence_type", "‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ": "target_evidence_type",
            "keymetric": "key_metric", "‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î": "key_metric",

            # Level 3: Steps
            "steps": "steps", "‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏¢‡πà‡∏≠‡∏¢": "steps",
            "step": "step", "‡∏•‡∏≥‡∏î‡∏±‡∏ö": "step",
            "description": "description", "‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î": "description",
            "responsible": "responsible", "‡∏ú‡∏π‡πâ‡∏£‡∏±‡∏ö‡∏ú‡∏¥‡∏î‡∏ä‡∏≠‡∏ö": "responsible",
            "verificationoutcome": "verification_outcome", "‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏´‡πá‡∏ô": "verification_outcome"
        }

        new_obj = {}
        for raw_key, raw_value in obj.items():
            # 1. Clean Key: ‡∏ï‡∏±‡∏î‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á, Newline ‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô Lowercase
            clean_k = str(raw_key).strip().lower().replace("_", "").replace(" ", "")
            
            # 2. Map Key: ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡πÉ‡∏ô Map ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ Key ‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏µ‡πà‡∏•‡πâ‡∏≤‡∏á‡πÅ‡∏•‡πâ‡∏ß
            target_key = FIELD_MAPPING.get(clean_k, clean_k)

            # 3. Value Normalization: ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏•‡∏Ç ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏•‡∏Ç‡∏à‡∏£‡∏¥‡∏á‡πÜ
            if target_key in ["failed_level", "step"]:
                try:
                    if isinstance(raw_value, str):
                        nums = re.findall(r"\d+", raw_value)
                        value = int(nums[0]) if nums else 0
                    else:
                        value = int(raw_value)
                except: value = 0
            else:
                value = raw_value

            # 4. Recursive Call: ‡∏ó‡∏≥‡∏ï‡πà‡∏≠‡πÉ‡∏ô‡∏ä‡∏±‡πâ‡∏ô‡∏•‡∏π‡∏Å
            new_obj[target_key] = action_plan_normalize_keys(value)
        return new_obj

    return obj
