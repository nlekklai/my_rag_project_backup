"""
llm_data_utils.py
Robust LLM + RAG utilities for SEAM assessment (CLEAN FINAL VERSION)
"""

import logging

logger = logging.getLogger(__name__)
if not logger.handlers:
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")


import time
import json
import hashlib
import uuid
import re
from datetime import datetime
from typing import List, Dict, Any, Optional, Union, Callable, TypeVar, Set
import json5
from utils.enabler_keyword_map import ENABLER_KEYWORD_MAP, DEFAULT_KEYWORDS
from langchain.retrievers import EnsembleRetriever
from langchain_core.documents import Document
from langchain_community.retrievers import BM25Retriever # FIX: Import BM25 ‡∏à‡∏≤‡∏Å community
import os
from pydantic import ValidationError

# --- ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° JSON Schema ---
try:
    from core.action_plan_schema import get_clean_action_plan_schema
    schema_json = json.dumps(get_clean_action_plan_schema(), ensure_ascii=False, indent=2)
except Exception as e:
    logger.error(f"Schema load failed: {e}")


# Optional: regex ‡πÅ‡∏ó‡∏ô re (‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤) ‚Äî ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡πá‡πÉ‡∏ä‡πâ re ‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤
try:
    import regex as re  # type: ignore
except ImportError:
    pass  # ‡πÉ‡∏ä‡πâ re ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ

# ===================================================================
# 1. Core Configuration (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô)
# ===================================================================
from config.global_vars import (
    INITIAL_TOP_K,
    MAX_EVAL_CONTEXT_LENGTH,
    DEFAULT_EMBED_BATCH_SIZE,
    RERANK_THRESHOLD,
    ANALYSIS_FINAL_K
)

# ===================================================================
# 2. Critical Utilities (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏à‡∏£‡∏¥‡∏á ‚Äî ‡πÑ‡∏°‡πà‡∏°‡∏µ fallback)
# ===================================================================
# üéØ FIX 1: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô Import ‡∏à‡∏≤‡∏Å _get_collection_name ‡πÑ‡∏õ‡πÄ‡∏õ‡πá‡∏ô get_doc_type_collection_key
from core.vectorstore import get_hf_embeddings
from utils.path_utils import (
    get_doc_type_collection_key, 
    _n, get_mapping_file_path, # <--- ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà‡∏à‡∏≤‡∏Å utils/path_utils
    get_vectorstore_collection_path,
    get_vectorstore_tenant_root_path,
    get_rubric_file_path
)
from core.json_extractor import (
    _robust_extract_json,
    _normalize_keys,
    _safe_int_parse,
    _extract_normalized_dict
)

# ===================================================================
# 3. Project Modules (‡∏ñ‡πâ‡∏≤‡∏´‡∏≤‡∏¢ ‚Üí ‡∏õ‡∏•‡πà‡∏≠‡∏¢‡πÉ‡∏´‡πâ error ‡∏ä‡∏±‡∏î ‡πÜ ‡πÑ‡∏õ‡πÄ‡∏•‡∏¢ ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏á‡∏µ‡∏¢‡∏ö)
# ===================================================================
from core.seam_prompts import (
    SYSTEM_ASSESSMENT_PROMPT,
    USER_ASSESSMENT_PROMPT,
    SYSTEM_ACTION_PLAN_PROMPT,
    ACTION_PLAN_PROMPT,
    SYSTEM_EVIDENCE_DESCRIPTION_PROMPT,
    EVIDENCE_DESCRIPTION_PROMPT,
    SYSTEM_LOW_LEVEL_PROMPT,
    USER_LOW_LEVEL_PROMPT,
    USER_LOW_LEVEL_PROMPT_TEMPLATE,
    USER_EVIDENCE_DESCRIPTION_TEMPLATE,
    EXCELLENCE_ADVICE_PROMPT, 
    SYSTEM_EXCELLENCE_PROMPT,
    SYSTEM_QUALITY_PROMPT,
    QUALITY_REFINEMENT_PROMPT
)

from core.vectorstore import VectorStoreManager, get_global_reranker, ChromaRetriever
from core.assessment_schema import CombinedAssessment, EvidenceSummary
from core.action_plan_schema import ActionPlanActions, ActionPlanResult

try:
    from core.assessment_schema import StatementAssessment
except ImportError:
    from pydantic import BaseModel
    class StatementAssessment(BaseModel):
        score: int = 0
        reason: str = ""

from langchain_core.documents import Document as LcDocument
# ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î Import ‡πÉ‡∏ô routers/llm_router.py

# ===================================================================
# 4. Constants
# ===================================================================
LOW_LEVEL_K: int = 3
_MOCK_FLAG = False
_MAX_LLM_RETRIES = 3

def set_mock_control_mode(enable: bool):
    global _MOCK_FLAG
    _MOCK_FLAG = bool(enable)
    logger.info(f"Mock control mode: {_MOCK_FLAG}")

# ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÉ‡∏ô core/llm_data_utils.py

def _create_where_filter(
    stable_doc_ids: Optional[Union[Set[str], List[str]]] = None,
    subject: Optional[str] = None,
    sub_topic: Optional[str] = None,  # üëà ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏°‡∏≤
    year: Optional[Union[int, str]] = None,
    enabler: Optional[str] = None,
    **kwargs  # üëà ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏° **kwargs ‡πÑ‡∏ß‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏±‡∏ô‡πÄ‡∏´‡∏ô‡∏µ‡∏¢‡∏ß‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï
) -> Dict[str, Any]:
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á Filter ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ChromaDB ‡∏ó‡∏µ‡πà‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô:
    """
    filters: List[Dict[str, Any]] = []

    # --- 1. ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Stable Doc IDs (‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î) ---
    if stable_doc_ids:
        ids_list = [str(i).strip() for i in (stable_doc_ids if isinstance(stable_doc_ids, (list, set)) else [stable_doc_ids]) if i]
        if ids_list:
            if len(ids_list) == 1:
                return {"stable_doc_uuid": ids_list[0]}
            else:
                return {"stable_doc_uuid": {"$in": ids_list}}

    # --- 2. ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Metadata ‡∏≠‡∏∑‡πà‡∏ô‡πÜ ---
    if year and str(year).strip():
        filters.append({"year": str(year).strip()})
    
    if enabler and str(enabler).strip():
        filters.append({"enabler": enabler.strip().upper()})

    if subject and str(subject).strip():
        filters.append({"subject": str(subject).strip()})

    # --- 3. ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ sub_topic (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏™‡πà‡∏á‡∏°‡∏≤) ---
    if sub_topic and str(sub_topic).strip():
        filters.append({"sub_topic": str(sub_topic).strip()})

    if not filters:
        return {}

    return filters[0] if len(filters) == 1 else {"$and": filters}


def retrieve_context_for_endpoint(
    vectorstore_manager,
    query: str = "",
    tenant: Optional[str] = None,
    year: Optional[Union[int, str]] = None,
    stable_doc_ids: Optional[Set[str]] = None,
    doc_type: Optional[str] = None,
    enabler: Optional[str] = None,
    subject: Optional[str] = None,
    sub_topic: Optional[str] = None,
    k_to_retrieve: int = 150, 
    k_to_rerank: int = 30,    
    strict_filter: bool = False,
    **kwargs
) -> Dict[str, Any]:
    """
    [ULTIMATE REVISED] Retrieval for Search Endpoint
    - ‡∏¢‡∏∂‡∏î Metadata ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢ (No Pydantic Error)
    - ‡πÉ‡∏ä‡πâ Deterministic MD5 Hash ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Deduplication
    - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Anchor Chunks ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á Context ‡∏ó‡∏µ‡πà‡∏°‡∏±‡πà‡∏ô‡∏Ñ‡∏á
    """
    start_time = time.time()
    vsm = vectorstore_manager

    # 1. Resolve collection
    clean_doc_type = str(doc_type or "document").strip().lower()
    collection_name = get_doc_type_collection_key(doc_type=clean_doc_type, enabler=enabler)
    
    chroma = vsm._load_chroma_instance(collection_name)
    if not chroma:
        logger.error(f"‚ùå Collection {collection_name} not found.")
        return {"top_evidences": [], "aggregated_context": "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", "retrieval_time": 0}

    # 2. Create where_filter
    where_filter = _create_where_filter(
        stable_doc_ids=list(stable_doc_ids) if stable_doc_ids else None, 
        subject=subject, 
        sub_topic=sub_topic, 
        year=year
    )

    # Dictionary ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Deduplication (‡πÉ‡∏ä‡πâ MD5 ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏õ‡πá‡∏ô Key)
    unique_map: Dict[str, LcDocument] = {}

    # =====================================================
    # ‚öì 2.1 ANCHOR RETRIEVAL (‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå)
    # =====================================================
    if stable_doc_ids:
        # ‡∏î‡∏∂‡∏á‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏£‡∏Å‡πÜ ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        anchors = chroma.get(where=where_filter, limit=10) 
        if anchors and anchors.get('documents'):
            for i in range(len(anchors['documents'])):
                content = anchors['documents'][i]
                md = anchors['metadatas'][i]
                
                # Deterministic MD5 Hash
                c_hash = hashlib.md5(content.encode()).hexdigest()
                uid = md.get("chunk_uuid") or f"anchor-{c_hash}"
                
                if uid not in unique_map:
                    # ‡∏â‡∏µ‡∏î‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÉ‡∏´‡πâ Anchor ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ï‡∏¥‡∏î‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡∏ï‡πâ‡∏ô‡πÜ ‡∏´‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á
                    md["score"] = 0.5 
                    md["is_anchor"] = True
                    unique_map[uid] = LcDocument(page_content=content, metadata=md)

    # =====================================================
    # üîç 2.2 SEMANTIC SEARCH
    # =====================================================
    search_query = query if (query and query != "*" and len(query) > 2) else ""
    
    # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö‡∏à‡∏≤‡∏Å Vector DB
    if search_query:
        docs = chroma.similarity_search(search_query, k=k_to_retrieve, filter=where_filter)
    else:
        # Fallback ‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏°‡πà‡∏°‡∏µ Query ‡πÉ‡∏´‡πâ‡∏î‡∏∂‡∏á‡πÅ‡∏ö‡∏ö‡∏Å‡∏ß‡∏≤‡∏î
        docs = chroma.similarity_search("*", k=k_to_retrieve, filter=where_filter)

    for d in docs:
        c_hash = hashlib.md5(d.page_content.encode()).hexdigest()
        md = d.metadata or {}
        uid = md.get("chunk_uuid") or c_hash
        if uid not in unique_map:
            unique_map[uid] = d

    candidates = list(unique_map.values())

    # =====================================================
    # üöÄ 3. BATCH RERANKING
    # =====================================================
    final_scored_docs = []
    reranker = getattr(vsm, "reranker", None)
    
    if reranker and candidates and search_query:
        try:
            batch_size = 100 
            logger.info(f"üöÄ Reranking {len(candidates)} candidates in batches...")
            for i in range(0, len(candidates), batch_size):
                batch = candidates[i : i + batch_size]
                # ‡πÉ‡∏ä‡πâ Reranker ‡∏Ç‡∏≠‡∏á LangChain (Compressor)
                scored_batch = reranker.compress_documents(documents=batch, query=search_query)
                final_scored_docs.extend(scored_batch)
        except Exception as e:
            logger.error(f"‚ö†Ô∏è Rerank failed: {e}")
            final_scored_docs = candidates
    else:
        final_scored_docs = candidates

    # =====================================================
    # 4. SORTING & SCORE INJECTION (‡∏´‡∏±‡∏ß‡πÉ‡∏à‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ 0.0000)
    # =====================================================
    def get_score(d) -> float:
        m = d.metadata or {}
        # ‡∏î‡∏∂‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏à‡∏≤‡∏Å‡∏ó‡∏∂‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ
        s = m.get("relevance_score") or m.get("score") or m.get("rerank_score") or 0.0
        try: return float(s)
        except: return 0.0

    final_scored_docs.sort(key=get_score, reverse=True)

    # =====================================================
    # 5. RESPONSE BUILD
    # =====================================================
    top_evidences = []
    aggregated_parts = []
    
    for doc in final_scored_docs[:k_to_rerank]:
        md = doc.metadata or {}
        text = doc.page_content.strip()
        score = get_score(doc)
        
        # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏•‡∏∞‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏≤‡πÉ‡∏´‡πâ Robust
        p_val = md.get("page") or md.get("page_label") or md.get("page_number") or "N/A"
        source_name = md.get('source') or md.get('source_filename') or md.get('file_name') or 'Unknown'
        s_uuid = md.get("stable_doc_uuid") or md.get("doc_id") or ""
        
        # SYNC SCORE ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤ Metadata ‡∏ó‡∏∏‡∏Å‡∏ï‡∏±‡∏ß
        md["score"] = score
        md["relevance_score"] = score
        
        evidence_item = {
            "doc_id": str(s_uuid),
            "chunk_uuid": str(md.get("chunk_uuid") or ""),
            "source": source_name,
            "text": text,
            "page": str(p_val),
            "score": score,
            "pdca_tag": md.get("pdca_tag", "Other"),
            "metadata": md
        }
        
        top_evidences.append(evidence_item)
        aggregated_parts.append(f"[‡πÑ‡∏ü‡∏•‡πå: {source_name}, ‡∏´‡∏ô‡πâ‡∏≤: {p_val}] {text}")

    retrieval_time = round(time.time() - start_time, 3)
    logger.info(f"üèÅ Finished: {len(top_evidences)} chunks in {retrieval_time}s")

    return {
        "top_evidences": top_evidences,
        "aggregated_context": "\n\n---\n\n".join(aggregated_parts) if aggregated_parts else "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•",
        "retrieval_time": retrieval_time,
        "total_candidates": len(candidates)
    }

# ------------------------
# Retrieval: retrieve_context_with_filter (Revised)
# ------------------------
def retrieve_context_with_filter(
    query: Union[str, List[str]],
    doc_type: str,
    tenant: Optional[str] = None,
    year: Optional[Union[int, str]] = None,
    enabler: Optional[str] = None,
    subject: Optional[str] = None,
    vectorstore_manager: Optional[Any] = None,
    mapped_uuids: Optional[List[str]] = None,
    stable_doc_ids: Optional[List[str]] = None,
    priority_docs_input: Optional[List[Any]] = None,
    sequential_chunk_uuids: Optional[List[str]] = None,
    sub_id: Optional[str] = None,
    level: Optional[int] = None,
    get_previous_level_docs: Optional[Callable[[int, str], List[Any]]] = None,
    top_k: int = 150, 
) -> Dict[str, Any]:
    """
    [FIXED VERSION] ‡∏¢‡∏∂‡∏î‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ (No Error) 
    ‡πÅ‡∏ï‡πà‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏¢‡∏±‡∏î‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏•‡∏á Metadata ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Rerank 0.0000
    """
    start_time = time.time()
    manager = vectorstore_manager
    queries_to_run = [query] if isinstance(query, str) else list(query or [""])
    
    # 1. Resolve Collection & Filter
    # ‡πÉ‡∏ä‡πâ helper ‡∏à‡∏≤‡∏Å utils ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Ç‡∏≠‡∏á‡∏ä‡∏∑‡πà‡∏≠ collection
    collection_name = get_doc_type_collection_key(doc_type, enabler or "KM")
    
    target_ids = set()
    if stable_doc_ids: target_ids.update([str(i) for i in stable_doc_ids])
    if mapped_uuids: target_ids.update([str(i) for i in mapped_uuids])
    if sequential_chunk_uuids: target_ids.update([str(i) for i in sequential_chunk_uuids])
    
    where_filter = _create_where_filter(
        stable_doc_ids=list(target_ids) if target_ids else None,
        subject=subject,
        year=year
    )

    # 2. Collect Chunks
    all_source_chunks = []

    # 2.1 Priority Docs
    if priority_docs_input:
        for doc in priority_docs_input:
            if not doc: continue
            if isinstance(doc, dict):
                pc = doc.get('page_content') or doc.get('text') or ''
                meta = doc.get('metadata') or {}
                if pc.strip():
                    all_source_chunks.append(LcDocument(page_content=pc, metadata=meta))
            elif hasattr(doc, 'page_content'):
                all_source_chunks.append(doc)

    # 2.2 Vector Retrieval
    try:
        full_retriever = manager.get_retriever(collection_name=collection_name)
        base_retriever = getattr(full_retriever, "base_retriever", full_retriever)
        
        search_kwargs = {"k": top_k}
        if where_filter: 
            search_kwargs["where"] = where_filter

        for q in queries_to_run:
            if not q: continue
            docs = base_retriever.invoke(q, config={"configurable": {"search_kwargs": search_kwargs}})
            if docs: all_source_chunks.extend(docs)
    except Exception as e:
        logger.error(f"Retrieval error: {e}")

    # 3. Deduplicate (‡πÉ‡∏ä‡πâ Hash ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥)
    unique_map: Dict[str, LcDocument] = {}
    for doc in all_source_chunks:
        if not doc or not doc.page_content.strip(): continue
        md = doc.metadata or {}
        # ‡πÉ‡∏ä‡πâ hashlib ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ ID ‡∏ó‡∏µ‡πà‡∏Ñ‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô‡∏ó‡∏∏‡∏Å‡∏£‡∏≠‡∏ö
        c_hash = hashlib.md5(doc.page_content.encode()).hexdigest()
        uid = str(md.get("chunk_uuid") or f"{md.get('stable_doc_uuid', 'unknown')}-{c_hash}")
        
        if uid not in unique_map:
            unique_map[uid] = doc

    candidates = list(unique_map.values())

    # 4. Batch Reranking
    final_scored_docs = []
    batch_size = 100 
    reranker = getattr(manager, "reranker", None)

    if reranker and candidates:
        main_query = queries_to_run[0]
        for i in range(0, len(candidates), batch_size):
            batch = candidates[i : i + batch_size]
            try:
                # üìå ‡∏Å‡∏∏‡∏ç‡πÅ‡∏à‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: Reranker ‡∏à‡∏∞‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÉ‡∏ô metadata
                scored_batch = reranker.compress_documents(batch, main_query)
                final_scored_docs.extend(scored_batch)
            except Exception as e:
                logger.error(f"Rerank Error: {e}")
                final_scored_docs.extend(batch)
    else:
        final_scored_docs = candidates

    # 5. Sorting & Score Injection (‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ 0.0000)
    def get_score(d) -> float:
        m = d.metadata or {}
        # ‡πÑ‡∏•‡πà‡∏´‡∏≤‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ
        s = m.get("relevance_score") or m.get("score") or m.get("rerank_score") or 0.0
        try: return float(s)
        except: return 0.0

    final_scored_docs.sort(key=get_score, reverse=True)

    # 6. Final Formatting (‡∏Ñ‡∏±‡∏î‡πÄ‡∏•‡∏∑‡∏≠‡∏Å K ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å)
    top_evidences = []
    aggregated_parts = []
    final_k = ANALYSIS_FINAL_K # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å config

    for doc in final_scored_docs[:final_k]:
        score = get_score(doc)
        # ‡∏Å‡∏£‡∏≠‡∏á Threshold ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡∏±‡πâ‡∏á‡πÑ‡∏ß‡πâ‡πÉ‡∏ô .env
        if score < RERANK_THRESHOLD and RERANK_THRESHOLD > 0:
            continue

        md = doc.metadata or {}
        text = doc.page_content.strip()
        
        # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö ingest.py
        page = str(md.get("page_label") or md.get("page_number") or md.get("page") or "N/A")
        source = md.get("source") or md.get("source_filename") or "Unknown"
        pdca = md.get("pdca_tag", "Other")

        # üéØ SYNC SCORE ‡πÄ‡∏Ç‡πâ‡∏≤ Metadata (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Loop ‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡πÅ‡∏•‡πâ‡∏ß‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ 0)
        md["score"] = score
        md["relevance_score"] = score
        md["rerank_score"] = score

        top_evidences.append({
            "doc_id": str(md.get("stable_doc_uuid") or md.get("doc_id") or ""),
            "chunk_uuid": str(md.get("chunk_uuid") or ""),
            "source": source,
            "text": text,
            "page": page,
            "pdca_tag": pdca,
            "score": score,
            "metadata": md
        })
        aggregated_parts.append(f"[{pdca}] [‡πÑ‡∏ü‡∏•‡πå: {source} ‡∏´‡∏ô‡πâ‡∏≤: {page}] {text}")

    return {
        "top_evidences": top_evidences,
        "aggregated_context": "\n\n---\n\n".join(aggregated_parts) if aggregated_parts else "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô",
        "retrieval_time": round(time.time() - start_time, 3),
        "total_candidates": len(candidates)
    }

# =====================================================================
# üõ† Helper: check_rubric_readiness (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏á‡∏µ‡∏¢‡∏ö‡∏•‡∏á)
# =====================================================================
def is_rubric_ready(tenant: str) -> bool:
    """ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏Ç‡∏≠‡∏á seam collection ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏û‡πà‡∏ô Warning ‡∏Å‡∏ß‡∏ô‡πÉ‡∏à """
    if not tenant:
        return False
    
    tenant_vs_root = get_vectorstore_tenant_root_path(tenant)
    chroma_path = os.path.join(tenant_vs_root, "seam")
    
    # ‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ True/False ‡πÄ‡∏á‡∏µ‡∏¢‡∏ö‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏î‡∏∂‡∏á Rubric ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    return os.path.exists(chroma_path)


# =====================================================================
# üöÄ Ultimate Version: retrieve_context_with_rubric (FIXED & REVISED)
# =====================================================================
def retrieve_context_with_rubric(
    vectorstore_manager,
    query: str,
    doc_type: str,
    enabler: Optional[str] = None,
    stable_doc_ids: Optional[Set[str]] = None,
    tenant: Optional[str] = None,
    year: Optional[Union[int, str]] = None,
    subject: Optional[str] = None,
    rubric_vectorstore_name: str = "seam", 
    top_k: int = 150,         # üöÄ ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏î‡∏∂‡∏á‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Reranker ‡∏°‡∏µ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô
    rubric_top_k: int = 15,  
    strict_filter: bool = True,
    k_to_rerank: int = 30    
) -> Dict[str, Any]:
    """
    [REVISED VERSION] ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Rubric + Evidence Retrieval ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏£‡∏∞‡∏ö‡∏ö Batch Reranking 
    ‡πÅ‡∏•‡∏∞ Content-Based Deduplication ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô Level 5 ‡∏´‡∏≤‡∏¢
    """
    start_time = time.time()
    vsm = vectorstore_manager

    # --- 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏™‡∏•‡∏±‡∏ö Collection ---
    if hasattr(vsm, 'doc_type') and vsm.doc_type != doc_type:
        logger.info(f"üîÑ Switching VSM doc_type to: {doc_type}")
        vsm.close()
        vsm.__init__(tenant=tenant, year=year, doc_type=doc_type, enabler=enabler)

    evidence_collection = get_doc_type_collection_key(doc_type, enabler or "KM")
    
    rubric_results = []
    # ‡πÉ‡∏ä‡πâ Dict ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥ Deduplication ‡∏î‡πâ‡∏ß‡∏¢ Content Hash ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ã‡πâ‡∏≥‡πÅ‡∏ï‡πà ID ‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô
    unique_evidence_map: Dict[str, LcDocument] = {}

    # --- 2. ‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á Rubrics (‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô) ---
    try:
        rubric_chroma = vsm._load_chroma_instance(rubric_vectorstore_name)
        if rubric_chroma:
            rubric_query = f"‡πÄ‡∏Å‡∏ì‡∏ë‡πå SE-AM {enabler} {subject or ''}: {query}"
            r_docs = rubric_chroma.similarity_search(rubric_query, k=rubric_top_k)
            for rd in r_docs:
                rubric_results.append({
                    "text": rd.page_content, 
                    "metadata": rd.metadata, 
                    "is_rubric": True
                })
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Rubric Retrieval Error: {e}")

    # --- 3. ‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á Evidence (‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô) ---
    try:
        evidence_chroma = vsm._load_chroma_instance(evidence_collection)
        if not evidence_chroma:
            return {"top_evidences": [], "rubric_context": rubric_results, "retrieval_time": 0}

        where_filter = None
        if stable_doc_ids:
            ids_list = [str(i).strip().lower() for i in stable_doc_ids if i]
            where_filter = {"stable_doc_uuid": ids_list[0]} if len(ids_list) == 1 else {"stable_doc_uuid": {"$in": ids_list}}
            
            # ‚öì 3.1 Fetch Anchor Chunks (‡∏™‡πà‡∏ß‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå ‡πÄ‡∏ä‡πà‡∏ô ‡∏´‡∏ô‡πâ‡∏≤ 1-5)
            anchors = evidence_chroma.get(where=where_filter, limit=10)
            if anchors and anchors.get('documents'):
                for i in range(len(anchors['documents'])):
                    content = anchors['documents'][i]
                    md = dict(anchors['metadatas'][i] or {}) # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô metadata ‡πÄ‡∏õ‡πá‡∏ô None
                    
                    # üéØ ‡πÅ‡∏Å‡πâ‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà 1: ‡πÉ‡∏ä‡πâ MD5 ‡πÅ‡∏ó‡∏ô hash() ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ ID ‡∏Ñ‡∏á‡∏ó‡∏µ‡πà (Deterministic)
                    content_hash = hashlib.md5(content.encode()).hexdigest()
                    uid = str(md.get("chunk_uuid") or f"anchor-{content_hash}")
                    
                    if uid not in unique_evidence_map:
                        # üéØ ‡πÅ‡∏Å‡πâ‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà 2: ‡∏â‡∏µ‡∏î‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏•‡∏á‡πÉ‡∏ô metadata ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á (Safe Injection)
                        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÉ‡∏´‡πâ Anchor ‡∏°‡∏µ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á (0.95) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏´‡∏•‡∏±‡∏Å
                        md["score"] = 0.95
                        md["relevance_score"] = 0.95
                        md["is_anchor"] = True
                        
                        unique_evidence_map[uid] = LcDocument(
                            page_content=content,
                            metadata=md
                        )

        # üîç 3.2 Semantic Search (‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢)
        search_results = evidence_chroma.similarity_search(query, k=top_k, filter=where_filter)
        for d in search_results:
            content_hash = str(hash(d.page_content))
            uid = d.metadata.get("chunk_uuid") or content_hash
            if uid not in unique_evidence_map:
                unique_evidence_map[uid] = d

        candidates = list(unique_evidence_map.values())

        # --- 4. BATCH RERANKING (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô OOM ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥) ---
        evidence_results = []
        reranker = get_global_reranker()
        
        if reranker and candidates and query:
            try:
                batch_size = 100 # üöÄ ‡πÅ‡∏ö‡πà‡∏á Batch ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏Ç‡∏≠‡∏á VRAM
                scored_candidates = []
                
                logger.info(f"üöÄ Batch Reranking {len(candidates)} chunks...")
                for i in range(0, len(candidates), batch_size):
                    batch = candidates[i : i + batch_size]
                    reranked_batch = reranker.compress_documents(documents=batch, query=query)
                    scored_candidates.extend(reranked_batch)
                
                # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å Batch
                scored_candidates = sorted(
                    scored_candidates, 
                    key=lambda x: getattr(x, "relevance_score", 0), 
                    reverse=True
                )
                
                for r in scored_candidates[:k_to_rerank]:
                    doc = r.document if hasattr(r, "document") else r
                    m = doc.metadata or {}
                    score = getattr(r, "relevance_score", 0.0)
                    
                    # üéØ Sync Score ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤ Metadata ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ
                    m["rerank_score"] = score
                    m["score"] = score
                    
                    evidence_results.append({
                        "text": doc.page_content,
                        "source_filename": m.get("source_filename") or m.get("source") or "Evidence",
                        "page_label": str(m.get("page_label") or m.get("page_number") or m.get("page") or "N/A"),
                        "doc_id": m.get("stable_doc_uuid") or m.get("doc_id"),
                        "chunk_uuid": m.get("chunk_uuid") or str(uuid.uuid4()),
                        "pdca_tag": m.get("pdca_tag") or "Content",
                        "rerank_score": score,
                        "is_evidence": True,
                        "metadata": m
                    })
            except Exception as e:
                logger.error(f"‚ö†Ô∏è Rerank failed: {e}")
                candidates = candidates[:k_to_rerank] # Fallback
        
        # ‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏°‡πà‡∏°‡∏µ Reranker ‡∏´‡∏£‡∏∑‡∏≠ Error ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏õ‡∏Å‡∏ï‡∏¥
        if not evidence_results:
            for d in candidates[:k_to_rerank]:
                m = d.metadata or {}
                evidence_results.append({
                    "text": d.page_content,
                    "source_filename": m.get("source_filename") or m.get("source") or "Evidence",
                    "page_label": str(m.get("page_label") or m.get("page_number") or m.get("page") or "N/A"),
                    "doc_id": m.get("stable_doc_uuid") or m.get("doc_id"),
                    "chunk_uuid": m.get("chunk_uuid") or str(uuid.uuid4()),
                    "pdca_tag": m.get("pdca_tag") or "Content",
                    "rerank_score": 0.0,
                    "is_evidence": True,
                    "metadata": m
                })

    except Exception as e:
        logger.error(f"‚ùå Evidence Retrieval Error: {e}", exc_info=True)

    retrieval_time = round(time.time() - start_time, 3)
    logger.info(f"‚úÖ Success: Retrieved {len(evidence_results)} evidence chunks in {retrieval_time}s")

    return {
        "top_evidences": evidence_results,
        "rubric_context": rubric_results,
        "retrieval_time": retrieval_time,
        "used_chunk_uuids": [e["chunk_uuid"] for e in evidence_results if e.get("chunk_uuid")]
    }

# ========================
#  retrieve_context_by_doc_ids (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö hydration ‡πÉ‡∏ô router)
# ========================
def retrieve_context_by_doc_ids(
    doc_uuids: List[str],
    doc_type: str,
    enabler: Optional[str] = None,
    vectorstore_manager = None,
    limit: int = 100, # üöÄ ‡πÄ‡∏û‡∏¥‡πà‡∏° limit ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô
    tenant: Optional[str] = None,
    year: Optional[Union[int, str]] = None,
) -> Dict[str, Any]:
    """
    [REVISED] ‡∏î‡∏∂‡∏á chunks ‡∏à‡∏≤‡∏Å stable_doc_uuid ‡∏´‡∏•‡∏≤‡∏¢‡∏ï‡∏±‡∏ß (‡πÉ‡∏ä‡πâ‡∏ï‡∏≠‡∏ô hydration sources)
    ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏∏‡∏õ‡∏µ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤ Collection ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏Å‡∏©‡∏≤ Metadata ‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô
    """
    start_time = time.time()
    vsm = vectorstore_manager or VectorStoreManager(tenant=tenant, year=year)
    
    # Resolve collection name ‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏õ‡∏µ‡πÅ‡∏•‡∏∞ enabler
    collection_name = get_doc_type_collection_key(doc_type=doc_type, enabler=enabler)

    chroma = vsm._load_chroma_instance(collection_name)
    if not chroma:
        logger.error(f"‚ùå Collection {collection_name} not found for hydration")
        return {"top_evidences": []}

    if not doc_uuids:
        return {"top_evidences": []}

    logger.info(f"üíß Hydration ‚Üí {len(doc_uuids)} doc IDs from {collection_name}")

    try:
        # ‡πÉ‡∏ä‡πâ Metadata filter ‡∏î‡∏∂‡∏á chunks ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏•‡∏¥‡∏™‡∏ï‡πå ID ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
        # ‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô list comprehension ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏ß‡∏£‡πå‡∏Ç‡∏≠‡∏á Type
        ids_to_query = [str(u) for u in doc_uuids if u]
        
        results = chroma._collection.get(
            where={"stable_doc_uuid": {"$in": ids_to_query}},
            limit=limit,
            include=["documents", "metadatas"]
        )
    except Exception as e:
        logger.error(f"‚ö†Ô∏è Hydration query failed: {e}")
        return {"top_evidences": []}

    evidences = []
    # üéØ ‡πÉ‡∏ä‡πâ Content-based Deduplication ‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô‡∏Å‡∏±‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ã‡πâ‡∏≥
    seen_contents = set()

    for doc_content, meta in zip(results.get("documents", []), results.get("metadatas", [])):
        if not doc_content or not doc_content.strip():
            continue
            
        content_hash = str(hash(doc_content))
        if content_hash in seen_contents:
            continue
        seen_contents.add(content_hash)

        p_val = meta.get("page_label") or meta.get("page_number") or meta.get("page") or "N/A"
        
        # üéØ Sync Score ‡∏´‡∏•‡∏≠‡∏Å (‡πÄ‡∏û‡∏£‡∏≤‡∏∞ Hydration ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ú‡πà‡∏≤‡∏ô Rerank ‡πÅ‡∏ï‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Logic ‡∏≠‡∏∑‡πà‡∏ô‡πÑ‡∏°‡πà‡∏û‡∏±‡∏á)
        score = meta.get("score") or meta.get("rerank_score") or 0.85

        evidences.append({
            "doc_id": meta.get("stable_doc_uuid") or meta.get("doc_id"),
            "chunk_uuid": meta.get("chunk_uuid"),
            "source": meta.get("source") or meta.get("source_filename") or "Unknown",
            "page": str(p_val),
            "text": doc_content.strip(),
            "pdca_tag": meta.get("pdca_tag", "Other"),
            "score": score,
            "metadata": meta # ‡πÅ‡∏ô‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏ï‡πá‡∏°‡πÑ‡∏ß‡πâ‡πÄ‡∏™‡∏°‡∏≠
        })

    logger.info(f"‚úÖ Hydration success: {len(evidences)} chunks from {len(doc_uuids)} docs")
    return {
        "top_evidences": evidences,
        "retrieval_time": round(time.time() - start_time, 3)
    }


def retrieve_context_for_low_levels(query: str, doc_type: str, enabler: Optional[str]=None,
                                 vectorstore_manager: Optional['VectorStoreManager']=None,
                                 top_k: int=LOW_LEVEL_K, initial_k: int=INITIAL_TOP_K,
                                 # üü¢ NEW: ‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠ arguments
                                 mapped_uuids: Optional[List[str]]=None,
                                 priority_docs_input: Optional[List[Any]] = None,
                                 sequential_chunk_uuids: Optional[List[str]] = None, 
                                 sub_id: Optional[str]=None, level: Optional[int]=None) -> Dict[str, Any]:
    """
    Retrieves a small, focused context for low levels (L1, L2) using a reduced k (LOW_LEVEL_K).
    """
    # ‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å ‡πÅ‡∏ï‡πà‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏ä‡πâ k ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
    return retrieve_context_with_filter(
        query=query,
        doc_type=doc_type,
        enabler=enabler,
        vectorstore_manager=vectorstore_manager,
        top_k=LOW_LEVEL_K,
        initial_k=initial_k,
        mapped_uuids=mapped_uuids,
        priority_docs_input=priority_docs_input,
        sequential_chunk_uuids=sequential_chunk_uuids, 
        sub_id=sub_id,
        level=level
    )

# ----------------------------------------------------
# Helper function: Summarize evidence list (minimal stub)
# ----------------------------------------------------
def _summarize_evidence_list_short(evidences: list, max_sentences: int = 3) -> str:
    """
    Provides a concise summary of evidence items.
    """
    if not evidences:
        return ""
    
    parts = []
    for ev in evidences[:max(1, min(len(evidences), max_sentences))]:
        if isinstance(ev, dict):
            fn = ev.get("source_filename") or ev.get("source") or ev.get("doc_id", "unknown")
            txt = ev.get("text") or ev.get("content") or ""
        else:
            fn = str(ev)
            txt = str(ev)
        txt_short = txt[:120].replace("\n", " ").strip()
        if txt_short:
            parts.append(f"‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå `{fn}`: {txt_short}...")
        else:
            parts.append(f"‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå `{fn}`")
    return " | ".join(parts)


# ULTIMATE FINAL VERSION: build_multichannel_context_for_level (OPTIMIZED)
# ‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó‡πÉ‡∏´‡∏°‡πà: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏Ñ‡πà BASELINE ‡πÅ‡∏•‡∏∞ AUXILIARY summaries ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
# ----------------------------------------------------
def build_multichannel_context_for_level(
    level: int,
    top_evidences: List[Dict[str, Any]],
    previous_levels_map: Optional[Dict[str, Any]] = None,
    previous_levels_evidence: Optional[List[Dict[str, Any]]] = None, # List ‡∏Ç‡∏≠‡∏á Chunks ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏à‡∏≤‡∏Å Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤
    max_main_context_tokens: int = 3000, 
    max_summary_sentences: int = 4,
    max_context_length: Optional[int] = None, 
    **kwargs
) -> Dict[str, Any]:
    """
    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏±‡∏Å‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Context Summary ‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏°‡∏≤
    ‡πÇ‡∏î‡∏¢‡πÄ‡∏ô‡πâ‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Baseline Summary (‡∏à‡∏≤‡∏Å Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤) ‡πÅ‡∏•‡∏∞ Auxiliary Summary (‡∏à‡∏≤‡∏Å Level ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô)
    """
    logger = logging.getLogger(__name__)
    K_MAIN = 5
    MIN_RELEVANCE_FOR_AUX = 0.4  # ‡∏Å‡∏£‡∏≠‡∏á aux ‡∏ó‡∏µ‡πà‡∏ï‡πà‡∏≥‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ

    # --- 1) Baseline Summary ---
    # ‡πÉ‡∏ä‡πâ List ‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏°‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß (previous_levels_evidence_list) ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å
    baseline_evidence = previous_levels_evidence or [] 

    summarizable_baseline = [
        item for item in baseline_evidence[:20] # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î 20 chunks ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß
        if isinstance(item, dict) and (item.get("text") or item.get("content"))
    ]
    
    # üü¢ FIX: ‡∏õ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô baseline
    if not summarizable_baseline:
        baseline_summary = "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏à‡∏≤‡∏Å Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤"
    else:
        baseline_summary = _summarize_evidence_list_short(
            summarizable_baseline,
            max_sentences=max_summary_sentences
        )

    # --- 2) Auxiliary Summary ---
    direct, aux_candidates = [], []

    for ev in top_evidences:
        if not isinstance(ev, dict):
            # ‡∏≠‡∏≤‡∏à‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå
            continue # ‡∏Ç‡πâ‡∏≤‡∏° chunks ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô dict ‡πÑ‡∏õ‡πÄ‡∏•‡∏¢

        # NEW: ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö tag ‡∏ó‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö‡πÄ‡∏ï‡πá‡∏°‡πÅ‡∏•‡∏∞‡∏¢‡πà‡∏≠
        tag = (ev.get("pdca_tag") or ev.get("PDCA") or "Other").upper()
        relevance = ev.get("rerank_score") or ev.get("score", 0.0)

        # PDCA Chunks ‡∏ñ‡∏π‡∏Å‡∏™‡πà‡∏á‡πÑ‡∏õ‡πÄ‡∏õ‡πá‡∏ô Direct Context (‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏ô Engine)
        # ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ô‡∏µ‡πâ‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡πÅ‡∏Ñ‡πà‡πÅ‡∏¢‡∏Å‡πÅ‡∏¢‡∏∞ ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Direct Context
        if tag in {"P", "PLAN", "D", "DO", "C", "CHECK", "A", "ACT"}:
            direct.append(ev)
        elif relevance >= MIN_RELEVANCE_FOR_AUX:  # ‡∏Å‡∏£‡∏≠‡∏á aux ‡∏ó‡∏µ‡πà‡∏ï‡πà‡∏≥‡πÄ‡∏Å‡∏¥‡∏ô
            aux_candidates.append(ev)

    # Logic ‡∏Å‡∏≤‡∏£‡∏¢‡πâ‡∏≤‡∏¢‡∏à‡∏≤‡∏Å aux ‡πÑ‡∏õ direct (K_MAIN) ‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡∏≠‡∏¢‡∏π‡πà (‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏±‡∏î/Debug ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏™‡πà‡∏á Direct ‡∏≠‡∏≠‡∏Å‡πÑ‡∏õ)
    if len(direct) < K_MAIN:
        need = K_MAIN - len(direct)
        direct.extend(aux_candidates[:need])
        aux_candidates = aux_candidates[need:]
        
    if len(direct) < K_MAIN:
        logger.warning(f"L{level}: Direct PDCA chunks ‡∏¢‡∏±‡∏á‡∏ô‡πâ‡∏≠‡∏¢ ({len(direct)}) ‡∏´‡∏•‡∏±‡∏á‡∏¢‡πâ‡∏≤‡∏¢‡∏à‡∏≤‡∏Å aux")

    aux_summary = _summarize_evidence_list_short(aux_candidates, max_sentences=3) if aux_candidates else "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏£‡∏≠‡∏á"

    # --- 3) Return ---
    debug_meta = {
        "level": level,
        "direct_count": len(direct),
        "aux_count": len(aux_candidates),
        # üü¢ FIX: ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏™‡∏£‡∏∏‡∏õ‡∏à‡∏£‡∏¥‡∏á
        "baseline_count": len(summarizable_baseline), 
        "max_context_length_received": max_context_length 
    }
    logger.info(f"Context L{level} ‚Üí Direct:{len(direct)} | Aux:{len(aux_candidates)} | Baseline:{len(summarizable_baseline)}")

    return {
        "baseline_summary": baseline_summary,
        "direct_context": "",  
        "aux_summary": aux_summary,
        "debug_meta": debug_meta,
    }

# ------------------------
# LLM fetcher
# ------------------------
def _fetch_llm_response(
    system_prompt: str, 
    user_prompt: str, 
    max_retries: int = 3,
    llm_executor: Any = None 
) -> str:
    """
    ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å LLM ‡∏ú‡πà‡∏≤‡∏ô LangChain/Ollama ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Format ‡∏ú‡∏¥‡∏î‡πÄ‡∏û‡∏µ‡πâ‡∏¢‡∏ô:
    - ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö JSON output ‡∏î‡πâ‡∏ß‡∏¢ Strict English Prompt
    - ‡πÉ‡∏ä‡πâ Regex Extraction ‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡πà‡∏ß‡∏ô { ... } ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡∏≠‡∏≠‡∏Å
    - Log raw response ‡πÄ‡∏ï‡πá‡∏°‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Debug
    - Retry ‡∏û‡∏£‡πâ‡∏≠‡∏° Exponential Backoff ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏Å‡∏¥‡∏î Error
    """
    global _MOCK_FLAG

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ LLM Instance ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    if llm_executor is None and not _MOCK_FLAG: 
        raise ConnectionError("LLM instance not initialized (Missing llm_executor).")

    # 1. üõ†Ô∏è ENFORCED PROMPT (‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡∏°‡∏±‡∏Å‡∏Ñ‡∏∏‡∏° Format ‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏•‡πá‡∏Å)
    enforced_system_prompt = system_prompt.strip() + (
        "\n\n"
        "### STRICT OUTPUT RULES ###\n"
        "1. ANSWER IN VALID JSON OBJECT ONLY.\n"
        "2. NO EXPLANATIONS, NO PREFACE, NO CONVERSATION.\n"
        "3. START WITH '{' AND END WITH '}'.\n"
        "4. DO NOT USE MARKDOWN CODE BLOCKS (```json).\n"
        "5. IF NO EVIDENCE FOUND, RETURN: {\"score\": 0, \"reason\": \"No evidence\", \"is_passed\": false}"
    )

    messages = [
        {"role": "system", "content": enforced_system_prompt},
        {"role": "user",   "content": user_prompt}
    ]

    for attempt in range(1, max_retries + 1):
        try:
            # --- MOCK MODE CASE ---
            if _MOCK_FLAG:
                mock_json = '{"score": 1, "reason": "Mock mode active", "is_passed": true}'
                logger.critical(f"LLM RAW RESPONSE (DEBUG MOCK): {mock_json}")
                return mock_json

            # --- ACTUAL LLM CALL (OLLAMA / LANGCHAIN) ---
            # ‡πÉ‡∏ä‡πâ temperature=0.0 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
            response = llm_executor.invoke(messages, config={"temperature": 0.0})
            
            # ‡∏î‡∏∂‡∏á Text ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏à‡∏≤‡∏Å Response Object
            raw_text = ""
            if hasattr(response, "content"):
                raw_text = str(response.content)
            elif isinstance(response, str):
                raw_text = response
            else:
                raw_text = str(response)

            # üîç ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏´‡πá‡∏ô‡πÉ‡∏ô Log ‡∏Ñ‡∏∑‡∏≠‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ (Log ‡∏Å‡πà‡∏≠‡∏ô Clean ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°‡πÇ‡∏°‡πÄ‡∏î‡∏•)
            logger.critical(f"LLM RAW RESPONSE (DEBUG): {raw_text[:1000]}{'...' if len(raw_text) > 1000 else ''}")

            # 2. üßπ CLEANING LOGIC (Regex Extraction)
            # ‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ LLM ‡∏ï‡∏≠‡∏ö "Based on the text... { ... }"
            raw_text_stripped = raw_text.strip()
            
            # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏õ‡∏µ‡∏Å‡∏Å‡∏≤‡∏Ñ‡∏π‡πà‡πÅ‡∏£‡∏Å { ... }
            json_match = re.search(r'(\{.*\})', raw_text_stripped, re.DOTALL)
            
            if json_match:
                extracted_json = json_match.group(1)
                try:
                    # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô JSON ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                    json.loads(extracted_json) 
                    return extracted_json
                except json.JSONDecodeError:
                    logger.warning(f"Extracted string is not valid JSON: {extracted_json[:100]}")
            
            # 3. üõ°Ô∏è FALLBACK: ‡∏ñ‡πâ‡∏≤ Regex ‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ ‡∏´‡∏£‡∏∑‡∏≠ Parse ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏î‡∏π‡∏ß‡πà‡∏≤ raw_text (‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏î‡∏´‡∏±‡∏ß‡∏ó‡πâ‡∏≤‡∏¢) ‡∏û‡∏≠‡∏•‡∏∏‡πâ‡∏ô‡πÄ‡∏õ‡πá‡∏ô JSON ‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°
            return raw_text_stripped

        except Exception as e:
            logger.error(f"LLM call failed (attempt {attempt}/{max_retries}): {e}")
            if attempt < max_retries:
                # Exponential backoff: 2s, 4s, 8s...
                time.sleep(2 ** attempt)  
            else:
                logger.critical("All LLM attempts failed ‚Äì returning safe fallback JSON")
                return '{"score": 0, "reason": "LLM_TIMEOUT_OR_FAILURE", "is_passed": false}'

    return '{"score": 0, "reason": "Unknown execution error"}'

# ------------------------
# Evaluation
# ------------------------
T = TypeVar("T", bound=BaseModel)

def _check_and_handle_empty_context(context: str, sub_id: str, level: int) -> Optional[Dict[str, Any]]:
    """
    Returns Failure result if context is empty or contains known error strings.
    Auto-fail with PDCA keys all set to 0.
    """
    if not context or "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á" in context or "ERROR:" in context.upper():
        logger.warning(f"Auto-FAIL L{level} for {sub_id}: Empty or Error Context detected from RAG.")
        context_preview = context.strip()[:100].replace("\n", " ") if context else "Empty Context"
        return {
            "score": 0,
            "reason": f"‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏î‡πâ‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á (Context: {context_preview}).",
            "is_passed": False,
            "P_Plan_Score": 0,
            "D_Do_Score": 0,
            "C_Check_Score": 0,
            "A_Act_Score": 0,
        }
    return None

def _get_context_for_level(context: str, level: int) -> str:
    """Return context string with appropriate length limit for each level."""
    if not context:
        return ""
    # L1-L2 ‡πÉ‡∏ä‡πâ context ‡∏¢‡∏≤‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô
    if level <= 2:
        return context[:6000]  
    # L3-L5 ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÉ‡∏ô global_vars ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î Latency
    return context[:MAX_EVAL_CONTEXT_LENGTH]  

# =========================
# Main Evaluation Function
# =========================
def evaluate_with_llm(
    context: str, 
    sub_criteria_name: str, 
    level: int, 
    statement_text: str, 
    sub_id: str, 
    llm_executor: Any = None, 
    pdca_phase: str = "",
    level_constraint: str = "",
    must_include_keywords: str = "",
    avoid_keywords: str = "",
    max_rerank_score: float = 0.0,
    max_evidence_strength: float = 10.0,
    specific_contextual_rule: str = "N/A",
    **kwargs
) -> Dict[str, Any]:
    """
    [ULTIMATE PRODUCTION v21.9.4] Standard Evaluation for L3+ 
    - ‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Early FAIL + UnboundLocalError
    - ‡πÑ‡∏°‡πà Early FAIL ‡∏ñ‡πâ‡∏≤ evidence ‡πÅ‡∏£‡∏á
    - ‡∏™‡πà‡∏á Extraction Keys + final_llm_context + raw_response
    - ‡∏ó‡∏ô‡∏ó‡∏≤‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Expert Re-eval ‡πÅ‡∏•‡∏∞ Post-process ‡πÄ‡∏ï‡πá‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö
    """
    import logging
    import json

    logger = logging.getLogger(__name__)

    # =================================================================
    # 1. Context Preparation
    # =================================================================
    context_to_send_eval = _get_context_for_level(context, level) if context else ""

    # üéØ ‡πÑ‡∏°‡πà Early FAIL ‡∏≠‡∏µ‡∏Å‡∏ï‡πà‡∏≠‡πÑ‡∏õ ‚Äî ‡πÉ‡∏´‡πâ LLM ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÄ‡∏≠‡∏á‡πÅ‡∏°‡πâ context ‡∏ß‡πà‡∏≤‡∏á‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß
    if not context_to_send_eval.strip():
        logger.warning(
            f"Context empty for {sub_id} L{level} - continuing evaluation "
            f"(Evidence Strength: {max_evidence_strength:.1f}/10.0)"
        )

    # =================================================================
    # 2. Additional Summaries
    # =================================================================
    baseline_summary = kwargs.get("baseline_summary", "").strip()
    aux_summary = kwargs.get("aux_summary", "").strip()

    # =================================================================
    # 3. Prompt Building (‡πÅ‡∏Å‡πâ UnboundLocalError ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ñ‡∏≤‡∏ß‡∏£)
    # =================================================================
    # ‡∏ï‡∏±‡πâ‡∏á default system_prompt ‡∏Å‡πà‡∏≠‡∏ô try ‡πÄ‡∏™‡∏°‡∏≠
    system_prompt = "You are an expert SE-AM auditor. Respond only with valid JSON."

    try:
        user_prompt = USER_ASSESSMENT_PROMPT.format(
            sub_criteria_name=sub_criteria_name,
            sub_id=sub_id,
            level=level,
            pdca_phase=pdca_phase or "‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ",
            statement_text=statement_text,
            context=context_to_send_eval[:32000],  # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô token overflow
            level_constraint=level_constraint or "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°",
            must_include_keywords=must_include_keywords or "‡πÑ‡∏°‡πà‡∏°‡∏µ",
            avoid_keywords=avoid_keywords or "‡πÑ‡∏°‡πà‡∏°‡∏µ",
            max_rerank_score=f"{max_rerank_score:.4f}",
            max_evidence_strength=f"{max_evidence_strength:.1f}",
            target_score_threshold=kwargs.get("target_score_threshold", 2),
            specific_contextual_rule=specific_contextual_rule.strip() if specific_contextual_rule != "N/A" else "‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå SE-AM ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô"
        )

        # ‡∏â‡∏µ‡∏î‡∏Å‡∏é‡∏û‡∏¥‡πÄ‡∏®‡∏©‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
        if specific_contextual_rule.strip() and specific_contextual_rule != "N/A":
            user_prompt += f"\n\n=== ‡∏Å‡∏é‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏ô‡∏µ‡πâ (‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏ï‡∏≤‡∏°‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏Ñ‡∏£‡πà‡∏á‡∏Ñ‡∏£‡∏±‡∏î) ===\n{specific_contextual_rule}\n=== ‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏Å‡∏é‡∏û‡∏¥‡πÄ‡∏®‡∏© ==="

        # ‡πÄ‡∏û‡∏¥‡πà‡∏° summary ‡∏à‡∏≤‡∏Å level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤
        if baseline_summary:
            user_prompt += f"\n\n--- ‡∏™‡∏£‡∏∏‡∏õ‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ (Baseline) ---\n{baseline_summary}"
        if aux_summary:
            user_prompt += f"\n\n--- ‡∏™‡∏£‡∏∏‡∏õ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÄ‡∏™‡∏£‡∏¥‡∏° (Auxiliary) ---\n{aux_summary}"

        # ‡πÉ‡∏ä‡πâ system_prompt ‡∏à‡∏≤‡∏Å template ‡∏ñ‡πâ‡∏≤‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
        system_prompt = SYSTEM_ASSESSMENT_PROMPT.format(
            max_evidence_strength=f"{max_evidence_strength:.1f}"
        )

    except KeyError as e:
        logger.error(f"Missing placeholder in prompt: {e}")
        # system_prompt ‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡πÄ‡∏õ‡πá‡∏ô default
        user_prompt = (
            f"‡πÄ‡∏Å‡∏ì‡∏ë‡πå: {sub_criteria_name} (L{level})\n"
            f"‡∏£‡∏´‡∏±‡∏™: {sub_id}\n"
            f"‡∏Å‡∏é‡∏û‡∏¥‡πÄ‡∏®‡∏©: {specific_contextual_rule}\n"
            f"‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {statement_text}\n"
            f"‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô: {context_to_send_eval[:15000]}"
        )

    # =================================================================
    # 4. JSON Schema Enforcement
    # =================================================================
    try:
        schema_json = json.dumps(CombinedAssessment.model_json_schema(), ensure_ascii=False, indent=2)
    except Exception as e:
        logger.warning(f"Schema generation failed: {e}")
        schema_json = '''
        {
          "score": 0.0,
          "reason": "string",
          "is_passed": false,
          "P_Plan_Score": 0.0,
          "D_Do_Score": 0.0,
          "C_Check_Score": 0.0,
          "A_Act_Score": 0.0
        }
        '''

    system_prompt += f"\n\n--- REQUIRED JSON SCHEMA ---\n{schema_json}\n"
    system_prompt += "\nCRITICAL: ‡∏ï‡∏≠‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞ JSON ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô"

    # =================================================================
    # 5. LLM Execution
    # =================================================================
    try:
        raw_response = _fetch_llm_response(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            max_retries=_MAX_LLM_RETRIES,
            llm_executor=llm_executor
        )

        logger.info(f"Raw Response ({sub_id} L{level}): {raw_response[:600]}...")

        parsed = _robust_extract_json(raw_response)
        if not isinstance(parsed, dict):
            logger.warning(f"JSON parse failed for {sub_id} L{level} - using fallback")
            parsed = {}

        # =================================================================
        # 6. Final Output ‚Äî ‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö low_level
        # =================================================================
        result = {
            "score": float(parsed.get("score", 0.0)),
            "reason": parsed.get("reason", "‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå").strip(),
            "is_passed": bool(parsed.get("is_passed", False)),
            "P_Plan_Score": float(parsed.get("P_Plan_Score", 0.0)),
            "D_Do_Score": float(parsed.get("D_Do_Score", 0.0)),
            "C_Check_Score": float(parsed.get("C_Check_Score", 0.0)),
            "A_Act_Score": float(parsed.get("A_Act_Score", 0.0)),
            # üéØ ‡πÄ‡∏û‡∏¥‡πà‡∏° Extraction Keys
            "Extraction_P": parsed.get("Extraction_P", parsed.get("‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô P", "-")),
            "Extraction_D": parsed.get("Extraction_D", parsed.get("‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô D", "-")),
            "Extraction_C": parsed.get("Extraction_C", parsed.get("‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô C", "-")),
            "Extraction_A": parsed.get("Extraction_A", parsed.get("‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô A", "-")),
            # üéØ ‡πÄ‡∏Å‡πá‡∏ö Context ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Expert Re-eval
            "final_llm_context": context_to_send_eval,
            "raw_llm_response": raw_response[:2000]
        }

        logger.info(f"Final Result {sub_id} L{level}: Score={result['score']:.1f} | Passed={result['is_passed']}")
        return result

    except Exception as e:
        logger.exception(f"Critical failure in evaluate_with_llm for {sub_id} L{level}: {e}")
        return {
            "score": 0.0,
            "reason": f"‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)}",
            "is_passed": False,
            "P_Plan_Score": 0.0, "D_Do_Score": 0.0,
            "C_Check_Score": 0.0, "A_Act_Score": 0.0,
            "Extraction_P": "-", "Extraction_D": "-", "Extraction_C": "-", "Extraction_A": "-",
            "final_llm_context": context_to_send_eval,
            "raw_llm_response": ""
        }
    
# =========================
# Patch for L1-L2 evaluation
# =========================

# 1Ô∏è‚É£ ‡πÄ‡∏û‡∏¥‡πà‡∏° context limit ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö L1/L2
def _get_context_for_level(context: str, level: int) -> str:
    """Return context string with appropriate length limit for each level."""
    if not context:
        return ""
    if level <= 2:
        return context[:6000]  # L1-L2 ‡πÉ‡∏ä‡πâ context ‡∏¢‡∏≤‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô
    return context[:MAX_EVAL_CONTEXT_LENGTH]  # L3-L5

def _extract_combined_assessment(parsed: Dict[str, Any], score_default_key: str = "score") -> Dict[str, Any]:
    """Helper to safely extract combined assessment results."""
    # üü¢ NEW: Extract all scores needed by seam_assessment.py (Action #1 logic)
    score = int(parsed.get(score_default_key, 0))
    is_passed = parsed.get("is_passed", score >= 1) # ‡πÉ‡∏ä‡πâ score >= 1 ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤ default ‡∏ñ‡πâ‡∏≤ LLM ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏™‡πà‡∏á is_passed

    result = {
        "score": score,
        "reason": parsed.get("reason", "No reason provided by LLM."),
        "is_passed": is_passed,
        "P_Plan_Score": int(parsed.get("P_Plan_Score", 0)),
        "D_Do_Score": int(parsed.get("D_Do_Score", 0)),
        "C_Check_Score": int(parsed.get("C_Check_Score", 0)),
        "A_Act_Score": int(parsed.get("A_Act_Score", 0)),
    }
    return result


# =================================================================
# ULTIMATE PRODUCTION: evaluate_with_llm_low_level (L1/L2 Multi-Enabler)
# =================================================================
def evaluate_with_llm_low_level(
    context: str,
    sub_criteria_name: str,
    level: int,
    statement_text: str,
    sub_id: str,
    llm_executor: Any = None,
    pdca_phase: str = "",
    level_constraint: str = "",
    must_include_keywords: str = "",
    avoid_keywords: str = "",
    max_rerank_score: float = 0.0,
    max_evidence_strength: float = 10.0,
    contextual_rules_map: Optional[Dict[str, Any]] = None,
    enabler_id: str = "KM",
    specific_contextual_rule: str = "N/A",
    **kwargs
) -> Dict[str, Any]:
    """
    [ULTIMATE PRODUCTION v21.9.4] Low-Level Evaluation (L1/L2)
    - ‡πÅ‡∏Å‡πâ UnboundLocalError (system_prompt)
    - ‡πÑ‡∏°‡πà Early FAIL ‡∏ñ‡πâ‡∏≤ evidence ‡πÅ‡∏£‡∏á
    - ‡∏™‡πà‡∏á Extraction Keys + final_llm_context + raw_response
    - ‡∏ó‡∏ô‡∏ó‡∏≤‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Expert Re-eval ‡πÅ‡∏•‡∏∞ Post-process ‡πÄ‡∏ï‡πá‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö
    """
    import logging

    logger = logging.getLogger(__name__)

    # =================================================================
    # 1. Context Preparation
    # =================================================================
    context_to_send_eval = _get_context_for_level(context, level) if context else ""

    # üéØ ‡πÑ‡∏°‡πà Early FAIL ‡∏≠‡∏µ‡∏Å‡∏ï‡πà‡∏≠‡πÑ‡∏õ ‚Äî ‡πÉ‡∏´‡πâ LLM ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÄ‡∏≠‡∏á‡πÅ‡∏°‡πâ context ‡∏ß‡πà‡∏≤‡∏á‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß
    # (‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á log warning ‡πÄ‡∏û‡∏∑‡πà‡∏≠ debug)
    if not context_to_send_eval.strip():
        logger.warning(
            f"Context empty for {sub_id} L{level} - continuing evaluation "
            f"(Evidence Strength: {max_evidence_strength:.1f}/10.0)"
        )

    # =================================================================
    # 2. Dynamic Plan Keywords
    # =================================================================
    plan_keywords = "‡∏ß‡∏¥‡∏™‡∏±‡∏¢‡∏ó‡∏±‡∏®‡∏ô‡πå, ‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢, ‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á, ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢, ‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô, ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå, ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå"
    if contextual_rules_map:
        sub_rules = contextual_rules_map.get(sub_id, {})
        level_rules = sub_rules.get(f"L{level}", {}) or sub_rules.get("L1", {})
        if level_rules and "plan_keywords" in level_rules:
            plan_keywords = level_rules["plan_keywords"]

    # =================================================================
    # 3. Prompt Building (‡πÅ‡∏Å‡πâ UnboundLocalError ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ñ‡∏≤‡∏ß‡∏£)
    # =================================================================
    # ‡∏ï‡∏±‡πâ‡∏á default system_prompt ‡∏Å‡πà‡∏≠‡∏ô try ‡πÄ‡∏™‡∏°‡∏≠
    system_prompt = "You are an expert SE-AM auditor. Respond only with valid JSON."

    try:
        # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÉ‡∏ä‡πâ template ‡πÄ‡∏ï‡πá‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Å‡πà‡∏≠‡∏ô
        system_prompt = SYSTEM_LOW_LEVEL_PROMPT.format(
            plan_keywords=plan_keywords,
            avoid_keywords=avoid_keywords or "‡πÑ‡∏°‡πà‡∏°‡∏µ"
        )
        system_prompt += "\n\nCRITICAL RULES:\n- ‡∏ï‡∏≠‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞ JSON ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô\n- ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô 0.0 - 2.0 ‡∏ï‡πà‡∏≠ PDCA phase"

        user_prompt = USER_LOW_LEVEL_PROMPT_TEMPLATE.format(
            sub_id=sub_id,
            sub_criteria_name=sub_criteria_name,
            level=level,
            statement_text=statement_text,
            level_constraint=level_constraint or "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏û‡∏¥‡πÄ‡∏®‡∏©",
            must_include_keywords=must_include_keywords or "‡πÑ‡∏°‡πà‡∏°‡∏µ",
            avoid_keywords=avoid_keywords or "‡πÑ‡∏°‡πà‡∏°‡∏µ",
            context=context_to_send_eval[:32000],  # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô token overflow
            max_rerank_score=f"{max_rerank_score:.4f}",
            max_evidence_strength=f"{max_evidence_strength:.1f}",
            specific_contextual_rule=specific_contextual_rule.strip() if specific_contextual_rule != "N/A" else "‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå SE-AM ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô"
        )

        # ‡∏â‡∏µ‡∏î‡∏Å‡∏é‡∏û‡∏¥‡πÄ‡∏®‡∏©‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
        if specific_contextual_rule.strip() and specific_contextual_rule != "N/A":
            user_prompt += f"\n\n=== ‡∏Å‡∏é‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏ô‡∏µ‡πâ ===\n{specific_contextual_rule}\n=== ‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏Å‡∏é‡∏û‡∏¥‡πÄ‡∏®‡∏© ==="

    except Exception as e:
        logger.error(f"Prompt formatting failed for {sub_id} L{level}: {e}")
        # system_prompt ‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡πÄ‡∏õ‡πá‡∏ô default ‡∏ó‡∏µ‡πà‡∏ï‡∏±‡πâ‡∏á‡πÑ‡∏ß‡πâ‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô
        user_prompt = (
            f"‡πÄ‡∏Å‡∏ì‡∏ë‡πå: {sub_criteria_name} (L{level})\n"
            f"‡∏£‡∏´‡∏±‡∏™: {sub_id}\n"
            f"‡∏Å‡∏é‡∏û‡∏¥‡πÄ‡∏®‡∏©: {specific_contextual_rule}\n"
            f"‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {statement_text}\n"
            f"‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô: {context_to_send_eval[:15000]}"
        )

    # =================================================================
    # 4. LLM Execution
    # =================================================================
    try:
        raw_response = _fetch_llm_response(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            max_retries=_MAX_LLM_RETRIES,
            llm_executor=llm_executor
        )

        logger.info(f"Raw LLM Response ({sub_id} L{level}): {raw_response[:500]}...")

        parsed = _robust_extract_json(raw_response)
        if not isinstance(parsed, dict):
            logger.warning(f"JSON parse failed for {sub_id} L{level} - using fallback")
            parsed = {}

        # =================================================================
        # 5. Final Output ‚Äî ‡πÄ‡∏û‡∏¥‡πà‡∏° Extraction Keys + Context ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Expert Loop
        # =================================================================
        result = {
            "score": float(parsed.get("score", 0.0)),
            "reason": parsed.get("reason", "‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå").strip(),
            "is_passed": bool(parsed.get("is_passed", False)),
            "P_Plan_Score": float(parsed.get("P_Plan_Score", 0.0)),
            "D_Do_Score": float(parsed.get("D_Do_Score", 0.0)),
            "C_Check_Score": float(parsed.get("C_Check_Score", 0.0)),
            "A_Act_Score": float(parsed.get("A_Act_Score", 0.0)),
            # üéØ ‡πÄ‡∏û‡∏¥‡πà‡∏° Extraction Keys ‚Äî ‡∏•‡∏î‡∏†‡∏≤‡∏£‡∏∞ Post-process
            "Extraction_P": parsed.get("Extraction_P", parsed.get("‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô P", parsed.get("Extraction_Plan", "-"))),
            "Extraction_D": parsed.get("Extraction_D", parsed.get("‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô D", parsed.get("Extraction_Do", "-"))),
            "Extraction_C": parsed.get("Extraction_C", parsed.get("‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô C", parsed.get("Extraction_Check", "-"))),
            "Extraction_A": parsed.get("Extraction_A", parsed.get("‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô A", parsed.get("Extraction_Act", "-"))),
            # üéØ ‡πÄ‡∏Å‡πá‡∏ö Context ‡∏î‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏¥‡∏° ‚Äî ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Expert Re-eval
            "final_llm_context": context_to_send_eval,
            "raw_llm_response": raw_response[:2000],  # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö debug
        }

        logger.info(f"Final Low-Level Result {sub_id} L{level}: Score={result['score']:.1f} | Passed={result['is_passed']}")
        return result

    except Exception as e:
        logger.exception(f"Critical failure in low_level_eval for {sub_id} L{level}: {e}")
        return {
            "score": 0.0,
            "reason": f"‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)}",
            "is_passed": False,
            "P_Plan_Score": 0.0, "D_Do_Score": 0.0,
            "C_Check_Score": 0.0, "A_Act_Score": 0.0,
            "Extraction_P": "-", "Extraction_D": "-", "Extraction_C": "-", "Extraction_A": "-",
            "final_llm_context": context_to_send_eval,
            "raw_llm_response": ""
        }
    
# ------------------------
# Summarize (FULL VERSION)
# ------------------------
def create_context_summary_llm(
    context: str, 
    sub_criteria_name: str, 
    level: int, 
    sub_id: str, 
    llm_executor: Any 
) -> Dict[str, Any]:
    logger = logging.getLogger("AssessmentApp")

    # 1. Validation ‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô
    if llm_executor is None: 
        return {"summary": "‡∏£‡∏∞‡∏ö‡∏ö LLM ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô", "suggestion_for_next_level": "‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠"}

    context_safe = (context or "").strip()
    if len(context_safe) < 50:
        return {
            "summary": "‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏Å‡∏¥‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏™‡∏£‡∏∏‡∏õ‡πÑ‡∏î‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô", 
            "suggestion_for_next_level": "‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö"
        }

    # 2. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Prompt ‡πÅ‡∏•‡∏∞ Parameter
    context_to_send = context_safe[:6000] # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Token Overflow
    next_level = min(level + 1, 5)

    try:
        human_prompt = USER_EVIDENCE_DESCRIPTION_TEMPLATE.format(
            sub_id=f"{sub_id} - {sub_criteria_name}",
            level=level,
            next_level=next_level,
            context=context_to_send
        )
    except Exception as e:
        logger.error(f"‚ùå Formatting Error: {e}")
        return {"summary": "Error formatting prompt", "suggestion_for_next_level": "N/A"}

    # ‡∏õ‡∏£‡∏±‡∏ö System Instruction ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ö‡∏ö "Zero-Tolerance"
    system_instruction = (
        f"{SYSTEM_EVIDENCE_DESCRIPTION_PROMPT}\n"
        "### IMPORTANT RULE ###\n"
        "RETURN ONLY A VALID JSON OBJECT. NO MARKDOWN. NO PREAMBLE. NO EXPLANATION.\n"
        "EXPECTED KEYS: \"summary\", \"suggestion_for_next_level\""
    )

    # 3. Execution Loop with Advanced Parsing
    max_retries = 2
    for attempt in range(1, max_retries + 1):
        try:
            logger.info(f"üîÑ Generating Summary {sub_id} L{level} (Attempt {attempt})")
            
            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å LLM (‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤ llm_executor ‡∏°‡∏µ‡πÄ‡∏°‡∏ò‡∏≠‡∏î generate)
            raw_response = llm_executor.generate(
                system=system_instruction, 
                prompts=[human_prompt]
            )

            # --- üéØ ‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ: Robust Text Extraction ---
            res_text = ""
            if hasattr(raw_response, 'generations'): 
                res_text = raw_response.generations[0][0].text
            elif hasattr(raw_response, 'content'):   
                res_text = raw_response.content
            else:
                res_text = str(raw_response)

            # --- üéØ ‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ: Cleaning & Regex Pre-processing ---
            res_text = res_text.strip()
            # ‡∏•‡∏ö Markdown Code Blocks ‡∏≠‡∏≠‡∏Å‡∏ñ‡πâ‡∏≤‡∏´‡∏•‡∏∏‡∏î‡∏°‡∏≤ (‡πÄ‡∏ä‡πà‡∏ô ```json ... ```)
            res_text = re.sub(r'```(?:json)?\n?|```', '', res_text).strip()
            
            # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏´‡∏≤ { ... } ‡∏Å‡πâ‡∏≠‡∏ô‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠
            match = re.search(r'\{.*\}', res_text, re.DOTALL)
            if match:
                json_str = match.group(0)
                try:
                    parsed = json.loads(json_str)
                except json.JSONDecodeError:
                    # ‡∏ñ‡πâ‡∏≤ JSON ‡∏û‡∏±‡∏á (‡πÄ‡∏ä‡πà‡∏ô ‡∏°‡∏µ " ‡∏ã‡πâ‡∏≠‡∏ô‡∏Å‡∏±‡∏ô) ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô Normalize ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ
                    parsed = _extract_normalized_dict(json_str)
            else:
                # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ { } ‡πÄ‡∏•‡∏¢ ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô Normalize ‡∏Å‡∏±‡∏ö Text ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
                parsed = _extract_normalized_dict(res_text)

            # 4. Final Value Validation
            if parsed and isinstance(parsed, dict):
                sum_text = parsed.get("summary") or parsed.get("‡∏™‡∏£‡∏∏‡∏õ")
                sug_text = parsed.get("suggestion_for_next_level") or parsed.get("‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥")

                if sum_text and str(sum_text).strip() != "":
                    return {
                        "summary": str(sum_text).strip(),
                        "suggestion_for_next_level": str(sug_text).strip() if sug_text else "‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ñ‡∏±‡∏î‡πÑ‡∏õ"
                    }
            
            logger.warning(f"‚ö†Ô∏è Attempt {attempt}: Invalid format, retrying with force rule...")
            human_prompt += "\n\nCRITICAL: You must return ONLY the JSON object. Do not say anything else."
            
        except Exception as e:
            logger.error(f"‚ùå Attempt {attempt} Failed: {e}")
            time.sleep(1.0) # Wait for local LLM to breathe

    # 5. Fallback - ‡∏ñ‡πâ‡∏≤‡∏û‡∏±‡∏á‡∏´‡∏°‡∏î‡∏à‡∏£‡∏¥‡∏á‡πÜ ‡πÉ‡∏´‡πâ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
    return {
        "summary": f"‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö {level} (‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏á)",
        "suggestion_for_next_level": f"‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Ç‡∏≠‡∏á Level {next_level} ‡πÉ‡∏ô‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô"
    }


def create_structured_action_plan(
    recommendation_statements: List[Dict[str, Any]],
    sub_id: str,
    sub_criteria_name: str,
    target_level: int = 5,
    llm_executor: Any = None,
    logger: logging.Logger = None,
    max_retries: int = 3,
    enabler_rules: Dict[str, Any] = {}
) -> List[Dict[str, Any]]:
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á Strategic Roadmap (Action Plan) ‡πÇ‡∏î‡∏¢‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏à‡∏≤‡∏Å Gap ‡∏ó‡∏µ‡πà‡∏û‡∏ö
    ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á 'root' key ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• 70B
    """
    if logger is None:
        logger = logging.getLogger(__name__)

    logger.info(f"üöÄ Generating Strategic Roadmap for {sub_id} (Target L{target_level})")

    # --- 1. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Mode ‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Context ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Prompt ---
    is_sustain_mode = not recommendation_statements
    is_quality_refinement = False
    
    if not is_sustain_mode:
        types = [s.get('recommendation_type') for s in recommendation_statements]
        if 'FAILED' not in types and 'GAP_ANALYSIS' not in types:
            is_quality_refinement = True

    # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Dynamic Params ‡∏ï‡∏≤‡∏°‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
    if is_sustain_mode:
        advice_focus = "‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏•‡∏¥‡∏® ‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏° ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏ô‡πÅ‡∏ö‡∏ö (Role Model)"
        dynamic_max_phases = 1
        max_steps = 5
    elif is_quality_refinement:
        advice_focus = "‡∏Å‡∏≤‡∏£‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏Ç‡πá‡∏á‡πÅ‡∏Å‡∏£‡πà‡∏á‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÄ‡∏ä‡∏¥‡∏á‡∏õ‡∏£‡∏∞‡∏à‡∏±‡∏Å‡∏©‡πå‡πÅ‡∏•‡∏∞‡∏£‡∏∞‡∏ö‡∏ö‡∏Å‡∏≤‡∏£‡∏ß‡∏±‡∏î‡∏ú‡∏• (Check)"
        dynamic_max_phases = 1
        max_steps = 3
    else:
        advice_focus = f"‡∏Å‡∏≤‡∏£‡∏õ‡∏¥‡∏î‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á (Gap Remediation) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏¢‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏™‡∏π‡πà Level {target_level}"
        dynamic_max_phases = 3 if target_level >= 4 else 2
        max_steps = 3

    # --- 2. ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ Gap (Statement Content) ---
    if is_sustain_mode:
        stmt_content = "‡∏ó‡∏∏‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡πÅ‡∏•‡πâ‡∏ß ‡πÄ‡∏ô‡πâ‡∏ô‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á"
    else:
        unique_statements = {}
        for s in recommendation_statements:
            reason = (s.get('reason') or s.get('statement') or "").strip()
            lvl = s.get('level', 0)
            if not reason: continue
            if reason not in unique_statements or lvl > unique_statements[reason]:
                unique_statements[reason] = lvl
        stmt_content = "\n".join([f"- [Level {v}] {k}" for k, v in unique_statements.items()])

    # --- 3. ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö Prompt (‡πÉ‡∏ä‡πâ Template ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏™‡πà‡∏á‡∏°‡∏≤) ---
    # ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ACTION_PLAN_PROMPT ‡∏Ñ‡∏∑‡∏≠‡∏ä‡∏∏‡∏î‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏ô‡∏¥‡∏¢‡∏≤‡∏°‡πÑ‡∏ß‡πâ
    human_prompt = ACTION_PLAN_PROMPT.format(
        sub_id=sub_id,
        sub_criteria_name=sub_criteria_name,
        target_level=target_level,
        recommendation_statements_list=stmt_content,
        advice_focus=advice_focus,
        max_phases=dynamic_max_phases,
        max_steps=max_steps,
        max_words_per_step=150,
        language="‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢"
    )

    # --- 4. Execution Loop (‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô JSON Error) ---
    for attempt in range(1, max_retries + 1):
        try:
            logger.debug(f"Attempt {attempt}/{max_retries} for {sub_id}")

            response = llm_executor.generate(
                system=SYSTEM_ACTION_PLAN_PROMPT,
                prompts=[human_prompt],
                temperature=0.0 # ‡πÉ‡∏ä‡πâ 0.0 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á JSON
            )

            # 4.1 Extract Text
            raw_text = ""
            if hasattr(response, 'generations') and response.generations:
                raw_text = response.generations[0][0].text
            elif hasattr(response, 'content'):
                raw_text = response.content
            else:
                raw_text = str(response)

            # 4.2 Extract JSON Array
            items = _extract_json_array_for_action_plan(raw_text, logger)
            if not items:
                logger.warning(f"‚ö†Ô∏è Attempt {attempt}: JSON extraction failed.")
                continue

            # 4.3 Normalize Keys (‡πÅ‡∏õ‡∏•‡∏á‡∏™‡∏≤‡∏£‡∏û‡∏±‡∏î‡∏ä‡∏∑‡πà‡∏≠ Key ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡πá‡∏Å)
            clean_items = action_plan_normalize_keys(items)

            # 4.4 [CRITICAL FIX] Flexible Validation
            try:
                # ‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô validate_flexible ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏ô ActionPlanResult
                # ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ LLM ‡∏û‡πà‡∏ô {"root": [...]} ‡∏´‡∏£‡∏∑‡∏≠ [...] ‡∏°‡∏≤‡∏™‡∏•‡∏±‡∏ö‡∏Å‡∏±‡∏ô
                validated = ActionPlanResult.validate_flexible(clean_items)
                
                logger.info(f"‚úÖ Action plan generated and validated for {sub_id}")
                
                # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô List ‡∏Ç‡∏≠‡∏á Dictionary ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Alias (Phase, Goal, Actions) 
                # ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏ü‡∏•‡πå DOCX ‡∏ô‡∏≥‡πÑ‡∏õ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡πÑ‡∏î‡πâ‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°
                return validated.model_dump(by_alias=True)
                
            except ValidationError as ve:
                logger.error(f"‚ùå Validation Error (Attempt {attempt}): {ve}")
                # ‡∏™‡πà‡∏á Error ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡πÉ‡∏ô Prompt ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ Retry (‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô)
                continue

        except Exception as e:
            logger.error(f"üí• Critical Error in Attempt {attempt}: {str(e)}")
            time.sleep(0.5 * attempt)

    # --- 5. Fallback Plan (‡∏ñ‡πâ‡∏≤‡∏û‡∏±‡∏á‡∏Ñ‡∏£‡∏ö‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á) ---
    logger.warning(f"üö® All attempts failed for {sub_id}. Returning Emergency Fallback.")
    return _get_emergency_fallback_plan(sub_id, sub_criteria_name, target_level, is_sustain_mode, is_quality_refinement)

# =================================================================
# 2. Key Normalizer: ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤ LLM ‡∏û‡πà‡∏ô Key ‡πÑ‡∏°‡πà‡∏ô‡∏¥‡πà‡∏á
# =================================================================
def action_plan_normalize_keys(obj: Any) -> Any:
    if isinstance(obj, list): return [action_plan_normalize_keys(i) for i in obj]
    if isinstance(obj, dict):
        field_mapping = {
            'phase': 'phase', 'goal': 'goal', 'actions': 'actions',
            'statementid': 'statement_id', 'statement_id': 'statement_id',
            'failedlevel': 'failed_level', 'failed_level': 'failed_level',
            'recommendation': 'recommendation',
            'targetevidencetype': 'target_evidence_type', 'target_evidence_type': 'target_evidence_type',
            'keymetric': 'key_metric', 'key_metric': 'key_metric',
            'steps': 'steps', 'step': 'step', 
            'description': 'description', 'responsible': 'responsible',
            'toolstemplates': 'tools_templates', 'tools_templates': 'tools_templates',
            'verificationoutcome': 'verification_outcome', 'verification_outcome': 'verification_outcome'
        }
        
        new_obj = {}
        for k, v in obj.items():
            # ‡∏Å‡∏ß‡∏≤‡∏î‡∏•‡πâ‡∏≤‡∏á Key ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÅ‡∏ï‡πà‡∏ï‡∏±‡∏ß‡∏û‡∏¥‡∏°‡∏û‡πå‡πÄ‡∏•‡πá‡∏Å‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö
            k_raw = str(k).lower().replace(' ', '').replace('_', '').strip()
            k_raw = re.sub(r'[^a-z0-9]', '', k_raw)
            
            target_key = field_mapping.get(k_raw) or k_raw
            
            # Enforcement: ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö Integer ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Level ‡πÅ‡∏•‡∏∞ Step
            if target_key in ['failed_level', 'step']:
                try:
                    if isinstance(v, (int, float)): v = int(v)
                    else:
                        nums = re.findall(r'\d+', str(v))
                        v = int(nums[0]) if nums else 0
                except: v = 0
            
            new_obj[target_key] = action_plan_normalize_keys(v)
        return new_obj
    return obj

# =================================================================
# 3. JSON Extractor: ‡∏£‡∏∞‡∏ö‡∏ö‡∏Å‡∏π‡πâ‡∏Ñ‡∏∑‡∏ô JSON ‡∏ó‡∏µ‡πà‡∏û‡∏±‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏à‡∏ô‡πÑ‡∏°‡πà‡∏à‡∏ö
# =================================================================
def _extract_json_array_for_action_plan(text: Any, logger: logging.Logger) -> List[Dict[str, Any]]:
    try:
        if not isinstance(text, str): text = str(text) if text is not None else ""
        if not text.strip(): return []

        # ‡∏•‡∏ö Markdown tags
        clean_text = re.sub(r'```(?:json)?\s*([\s\S]*?)\s*```', r'\1', text, flags=re.IGNORECASE).strip()

        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï JSON
        start_idx = clean_text.find('[')
        end_idx = clean_text.rfind(']')

        if start_idx == -1:
            start_idx = clean_text.find('{')
            end_idx = clean_text.rfind('}')
            if start_idx == -1: return []
            json_candidate = clean_text[start_idx:end_idx + 1]
        else:
            json_candidate = clean_text[start_idx:end_idx + 1]

        # ‡∏•‡πâ‡∏≤‡∏á Control characters
        json_candidate = "".join(char for char in json_candidate if ord(char) >= 32 or char in "\n\r\t")

        def try_parse(content):
            try:
                data = json5.loads(content)
                return data if isinstance(data, list) else [data]
            except Exception: return None

        # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏° Parse ‡πÅ‡∏•‡∏∞‡∏ã‡πà‡∏≠‡∏°‡πÅ‡∏ã‡∏°
        result = try_parse(json_candidate)
        if not result:
            repaired = json_candidate.replace('‚Äú', '"').replace('‚Äù', '"').replace("'", '"')
            result = try_parse(repaired)
        
        # ‡∏Å‡∏£‡∏ì‡∏µ LLM ‡∏ï‡∏±‡∏î‡∏à‡∏ö (Truncated) ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏õ‡∏¥‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡πÉ‡∏´‡πâ
        if not result:
            for suffix in ["]", "}", "}]", "}\n]"]:
                result = try_parse(json_candidate + suffix)
                if result: break

        return result or []
    except Exception as e:
        logger.error(f"Extraction failed: {str(e)}")
        return []

# =================================================================
# 4. Emergency Fallback (Revised for All Modes)
# =================================================================
def _get_emergency_fallback_plan(sub_id, sub_criteria_name, target_level, is_sustain_mode, is_quality_refinement):
    if is_sustain_mode:
        title = "Continuous Excellence Plan"
        rec = "‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡πÅ‡∏•‡∏∞‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏ö‡πà‡∏á‡∏õ‡∏±‡∏ô‡∏≠‡∏á‡∏Ñ‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏™‡∏π‡πà‡∏†‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å (Best Practice Sharing)"
    elif is_quality_refinement:
        title = "Quality Evidence Reinforcement"
        rec = "‡∏à‡∏±‡∏î‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£ (Check) ‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏¢‡∏¥‡πà‡∏á‡∏Ç‡∏∂‡πâ‡∏ô"
    else:
        title = "Gap Remediation Roadmap"
        rec = f"‡πÄ‡∏£‡πà‡∏á‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏£‡∏∞‡∏î‡∏±‡∏ö {target_level} ‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏ó‡∏≥‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏ß‡∏á‡∏à‡∏£ PDCA ‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå"
        
    return [{
        "phase": f"Phase: {title}",
        "goal": f"‡∏¢‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô {sub_criteria_name}",
        "actions": [{
            "statement_id": sub_id, 
            "failed_level": target_level,
            "recommendation": rec, 
            "target_evidence_type": "Evidence Pack / KM Dashboard",
            "key_metric": "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô 100%",
            "steps": [
                {
                    "Step": 1, 
                    "Description": "‡∏ó‡∏ö‡∏ó‡∏ß‡∏ô‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏ó‡∏≥‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô", 
                    "Responsible": "KM Working Team", 
                    "Tools_Templates": "Gap Analysis Template", 
                    "Verification_Outcome": "‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏£‡∏∏‡∏õ‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç"
                }
            ]
        }]
    }]
