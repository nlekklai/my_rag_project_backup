# -*- coding: utf-8 -*-
# core/start_assessment.py - Optimized for Mac & GPU Server

import os
import sys
import logging
import argparse
import time
import uuid
import multiprocessing
from typing import Optional, Dict, Any

# -------------------- PATH SETUP --------------------
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
if project_root not in sys.path:
    sys.path.append(project_root)

# -------------------- IMPORT CORE --------------------
from models.llm import create_llm_instance
from database import init_db, db_create_task, db_finish_task, db_update_task_status 
from config.global_vars import (
    EVIDENCE_DOC_TYPES, DEFAULT_ENABLER, DEFAULT_LLM_MODEL_NAME, 
    DEFAULT_TENANT, DEFAULT_YEAR
) 

try:
    from core.seam_assessment import SEAMPDCAEngine, AssessmentConfig 
    from core.vectorstore import load_all_vectorstores
    
    # üéØ Robust Document Map Loader
    try:
        from core.vectorstore import load_document_map
    except ImportError:
        try:
            from core.vectorstore import load_doc_id_mapping as load_document_map
            print("üí° Note: Using 'load_doc_id_mapping' as 'load_document_map'")
        except ImportError:
            def load_document_map(*args, **kwargs): return {}
            print("‚ö†Ô∏è Warning: No document mapping function found.")

except Exception as e:
    print(f"‚ùå FATAL: Missing critical modules: {e}", file=sys.stderr)
    raise

# -------------------- LOGGING SETUP --------------------
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# -------------------- ARGUMENT PARSING --------------------
def parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(description="SEAM PDCA Assessment Runner (Production Mode)")
    p.add_argument("--sub", type=str, default="all", help="Sub-Criteria ID (e.g., 1.1)")
    p.add_argument("--enabler", type=str, default=DEFAULT_ENABLER, help="Enabler (KM/IT/...)")
    p.add_argument("--target_level", type=int, default=5, help="Max target maturity level")
    p.add_argument("--export", action="store_true", help="Save results to JSON")
    p.add_argument("--mock", choices=["none", "random", "control"], default="none", help="Mock mode")
    p.add_argument("--sequential", action="store_true", help="Force sequential execution")
    p.add_argument("--tenant", type=str, default=DEFAULT_TENANT, help="Tenant ID")
    p.add_argument("--year", type=int, default=DEFAULT_YEAR, help="Year")
    p.add_argument("--min-retry-score", type=float, default=0.65)
    p.add_argument("--max-retrieval-attempts", type=int, default=3)
    p.add_argument("--record-id", type=str, default=None, help="Custom record ID")
    return p.parse_args()

# -------------------- MAIN EXECUTION --------------------
def main():
    args = parse_args()
    
    # 1. Initialize Record ID
    record_id = args.record_id if args.record_id else uuid.uuid4().hex[:12]
    run_mode = "Sequential" if args.sequential else "Parallel"
    logger.info(f"üöÄ Runner Started | ID: {record_id} | Mode: {run_mode}")
    start_ts = time.time()

    # 2. Database Initialization
    try:
        init_db()
        db_create_task(
            record_id=record_id, tenant=args.tenant, year=str(args.year),
            enabler=args.enabler, sub_criteria=args.sub, user_id="CLI_SYSTEM"
        )
        logger.info(f"‚úÖ Database Task Registered: {record_id}")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è DB Registration Warning: {e}")

    # 3. Resource Loading (Vector Store & Document Map)
    vsm = None
    if args.mock == "none":
        try:
            vsm = load_all_vectorstores(
                tenant=args.tenant, year=str(args.year),
                doc_types=EVIDENCE_DOC_TYPES, enabler_filter=args.enabler
            )
        except Exception as e:
            logger.error(f"‚ùå VSM Load failed: {e}")
            sys.exit(1)

    document_map = {}
    try:
        raw_map = load_document_map(EVIDENCE_DOC_TYPES, args.tenant, str(args.year), args.enabler)
        if isinstance(raw_map, dict) and raw_map:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô dict ‡∏ã‡πâ‡∏≠‡∏ô dict ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            sample_val = next(iter(raw_map.values()))
            if isinstance(sample_val, dict):
                document_map = {k: v.get("file_name", k) for k, v in raw_map.items()}
            else:
                document_map = raw_map
        logger.info(f"üéØ Loaded {len(document_map)} document mappings.")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Document map warning: {e}")

    # Initialize LLM
    llm = create_llm_instance(model_name=DEFAULT_LLM_MODEL_NAME, temperature=0.0)

    # 4. Engine Configuration
    config = AssessmentConfig(
        enabler=args.enabler, tenant=args.tenant, year=args.year,
        target_level=args.target_level, mock_mode=args.mock,
        force_sequential=args.sequential, model_name=DEFAULT_LLM_MODEL_NAME,
        min_retry_score=args.min_retry_score, max_retrieval_attempts=args.max_retrieval_attempts
    )
    
    engine = SEAMPDCAEngine(
        config=config, llm_instance=llm, logger_instance=logger,             
        doc_type=EVIDENCE_DOC_TYPES, vectorstore_manager=vsm, 
        document_map=document_map, record_id=record_id
    )

    # 5. Run Assessment
    final_results = None
    try:
        final_results = engine.run_assessment(
            target_sub_id=args.sub, export=args.export, 
            vectorstore_manager=vsm, sequential=args.sequential,
            document_map=document_map, record_id=record_id
        )

        # Persistence: ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏á DB (‡πÄ‡∏û‡∏¥‡πà‡∏° Safe Check)
        if final_results is not None:
            db_finish_task(record_id, final_results)
            logger.info(f"üíæ Results saved to database: {record_id}")
        else:
            logger.error("‚ùå run_assessment returned None")

    except Exception as e:
        logger.exception(f"‚ùå Engine execution failed: {e}")
        db_update_task_status(record_id, 0, f"Error: {str(e)}", status="FAILED")
        sys.exit(1)

    # 6. Final Summary Extraction (Ironclad Logic)
    duration_s = time.time() - start_ts
    
    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô AttributeError
    summary_display = {
        "level": "L0",
        "score": 0.0,
        "path": "N/A"
    }

    if isinstance(final_results, dict):
        summary = final_results.get("summary", {})
        summary_display["level"] = summary.get("overall_level_label", "L0")
        summary_display["score"] = summary.get("overall_avg_score", 0.0)
        summary_display["path"] = final_results.get("export_path_used", "N/A")

        # [Safe Guard] ‡∏Å‡∏£‡∏ì‡∏µ‡∏£‡∏±‡∏ô‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡πÅ‡∏ï‡πà‡∏ú‡∏•‡∏£‡∏ß‡∏° (summary) ‡∏û‡∏±‡∏á ‡πÉ‡∏´‡πâ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å subcriteria_results
        if summary_display["score"] == 0:
            results = final_results.get("subcriteria_results", [])
            if results and isinstance(results[0], dict):
                summary_display["score"] = results[0].get("weighted_score", 0.0)

    # üèÅ Display Summary UI
    print("\n" + "‚ïê"*65)
    print(f" üèÅ  ASSESSMENT COMPLETE | ID: {record_id}")
    print("‚ïê"*65)
    print(f" [MODE]        : {run_mode}")
    print(f" [RESULT]      : Level {summary_display['level']}")
    print(f" [SCORE]       : {summary_display['score']:.2f} / 5.00")
    print(f" [DURATION]    : {duration_s:.2f} seconds")
    print("-" * 65)
    if args.export:
        print(f" üíæ Exported to: {summary_display['path']}")
    print("‚ïê"*65 + "\n")

if __name__ == "__main__":
    # ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô‡∏ö‡∏ô Mac (ARM) ‡πÅ‡∏•‡∏∞‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ï‡∏≠‡∏ô‡∏ó‡∏≥ Multiprocessing
    multiprocessing.freeze_support()
    main()