# -*- coding: utf-8 -*-
# core/start_assessment.py - Optimized for Mac & GPU Server

import os
import sys
import logging
import argparse
import time
import uuid
import multiprocessing
from typing import Optional, Dict, Any

# -------------------- PATH SETUP --------------------
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
if project_root not in sys.path:
    sys.path.append(project_root)

# -------------------- IMPORT CORE --------------------
from models.llm import create_llm_instance
from database import init_db, db_create_task, db_finish_task, db_update_task_status 
from config.global_vars import (
    EVIDENCE_DOC_TYPES, DEFAULT_ENABLER, DEFAULT_LLM_MODEL_NAME, 
    DEFAULT_TENANT, DEFAULT_YEAR
) 

# -------------------- LOGGING SETUP --------------------
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# -------------------- IMPORT CORE --------------------
from models.llm import create_llm_instance
from database import init_db, db_create_task, db_finish_task, db_update_task_status 

# ‚úÖ ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÇ‡∏´‡∏•‡∏î mapping ‡∏à‡∏≤‡∏Å path_utils ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
try:
    from utils.path_utils import load_doc_id_mapping as load_document_map
    logger.info("‚úÖ Successfully linked 'load_doc_id_mapping' from path_utils")
except ImportError:
    # Fallback ‡∏Å‡∏£‡∏ì‡∏µ‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡∏à‡∏£‡∏¥‡∏á‡πÜ (‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡∏ñ‡πâ‡∏≤ path_utils ‡∏≠‡∏¢‡∏π‡πà‡∏ñ‡∏π‡∏Å‡∏ó‡∏µ‡πà)
    def load_document_map(*args, **kwargs): return {}
    print("‚ö†Ô∏è Warning: No document mapping function found in utils.path_utils")

try:
    from core.seam_assessment import SEAMPDCAEngine, AssessmentConfig 
    from core.vectorstore import load_all_vectorstores
    # ‡∏•‡∏ö‡∏™‡πà‡∏ß‡∏ô Import load_document_map ‡πÄ‡∏Å‡πà‡∏≤‡πÜ ‡∏ó‡∏¥‡πâ‡∏á‡πÑ‡∏õ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢
except Exception as e:
    print(f"‚ùå FATAL: Missing critical modules: {e}", file=sys.stderr)
    raise

# -------------------- ARGUMENT PARSING --------------------
def parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(description="SEAM PDCA Assessment Runner (Production Mode)")
    p.add_argument("--sub", type=str, default="all", help="Sub-Criteria ID (e.g., 1.1)")
    p.add_argument("--enabler", type=str, default=DEFAULT_ENABLER, help="Enabler (KM/IT/...)")
    p.add_argument("--target_level", type=int, default=5, help="Max target maturity level")
    p.add_argument("--export", action="store_true", help="Save results to JSON")
    p.add_argument("--mock", choices=["none", "random", "control"], default="none", help="Mock mode")
    p.add_argument("--sequential", action="store_true", help="Force sequential execution")
    p.add_argument("--tenant", type=str, default=DEFAULT_TENANT, help="Tenant ID")
    p.add_argument("--year", type=int, default=DEFAULT_YEAR, help="Year")
    p.add_argument("--min-retry-score", type=float, default=0.65)
    p.add_argument("--max-retrieval-attempts", type=int, default=3)
    p.add_argument("--record-id", type=str, default=None, help="Custom record ID")
    return p.parse_args()

# -------------------- MAIN EXECUTION --------------------
def main():
    args = parse_args()
    
    # 1. Initialize Record ID
    record_id = args.record_id if args.record_id else uuid.uuid4().hex[:12]
    run_mode = "Sequential" if args.sequential else "Parallel"
    logger.info(f"üöÄ Runner Started | ID: {record_id} | Mode: {run_mode}")
    start_ts = time.time()

    # 2. Database Initialization
    try:
        init_db()
        db_create_task(
            record_id=record_id, tenant=args.tenant, year=str(args.year),
            enabler=args.enabler, sub_criteria=args.sub, user_id="CLI_SYSTEM"
        )
        logger.info(f"‚úÖ Database Task Registered: {record_id}")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è DB Registration Warning: {e}")

    # 3. Resource Loading (Vector Store & Document Map)
    vsm = None
    if args.mock == "none":
        try:
            vsm = load_all_vectorstores(
                tenant=args.tenant, year=str(args.year),
                doc_types=EVIDENCE_DOC_TYPES, enabler_filter=args.enabler
            )
        except Exception as e:
            logger.error(f"‚ùå VSM Load failed: {e}")
            sys.exit(1)

    document_map = {}
    try:
        raw_map = load_document_map(EVIDENCE_DOC_TYPES, args.tenant, str(args.year), args.enabler)
        if isinstance(raw_map, dict) and raw_map:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô dict ‡∏ã‡πâ‡∏≠‡∏ô dict ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            sample_val = next(iter(raw_map.values()))
            if isinstance(sample_val, dict):
                document_map = {k: v.get("file_name", k) for k, v in raw_map.items()}
            else:
                document_map = raw_map
        logger.info(f"üéØ Loaded {len(document_map)} document mappings.")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Document map warning: {e}")

    # Initialize LLM
    llm = create_llm_instance(model_name=DEFAULT_LLM_MODEL_NAME, temperature=0.0)

    # 4. Engine Configuration
    config = AssessmentConfig(
        enabler=args.enabler, tenant=args.tenant, year=args.year,
        target_level=args.target_level, mock_mode=args.mock,
        force_sequential=args.sequential, model_name=DEFAULT_LLM_MODEL_NAME,
        min_retry_score=args.min_retry_score, max_retrieval_attempts=args.max_retrieval_attempts
    )
    
    engine = SEAMPDCAEngine(
        config=config, llm_instance=llm, logger_instance=logger,             
        doc_type=EVIDENCE_DOC_TYPES, vectorstore_manager=vsm, 
        document_map=document_map, record_id=record_id
    )

    # 5. Run Assessment
    final_results = None
    try:
        final_results = engine.run_assessment(
            target_sub_id=args.sub, export=args.export, 
            vectorstore_manager=vsm, sequential=args.sequential,
            document_map=document_map, record_id=record_id
        )

        # Persistence: ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏á DB (‡πÄ‡∏û‡∏¥‡πà‡∏° Safe Check)
        if final_results is not None:
            db_finish_task(record_id, final_results)
            logger.info(f"üíæ Results saved to database: {record_id}")
        else:
            logger.error("‚ùå run_assessment returned None")

    except Exception as e:
        logger.exception(f"‚ùå Engine execution failed: {e}")
        db_update_task_status(record_id, 0, f"Error: {str(e)}", status="FAILED")
        sys.exit(1)

    # ---------------------------------------------------------------
    # 6. Final Summary Extraction (CLI Display Logic) - REVISED v2026
    # ---------------------------------------------------------------
    duration_s = time.time() - start_ts
    
    # ‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Error
    summary_display = {
        "level": "L0",
        "score": 0.0,
        "path": "N/A"
    }

    if isinstance(final_results, dict):
        # üõ°Ô∏è Step 1: Unwrap ‡∏ä‡∏±‡πâ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö Payload)
        data = final_results.get("result") or final_results.get("assessment_result") or final_results
        res_summary = data.get("result_summary") or data.get("summary") or {}
        sub_details = data.get("sub_criteria_results") or data.get("sub_criteria_details") or []
        
        # üõ°Ô∏è Step 2: Extract Level (Maturity)
        # ‡∏•‡∏≠‡∏á‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å Summary ‡∏£‡∏ß‡∏°‡∏Å‡πà‡∏≠‡∏ô ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ (‡∏Å‡∏£‡∏ì‡∏µ‡∏£‡∏±‡∏ô --sub) ‡πÉ‡∏´‡πâ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô
        lvl = res_summary.get("maturity_level")
        if lvl is None:
            lvl = data.get("highest_full_level")
        if lvl is None and sub_details:
            lvl = sub_details[0].get("highest_full_level") or sub_details[0].get("maturity_level")
        
        # ‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏ß‡πà‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç (Default 0)
        try:
            lvl_int = int(lvl) if lvl is not None else 0
            summary_display["level"] = f"L{lvl_int}"
        except:
            summary_display["level"] = str(lvl or "L0")

        # üõ°Ô∏è Step 3: Extract Score (Weighted Score)
        # ‡∏ï‡∏£‡∏£‡∏Å‡∏∞: ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°‡πÄ‡∏õ‡πá‡∏ô 0 (‡∏≠‡∏≤‡∏à‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏£‡∏±‡∏ô‡∏Ç‡πâ‡∏≠‡πÄ‡∏î‡∏µ‡∏¢‡∏ß) ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ç‡∏≠‡∏á Sub-criteria ‡∏ô‡∏±‡πâ‡∏ô‡πÜ ‡πÅ‡∏ó‡∏ô
        score = res_summary.get("total_weighted_score") or data.get("weighted_score")
        
        if not score or float(score) == 0:
            if sub_details:
                # ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠‡πÉ‡∏ô List (‡∏Å‡∏£‡∏ì‡∏µ‡∏£‡∏∞‡∏ö‡∏∏ --sub)
                score = sub_details[0].get("weighted_score") or sub_details[0].get("score")

        summary_display["score"] = float(score or 0.0)
        summary_display["path"] = data.get("export_path") or data.get("export_path_used") or "N/A"

    # ---------------------------------------------------------------
    # üèÅ Display Summary UI (Console Output)
    # ---------------------------------------------------------------
    print("\n" + "‚ïê"*65)
    print(f" üèÅ  ASSESSMENT COMPLETE | ID: {record_id}")
    print("‚ïê"*65)
    print(f" [MODE]        : {run_mode}")
    print(f" [RESULT]      : {summary_display['level']}") # ‡∏à‡∏∞‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏õ‡πá‡∏ô L5 ‡πÅ‡∏ó‡∏ô L0
    
    # ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏∑‡∏≠ 20.00 ‡πÅ‡∏•‡∏∞ Weight ‡∏Ñ‡∏∑‡∏≠ 4.0 ‡∏à‡∏∞‡∏´‡∏°‡∏≤‡∏¢‡∏ñ‡∏∂‡∏á‡πÑ‡∏î‡πâ Level 5 ‡πÄ‡∏ï‡πá‡∏°‡πÉ‡∏ô‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏±‡πâ‡∏ô
    print(f" [SCORE]       : {summary_display['score']:.2f}") 
    print(f" [DURATION]    : {duration_s:.2f} seconds")
    print("-" * 65)
    if args.export:
        print(f" üíæ Exported to: {summary_display['path']}")
    print("‚ïê"*65 + "\n")

if __name__ == "__main__":
    # ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô‡∏ö‡∏ô Mac (ARM) ‡πÅ‡∏•‡∏∞‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ï‡∏≠‡∏ô‡∏ó‡∏≥ Multiprocessing
    multiprocessing.freeze_support()
    main()