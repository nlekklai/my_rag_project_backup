# -*- coding: utf-8 -*-
# core/start_assessment.py - Optimized for Mac & GPU Server

import os
import sys
import logging
import argparse
import time
import uuid
import multiprocessing
from typing import Optional, Dict, Any

# -------------------- PATH SETUP --------------------
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
if project_root not in sys.path:
    sys.path.append(project_root)

# -------------------- IMPORT CORE --------------------
from models.llm import create_llm_instance
from database import init_db, db_create_task, db_finish_task, db_update_task_status 
from config.global_vars import (
    EVIDENCE_DOC_TYPES, DEFAULT_ENABLER, DEFAULT_LLM_MODEL_NAME, 
    DEFAULT_TENANT, DEFAULT_YEAR
) 

# -------------------- LOGGING SETUP --------------------
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# -------------------- IMPORT CORE --------------------
from models.llm import create_llm_instance
from database import init_db, db_create_task, db_finish_task, db_update_task_status 

# âœ… à¸™à¸³à¹€à¸‚à¹‰à¸²à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¹‚à¸«à¸¥à¸” mapping à¸ˆà¸²à¸ path_utils à¹‚à¸”à¸¢à¸•à¸£à¸‡
try:
    from utils.path_utils import load_doc_id_mapping as load_document_map
    logger.info("âœ… Successfully linked 'load_doc_id_mapping' from path_utils")
except ImportError:
    # Fallback à¸à¸£à¸“à¸µà¸«à¸²à¹„à¸¡à¹ˆà¹€à¸ˆà¸­à¸ˆà¸£à¸´à¸‡à¹† (à¹„à¸¡à¹ˆà¸„à¸§à¸£à¹€à¸à¸´à¸”à¸‚à¸¶à¹‰à¸™à¸–à¹‰à¸² path_utils à¸­à¸¢à¸¹à¹ˆà¸–à¸¹à¸à¸—à¸µà¹ˆ)
    def load_document_map(*args, **kwargs): return {}
    print("âš ï¸ Warning: No document mapping function found in utils.path_utils")

try:
    from core.seam_assessment import SEAMPDCAEngine, AssessmentConfig 
    from core.vectorstore import load_all_vectorstores
    # à¸¥à¸šà¸ªà¹ˆà¸§à¸™ Import load_document_map à¹€à¸à¹ˆà¸²à¹† à¸—à¸´à¹‰à¸‡à¹„à¸›à¹„à¸”à¹‰à¹€à¸¥à¸¢
except Exception as e:
    print(f"âŒ FATAL: Missing critical modules: {e}", file=sys.stderr)
    raise

# -------------------- ARGUMENT PARSING --------------------
def parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(description="SEAM PDCA Assessment Runner (Production Mode)")
    p.add_argument("--sub", type=str, default="all", help="Sub-Criteria ID (e.g., 1.1)")
    p.add_argument("--enabler", type=str, default=DEFAULT_ENABLER, help="Enabler (KM/IT/...)")
    p.add_argument("--target_level", type=int, default=5, help="Max target maturity level")
    p.add_argument("--export", action="store_true", help="Save results to JSON")
    p.add_argument("--mock", choices=["none", "random", "control"], default="none", help="Mock mode")
    p.add_argument("--sequential", action="store_true", help="Force sequential execution")
    p.add_argument("--tenant", type=str, default=DEFAULT_TENANT, help="Tenant ID")
    p.add_argument("--year", type=int, default=DEFAULT_YEAR, help="Year")
    p.add_argument("--min-retry-score", type=float, default=0.65)
    p.add_argument("--max-retrieval-attempts", type=int, default=3)
    p.add_argument("--record-id", type=str, default=None, help="Custom record ID")
    return p.parse_args()

# -------------------- MAIN EXECUTION --------------------
def main():
    args = parse_args()
    
    # 1. Initialize Record ID
    record_id = args.record_id if args.record_id else uuid.uuid4().hex[:12]
    run_mode = "Sequential" if args.sequential else "Parallel"
    logger.info(f"ğŸš€ Runner Started | ID: {record_id} | Mode: {run_mode}")
    start_ts = time.time()

    # 2. Database Initialization
    try:
        init_db()
        db_create_task(
            record_id=record_id, tenant=args.tenant, year=str(args.year),
            enabler=args.enabler, sub_criteria=args.sub, user_id="CLI_SYSTEM"
        )
        logger.info(f"âœ… Database Task Registered: {record_id}")
    except Exception as e:
        logger.warning(f"âš ï¸ DB Registration Warning: {e}")

    # 3. Resource Loading (Vector Store & Document Map)
    vsm = None
    if args.mock == "none":
        try:
            vsm = load_all_vectorstores(
                tenant=args.tenant, year=str(args.year),
                doc_types=EVIDENCE_DOC_TYPES, enabler_filter=args.enabler
            )
        except Exception as e:
            logger.error(f"âŒ VSM Load failed: {e}")
            sys.exit(1)

    document_map = {}
    try:
        raw_map = load_document_map(EVIDENCE_DOC_TYPES, args.tenant, str(args.year), args.enabler)
        if isinstance(raw_map, dict) and raw_map:
            # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸§à¹ˆà¸²à¹€à¸›à¹‡à¸™ dict à¸‹à¹‰à¸­à¸™ dict à¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆ
            sample_val = next(iter(raw_map.values()))
            if isinstance(sample_val, dict):
                document_map = {k: v.get("file_name", k) for k, v in raw_map.items()}
            else:
                document_map = raw_map
        logger.info(f"ğŸ¯ Loaded {len(document_map)} document mappings.")
    except Exception as e:
        logger.warning(f"âš ï¸ Document map warning: {e}")

    # Initialize LLM
    llm = create_llm_instance(model_name=DEFAULT_LLM_MODEL_NAME, temperature=0.0)

    # 4. Engine Configuration
    config = AssessmentConfig(
        enabler=args.enabler, tenant=args.tenant, year=args.year,
        target_level=args.target_level, mock_mode=args.mock,
        force_sequential=args.sequential, model_name=DEFAULT_LLM_MODEL_NAME,
        min_retry_score=args.min_retry_score, max_retrieval_attempts=args.max_retrieval_attempts
    )
    
    engine = SEAMPDCAEngine(
        config=config, llm_instance=llm, logger_instance=logger,             
        doc_type=EVIDENCE_DOC_TYPES, vectorstore_manager=vsm, 
        document_map=document_map, record_id=record_id
    )

    # 5. Run Assessment
    final_results = None
    try:
        final_results = engine.run_assessment(
            target_sub_id=args.sub, export=args.export, 
            vectorstore_manager=vsm, sequential=args.sequential,
            document_map=document_map, record_id=record_id
        )

        # Persistence: à¸šà¸±à¸™à¸—à¸¶à¸à¸œà¸¥à¸¥à¸‡ DB (à¹€à¸à¸´à¹ˆà¸¡ Safe Check)
        if final_results is not None:
            db_finish_task(record_id, final_results)
            logger.info(f"ğŸ’¾ Results saved to database: {record_id}")
        else:
            logger.error("âŒ run_assessment returned None")

    except Exception as e:
        logger.exception(f"âŒ Engine execution failed: {e}")
        db_update_task_status(record_id, 0, f"Error: {str(e)}", status="FAILED")
        sys.exit(1)

    # 6. Final Summary Extraction (Production-Ready Logic)
    duration_s = time.time() - start_ts
    
    summary_display = {
        "level": "L0",
        "score": 0.0,
        "path": "N/A"
    }

    if isinstance(final_results, dict):
        # 1. à¸¥à¸­à¸‡à¸”à¸¶à¸‡à¸ˆà¸²à¸à¸•à¸±à¸§à¸£à¸§à¸¡à¸£à¸°à¸”à¸±à¸šà¸šà¸™à¸ªà¸¸à¸”à¸à¹ˆà¸­à¸™
        res_summary = final_results.get("result_summary", {}) 
        summary_display["level"] = res_summary.get("maturity_level", "L0")
        summary_display["score"] = res_summary.get("total_weighted_score", 0.0)
        summary_display["path"] = final_results.get("export_path_used", "N/A")

        # 2. [Safe Guard] à¸«à¸²à¸à¸”à¹‰à¸²à¸™à¸šà¸™à¹€à¸›à¹‡à¸™ 0 (à¸­à¸²à¸ˆà¹€à¸à¸£à¸²à¸° Bug à¹ƒà¸™ Aggregator) à¹ƒà¸«à¹‰ Loop à¸«à¸²à¸ˆà¸²à¸à¸£à¸²à¸¢à¸¥à¸°à¹€à¸­à¸µà¸¢à¸”
        if summary_display["score"] == 0:
            details = final_results.get("sub_criteria_details", [])
            for d in details:
                sub_results = d.get("sub_criteria_results", [])
                if sub_results:
                    # à¸„à¸³à¸™à¸§à¸“à¸«à¸²à¸„à¸°à¹à¸™à¸™à¸£à¸§à¸¡à¸ˆà¸²à¸ weight à¸‚à¸­à¸‡à¸—à¸¸à¸ level à¸—à¸µà¹ˆà¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¸œà¹ˆà¸²à¸™
                    total_score = sum(
                        item.get("weight", 0) 
                        for item in sub_results 
                        if item.get("level_details")
                    )
                    if total_score > 0:
                        summary_display["score"] = total_score
                        # à¸–à¹‰à¸²à¹€à¸ˆà¸­à¸•à¸±à¸§à¸—à¸µà¹ˆà¸¡à¸µà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹à¸¥à¹‰à¸§ à¹ƒà¸«à¹‰à¸«à¸¢à¸¸à¸”à¸«à¸²à¸—à¸±à¸™à¸—à¸µ
                        break

    # ğŸ Display Summary UI
    print("\n" + "â•"*65)
    print(f" ğŸ  ASSESSMENT COMPLETE | ID: {record_id}")
    print("â•"*65)
    print(f" [MODE]        : {run_mode}")
    print(f" [RESULT]      : Level {summary_display['level']}")
    print(f" [SCORE]       : {summary_display['score']:.2f} / 5.00")
    print(f" [DURATION]    : {duration_s:.2f} seconds")
    print("-" * 65)
    if args.export:
        print(f" ğŸ’¾ Exported to: {summary_display['path']}")
    print("â•"*65 + "\n")

if __name__ == "__main__":
    # à¸ªà¸³à¸„à¸±à¸à¸¡à¸²à¸à¸ªà¸³à¸«à¸£à¸±à¸šà¸à¸²à¸£à¸£à¸±à¸™à¸šà¸™ Mac (ARM) à¹à¸¥à¸°à¸›à¹‰à¸­à¸‡à¸à¸±à¸™à¸›à¸±à¸à¸«à¸²à¸•à¸­à¸™à¸—à¸³ Multiprocessing
    multiprocessing.freeze_support()
    main()