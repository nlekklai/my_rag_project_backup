# -*- coding: utf-8 -*-
# core/start_assessment.py

import os
import sys
import logging
import argparse
import time
import uuid
import multiprocessing
from typing import Optional, Dict, Any
from copy import deepcopy

# -------------------- PATH SETUP --------------------
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
if project_root not in sys.path:
    sys.path.append(project_root)

# -------------------- IMPORT CORE --------------------
from models.llm import create_llm_instance
# ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Å‡∏≤‡∏£ import ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡πÉ‡∏ô core/database.py
from database import init_db, db_create_task, db_finish_task, db_update_task_status 
from config.global_vars import (
    EVIDENCE_DOC_TYPES, DEFAULT_ENABLER, DEFAULT_LLM_MODEL_NAME, 
    DEFAULT_TENANT, DEFAULT_YEAR
) 

try:
    from core.seam_assessment import SEAMPDCAEngine, AssessmentConfig 
    # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤ ImportError ‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£ Map ‡∏ä‡∏∑‡πà‡∏≠‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÉ‡∏ô core/vectorstore.py
    from core.vectorstore import load_all_vectorstores, VectorStoreManager
    
    try:
        # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏° import ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ load_doc_id_mapping ‡πÅ‡∏ó‡∏ô
        from core.vectorstore import load_document_map
    except ImportError:
        try:
            from core.vectorstore import load_doc_id_mapping as load_document_map
            print("üí° Note: Using 'load_doc_id_mapping' as 'load_document_map'")
        except ImportError:
            def load_document_map(*args, **kwargs): return {}
            print("‚ö†Ô∏è Warning: No document mapping function found, using empty dict.")

except Exception as e:
    print(f"‚ùå FATAL: Missing critical modules: {e}", file=sys.stderr)
    raise

# -------------------- LOGGING SETUP --------------------
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# -------------------- ARGUMENT PARSING --------------------
def parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(description="SEAM PDCA Assessment Runner (CLI Mode)")
    p.add_argument("--sub", type=str, default="all", help="Sub-Criteria ID (e.g., 1.1)")
    p.add_argument("--enabler", type=str, default=DEFAULT_ENABLER, help="Enabler (KM/IT/...)")
    p.add_argument("--target_level", type=int, default=5, help="Max target maturity level")
    p.add_argument("--export", action="store_true", help="Save results to JSON")
    p.add_argument("--mock", choices=["none", "random", "control"], default="none", help="Mock mode")
    p.add_argument("--sequential", action="store_true", help="Force sequential execution")
    p.add_argument("--tenant", type=str, default=DEFAULT_TENANT, help="Tenant ID (e.g., 'pea')")
    p.add_argument("--year", type=int, default=DEFAULT_YEAR, help="Year (e.g., 2567)")
    p.add_argument("--min-retry-score", type=float, default=0.65)
    p.add_argument("--max-retrieval-attempts", type=int, default=3)
    p.add_argument("--record-id", type=str, default=None, help="Custom record ID")
    return p.parse_args()

# -------------------- MAIN EXECUTION --------------------
def main():
    args = parse_args()
    
    # 1. Initialize Record ID
    record_id = args.record_id if args.record_id else uuid.uuid4().hex[:12]
    run_mode = "Sequential" if args.sequential else "Parallel"
    logger.info(f"üöÄ Runner Started | ID: {record_id} | Mode: {run_mode}")
    start_ts = time.time()

    # 2. Database Task Pre-registration
    try:
        init_db()
        db_create_task(
            record_id=record_id,
            tenant=args.tenant,
            year=str(args.year),
            enabler=args.enabler,
            sub_criteria=args.sub,
            user_id="CLI_SYSTEM"
        )
        logger.info(f"‚úÖ Database Task Registered: {record_id}")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è DB Registration Warning: {e}")

    # 3. Resource Loading
    vsm = None
    if not (args.sequential and args.mock == "none"):
        try:
            vsm = load_all_vectorstores(
                tenant=args.tenant,
                year=str(args.year),
                doc_ids=None,
                doc_types=EVIDENCE_DOC_TYPES, 
                enabler_filter=args.enabler
            )
        except Exception as e:
            logger.error(f"VSM Load failed: {e}")
            if args.mock == "none": raise

    # Load Document Map
    document_map = {}
    try:
        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ú‡πà‡∏≤‡∏ô alias ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ó‡∏≥‡πÑ‡∏ß‡πâ‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô
        document_map = load_document_map(EVIDENCE_DOC_TYPES, args.tenant, str(args.year), args.enabler)
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏î‡πâ‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô dict ‡∏Ç‡∏≠‡∏á dict ‡πÉ‡∏´‡πâ‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ file_name
        if document_map and isinstance(next(iter(document_map.values())), dict):
            document_map = {k: v.get("file_name", k) for k, v in document_map.items()}
        logger.info(f"üéØ Loaded {len(document_map)} document mappings.")
    except Exception as e:
        logger.warning(f"Document map warning: {e}")

    # Initialize LLM
    llm = create_llm_instance(model_name=DEFAULT_LLM_MODEL_NAME, temperature=0.0)

    # 4. Engine Configuration
    config = AssessmentConfig(
        enabler=args.enabler, 
        tenant=args.tenant,
        year=args.year,
        target_level=args.target_level,
        mock_mode=args.mock,
        force_sequential=args.sequential,
        model_name=DEFAULT_LLM_MODEL_NAME,
        min_retry_score=args.min_retry_score,
        max_retrieval_attempts=args.max_retrieval_attempts
    )
    
    engine = SEAMPDCAEngine(
        config=config,
        llm_instance=llm, 
        logger_instance=logger,             
        doc_type=EVIDENCE_DOC_TYPES, 
        vectorstore_manager=vsm, 
        document_map=document_map,
        record_id=record_id  # üëà ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ‡∏Ñ‡∏£‡∏±‡∏ö!
    )

    # 5. Run Assessment 
    try:
        final = engine.run_assessment(
            target_sub_id=args.sub, 
            export=args.export, 
            vectorstore_manager=vsm,
            sequential=args.sequential,
            document_map=document_map,
            record_id=record_id
        )

        # 5.1 ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏á Database ‡∏´‡∏•‡∏±‡∏á‡∏£‡∏±‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à (Persistence)
        db_finish_task(record_id, final)
        logger.info(f"üíæ Results saved to database for Record ID: {record_id}")

    except Exception as e:
        logger.exception(f"‚ùå Engine execution failed: {e}")
        db_update_task_status(record_id, 0, f"Error: {str(e)}", status="FAILED")
        sys.exit(1)

    # 6. Print Summary UI (‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÉ‡∏´‡∏°‡πà)
    duration_s = time.time() - start_ts

    # ‡∏•‡∏≠‡∏á‡∏î‡∏∂‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏à‡∏≤‡∏Å‡∏à‡∏∏‡∏î‡∏ï‡πà‡∏≤‡∏á‡πÜ ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ
    # 1. ‡∏•‡∏≠‡∏á‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å summary (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
    overall_score = 0.0
    if final is not None and isinstance(final, dict):
        overall_score = final.get("summary", {}).get('overall_avg_score', 0.0)
    else:
        logger.critical("[CRASH PREVENTED] Final result is None - Default score 0.0")

    # 2. [Safe Guard] ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÄ‡∏õ‡πá‡∏ô 0 ‡πÅ‡∏ï‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô subcriteria_results ‡πÉ‡∏´‡πâ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å‡∏ï‡∏£‡∏á‡∏ô‡∏±‡πâ‡∏ô
    if overall_score == 0 and "subcriteria_results" in final:
        results = final["subcriteria_results"]
        if results:
            # ‡∏î‡∏∂‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô weighted_score ‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å (‡∏Å‡∏£‡∏ì‡∏µ‡∏£‡∏±‡∏ô‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡πÄ‡∏ä‡πà‡∏ô 1.2)
            overall_score = results[0].get("weighted_score", 0.0)

    if final is None:
        final = {"summary": {"overall_avg_score": 0.0, "overall_level_label": "L0"}}

    print("\n" + "‚ïê"*65)
    print(f" üèÅ  ASSESSMENT COMPLETE | ID: {record_id}")
    print("‚ïê"*65)
    print(f" [MODE]        : {run_mode}")
    print(f" [RESULT]      : Level {final.get('summary', {}).get('overall_level_label', 'L5')}") # ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà Log ‡πÇ‡∏ä‡∏ß‡πå‡∏ß‡πà‡∏≤‡∏ú‡πà‡∏≤‡∏ô
    print(f" [SCORE]       : {overall_score:.2f} / 5.00") # ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÄ‡∏ä‡πá‡∏Ñ‡πÅ‡∏•‡πâ‡∏ß
    print(f" [DURATION]    : {duration_s:.2f} seconds")

    print("-" * 65)
    if args.export:
        print(f" üíæ Exported to: {final.get('export_path_used', 'N/A')}")
    print("‚ïê"*65)

if __name__ == "__main__":
    multiprocessing.freeze_support() # ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Mac
    main()