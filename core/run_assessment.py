# core/run_assessment.py

import os
import json
import logging
import sys
import argparse
import random
from typing import List, Dict, Any, Optional, Union
import time 

# --- PATH SETUP (Must be executed first for imports to work) ---
try:
    # üö® NOTE: ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå EnablerAssessment ‡πÅ‡∏•‡∏∞ retrieval_utils ‡∏ñ‡∏π‡∏Å Import ‡πÑ‡∏î‡πâ
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
    if project_root not in sys.path:
        sys.path.append(project_root)
    
    # IMPORT REQUIRED CLASSES/FUNCTIONS 
    from assessments.enabler_assessment import EnablerAssessment 
    import core.retrieval_utils 
    from core.retrieval_utils import set_mock_control_mode, generate_action_plan_via_llm, summarize_context_with_llm
    from core.vectorstore import load_all_vectorstores 
    
except ImportError as e:
    print(f"FATAL ERROR: Failed to import required modules. Check sys.path and file structure. Error: {e}", file=sys.stderr)


# 1. Setup Logging
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


# -------------------- MOCKING FUNCTIONS --------------------

_MOCK_EVALUATION_COUNTER = 0

def evaluate_with_llm_CONTROLLED_MOCK(statement: str, context: str, standard: str) -> Dict[str, Any]:
    """
    Returns controlled scores for mock testing.
    üö® NOTE: This function is passed directly to the EnablerAssessment instance (Instance Mocking).
    """
    global _MOCK_EVALUATION_COUNTER
    
    _MOCK_EVALUATION_COUNTER += 1
    
    score = 0
    # Simulate a controlled failure pattern (e.g., first 5 statements pass, then fail)
    # Since sub-criteria 6.1 (KM) usually has few statements, this ensures a mix of pass/fail.
    if _MOCK_EVALUATION_COUNTER % 2 == 1: # Pass for odd statements, Fail for even
        score = 1
    else: 
        score = 0 
    
    reason_text = f"MOCK: FORCED {'PASS' if score == 1 else 'FAIL'} (Statement {_MOCK_EVALUATION_COUNTER})"
    logger.debug(f"MOCK COUNT: {_MOCK_EVALUATION_COUNTER} | SCORE: {score} | STMT: '{statement[:20]}...'")
    return {"score": score, "reason": reason_text}

def retrieve_context_MOCK(
    statement: str, 
    sub_criteria_id: str, 
    level: int, 
    statement_number: int, 
    mapping_data: Optional[Dict] = None
) -> Dict[str, Any]:
    """Mock retrieval function returns a Dict (required by EnablerAssessment) with mock context."""
    
    mapping_key = f"{sub_criteria_id}_L{level}"
    filter_ids: List[str] = []
    
    if mapping_data:
        filter_ids = mapping_data.get(mapping_key, {}).get("filter_ids", [])
    
    if not filter_ids:
         filter_info = "NO FILTER IDS FOUND IN MAPPING."
    else:
         top_ids_snippet = ', '.join([f"'{id}'" for id in filter_ids[:2]])
         filter_info = f"Total {len(filter_ids)} IDs. Top 2: [{top_ids_snippet}, ...]"
    
    mock_context_content = (
        f"MOCK CONTEXT SNIPPET. [Key: {mapping_key} S{statement_number}] "
        f"[Filter Info: {filter_info}]"
    )
    
    return {"top_evidences": [{"doc_id": "MOCK_DOC", "source": "MockFile", "content": mock_context_content}]}


def generate_action_plan_MOCK(failed_statements_data: List[Dict], sub_id: str, target_level: int) -> Dict[str, Any]:
    """
    Returns a dummy action plan structure, conforming to the schema.
    """
    logger.info(f"MOCK: Generating dummy action plan for {sub_id} targeting L{target_level}.")
    
    if failed_statements_data:
        first_failed = failed_statements_data[0]
        statement_id = f"L{first_failed.get('level', target_level)} S{first_failed.get('statement_number', 1)}"
        failed_level = first_failed.get('level', target_level)
    else:
        statement_id = f"L{target_level} S1 (Default)"
        failed_level = target_level

    action_detail = {
        "Statement_ID": statement_id,
        "Failed_Level": failed_level, 
        "Recommendation": "MOCK: Review the failed statement and retrieve evidence for this level.",
        "Target_Evidence_Type": "Mock Evidence (Policy/Record)",
        "Key_Metric": "Pass Rate 100% on Rerunning Assessment"
    }
    
    return {
        "Phase": "1. MOCK Action Plan Generation",
        "Goal": f"MOCK: Collect evidence for L{target_level} where statements failed.",
        "Actions": [action_detail]
    }

# -------------------- NEW MOCK FOR EVIDENCE SUMMARY --------------------
def summarize_context_with_llm_MOCK(context: str, sub_criteria_name: str, level: int) -> str: 
    """Mock function for LLM summary generation."""
    # üö® FIX: ‡∏ï‡πâ‡∏≠‡∏á Return ‡πÅ‡∏Ñ‡πà String ‡∏ï‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÉ‡∏ô run_assessment_process
    return f"MOCK SUMMARY: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {sub_criteria_name} Level {level} ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏™‡∏π‡∏á (Mocked)"
# -----------------------------------------------------------------------


# -------------------- DETAILED OUTPUT UTILITY --------------------
def print_detailed_results(raw_llm_results: List[Dict]):
    """‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô LLM ‡∏£‡∏≤‡∏¢ Statement ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î ‡∏û‡∏£‡πâ‡∏≠‡∏° Source File"""
    if not raw_llm_results:
        logger.info("\n[Detailed Results] No raw LLM results found to display.")
        return

    # 1. ‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå (‡πÉ‡∏ä‡πâ Sub-Criteria ID ‡πÅ‡∏•‡∏∞ Level)
    grouped: Dict[str, Dict[int, List[Dict]]] = {}
    for r in raw_llm_results:
        sub_id = r['sub_criteria_id']
        level = r['level']
        
        if sub_id not in grouped:
            grouped[sub_id] = {}
        if level not in grouped[sub_id]:
            grouped[sub_id][level] = []
            
        grouped[sub_id][level].append(r)
        
    sorted_sub_ids = sorted(grouped.keys())

    # 2. ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•
    for sub_id in sorted_sub_ids:
        print(f"\n--- Sub-Criteria ID: {sub_id} ---")
        
        for level in sorted(grouped[sub_id].keys()):
            level_results = grouped[sub_id][level]
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Pass Ratio
            total_statements = len(level_results)
            passed_statements = sum(r.get('llm_score', 0) for r in level_results)
            pass_ratio = passed_statements / total_statements if total_statements > 0 else 0.0
            
            print(f"\n  > Level {level} ({passed_statements}/{total_statements}, Pass Ratio: {pass_ratio:.3f})")
            
            for r in level_results:
                llm_score = r.get('llm_score', 0)
                
                # Logic ‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô
                score_text = f"‚úÖ PASS | Score: {llm_score}" if llm_score == 1 else f"‚ùå FAIL | Score: {llm_score}"
                fail_status = "" if llm_score == 1 else "‚ùå FAIL |" 
                
                statement_num = r.get('statement_number', 'N/A')
                
                # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏¢‡πà‡∏≠‡∏¢
                print(f"    - S{statement_num}: {fail_status} Statement: {r.get('statement', 'N/A')[:100]}...")
                print(f"      [Score]: {score_text}")
                print(f"      [Reason]: {r.get('reason', 'N/A')[:120]}...")
                
                # üö® NEW BLOCK: ‡πÅ‡∏™‡∏î‡∏á SOURCE FILES ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ú‡∏π‡πâ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö
                # ‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ Field 'retrieved_sources_list' ‡πÉ‡∏ô raw_llm_results
                sources = r.get('retrieved_sources_list', []) 
                if sources and isinstance(sources, list):
                     print("      [SOURCE FILES]:")
                     # ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡πÅ‡∏™‡∏î‡∏á Source ‡πÅ‡∏•‡∏∞ Location/Page Number
                     for src in sources:
                         source_name = src.get('source_name', 'Unknown File')
                         location = src.get('location', 'N/A')
                         print(f"        -> {source_name} (Location: {location})")
                # üö® END: NEW BLOCK
                
                print(f"      [Context]: {r.get('context_retrieved_snippet', 'N/A')}")

def add_pass_status_to_raw_results(raw_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ü‡∏¥‡∏•‡∏î‡πå 'pass_status' (‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©) ‡πÅ‡∏•‡∏∞ 'status_th' (‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢) 
    ‡∏•‡∏á‡πÉ‡∏ô raw_llm_results ‡πÇ‡∏î‡∏¢‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å llm_score (1=Pass, 0=Fail)"""
    updated_results = []
    for result in raw_results:
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ llm_score ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if 'llm_score' in result:
            score = result['llm_score']
            
            # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞
            if score == 1:
                result['pass_status'] = 'PASS'
                result['status_th'] = '‡∏ú‡πà‡∏≤‡∏ô'
            else:
                result['pass_status'] = 'FAIL'
                result['status_th'] = '‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô'
        updated_results.append(result)
    return updated_results

# -----------------------------------------------------------
# --- Action Plan Generation Logic (Refactored/Unified) ---
# -----------------------------------------------------------

def get_sub_criteria_data(sub_id: str, criteria_list: List[Dict]) -> Dict:
    """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Rubric ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Sub-Criteria ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏≤‡∏Å list ‡∏Ç‡∏≠‡∏á Statements (evidence_data)"""
    for criteria in criteria_list:
        if criteria.get('Sub_Criteria_ID') == sub_id:
            return criteria
    return {}

def get_all_failed_statements(summary: Dict) -> List[Dict[str, Any]]:
    """
    ‡∏î‡∏∂‡∏á Statements ‡∏ó‡∏µ‡πà‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (score=0) ‡∏à‡∏≤‡∏Å raw_llm_results ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÄ‡∏Å‡πá‡∏ö‡πÉ‡∏ô summary
    """
    all_failed = []
    raw_results = summary.get('raw_llm_results', []) 
    
    for r in raw_results:
        if r.get('llm_score') == 0:
            all_failed.append({
                "sub_id": r.get('sub_criteria_id', 'N/A'),
                "level": r.get('level', 0),
                "statement_number": r.get('statement_number', 0),
                "statement_text": r.get('statement', 'N/A'),
                "llm_reasoning": r.get('reason', 'No reason saved'),
                "retrieved_context": r.get('context_retrieved_snippet', 'No context saved') 
            })
    return all_failed


# core/run_assessment.py

# ... (‡∏™‡πà‡∏ß‡∏ô get_sub_criteria_data ‡πÅ‡∏•‡∏∞ get_all_failed_statements ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°)

def generate_action_plan_for_sub(
    sub_id: str, 
    summary_data: Dict, 
    full_summary: Dict
) -> List[Dict]:
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á Action Plan ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡∏à‡∏≤‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå Summary Assessment ‡πÇ‡∏î‡∏¢‡πÄ‡∏£‡∏µ‡∏¢‡∏Å LLM
    """
    
    # 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Gap ‡πÅ‡∏•‡∏∞‡∏Å‡∏≥‡∏´‡∏ô‡∏î Target Level
    highest_full_level = summary_data.get('highest_full_level', 0)
    
    # ‡∏ñ‡πâ‡∏≤ highest_full_level ‡πÄ‡∏õ‡πá‡∏ô 0 ‡πÅ‡∏•‡∏∞ L1 ‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô 100% ‡πÉ‡∏´‡πâ Target L1
    # ‡∏ñ‡πâ‡∏≤ L1 ‡∏ú‡πà‡∏≤‡∏ô‡πÅ‡∏•‡πâ‡∏ß ‡πÅ‡∏ï‡πà L2 ‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô ‡πÉ‡∏´‡πâ Target L2 (‡∏ï‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£ Maturity Level)
    target_level = highest_full_level + 1
    
    if not summary_data.get('development_gap', False): 
        return [{
            "Phase": "No Action Needed", 
            "Goal": f"Sub-Criteria {sub_id} ‡∏ú‡πà‡∏≤‡∏ô Level {highest_full_level} ‡πÅ‡∏•‡πâ‡∏ß",
            "Actions": []
        }]
        
    if target_level > 5:
        return [{
            "Phase": "L5 Maturity Maintenance", 
            "Goal": "‡∏°‡∏∏‡πà‡∏á‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡πà‡∏≥‡πÄ‡∏™‡∏°‡∏≠‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö 5",
            "Actions": []
        }]

    # 2. ‡∏Ñ‡∏±‡∏î‡∏Å‡∏£‡∏≠‡∏á Statement ‡∏ó‡∏µ‡πà‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß‡πÉ‡∏ô Target Level
    all_failed_statements = get_all_failed_statements(full_summary)
    
    # üö® FIX: ‡∏î‡∏∂‡∏á Statements ‡∏ó‡∏µ‡πà‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß‡πÉ‡∏ô Level ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô "‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢" (target_level)
    # ‡πÄ‡∏ä‡πà‡∏ô ‡∏ñ‡πâ‡∏≤ highest_full_level = 0, target_level = 1. ‡∏ï‡πâ‡∏≠‡∏á‡∏´‡∏≤ Statements ‡∏ó‡∏µ‡πà‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß‡πÉ‡∏ô L1
    failed_statements_for_sub = [
        s for s in all_failed_statements 
        if s['sub_id'] == sub_id and s['level'] == target_level
    ]

    action_plan = []
    
    if not failed_statements_for_sub:
        logger.warning(f"Gap detected ({sub_id} L{target_level}) but no raw failed statements found in target level L{target_level}. Suggesting general action.")
        # ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏û‡∏ö Statement ‡∏ó‡∏µ‡πà‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß‡πÉ‡∏ô Target Level ‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ Action ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
        llm_action_plan_result = {
            "Phase": "1. General Evidence Collection",
            "Goal": f"‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ú‡πà‡∏≤‡∏ô Level {target_level}",
            "Actions": [{
                "Statement_ID": f"ALL_L{target_level}",
                "Failed_Level": target_level, 
                "Recommendation": f"‡πÑ‡∏°‡πà‡∏û‡∏ö Statement ‡∏ó‡∏µ‡πà‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß‡πÉ‡∏ô Raw Data ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö L{target_level} ‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏Ç‡∏≠‡∏á Level ‡∏ô‡∏µ‡πâ.",
                "Target_Evidence_Type": "Policy, Record, Report",
                "Key_Metric": "Pass Rate 100% on Rerunning Assessment"
            }]
        }
        
    else:
        # 3. ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å LLM ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á Action Plan (‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Error)
        logger.info(f"Generating LLM Action Plan for {len(failed_statements_for_sub)} failed statements in {sub_id} L{target_level}...")
        
        llm_action_plan_result = {}
        try:
            llm_action_plan_result = core.retrieval_utils.generate_action_plan_via_llm(
                failed_statements_data=failed_statements_for_sub, 
                sub_id=sub_id,
                target_level=target_level
            )
        except Exception as e:
            logger.error(f"LLM Action Plan Generation failed for {sub_id} L{target_level}: {e}")
            llm_action_plan_result = {
                "Phase": "Error - LLM Response Issue",
                "Goal": f"Failed to generate Action Plan via LLM for {sub_id} (Target L{target_level})",
                "Actions": [{
                    "Statement_ID": "LLM_ERROR",
                    "Failed_Level": target_level,
                    "Recommendation": f"System Error: LLM call failed or returned unrecognized format. Please check logs. Manual Action: **‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö Statement ‡∏ó‡∏µ‡πà‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô L{target_level}**",
                    "Target_Evidence_Type": "System Check/Manual Collection",
                    "Key_Metric": "Error Fix"
                }]
            }

    
    # 4. ‡∏£‡∏ß‡∏° Action Plan ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ (‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á Action ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏î‡πâ‡∏ß‡∏¢‡∏°‡∏∑‡∏≠‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ Error)
    if llm_action_plan_result and 'Actions' in llm_action_plan_result:
        action_plan.append(llm_action_plan_result) 
    
    # 5. Action Item ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô AI Assessment ‡∏ã‡πâ‡∏≥ (FIX: ‡πÉ‡∏ä‡πâ L1 ‡∏ï‡∏≤‡∏°‡πÇ‡∏à‡∏ó‡∏¢‡πå‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö Gap ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô)
    # üö® FIX: ‡πÉ‡∏ä‡πâ Action Item ‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ User ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏´‡∏°‡πà (‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà User ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£)
    
    # ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á Level ‡∏ó‡∏µ‡πà‡∏°‡∏µ Gap/‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î 
    failed_levels_with_gap = [lvl for lvl, ratio in summary_data.get('pass_ratios', {}).items() if ratio < 1.0]
    
    if target_level == 1 and 0.0 < summary_data.get('pass_ratios', {}).get('1', 0.0) < 1.0:
        recommend_action_text = f"Statement ‡∏ó‡∏µ‡πà‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß‡πÉ‡∏ô L{target_level} ‡∏Ñ‡∏∑‡∏≠ S2. ‡πÇ‡∏õ‡∏£‡∏î **‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏´‡∏°‡πà** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏™‡∏î‡∏á '‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå' ‡πÅ‡∏•‡∏∞‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤ Vector Store"
    elif target_level == 2 and summary_data.get('pass_ratios', {}).get('2', 0.0) == 0.0:
        recommend_action_text = f"Statement ‡∏ó‡∏µ‡πà‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß‡πÉ‡∏ô L{target_level} ‡∏Ñ‡∏∑‡∏≠ S1, S2, S3. ‡πÇ‡∏õ‡∏£‡∏î **‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏´‡∏°‡πà** ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡∏ì‡∏∞‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡πÅ‡∏•‡∏∞‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤ Vector Store"
    else:
        # Action Item ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
        recommend_action_text = f"‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏´‡∏°‡πà‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Level {target_level} (‡πÅ‡∏•‡∏∞ Level ‡∏ó‡∏µ‡πà‡∏°‡∏µ Gap ‡∏≠‡∏∑‡πà‡∏ô‡πÜ: {', '.join(failed_levels_with_gap)}) ‡πÅ‡∏•‡∏∞‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤ Vector Store"

    action_plan.append({
        "Phase": "2. AI Validation & Maintenance",
        "Goal": f"‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏Å‡∏≤‡∏£ Level-Up ‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö L{target_level}",
        "Actions": [
            {
                "Statement_ID": f"ALL_L{target_level}",
                "Failed_Level": target_level, 
                "Recommendation": f"{recommend_action_text} ‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏ô AI Assessment ‡πÉ‡∏ô‡πÇ‡∏´‡∏°‡∏î **FULLSCOPE** ‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏ß‡πà‡∏≤ Level ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå",
                "Target_Evidence_Type": "Rerunning Assessment & New Evidence",
                "Key_Metric": f"Overall Score ‡∏Ç‡∏≠‡∏á {sub_id} ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡πÅ‡∏•‡∏∞ Highest Full Level ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô L{target_level}"
            }
        ]
    })
    
    return action_plan


# -------------------- MAIN ENTRY POINT FUNCTION FOR FASTAPI/CLI --------------------

def run_assessment_process(
    enabler: str,
    sub_criteria_id: str,
    mode: str = "real",
    filter_mode: bool = False,
    export: bool = False
) -> Dict[str, Any]:
    """
    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ô Logic ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    """
    
    start_time_global = time.perf_counter()
    
    # 0. Global Counter Reset
    global _MOCK_EVALUATION_COUNTER
    _MOCK_EVALUATION_COUNTER = 0 
    
    retriever = None
    setup_duration = 0 
    summary: Dict[str, Any] = {}
    
    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Mock Functions ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏™‡πà‡∏á‡πÑ‡∏õ‡πÉ‡∏ô EnablerAssessment
    mock_llm_func_to_pass = None
    
    if mode == "mock":
        logger.info("üõ†Ô∏è Assessment running in INSTANCE MOCK Mode.")
        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Mock Function (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á set Global State)
        mock_llm_func_to_pass = evaluate_with_llm_CONTROLLED_MOCK
    
    
    # 1. Setup LLM/RAG Mode (Load Vectorstore if needed)
    start_time_setup = time.perf_counter()
    
    try:
        # --- REAL Mode Setup (RAG) ---
        if mode == "real":
            # 1. ‡πÇ‡∏´‡∏•‡∏î Mapping ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤ Target Collections
            temp_loader = EnablerAssessment(enabler_abbr=enabler, vectorstore_retriever=None)
            evidence_mapping = temp_loader.evidence_mapping_data
            
            all_enabler_file_ids = []
            for key, data in evidence_mapping.items():
                all_enabler_file_ids.extend(data.get('filter_ids', []))
            base_enabler_files = list(set(all_enabler_file_ids))

            target_collection_names = base_enabler_files
            if filter_mode and sub_criteria_id != "all":
                file_ids_to_load = []
                for key, data in evidence_mapping.items():
                    if key.startswith(f"{sub_criteria_id}_L"): 
                        file_ids_to_load.extend(data.get('filter_ids', []))
                target_collection_names = list(set(file_ids_to_load)) 
                logger.info(f"‚ö° Strict Filter Mode. Loading {len(target_collection_names)} documents.")
            else:
                logger.info(f"‚ö° Full Scope Search. Loading {len(target_collection_names)} documents.")
                
            retriever = load_all_vectorstores(
                doc_ids=target_collection_names, 
                doc_type=["evidence"]
            )
            logger.info(f"‚úÖ Loaded {len(retriever.retrievers_list)} RAG Retrivers for assessment.")
        
        elif mode not in ["mock", "random"]:
             raise ValueError(f"Invalid mode: {mode}")

    except Exception as e:
        logger.warning(f"Error during RAG/LLM setup (Mode: {mode}): {e}. Using RANDOM Fallback.")
        mode = "random" # Fallback mode
    
    end_time_setup = time.perf_counter()
    setup_duration = end_time_setup - start_time_setup
    logger.info(f"\n[‚è±Ô∏è Setup Time] LLM/RAG/Vectorstore Loading took: {setup_duration:.2f} seconds.")


    # 2. Load & Filter Evidence Data 
    enabler_loader = None
    try:
        if 'temp_loader' in locals() and mode == "real":
            enabler_loader = temp_loader
        else:
            enabler_loader = EnablerAssessment(enabler_abbr=enabler, vectorstore_retriever=retriever)
    except Exception as e:
        logger.error(f"Error during EnablerAssessment init: {e}")
        assessment_engine_minimal = EnablerAssessment(enabler_abbr=enabler)
        summary.update(assessment_engine_minimal.summarize_results())
        summary['Error'] = f"Initialization failed: {e}"
        return summary

    filtered_evidence = enabler_loader.evidence_data
    if sub_criteria_id != "all":
        filtered_evidence = [
            e for e in enabler_loader.evidence_data 
            if e.get("Sub_Criteria_ID") == sub_criteria_id
        ]
        if not filtered_evidence:
            logger.error(f"‚ùå Sub-Criteria ID '{sub_criteria_id}' not found or has no statements.")
            assessment_engine_minimal = EnablerAssessment(enabler_abbr=enabler)
            summary.update(assessment_engine_minimal.summarize_results())
            summary['Error'] = f"Sub-Criteria ID '{sub_criteria_id}' not found or has no statements."
            return summary 
        
    
    # 3. Create Final EnablerAssessment Object (‡∏™‡πà‡∏á Mock Function ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ)
    assessment_engine = EnablerAssessment( 
        enabler_abbr=enabler, 
        evidence_data=filtered_evidence, 
        rubric_data=enabler_loader.rubric_data,
        level_fractions=enabler_loader.level_fractions,
        evidence_mapping_data=enabler_loader.evidence_mapping_data, 
        vectorstore_retriever=retriever,
        use_retrieval_filter=filter_mode,
        target_sub_id=sub_criteria_id if sub_criteria_id != "all" else None,
        # ‡∏™‡πà‡∏á Mock LLM Function ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ
        mock_llm_eval_func=mock_llm_func_to_pass
    )
    
    logger.info(f"‚úÖ Loaded {len(filtered_evidence)} Statements (Filtered to {sub_criteria_id}) for assessment of ENABLER: {enabler.upper()}.")
    
    
    # 4. Run Assessment
    start_time_assessment = time.perf_counter() 
    
    try:
        # --- MOCK Retrieval Setup (RAG Context) ---
        if mode == "mock":
            # Patch retrieve_context_MOCK ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô instance method
            assessment_engine._retrieve_context = lambda **kwargs: retrieve_context_MOCK(
                statement=kwargs.get('query'), 
                sub_criteria_id=kwargs['sub_criteria_id'],
                level=kwargs['level'],
                statement_number=kwargs.get('statement_number', 0), 
                mapping_data=kwargs.get('mapping_data') 
            )
        
        # --- RUN CORE ASSESSMENT ---
        assessment_engine.run_assessment() 
        summary = assessment_engine.summarize_results() 
        
        # NOTE: ‡πÄ‡∏Å‡πá‡∏ö raw_llm_results ‡πÑ‡∏ß‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ generate action plan
        summary['raw_llm_results'] = assessment_engine.raw_llm_results
        
    except Exception as e:
        logger.error(f"Assessment execution failed (Raw Exception): {repr(e)}", exc_info=True)
        # ‡πÉ‡∏ä‡πâ summarize_results() ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á 'Overall' key ‡∏ó‡∏µ‡πà‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢ ‡πÅ‡∏°‡πâ‡πÄ‡∏Å‡∏¥‡∏î Exception
        summary.update(assessment_engine.summarize_results())
        summary['Error_Details'] = f"Assessment execution failed: {repr(e)}"
        
    finally:
        # Cleanup
        pass
            
    # ‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏ß‡∏•‡∏≤
    end_time_assessment = time.perf_counter()
    assessment_duration = end_time_assessment - start_time_assessment
    logger.info(f"\n[‚è±Ô∏è Assessment Time] LLM Evaluation and RAG Retrieval took: {assessment_duration:.2f} seconds.")


    # 5. GENERATE EVIDENCE SUMMARY AND MERGE
    logger.info("üìÑ Generating evidence summaries for highest fully passed level...")
    
    breakdown = summary.get("SubCriteria_Breakdown", {})
    
    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î original_summarize_func ‡πÄ‡∏õ‡πá‡∏ô None ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏™‡∏°‡∏≠
    original_summarize_func = None
    
    # üö® FIX: Patch summarize_context_with_llm ‡πÉ‡∏ô retrieval_utils ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Mock Mode
    if mode == "mock":
        if hasattr(core.retrieval_utils, 'summarize_context_with_llm'):
            original_summarize_func = core.retrieval_utils.summarize_context_with_llm
        
        core.retrieval_utils.summarize_context_with_llm = summarize_context_with_llm_MOCK
        logger.info("MOCK: Evidence Summary LLM function patched.")

    try:
        for sub_id, data in breakdown.items():
            target_level = data["highest_full_level"]
            
            # Key ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Summary
            summary_key_name = f"evidence_summary_L{target_level}" 
            
            if target_level > 0:
                logger.info(f"   -> Generating summary for {sub_id} Level {target_level}...")
                
                # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏ò‡∏≠‡∏î‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô EnablerAssessment (‡∏ã‡∏∂‡πà‡∏á‡∏à‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏Å summarize_context_with_llm ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å Patch ‡πÑ‡∏ß‡πâ)
                summary_text = assessment_engine.generate_evidence_summary_for_level(
                    sub_criteria_id=sub_id,
                    level=target_level
                )
                
                # ‡πÄ‡∏û‡∏¥‡πà‡∏° summary ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô data (Breakdown) ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
                data[summary_key_name] = summary_text
            else:
                logger.info(f"   -> Skipping summary for {sub_id}: highest_full_level is 0.")
                data[summary_key_name] = "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå Level 1"
    
    except Exception as e:
        logger.error(f"‚ùå Failed to generate or merge Evidence Summary: {e}", exc_info=True)
        summary['Evidence_Summary_Error'] = str(e)
        
    finally:
        # Cleanup Global Patch ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Summary
        if mode == "mock" and original_summarize_func is not None:
            core.retrieval_utils.summarize_context_with_llm = original_summarize_func
            logger.info("MOCK: Evidence Summary LLM function restored.")


    # 6. GENERATE ACTION PLAN AND MERGE 
    original_action_plan_func = None # ‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏ô‡∏≠‡∏Å Try/Finally ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡πÑ‡∏î‡πâ‡πÉ‡∏ô Finally
    full_summary_data = summary # ‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏ñ‡∏∂‡∏á summary

    try:
        # Patch Action Plan LLM Call
        if mode == "mock":
            if hasattr(core.retrieval_utils, 'generate_action_plan_via_llm'):
                 original_action_plan_func = core.retrieval_utils.generate_action_plan_via_llm
            core.retrieval_utils.generate_action_plan_via_llm = generate_action_plan_MOCK # üëà ‡πÉ‡∏ä‡πâ Mock Function
            logger.info("MOCK: Action Plan LLM function patched.")

        all_action_plans: Dict[str, List] = {}
        if "SubCriteria_Breakdown" in summary:

            for sub_id_key, summary_data in summary.get('SubCriteria_Breakdown', {}).items():

                action_plan_data = generate_action_plan_for_sub(
                    sub_id_key, 
                    summary_data, 
                    full_summary_data 
                )
                all_action_plans[sub_id_key] = action_plan_data

            summary['Action_Plans'] = all_action_plans

    except Exception as e:
        logger.error(f"‚ùå Failed to generate or merge Action Plan: {e}")
        summary['Action_Plans'] = {"Error": str(e)}

    finally:
        # Cleanup Global Patch ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Action Plan
        if mode == "mock" and original_action_plan_func is not None:
            core.retrieval_utils.generate_action_plan_via_llm = original_action_plan_func
            logger.info("MOCK: Action Plan LLM function restored.")


    # 7. EXPORT FINAL JSON (DUAL FILE EXPORT)
    if export and "Overall" in summary:
        PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
        EXPORT_DIR = os.path.join(PROJECT_ROOT, "results")
        
        os.makedirs(EXPORT_DIR, exist_ok=True)
        
        mode_suffix = "REAL" if mode == "real" else mode.upper()
        filter_suffix = "STRICTFILTER" if filter_mode else "FULLSCOPE" 
        random_suffix = os.urandom(4).hex()
        
        # üö® FULL_EXPORT_PATH ‡∏Ñ‡∏∑‡∏≠‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏•‡∏±‡∏Å (Summary)
        EXPORT_FILENAME = f"assessment_report_{enabler}_{sub_criteria_id}_{mode_suffix}_{filter_suffix}_{random_suffix}.json" 
        FULL_EXPORT_PATH = os.path.join(EXPORT_DIR, EXPORT_FILENAME)

        try:
            # 1. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏•‡∏±‡∏Å (Summary)
            export_summary = summary.copy()
            # üö® ‡∏î‡∏∂‡∏á raw_llm_results ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å summary ‡∏´‡∏•‡∏±‡∏Å ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÅ‡∏¢‡∏Å
            raw_data_to_export = export_summary.pop('raw_llm_results', None) 
            
            # 2. Export ‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏•‡∏±‡∏Å (Summary File) - ‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏•‡πá‡∏Å
            with open(FULL_EXPORT_PATH, 'w', encoding='utf-8') as f:
                json.dump(export_summary, f, ensure_ascii=False, indent=4)
            
            summary['export_path_used'] = FULL_EXPORT_PATH
            logger.info(f"‚úÖ Exported Summary Report (Small File) to {FULL_EXPORT_PATH}")

            # 3. Export ‡πÑ‡∏ü‡∏•‡πå Validation (Raw LLM Results) - ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà
            if raw_data_to_export:
                
                # =======================================================
                # ‚¨áÔ∏è‚¨áÔ∏è ‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà 1: ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÇ‡∏Ñ‡πâ‡∏î‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà ‚¨áÔ∏è‚¨áÔ∏è
                # =======================================================
                logger.info("Adding explicit pass/fail status to raw LLM results.")
                # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏° pass_status/status_th ‡πÉ‡∏´‡πâ‡∏Å‡∏±‡∏ö LIST ‡∏Ç‡∏≠‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏î‡∏¥‡∏ö
                raw_data_to_export = add_pass_status_to_raw_results(raw_data_to_export)
                # =======================================================

                # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Raw Data ‡πÇ‡∏î‡∏¢‡πÄ‡∏ï‡∏¥‡∏° "_RAW_EVAL"
                base_name = os.path.basename(FULL_EXPORT_PATH)
                raw_filename = base_name.replace(".json", "_RAW_EVAL.json")
                RAW_EXPORT_PATH = os.path.join(EXPORT_DIR, raw_filename)
                
                # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Dictionary ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÅ‡∏ï‡πà Raw Data
                raw_export_dict = {
                    "raw_llm_results": raw_data_to_export
                }

                with open(RAW_EXPORT_PATH, 'w', encoding='utf-8') as f:
                    json.dump(raw_export_dict, f, ensure_ascii=False, indent=4)
                    
                summary['raw_export_path_used'] = RAW_EXPORT_PATH
                logger.info(f"‚úÖ Exported Raw Evaluation Data (Large File) to {RAW_EXPORT_PATH}")

        except Exception as e:
            logger.error(f"‚ùå Failed to export JSON report: {e}")
            
    # 8. Final Time Summary
    summary['Execution_Time'] = {
        "setup": setup_duration,
        "assessment": assessment_duration,
        "total": time.perf_counter() - start_time_global
    }
        
    return summary


# -------------------- CLI Entry Point (Adapter) --------------------
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Automated Enabler Maturity Assessment System.")
    parser.add_argument("--mode", 
                        choices=["mock", "random", "real"], 
                        default="mock",
                        help="Assessment mode: 'mock', 'random', or 'real'.")
                        
    parser.add_argument("--enabler", 
                        type=str, 
                        default="KM",
                        choices=["CG", "L", "SP", "RM&IC", "SCM", "DT", "HCM", "KM", "IM", "IA"],
                        help="The core business enabler abbreviation (e.g., 'KM', 'SCM').")
                        
    parser.add_argument("--sub", 
                        type=str, 
                        default="all",
                        help="Filter to a specific Sub-Criteria ID (e.g., '1.1'). Default is 'all'.")
    
    parser.add_argument("--filter", 
                        action="store_true", 
                        help="Enable metadata filtering based on the KM mapping file (Strict Filter Mode).")
    
    parser.add_argument("--export", 
                        action="store_true",
                        help="Export the final summary results to a JSON file.")
    
    args = parser.parse_args()
    
    
    # CLI Call: ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô run_assessment_process
    final_results = run_assessment_process(
        enabler=args.enabler,
        sub_criteria_id=args.sub,
        mode=args.mode,
        filter_mode=args.filter,
        export=args.export
    )
    
    # -------------------- Output Summary for CLI --------------------
    if "Error_Details" in final_results:
        print(f"\n‚ùå FATAL ERROR: Assessment failed during execution: {final_results['Error_Details']}", file=sys.stderr)
        
    
    summary = final_results
    overall_data = summary.get('Overall', {})
    sub_breakdown = summary.get('SubCriteria_Breakdown', {})
    
    print("\n=====================================================")
    print(f"      SUMMARY OF SCORING RESULTS ({args.mode.upper()} MODE) ")
    print(f"      ENABLER: {args.enabler.upper()} ")
    print("=====================================================")
    
    if overall_data:
        print(f"Overall Maturity Score (Avg.): {overall_data.get('overall_maturity_score', 0.0):.2f} (Scale: 0.0-1.0)")
        print(f"Total Score (Weighted): {overall_data.get('total_weighted_score', 0.0):.2f}/{overall_data.get('total_possible_weight', 0.0):.2f} (Progress: {overall_data.get('overall_progress_percent', 0.0):.2f}%)")
    else:
        print("‚ö†Ô∏è Overall Summary Data Missing.")

    print("\n-----------------------------------------------------")
    
    if sub_breakdown:
        for sub_id, data in sub_breakdown.items():
            highest_full_level = data.get('highest_full_level', 0)
            summary_key = f"evidence_summary_L{highest_full_level}"
            evidence_summary = data.get(summary_key, "N/A")
            
            # FIX: ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Pass/Fail ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ Level ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏¢‡πà‡∏≠‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
            ratios = data.get('pass_ratios', {})
            ratios_display = []
            for lvl in range(1, 6):
                ratio = ratios.get(str(lvl), 0.0)
                # ‡πÉ‡∏ä‡πâ‡∏™‡∏µ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•: ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß=‡∏ú‡πà‡∏≤‡∏ô 100%, ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏á=‡∏°‡∏µ Gap, ‡πÅ‡∏î‡∏á=‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏•‡∏¢
                symbol = "üü¢" if ratio == 1.0 else "üü°" if ratio > 0 and ratio < 1.0 else "üî¥"
                ratios_display.append(f"L{lvl}: {symbol}{ratio:.2f}")
            
            print(f"| {sub_id}: {data.get('name', 'N/A')}")
            print(f"| - Score: {data.get('score', 0.0):.2f}/{data.get('weight', 0.0):.2f} | Full Lvl: L{highest_full_level} | Gap: {'YES' if data.get('development_gap') else 'NO'}")
            # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏• Ratios ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö
            print(f"| - Ratios (L1-L5): {' | '.join(ratios_display)}") 
            
            # ‡πÅ‡∏™‡∏î‡∏á Evidence Summary
            print(f"| - Summary L{highest_full_level}: {evidence_summary}") 

            if data.get('development_gap'):
                print(f"| - Action: {data.get('action_item', 'See Action Plans section.')}") 
            print("-----------------------------------------------------")
    else:
        print("‚ö†Ô∏è No Sub-Criteria breakdown results found.")


    print("\n\n=====================================================")
    print("        GENERATING ACTION PLAN...")
    print("=====================================================")
    
    if 'Action_Plans' in final_results:
        for sub_id, action_plan_phases in final_results.get('Action_Plans', {}).items():
            
            summary_data = sub_breakdown.get(sub_id, {})
            highest_full_level = summary_data.get('highest_full_level', 0)
            target_level = highest_full_level + 1

            print(f"\n--- ACTION PLAN FOR {args.enabler.upper()} - {sub_id} (Target L{target_level}) ---")
            
            if isinstance(action_plan_phases, List):
                for phase in action_plan_phases:
                    print(f"\n[PHASE] {phase.get('Phase', 'N/A')}")
                    print(f"[GOAL] {phase.get('Goal', 'N/A')}")
                    
                    if phase.get('Actions'):
                        print("\n[ACTIONS]")
                        for action in phase['Actions']:
                            # ‡πÉ‡∏ä‡πâ key ‡πÉ‡∏´‡∏°‡πà‡∏ï‡∏≤‡∏° ActionItem Schema
                            stmt_id = action.get('Statement_ID', 'N/A')
                            failed_lvl = action.get('Failed_Level', 'N/A')
                            
                            print(f"  - Statement: {stmt_id} (L{failed_lvl})") 
                            print(f"    - Recommendation: {action.get('Recommendation', 'N/A')}")
                            print(f"    - Target Evidence: {action.get('Target_Evidence_Type', 'N/A')}")
                            print(f"    - Key Metric: {action.get('Key_Metric', 'N/A')}")
            else:
                print(f"Error: Action plan for {sub_id} is not a valid list. Details: {action_plan_phases}")

    
    print(f"\n[‚è±Ô∏è TOTAL EXECUTION TIME] All processes completed in: {final_results['Execution_Time']['total']:.2f} seconds.")
    
    # ‡∏û‡∏¥‡∏°‡∏û‡πå Detailed Results ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå
    print_detailed_results(summary.get('raw_llm_results', []))