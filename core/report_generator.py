import json
import argparse
import os
import sys
import logging
from typing import Dict, Any, List
from pathlib import Path

# 1. Setup Logging
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# -------------------- Module Import Logic --------------------
# 1. Import NARRATIVE PROMPT
NARRATIVE_REPORT_PROMPT = None
SYSTEM_NARRATIVE_PROMPT = None
try:
    # üìù IMPORT ‡∏ó‡∏±‡πâ‡∏á PROMPT Template ‡πÅ‡∏•‡∏∞ System Instruction
    from core.rag_prompts import NARRATIVE_REPORT_PROMPT, SYSTEM_NARRATIVE_PROMPT 
    logger.info("Successfully imported NARRATIVE_REPORT_PROMPT and SYSTEM_NARRATIVE_PROMPT from core.rag_prompts.")
    NARRATIVE_REPORT_TEMPLATE_TEXT = NARRATIVE_REPORT_PROMPT.template 
except ImportError:
    NARRATIVE_REPORT_PROMPT = None
    SYSTEM_NARRATIVE_PROMPT = None
    NARRATIVE_REPORT_TEMPLATE_TEXT = "[ERROR: Failed to import NARRATIVE_REPORT_PROMPT. Check your core.rag_prompts.py and Python path.]"
    logger.error("Could not import NARRATIVE_REPORT_PROMPT.")

# 2. Import REAL LLM Function
_LLM_NARRATIVE_ENABLED = False
try:
    # üö® NEW: Import REAL LLM function from retrieval_utils üö®
    from core.retrieval_utils import generate_narrative_report_via_llm_real 
    _LLM_NARRATIVE_ENABLED = True
    logger.info("Successfully imported REAL LLM function for narrative report.")
except ImportError:
    logger.warning("Could not import REAL LLM function (generate_narrative_report_via_llm_real). Narrative report will use MOCK FALLBACK.")


# --- Utility Functions for Data Synthesis (NEW LOGIC) ---

def calculate_overall_level(score: float) -> int:
    """Converts the overall maturity score (0.0 to 5.0) to a Maturity Level (L0 to L4)."""
    if 0.00 <= score < 1.00:
        return 0
    elif 1.00 <= score < 2.00:
        return 1
    elif 2.00 <= score < 3.00:
        return 2
    elif 3.00 <= score < 4.00:
        return 3
    elif 4.00 <= score <= 5.00:
        return 4
    return 0

def find_top_bottom_criteria(breakdown: Dict[str, Any]) -> dict:
    """
    Finds the Sub-Criteria with the highest and lowest scores, 
    and extracts relevant contextual data for the LLM.
    """
    
    # Filter out any criteria with non-numeric scores
    criteria_list = [
        {
            'score': data['score'],
            'id': key,
            'name': data['name'],
            'action_item': data.get('action_item', 'N/A'),
            'highest_full_level': data.get('highest_full_level', 0)
        }
        for key, data in breakdown.items() if isinstance(data.get('score'), (int, float))
    ]
    
    if not criteria_list:
        return {
            'top_criteria': {},
            'bottom_criteria': {}
        }

    # Sort criteria by score
    criteria_list.sort(key=lambda x: x['score'], reverse=True)

    top_criteria = criteria_list[0]
    bottom_criteria = criteria_list[-1]
    
    return {
        'top_criteria': top_criteria,
        'bottom_criteria': bottom_criteria
    }

def extract_top_goals(data: Dict[str, Any], limit: int = 3) -> List[str]:
    """Extracts unique goals from the action plans across all criteria."""
    
    unique_goals = set()
    
    # Iterate through each sub-criteria's action plans
    for sub_data in data.get("SubCriteria_Breakdown", {}).values():
        # Check if the action_plan_json exists and is a dictionary
        if "action_plan_json" in sub_data and isinstance(sub_data["action_plan_json"], dict):
            plan_json = sub_data["action_plan_json"]
            
            # Action plan JSON is structured as an array of phases
            for phase in plan_json.get("phases", []):
                goal = phase.get("Goal")
                if goal and isinstance(goal, str):
                    unique_goals.add(goal.strip())
                    
    # Convert set back to list for final output, taking the top N
    return list(unique_goals)[:limit]


# -------------------- NARRATIVE LLM FUNCTION (Updated for Real LLM Integration) --------------------

def generate_narrative_report_via_llm(summary_data: Dict[str, Any]) -> str:
    """
    ‡πÉ‡∏ä‡πâ LLM ‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô summary_data ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡πÄ‡∏ä‡∏¥‡∏á‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢ 4 ‡∏™‡πà‡∏ß‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö CEO 
    ‡πÇ‡∏î‡∏¢‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô LLM ‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏≤‡∏Å core/retrieval_utils.py (‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô)
    """
    
    enabler_name = summary_data.get('Overall', {}).get('enabler', 'UNKNOWN ENABLER')

    # üö® FALLBACK LOGIC üö®
    if not _LLM_NARRATIVE_ENABLED or NARRATIVE_REPORT_PROMPT is None or SYSTEM_NARRATIVE_PROMPT is None:
        # ‡πÉ‡∏ä‡πâ MOCK FALLBACK ‡∏´‡∏≤‡∏Å LLM Function, Prompt Template ‡∏´‡∏£‡∏∑‡∏≠ System Prompt ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
        overall_progress = summary_data.get('Overall', {}).get('overall_progress_percent', 0.0)
        
        fallback_error_info = ""
        if not _LLM_NARRATIVE_ENABLED:
            fallback_error_info += " (LLM Function Not Imported)"
        if NARRATIVE_REPORT_PROMPT is None or SYSTEM_NARRATIVE_PROMPT is None:
            fallback_error_info += " (Prompt Template Missing)"
        
        return f"""
[üö® ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡πÄ‡∏ä‡∏¥‡∏á‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏î‡∏¢ MOCK FALLBACK {fallback_error_info} ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {enabler_name} üö®]

## 1. ‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£ (Overall Maturity Summary)

‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏ö‡∏ö LLM ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÉ‡∏ô‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏à‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡πà‡∏≤ Summary ‡∏´‡∏•‡∏±‡∏Å: ‡∏ß‡∏∏‡∏í‡∏¥‡∏†‡∏≤‡∏ß‡∏∞ {enabler_name} ‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°: **Overall Progress ‡∏ó‡∏µ‡πà {overall_progress:.2f}%** ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡∏à‡∏≤‡∏Å‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô (‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå)

## 2. ‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞‡πÄ‡∏ä‡∏¥‡∏á‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£ (Strategic Recommendations)

‡πÇ‡∏õ‡∏£‡∏î‡∏£‡∏±‡∏ô‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏°‡∏µ Flag `--narrative` ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡πÅ‡∏ö‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á (Structured Markdown) ‡πÅ‡∏•‡∏∞ Action Plan ‡πÇ‡∏î‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î

---
[System Info: Real LLM call was disabled or failed to initialize.]
"""
    
    # -------------------- REAL LLM CALL LOGIC --------------------
    try:
        # --- NEW: Data Pre-processing ---
        overall_data = summary_data.get("Overall", {})
        overall_score = overall_data.get("overall_maturity_score", 0.0)
        progress = overall_data.get("overall_progress_percent", 0.0)

        # a. Calculate overall level and progress
        overall_level = calculate_overall_level(overall_score)
        overall_progress = progress # already a float percentage

        # b. Find top/bottom criteria for Section 2
        sub_breakdown = summary_data.get("SubCriteria_Breakdown", {})
        criteria_analysis = find_top_bottom_criteria(sub_breakdown)
        top_c = criteria_analysis['top_criteria']
        bottom_c = criteria_analysis['bottom_criteria']
        
        # c. Extract top strategic goals for Section 4
        top_goals = extract_top_goals(summary_data, limit=3)
        
        # Format goals for better prompt injection (e.g., list format)
        formatted_goals = "\n- " + "\n- ".join(top_goals) if top_goals else "‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏ä‡∏¥‡∏á‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏à‡∏≤‡∏Å Action Plan."
        
        # Dumping JSON to pass to LLM (for context)
        data_json_string = json.dumps(summary_data, indent=2, ensure_ascii=False)
        # --- END NEW: Data Pre-processing ---
        
        # 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á Prompt ‡∏â‡∏ö‡∏±‡∏ö‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå (Human Message)
        final_prompt_text = NARRATIVE_REPORT_PROMPT.format(
            summary_data=data_json_string,
            enabler_name=enabler_name,
            overall_level=overall_level,
            overall_progress=overall_progress,
            overall_maturity_score=overall_score,
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏∂‡∏Å‡∏Ç‡∏≠‡∏á‡∏à‡∏∏‡∏î‡πÅ‡∏Ç‡πá‡∏á/‡∏à‡∏∏‡∏î‡∏≠‡πà‡∏≠‡∏ô
            top_criteria_id=top_c.get('id', 'N/A'),
            top_criteria_name=top_c.get('name', 'N/A'),
            top_criteria_score=top_c.get('score', 0.0),
            top_criteria_summary=top_c.get('action_item', 'N/A'), # ‡πÉ‡∏ä‡πâ action_item ‡πÄ‡∏õ‡πá‡∏ô summary/description
            top_criteria_level=top_c.get('highest_full_level', 0),
            
            bottom_criteria_id=bottom_c.get('id', 'N/A'),
            bottom_criteria_name=bottom_c.get('name', 'N/A'),
            bottom_criteria_score=bottom_c.get('score', 0.0),
            bottom_criteria_summary=bottom_c.get('action_item', 'N/A'), # ‡πÉ‡∏ä‡πâ action_item ‡πÄ‡∏õ‡πá‡∏ô summary/description
            bottom_criteria_level=bottom_c.get('highest_full_level', 0),
            
            top_goals=formatted_goals
        )
        
        # 2. CALL REAL LLM FUNCTION (sending System Instruction)
        real_llm_response_text = generate_narrative_report_via_llm_real(
            prompt_text=final_prompt_text,
            system_instruction=SYSTEM_NARRATIVE_PROMPT 
        )
        
        # 3. ‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏à‡∏≤‡∏Å LLM ‡∏à‡∏£‡∏¥‡∏á
        # Remove any unwanted triple backticks or leading/trailing whitespace
        final_report = real_llm_response_text.strip().replace("```markdown", "").replace("```", "")
        return final_report.strip()

    except Exception as e:
        logger.error(f"Error during LLM narrative generation: {e}", exc_info=True)
        return f"ERROR: ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡πÄ‡∏ä‡∏¥‡∏á‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢: {e}"

# -------------------- STRUCTURED MARKDOWN FUNCTION --------------------
def generate_feedback_report(summary: Dict[str, Any]) -> str:
    """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á Markdown ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ú‡∏π‡πâ‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏á‡∏≤‡∏ô (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ LLM)"""
    report_parts = []
    
    sub_breakdown = summary.get('SubCriteria_Breakdown', {})
    action_plans_raw = summary.get('Action_Plans', {})
    
    # Flatten Action Plans for easy lookup by Sub_ID
    action_plans = {}
    for sub_list in action_plans_raw.values():
        for plan in sub_list:
            # Try to extract the Sub ID from the first action's Statement_ID
            first_statement_id = plan.get('Actions', [{}])[0].get('Statement_ID', 'N/A')
            parts = first_statement_id.split(' ')
            sub_id = parts[0] if '.' in parts[0] else None 

            if sub_id:
                action_plans[sub_id] = plan
            elif plan.get('Actions') and plan['Actions'][0].get('Failed_Level'):
                for key, value in action_plans_raw.items():
                    if value == sub_list:
                        action_plans[key] = plan
                        break


    passed_subs = {}
    failed_subs = {}

    for sub_id, data in sub_breakdown.items():
        highest_full_level = data.get('highest_full_level', 0)
        
        if highest_full_level >= 1:
            passed_subs[sub_id] = data
        else:
            failed_subs[sub_id] = data

    report_parts.append("\n## 1. ‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô (Highest Full Level >= 1)")
    report_parts.append("‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÅ‡∏•‡πâ‡∏ß")
    report_parts.append("| ‡∏£‡∏´‡∏±‡∏™ | ‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠ | ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô (Score) | Level ‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏ï‡πá‡∏° | ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Level ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ |")
    report_parts.append("| :---: | :--- | :---: | :---: | :--- |")
    
    for sub_id, data in passed_subs.items():
        highest_full = data.get('highest_full_level', 0)
        score = data.get('score', 0.0)
        action_detail = "‡∏î‡∏π‡πÅ‡∏ú‡∏ô‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡πÇ‡∏î‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô Action Plans"
        
        # Lookup the relevant Action Plan goal
        if action_plans.get(sub_id):
            action_detail = action_plans[sub_id].get('Goal', action_detail)
            
        action_detail = action_detail.replace('MOCK:', '').strip()
        if len(action_detail) > 100:
            action_detail = action_detail[:97] + "..."
            
        report_parts.append(f"| {sub_id} | {data.get('name')} | {score:.2f} | L{highest_full} | {action_detail} |")

    report_parts.append("\n\n## 2. ‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏£‡πà‡∏á‡∏û‡∏±‡∏í‡∏ô‡∏≤ (Highest Full Level = 0)")
    report_parts.append("‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå Level 1 ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏£‡πà‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏£‡∏∞‡∏ö‡∏ö‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô")
    report_parts.append("| ‡∏£‡∏´‡∏±‡∏™ | ‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠ | ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô (Score) | ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ö‡∏£‡∏£‡∏•‡∏∏ Level 1 |")
    report_parts.append("| :---: | :--- | :---: | :--- |")
    
    for sub_id, data in failed_subs.items():
        score = data.get('score', 0.0)
        recommendation = data.get('action_item', '‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏£‡∏∏‡∏õ.')
        
        # Lookup the first specific action for Level 1
        if action_plans.get(sub_id) and action_plans[sub_id].get('Actions'):
            first_action = action_plans[sub_id]['Actions'][0]
            # Use the detailed recommendation from the Action Plan
            rec_text = first_action.get('Recommendation', recommendation)
            recommendation = f"‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Statement {first_action.get('Statement_ID')} ‡πÄ‡∏û‡∏∑‡πà‡∏≠: {rec_text}"
            
        recommendation = recommendation.replace('MOCK:', '').strip()
        if len(recommendation) > 120:
            recommendation = recommendation[:117] + "..."
            
        report_parts.append(f"| {sub_id} | {data.get('name')} | {score:.2f} | {recommendation} |")

    return "\n".join(report_parts)


# -------------------- CLI Entry Point --------------------

def run_report_from_json(file_path: str, use_narrative_llm: bool, output_file: str = None):
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏±‡∏ô‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡πÇ‡∏î‡∏¢‡∏£‡∏±‡∏ö path ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå JSON ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)"""
    try:
        if not os.path.exists(file_path):
             raise FileNotFoundError(f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà Path: {file_path}")

        with open(file_path, 'r', encoding='utf-8') as f:
            summary_data = json.load(f)
            
        print(f"\n‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå: {file_path}")
        
        # ‡∏û‡∏¥‡∏°‡∏û‡πå‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏£‡∏ß‡∏°‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô
        overall = summary_data.get('Overall', {})
        if overall:
            print("=====================================================")
            print("        OVERALL ASSESSMENT SUMMARY")
            print(f"Enabler: {overall.get('enabler', 'N/A')}")
            print(f"Overall Maturity Score (Avg.): {overall.get('overall_maturity_score', 0.0):.2f}")
            print(f"Total Score (Weighted): {overall.get('total_weighted_score', 0.0):.2f}/{overall.get('total_possible_weight', 0.0):.2f}")
            print(f"Progress: {overall.get('overall_progress_percent', 0.0):.2f}%")
            print("=====================================================")
        
        # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏´‡∏°‡∏î‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô
        if use_narrative_llm:
            logger.info("Generating narrative report using LLM...")
            # üö® LLM Call ‡∏ñ‡∏π‡∏Å‡∏¢‡πâ‡∏≤‡∏¢‡∏°‡∏≤‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà üö®
            final_report_text = generate_narrative_report_via_llm(summary_data) 
            report_type = "LLM NARRATIVE (CEO Format)"
        else:
            logger.info("Generating structured markdown report...")
            final_report_text = generate_feedback_report(summary_data)
            report_type = "STRUCTURED MARKDOWN (Operator Format)"
            
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏û‡∏¥‡∏°‡∏û‡πå Feedback Report ‡∏•‡∏á console
        print(f"\n\n=====================================================")
        print(f"      FINAL FEEDBACK REPORT ({report_type})")
        print("=====================================================")
        print(final_report_text)
        print("\n=====================================================")

        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå
        if output_file:
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á directory ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ
            os.makedirs(os.path.dirname(output_file) or '.', exist_ok=True)
            with open(output_file, 'w', encoding='utf-8') as outfile:
                outfile.write(final_report_text)
            print(f"\n‚úÖ SUCCESS: ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô ({report_type}) ‡∏•‡∏á‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå: {output_file}")


    except FileNotFoundError as e:
        print(f"‚ùå ERROR: {e}", file=sys.stderr)
    except json.JSONDecodeError:
        print(f"‚ùå ERROR: ‡πÑ‡∏ü‡∏•‡πå {file_path} ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö JSON ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á", file=sys.stderr)
    except Exception as e:
        print(f"‚ùå ERROR: ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô: {e}", file=sys.stderr)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Generate a Feedback Report from a saved Assessment JSON file.")
    parser.add_argument("--file", 
                        type=str, 
                        required=True,
                        help="Path to the assessment result JSON file.")
    
    parser.add_argument("--narrative", 
                        action="store_true",
                        help="Generate a human-readable, narrative summary using LLM for CEO.")
    
    parser.add_argument("--output", 
                        type=str,
                        default=None,
                        help="Optional path to save the generated report to a file (e.g., reports/final_summary.txt).")
    
    args = parser.parse_args()
    run_report_from_json(args.file, args.narrative, args.output)
