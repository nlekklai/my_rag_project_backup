# core/ingest.py
# ‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏ï‡πá‡∏°: Multi-Tenant + Multi-Year (‡∏£‡∏±‡∏ê‡∏ß‡∏¥‡∏™‡∏≤‡∏´‡∏Å‡∏¥‡∏à‡πÑ‡∏ó‡∏¢ Ready)
# ‡∏£‡∏ß‡∏°‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: Path Isolation, get_vectorstore, ingest_all_files, list_documents, wipe_vectorstore

import os
import re
import sys
import logging
import unicodedata
import json
import uuid
import glob
import hashlib
import shutil
from datetime import datetime, timezone
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Optional, Set, Iterable, Dict, Any, Union, Tuple, TypedDict, Literal # üü¢ FIX: ‡πÄ‡∏û‡∏¥‡πà‡∏° Literal
import pandas as pd
import numpy as np
from pydantic import ValidationError
from collections import defaultdict # üü¢ FIX 1: ‡πÄ‡∏û‡∏¥‡πà‡∏° defaultdict


# LangChain loaders
from langchain_community.document_loaders import (
    PyPDFLoader,
    UnstructuredPDFLoader,
    CSVLoader,
    UnstructuredWordDocumentLoader,
    UnstructuredExcelLoader,
    TextLoader,
    UnstructuredPowerPointLoader,
    UnstructuredFileLoader
)

import fitz  # PyMuPDF

from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter 

# üí° ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: ‡πÉ‡∏ä‡πâ langchain_chroma ‡πÅ‡∏•‡∏∞ langchain_huggingface ‡πÅ‡∏ó‡∏ô
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings

try:
    from langchain_community.vectorstores.utils import filter_complex_metadata as _imported_filter_complex_metadata
except ImportError:
    _imported_filter_complex_metadata = None


# -------------------- Global Config --------------------
# üìå ASSUME: config.global_vars ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ
from config.global_vars import (
    SUPPORTED_TYPES,
    SUPPORTED_DOC_TYPES,
    DEFAULT_ENABLER,
    SUPPORTED_ENABLERS,
    EVIDENCE_DOC_TYPES,
    DEFAULT_DOC_TYPES,
    CHUNK_OVERLAP,
    CHUNK_SIZE,
    DEFAULT_TENANT, 
    DEFAULT_YEAR,
    EMBEDDING_MODEL_NAME,
    DATA_STORE_ROOT,
    SUPPORTED_DOC_TYPES
)

# -------------------- [NEW] Import Path Utilities --------------------
from utils.path_utils import (
    get_document_source_dir,
    get_doc_type_collection_key,
    get_vectorstore_collection_path,
    get_mapping_file_path,
    get_vectorstore_tenant_root_path, # ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö wipe
    get_evidence_mapping_file_path, # ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Evidence Map
    load_doc_id_mapping,
    save_doc_id_mapping,
    # üí° FIX: ‡πÄ‡∏û‡∏¥‡πà‡∏° load/save_evidence_mapping
    load_evidence_mapping,
    save_evidence_mapping,
    get_normalized_metadata,
    create_stable_uuid_from_path,
    parse_collection_name,
    get_mapping_tenant_root_path,
    _update_evidence_mapping,
    get_mapping_key_from_physical_path
)
# ---------------------------------------------------------------------

# Logging
logging.basicConfig(
    filename="ingest.log",
    level=logging.DEBUG, 
    format="%(asctime)s | %(levelname)s | %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger(__name__)

try:
    import pytesseract
    # üìå Comment out or adjust path based on target OS
    # pytesseract.pytesseract.tesseract_cmd = '/opt/homebrew/bin/tesseract' 
    logger.info("‚úÖ Pytesseract module loaded.")
except ImportError:
    logger.warning("Pytesseract not installed. Tesseract OCR may fail.")
except Exception as e:
    logger.error(f"Failed to set pytesseract path: {e}")

# --- Document Info Model ---
class DocInfo(TypedDict):
    doc_id: str             # Stable UUID
    doc_id_key: str         # Filename ID Key (normalized name)
    filename: str
    filepath: str
    doc_type: str           # Collection name (e.g., 'document', 'evidence')
    enabler: Optional[str]  # Enabler code (e.g., 'KM')
    upload_date: str        # ISO format
    chunk_count: int
    status: str             # "Ingested" | "Pending" | "Error"
    size: int               # File size in bytes

# -------------------- Log Noise Suppression (NEW) --------------------
import warnings

warnings.filterwarnings(
    "ignore", 
    "Cannot set gray non-stroke color because", 
    category=UserWarning,
    module='pdfminer' 
)
logging.getLogger('pdfminer').setLevel(logging.ERROR)
logging.getLogger('pdfminer.pdfinterp').setLevel(logging.ERROR)
logging.getLogger('unstructured').setLevel(logging.ERROR)
logging.getLogger('pypdf').setLevel(logging.ERROR)

# -------------------- [REMOVED/REPLACED] Path Builders --------------------
# üìå ‡∏ñ‡∏π‡∏Å‡∏•‡∏ö‡∏≠‡∏≠‡∏Å‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏ñ‡∏π‡∏Å‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏î‡πâ‡∏ß‡∏¢‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏à‡∏≤‡∏Å utils/path_utils.py:
# build_tenant_base_path
# get_collection_parent_dir
# get_target_dir (‡∏ñ‡∏π‡∏Å‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏î‡πâ‡∏ß‡∏¢ get_doc_type_collection_key)
# _get_source_dir (‡∏ñ‡∏π‡∏Å‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏î‡πâ‡∏ß‡∏¢ get_document_source_dir)
# --------------------------------------------------------------------------

def _parse_collection_name(
    collection_name: str, 
) -> Tuple[str, Optional[str]]:
    """
    Parses a collection name back into doc_type and enabler, handling both 
    Multi-Tenant/Year structure (Fallback) and the simple structure.
    
    Collection IDs ‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á: 'evidence_km', 'document'
    """
    collection_name_lower = collection_name.lower()
    
    # üìå NEW: ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Prefix 'rag_' ‡∏Å‡πà‡∏≠‡∏ô (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ô‡πÑ‡∏î‡πâ‡∏Å‡∏±‡∏ö VSM)
    if collection_name_lower.startswith("rag_"):
         collection_name_lower = collection_name_lower[4:]

    # 1. ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö DocType_Enabler (‡πÄ‡∏ä‡πà‡∏ô evidence_km)
    if collection_name_lower.startswith(f"{EVIDENCE_DOC_TYPES.lower()}_"):
        # Split ‡πÅ‡∏Ñ‡πà‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß: evidence_km -> ['evidence', 'km']
        parts_old = collection_name_lower.split("_", 1) 
        if len(parts_old) == 2:
            doc_type = parts_old[0]
            enabler_candidate = parts_old[1].upper()
            
            if enabler_candidate in SUPPORTED_ENABLERS: # üéØ FIX: ‡πÄ‡∏ä‡πá‡∏Ñ Enabler ‡∏Å‡∏±‡∏ö Global List
                 return doc_type, enabler_candidate
        
    # 2. ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö DocType (‡πÄ‡∏ä‡πà‡∏ô document)
    if collection_name_lower in [dt.lower() for dt in SUPPORTED_DOC_TYPES]:
        return collection_name_lower, None
        
    # 3. Fallback to the original name if no match is found (‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô Doc Type ‡∏ê‡∏≤‡∏ô)
    return collection_name_lower, None


# -------------------- Helper: safe metadata filter --------------------
def _safe_filter_complex_metadata(meta: Any) -> Dict[str, Any]:
    """Ensure metadata is serializable and safe for Chroma / storage. (No Change)"""
    if not isinstance(meta, dict):
        if hasattr(meta, "items"):
            meta_dict = dict(meta.items())
        else:
            return {} 
    else:
        meta_dict = meta
        
    clean = {}

    for k, v in meta_dict.items():
        if v is None:
            continue
            
        if isinstance(v, (str, int, float, bool)):
            clean[k] = v
        elif isinstance(v, dict):
            try:
                clean[k] = json.dumps(v)
            except TypeError:
                clean[k] = str(v)
                
        elif isinstance(v, (list, tuple)):
            if len(v) == 1:
                item = v[0]
                if isinstance(item, (str, int, float, bool)):
                    clean[k] = item 
                    continue 
                elif isinstance(item, (np.floating, np.integer)):
                    clean[k] = item.item() 
                    continue 
            
            try:
                clean[k] = json.dumps([str(x) for x in v]) 
            except Exception:
                clean[k] = str(v)
                
        elif isinstance(v, (np.floating, np.integer)):
            clean[k] = v.item()
        else:
            try:
                clean[k] = str(v)
            except Exception:
                continue
                
    if _imported_filter_complex_metadata:
        try:
            return _imported_filter_complex_metadata(clean)
        except Exception as e:
            logger.debug(f"LangChain filter failed after local cleanup: {e}")
            pass

    return clean

# -------------------- Normalization utility --------------------
def _normalize_doc_id(raw_id: str, file_content: bytes = None) -> str:
    """Generates the 34-character reference ID Key. (No Change)"""
    normalized = re.sub(r'[^a-zA-Z0-9]', '', raw_id).lower()
    if len(normalized) > 28:
        normalized = normalized[:28]
    hash_suffix = '000000'
    if file_content:
        hash_suffix = hashlib.sha1(file_content).hexdigest()[:6]
    final_id = (normalized + hash_suffix).ljust(34, '0')
    return final_id

# -------------------- Text Cleaning --------------------
def clean_text(text: str) -> str:
    """Basic text cleaning utility. (No Change in Logic)"""
    if not text: return ""
    text = text.replace('\xa0', ' ').replace('\u200b', '').replace('\u00ad', '')
    text = re.sub(r'[\uFFFD\u2000-\u200F\u2028-\u202F\u2060-\u206F\uFEFF]', '', text)
    text = re.sub(r'([‡∏Å-‡πô])\s{1,3}(?=[‡∏Å-‡πô])', r'\1', text) 
    ocr_replacements = {"‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏ô": "‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô", "‡∏Ñ‡∏ì‡∏∞‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏£": "‡∏Ñ‡∏ì‡∏∞‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£"}
    for bad, good in ocr_replacements.items(): text = text.replace(bad, good)
    text = re.sub(r'[^\x09\x0A\x0D\x20-\x7E\u0E00-\u0E7F]', '', text) 
    text = re.sub(r'\(\s+', '(', text); text = re.sub(r'\s+\)', ')', text)
    text = re.sub(r'\r\n', '\n', text); text = re.sub(r'\n{3,}', '\n\n', text)
    text = re.sub(r'[ \t]{2,}', ' ', text); text = re.sub(r'\s{2,}', ' ', text)
    return text.strip()


def _is_pdf_image_only(file_path: str) -> bool:
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ PDF ‡πÄ‡∏õ‡πá‡∏ô image-only ‡∏´‡∏£‡∏∑‡∏≠‡∏°‡∏µ text layer (No Change)"""
    try:
        doc = fitz.open(file_path)
        for page in doc:
            text = page.get_text().strip()
            if text:
                return False  
        return True  
    except Exception as e:
        logger.warning(f"Cannot check PDF text layer for {file_path}: {e}")
        return True  


def _load_document_with_loader(file_path: str, loader_class: Any) -> List[Document]:
    """Helper function to load a document using a specific LangChain loader class. (Modified for Image Fallback)"""
    raw_docs: List[Any] = [] 
    ext = "." + file_path.lower().split('.')[-1]
    
    # --- 1. Handle Known Loaders (CSV) ---
    if loader_class is CSVLoader:
        try:
            # üí° FIX: ‡πÄ‡∏û‡∏¥‡πà‡∏° csv_args={"delimiter": "|"} ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ Pipe
            # üìå ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå FAQ .csv ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ | ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÅ‡∏ö‡πà‡∏á‡πÅ‡∏ó‡∏ô ,
            loader = loader_class(
                file_path, 
                encoding='utf-8', 
                csv_args={"delimiter": "|"} 
            )
            raw_docs = loader.load()
        except Exception as e:
            logger.error(f"‚ùå LOADER FAILED: CSVLoader for {os.path.basename(file_path)} raised: {type(e).__name__} ({e})")
            return []
    
    # --- 2. Handle PDF (Text/Image-Only) ---
    elif ext == ".pdf":
        try:
            if _is_pdf_image_only(file_path):
                logger.info(f"PDF is image-only, using OCR loader: {file_path}")
                # üìå FIX: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô mode="elements" ‡πÄ‡∏õ‡πá‡∏ô mode="single" ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Image-Only PDF
                loader = UnstructuredFileLoader(file_path, mode="single", languages=['tha','eng'])
            else:
                logger.info(f"PDF has text layer, using PyPDFLoader: {file_path}")
                # PyPDFLoader ‡∏°‡∏±‡∏Å‡∏à‡∏∞‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö PDF ‡∏ó‡∏µ‡πà‡∏°‡∏µ text layer 
                loader = PyPDFLoader(file_path) 
            raw_docs = loader.load()
        except Exception as e:
             logger.error(f"‚ùå LOADER FAILED: PDF Loader for {os.path.basename(file_path)} raised: {type(e).__name__} ({e})")
             return []
    
    # --- 3. Handle Images (JPG, PNG) with Fallback ---
    elif ext in [".jpg", ".jpeg", ".png"]:
        
        # 3.1 Primary Attempt: UnstructuredFileLoader (Robust OCR, but can fail with TypeError)
        try:
            logger.info(f"Reading image file using UnstructuredFileLoader (Primary OCR): {file_path} ...")
            
            # üìå FIX 1: ‡πÉ‡∏ä‡πâ mode="elements" (‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î) ‡πÅ‡∏•‡∏∞ languages
            loader = UnstructuredFileLoader(file_path, mode="elements", languages=['tha','eng']) 
            raw_docs = loader.load()
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏î‡πâ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏£‡∏¥‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            if any(doc.page_content and doc.page_content.strip() for doc in raw_docs):
                return raw_docs
            
            # ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ (OCR failed silently), ‡∏•‡∏≠‡∏á Fallback
            raise RuntimeError("Unstructured OCR failed to extract text content.") 
            
        except Exception as primary_e:
            
            # üìå FIX 3: ‡πÉ‡∏ä‡πâ UnstructuredFileLoader (mode="single") ‡πÄ‡∏õ‡πá‡∏ô Fallback OCR
            try:
                logger.warning(
                    f"‚ö†Ô∏è Primary image loader failed with {type(primary_e).__name__}. "
                    f"Falling back to simpler UnstructuredFileLoader (mode='single') for {os.path.basename(file_path)}."
                )
                # ‡πÉ‡∏ä‡πâ mode="single" ‡∏ã‡∏∂‡πà‡∏á‡∏á‡πà‡∏≤‡∏¢‡∏Å‡∏ß‡πà‡∏≤ mode="elements" 
                loader = UnstructuredFileLoader(file_path, mode="single", languages=['tha','eng']) 
                raw_docs = loader.load()
                
                if raw_docs and raw_docs[0].page_content.strip():
                     logger.info("‚úÖ Fallback Unstructured OCR (single mode) successful.")
                     return raw_docs

            except Exception as fallback_e:
                logger.error(
                    f"‚ùå FALLBACK FAILED: Simpler Unstructured OCR also failed for {os.path.basename(file_path)} "
                    f"with {type(fallback_e).__name__}."
                )
                return [] # Image file fully failed to load
            
            # ‡∏ñ‡πâ‡∏≤ Fallback ‡πÅ‡∏•‡πâ‡∏ß‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ ‡∏Å‡πá‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á
            return []

    # --- 4. Handle Other File Types ---
    else:
        try:
            loader = loader_class(file_path)
            raw_docs = loader.load()
        except Exception as e:
            loader_name = getattr(loader_class, '__name__', 'UnknownLoader')
            logger.error(f"‚ùå LOADER FAILED: {os.path.basename(file_path)} - {loader_name} raised: {type(e).__name__} ({e})")
            return []
        
    
    # --- 5. Post-Processing & Filtering ---
    if raw_docs:
        original_count = len(raw_docs)
        
        filtered_docs = [
            doc for doc in raw_docs 
            if isinstance(doc, Document) and doc.page_content is not None and doc.page_content.strip()
        ]
        
        if len(filtered_docs) < original_count:
            logger.warning(
                f"‚ö†Ô∏è Loader returned {original_count - len(filtered_docs)} empty/None documents "
                f"for {os.path.basename(file_path)}. Filtered to {len(filtered_docs)} valid documents."
            )
        
        if not filtered_docs and original_count > 0:
             logger.warning(f"‚ö†Ô∏è Loader returned documents but all were empty/invalid for {os.path.basename(file_path)}. Returning 0 valid documents.")
             return []
             
        return filtered_docs
    
    return [] # Return empty list if raw_docs is empty (e.g., file was empty or loading failed silently)

# -------------------- Loaders --------------------
FILE_LOADER_MAP = {
    ".pdf": lambda p: _load_document_with_loader(p, UnstructuredFileLoader), 
    ".docx": lambda p: _load_document_with_loader(p, UnstructuredWordDocumentLoader),
    ".txt": lambda p: _load_document_with_loader(p, TextLoader),
    ".xlsx": lambda p: _load_document_with_loader(p, UnstructuredExcelLoader),
    ".pptx": lambda p: _load_document_with_loader(p, UnstructuredPowerPointLoader),
    ".md": lambda p: _load_document_with_loader(p, TextLoader), 
    ".csv": lambda p: _load_document_with_loader(p, CSVLoader),
    ".jpg": lambda p: _load_document_with_loader(p, UnstructuredFileLoader), 
    ".jpeg": lambda p: _load_document_with_loader(p, UnstructuredFileLoader),
    ".png": lambda p: _load_document_with_loader(p, UnstructuredFileLoader),
}

# -------------------- Normalization utility --------------------
def normalize_loaded_documents(raw_docs: List[Any], source_path: Optional[str] = None) -> List[Document]:
    """Converts raw loaded documents into clean LangChain Document objects. (No Change)"""
    normalized: List[Document] = []
    for idx, item in enumerate(raw_docs):
        try:
            if isinstance(item, Document): doc = item
            else: doc = Document(page_content=str(item), metadata={})
            
            doc.page_content = unicodedata.normalize("NFKC", doc.page_content or "").strip() 
            
            if not doc.page_content:
                logger.warning(f"‚ö†Ô∏è Doc #{idx} from loader has no content (Empty/None). Skipping normalization for this document.")
                continue 
            
            if not isinstance(doc.metadata, dict): doc.metadata = {"_raw_meta": str(doc.metadata)}
            if source_path: doc.metadata.setdefault("source_file", os.path.basename(source_path))
            try: doc.metadata = _safe_filter_complex_metadata(doc.metadata)
            except Exception: doc.metadata = {"source_file": os.path.basename(source_path)} if source_path else {}
            normalized.append(doc)
            
        except Exception as e:
            logger.warning(f"normalize_loaded_documents: skipping item #{idx} due to error: {e}")
            continue
    return normalized

# üìå Global Text Splitter Configuration (No Change)
TEXT_SPLITTER = RecursiveCharacterTextSplitter(
    chunk_size=CHUNK_SIZE,              
    chunk_overlap=CHUNK_OVERLAP,            
    separators=[
        "\n\n",                   
        "\n- ",                   
        "\n‚Ä¢ ",                   
        " ",                      
        ""
    ]   ,
    length_function=len,
    is_separator_regex=False
)

# ------------------------------------------------------------------
# SE-AM Sub-topic Mapping (‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤ 3-15 ‡∏Ç‡∏≠‡∏á SE-AM Manual Book 2566)
# ------------------------------------------------------------------
SEAM_SUBTOPIC_MAP = {
    # CG
    "1.1": "CG-1.1", "1-1": "CG-1.1",
    # SP
    "2.1": "SP-2.1", "2-1": "SP-2.1",
    # RM&IC
    "3.1": "RMIC-3.1", "3-1": "RMIC-3.1",
    # SCM
    "4.1": "SCM-4.1", "4-1": "SCM-4.1",
    # DT
    "5.1": "DT-5.1", "5-1": "DT-5.1",
    # HCM
    "6.1": "HCM-6.1", "6-1": "HCM-6.1", "6.2": "HCM-6.2", "6.3": "HCM-6.3", "6.4": "HCM-6.4",
    "6.5": "HCM-6.5", "6.6": "HCM-6.6", "6.7": "HCM-6.7",
    # KM & IM
    "7.1": "KM-7.1", "7-1": "KM-7.1",
    "7.20": "IM-7.20", "7-20": "IM-7.20",
    # IA
    "8.1": "IA-8.1", "8-1": "IA-8.1",
}

# Keywords ‡∏ó‡∏µ‡πà‡∏ö‡πà‡∏á‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ï‡πà‡∏≤‡∏á ‡πÜ
LEVEL_KEYWORDS = ["‡∏£‡∏∞‡∏î‡∏±‡∏ö 1", "‡∏£‡∏∞‡∏î‡∏±‡∏ö 2", "‡∏£‡∏∞‡∏î‡∏±‡∏ö 3", "‡∏£‡∏∞‡∏î‡∏±‡∏ö 4", "‡∏£‡∏∞‡∏î‡∏±‡∏ö 5"]

def _detect_sub_topic_and_page(text: str) -> Dict[str, Any]:
    """
    ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö sub_topic ‡πÅ‡∏•‡∏∞ page number ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ç‡∏≠‡∏á chunk
    """
    result = {"sub_topic": None, "page_number": None}

    # 1. ‡∏à‡∏±‡∏ö page number (‡πÄ‡∏ä‡πà‡∏ô "‡∏´‡∏ô‡πâ‡∏≤ 1-1", "‡∏´‡∏ô‡πâ‡∏≤ 243")
    page_match = re.search(r'‡∏´‡∏ô‡πâ‡∏≤\s*(\d+(?:-\d+)?)', text)
    if page_match:
        result["page_number"] = page_match.group(1)

    # 2. ‡∏à‡∏±‡∏ö sub_topic ‡πÄ‡∏ä‡πà‡∏ô "4.1", "7-20", "KM topic 4.1"
    for pattern, code in [
        (r'(?:KM|topic)?\s*(\d+\.\d+)', None),
        (r'(\d+-\d+)', None),
        (r'(\d+\.\d+)', None),
    ]:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            key = match.group(1).replace("-", ".")
            if key in SEAM_SUBTOPIC_MAP:
                result["sub_topic"] = SEAM_SUBTOPIC_MAP[key]
                break

    # 3. ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á‡∏à‡∏±‡∏ö‡∏à‡∏≤‡∏Å‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÄ‡∏ï‡πá‡∏° (‡πÄ‡∏ä‡πà‡∏ô "4.1 ‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡∏ö‡∏ö")
    if not result["sub_topic"]:
        for key, code in SEAM_SUBTOPIC_MAP.items():
            if key.replace(".", "-") in text or key in text:
                result["sub_topic"] = code
                break

    return result


# ------------------------------------------------------------------
# load_and_chunk_document ‚Äì ‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏™‡∏∏‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö SE-AM (‡πÉ‡∏ä‡πâ Key ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß: chunk_uuid)
# ------------------------------------------------------------------
def load_and_chunk_document(
    file_path: str,
    stable_doc_uuid: str,
    doc_type: str,
    enabler: Optional[str] = None,
    subject: Optional[str] = None,
    year: Optional[int] = None,
    version: str = "v1",
    metadata: Optional[Dict[str, Any]] = None,
    ocr_pages: Optional[Iterable[int]] = None
) -> List[Document]:
    """
    Load + Clean + Chunk + ‡πÉ‡∏™‡πà sub_topic + page_number ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
    """
    
    file_extension = os.path.splitext(file_path)[1].lower()
    loader_func = FILE_LOADER_MAP.get(file_extension)
    
    if not loader_func:
        logger.error(f"No loader found for {file_extension}")
        return []

    # --- Load Document ---
    try:
        # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤ loader_func ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ OCR ‡πÅ‡∏•‡∏∞ Error ‡πÑ‡∏î‡πâ‡∏î‡∏µ
        raw_docs = loader_func(file_path)
    except Exception as e:
        # Handle exceptions including ValidationError (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
        logger.error(f"Load failed: {file_path} | {e}")
        raw_docs = []
        
    if not raw_docs:
        logger.warning(f"No content loaded from {os.path.basename(file_path)}")
        return []

    # --- Normalize to Document objects ---
    docs = []
    for doc in raw_docs:
        if isinstance(doc, Document):
            docs.append(doc)
        else:
            logger.warning(f"Non-Document object skipped: {type(doc)}")

    # --- Inject Base Metadata ---
    # base_metadata ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠ base_metadata ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏™‡πà‡∏á‡∏ú‡πà‡∏≤‡∏ô‡πÑ‡∏õ‡∏ó‡∏∏‡∏Å chunk
    base_metadata = {
        "doc_type": doc_type,
        "doc_id": stable_doc_uuid, # ‡πÉ‡∏ä‡πâ doc_id ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏±‡∏Å‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏ñ‡∏∂‡∏á Stable UUID
        "stable_doc_uuid": stable_doc_uuid, # ‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏î‡∏¥‡∏° (‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ)
        "source_filename": os.path.basename(file_path),
        "source": os.path.basename(file_path), # ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô Path ‡∏ó‡∏µ‡πà clean ‡∏Å‡∏ß‡πà‡∏≤‡∏ô‡∏µ‡πâ
        "version": version,
    }
    if enabler: base_metadata["enabler"] = enabler
    if subject: base_metadata["subject"] = subject.strip()
    if year: base_metadata["year"] = year
    
    # ‡∏£‡∏ß‡∏° injected_metadata ‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏°‡∏≤‡∏à‡∏≤‡∏Å process_document
    if metadata: 
        base_metadata.update(metadata) 

    for d in docs:
        d.metadata.update(base_metadata)
        d.metadata = _safe_filter_complex_metadata(d.metadata)

    # --- Split into chunks ---
    try:
        # TEXT_SPLITTER ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏õ‡πá‡∏ô LangChain RecursiveCharacterTextSplitter ‡∏´‡∏£‡∏∑‡∏≠ seggmenter ‡∏ó‡∏µ‡πà‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô
        chunks = TEXT_SPLITTER.split_documents(docs)
    except Exception as e:
        logger.error(f"Split failed: {e}")
        chunks = docs

    # --- Clean text & Inject per-chunk metadata ---
    final_chunks = []
    # üí° FIX: ‡πÉ‡∏ä‡πâ start=1 ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏° ‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ format string ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ index ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏Ñ‡∏á‡∏ó‡∏µ‡πà (‡πÄ‡∏ä‡πà‡∏ô 0001)
    for idx, chunk in enumerate(chunks, start=1):
        if not isinstance(chunk, Document):
            continue

        # Clean text
        chunk.page_content = clean_text(chunk.page_content)

        # Detect sub_topic & page_number
        detected = _detect_sub_topic_and_page(chunk.page_content)
        if detected["sub_topic"]:
            chunk.metadata["sub_topic"] = detected["sub_topic"]
        if detected["page_number"]:
            page_val = detected["page_number"]
            chunk.metadata["page_number"] = page_val
            chunk.metadata["page"] = f"P{page_val}"  # ‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏õ‡πá‡∏ô P45 ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ!

        # üü¢ CRITICAL FIX: ‡πÉ‡∏ä‡πâ Key "chunk_uuid" ‡πÄ‡∏õ‡πá‡∏ô Key ‡∏´‡∏•‡∏±‡∏Å‡πÄ‡∏û‡∏µ‡∏¢‡∏á Key ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
        chunk_id_prefix = stable_doc_uuid[:16] # ‡πÉ‡∏ä‡πâ‡πÅ‡∏Ñ‡πà 16 ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å‡∏Ç‡∏≠‡∏á UUID ‡∏Å‡πá‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠
        
        # 1. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î ID ‡∏´‡∏•‡∏±‡∏Å‡πÄ‡∏õ‡πá‡∏ô chunk_uuid
        unique_chunk_id = f"{chunk_id_prefix}-{idx:04d}" 
        chunk.metadata["chunk_uuid"] = unique_chunk_id
        
        # 2. ‚úÖ ‡∏•‡∏ö chunk_id ‡∏ó‡∏¥‡πâ‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ Key ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
        if "chunk_id" in chunk.metadata:
            del chunk.metadata["chunk_id"]
        
        chunk.metadata["chunk_index"] = idx
        
        chunk.metadata = _safe_filter_complex_metadata(chunk.metadata)

        final_chunks.append(chunk)

    logger.info(f"Loaded {os.path.basename(file_path)} ‚Üí {len(final_chunks)} chunks | "
                f"sub_topic detected: {len([c for c in final_chunks if c.metadata.get('sub_topic')])}")
    return final_chunks

# -------------------- [REVISED] Process single document (Cleaned & Final) --------------------
def process_document(
    file_path: str,
    file_name: str,
    stable_doc_uuid: str, 
    doc_type: Optional[str] = None,
    enabler: Optional[str] = None, 
    subject: Optional[str] = None,  
    base_path: str = "", # üí° FIX: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô Default ‡∏à‡∏≤‡∏Å VECTORSTORE_DIR ‡πÄ‡∏õ‡πá‡∏ô String ‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤
    year: Optional[int] = None,
    tenant: Optional[str] = None, 
    version: str = "v1",
    metadata: dict = None,
    source_name_for_display: Optional[str] = None,
    ocr_pages: Optional[Iterable[int]] = None
) -> Tuple[List[Document], str, str]: 
    
    # üìå ASSUME: _normalize_doc_id, DEFAULT_DOC_TYPES, EVIDENCE_DOC_TYPES, DEFAULT_ENABLER ‡∏ñ‡∏π‡∏Å Import ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
    raw_doc_id_input = os.path.splitext(file_name)[0]
    filename_doc_id_key = _normalize_doc_id(raw_doc_id_input) 
            
    doc_type = doc_type or DEFAULT_DOC_TYPES
    
    resolved_enabler = None
    if doc_type.lower() == EVIDENCE_DOC_TYPES.lower():
        resolved_enabler = (enabler or DEFAULT_ENABLER).upper()

    # üü¢ ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏° Metadata ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÑ‡∏ß‡πâ‡πÉ‡∏ô injected_metadata ‡∏ì ‡∏à‡∏∏‡∏î‡∏ô‡∏µ‡πâ
    injected_metadata = metadata or {}
    
    # 1. ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å Resolve
    injected_metadata["doc_type"] = doc_type
    injected_metadata["original_stable_id"] = filename_doc_id_key[:32].lower() # ‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô Reference ID
    
    if resolved_enabler:
        injected_metadata["enabler"] = resolved_enabler
    if tenant: 
        injected_metadata["tenant"] = tenant
        
    # üí° FIX: ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏° year ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô injected_metadata ‡∏î‡πâ‡∏ß‡∏¢ (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Ñ‡πà‡∏≤)
    if year is not None: 
        injected_metadata["year"] = year
        
    if subject: 
        injected_metadata["subject"] = subject
        
    filter_id_value = filename_doc_id_key 
    logger.critical(f"================== START DEBUG INGESTION: {file_name} ==================")
    logger.critical(f"üîç DEBUG ID (stable_doc_uuid, 64-char Hash): {len(stable_doc_uuid)}-char: {stable_doc_uuid[:34]}...")
    logger.critical(f"‚úÖ FINAL ID TO STORE (34-char Ref ID): {len(filter_id_value)}-char: {filter_id_value[:34]}...")

    # üéØ ‡∏™‡πà‡∏á Metadata ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ú‡πà‡∏≤‡∏ô dict ‡πÑ‡∏õ‡πÉ‡∏´‡πâ load_and_chunk_document
    chunks = load_and_chunk_document(
        file_path=file_path,
        stable_doc_uuid=stable_doc_uuid,
        doc_type=doc_type, 
        enabler=resolved_enabler, 
        subject=subject, 
        # base_path ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏™‡πà‡∏á‡πÅ‡∏•‡πâ‡∏ß ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
        version=version,
        metadata=injected_metadata, # <--- **‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:** ‡∏°‡∏µ year ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ô‡∏µ‡πâ‡πÅ‡∏•‡πâ‡∏ß
        ocr_pages=ocr_pages
    )
    
    if chunks:
         logger.debug(f"Chunk metadata preview: {chunks[0].metadata}")
        
    return chunks, stable_doc_uuid, doc_type

# -------------------- Vectorstore / Mapping Utilities --------------------
_VECTORSTORE_SERVICE_CACHE: dict = {}

def get_vectorstore(
    collection_name: str = "default",
    tenant: str = "pea",
    year: int = 2568,
    base_path: str = "" # üí° FIX: ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç VECTORSTORE_DIR ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ô‡∏¥‡∏¢‡∏≤‡∏° ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô String ‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤
) -> Chroma:
    """
    ‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô Multi-Tenant/Multi-Year ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ Path Utility ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Path
    """

    # === 1. ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏á ‡πÜ ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ï‡∏¥‡∏° prefix ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ ===
    if len(collection_name) < 3:
        logger.warning(
            f"Collection name '{collection_name}' ‡∏Ñ‡πà‡∏≠‡∏ô‡∏Ç‡πâ‡∏≤‡∏á‡∏™‡∏±‡πâ‡∏ô ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 6 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£ "
            f"(‡πÄ‡∏ä‡πà‡∏ô evidence_km, km42l103) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏ä‡∏ô‡∏Å‡∏±‡∏ô‡∏Ç‡∏≠‡∏á‡∏ä‡∏∑‡πà‡∏≠"
        )

    # === 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á path ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á PEA ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Path Utility ===
    try:
        # üéØ REVISED: ‡πÉ‡∏ä‡πâ parse_collection_name ‡∏à‡∏≤‡∏Å path_utils.py
        doc_type_for_path, enabler_for_path = parse_collection_name(collection_name)
        
        # üéØ FIX: ‡πÉ‡∏ä‡πâ get_vectorstore_collection_path ‡∏à‡∏≤‡∏Å path_utils.py
        persist_directory = get_vectorstore_collection_path(
            tenant=tenant,
            # Path Utility ‡∏à‡∏∞‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡πÉ‡∏ä‡πâ year/enabler ‡∏Å‡πá‡∏ï‡πà‡∏≠‡πÄ‡∏°‡∏∑‡πà‡∏≠ doc_type ‡πÄ‡∏õ‡πá‡∏ô Evidence
            year=year, 
            doc_type=doc_type_for_path,
            enabler=enabler_for_path
        )
        
    except Exception as e:
        logger.error(f"‚ùå Failed to generate vectorstore path using path_utils: {e}. Using simple fallback path.")
        # Fallback Path ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ Dependency ‡∏Å‡∏±‡∏ö Global Constant ‡πÄ‡∏î‡∏¥‡∏°
        persist_directory = os.path.join(tenant, str(year), collection_name)
        logger.warning(f"‚ö†Ô∏è Warning: Fallback path used. Result: {persist_directory}")

    cache_key = persist_directory

    # === 3. Cache HIT ===
    if cache_key in _VECTORSTORE_SERVICE_CACHE:
        logger.debug(f"Cache HIT ‚Üí Reusing vectorstore: {persist_directory}")
        return _VECTORSTORE_SERVICE_CACHE[cache_key]

    # === 4. Embedding model (‡πÅ‡∏ä‡∏£‡πå‡∏ï‡∏±‡∏ß‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏ï‡∏•‡∏≠‡∏î process) ===
    embeddings = _VECTORSTORE_SERVICE_CACHE.get("embeddings_model")

    if not embeddings:
        # üìå ASSUME: EMBEDDING_MODEL_NAME ‡∏ñ‡∏π‡∏Å Import ‡∏à‡∏≤‡∏Å config/global_vars
        logger.info(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î {EMBEDDING_MODEL_NAME} (SOTA Multilingual 2024) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á Retrieval")

        try:
            embeddings = HuggingFaceEmbeddings(
                model_name= EMBEDDING_MODEL_NAME,
                model_kwargs={
                    "device": "cpu", # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô "cuda" ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ GPU 
                },  
                encode_kwargs={
                    "normalize_embeddings": True, 
                    "batch_size": 32,
                }
            )
            _VECTORSTORE_SERVICE_CACHE["embeddings_model"] = embeddings
            logger.info(f"{EMBEDDING_MODEL_NAME} ‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡πÅ‡∏•‡∏∞‡πÅ‡∏ä‡∏£‡πå‡∏ï‡∏•‡∏≠‡∏î process")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to load {EMBEDDING_MODEL_NAME}: {e}")
            logger.warning("‚ö†Ô∏è Falling back to paraphrase-multilingual-MiniLM-L12-v2")
            # ‡πÉ‡∏ä‡πâ Fallback model ‡∏ï‡∏±‡∏ß‡πÄ‡∏î‡∏¥‡∏°
            embeddings = HuggingFaceEmbeddings(
                model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
                model_kwargs={"device": "cpu"}
            )
            _VECTORSTORE_SERVICE_CACHE["embeddings_model"] = embeddings


    # === 5. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÇ‡∏´‡∏•‡∏î Chroma ===
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á directory ‡∏ï‡∏≤‡∏° Path ‡∏ó‡∏µ‡πà‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏à‡∏≤‡∏Å Path Utility (‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á DATA_STORE_ROOT)
    os.makedirs(persist_directory, exist_ok=True) 

    vectorstore = Chroma(
        collection_name=collection_name,           # ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏î‡∏¥‡∏°‡∏ï‡∏£‡∏á ‡πÜ
        persist_directory=persist_directory,
        embedding_function=embeddings
    )

    _VECTORSTORE_SERVICE_CACHE[cache_key] = vectorstore

    logger.info(
        f"Vectorstore ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô!\n"
        f"   Collection  : {collection_name}\n"
        f"   Path        : {persist_directory}"
    )

    return vectorstore


# -------------------- Load / Save / Management Mapping DB --------------------

def _get_doc_map_key(doc_type: str, enabler: Optional[str]) -> str:
    """Helper for internal dict key for mapping DB."""
    doc_type_lower = doc_type.lower()
    if doc_type_lower == EVIDENCE_DOC_TYPES.lower() and enabler:
        return f"{doc_type_lower}_{enabler.upper()}"
    return doc_type_lower


_MAPPING_DB_CACHE: Dict[str, Dict[str, Any]] = {}

# -------------------- API Helper: Get UUIDs for RAG Filtering --------------------
# üìå REVISED: ‡πÄ‡∏û‡∏¥‡πà‡∏° tenant ‡πÅ‡∏•‡∏∞ year
def get_stable_uuids_by_doc_type(doc_types: List[str], tenant: str = "pwa", year: int = 2568) -> List[str]:
    """Retrieves Stable UUIDs for RAG filtering based on document types (Multi-Tenant/Year)."""
    if not doc_types: return []
    doc_type_set = {dt.lower() for dt in doc_types}
    target_uuids = []

    # ‡πÇ‡∏´‡∏•‡∏î mapping db ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö doc_types ‡∏ó‡∏µ‡πà‡∏£‡πâ‡∏≠‡∏á‡∏Ç‡∏≠
    for dt_req in doc_type_set:
        
        doc_mapping_db = load_doc_id_mapping(dt_req, tenant, year)
        
        for s_uuid, entry in doc_mapping_db.items():
            # NOTE: ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á Tenant/Year/Doc Type ‡∏ñ‡∏π‡∏Å‡∏ó‡∏≥‡πÉ‡∏ô load_doc_id_mapping ‡πÅ‡∏•‡πâ‡∏ß
            target_uuids.append(s_uuid)

    return list(set(target_uuids))

def ingest_all_files(
    doc_types: List[str],
    tenant: str = DEFAULT_TENANT,
    year: Optional[Union[str, int]] = None,
    enabler: Optional[str] = None,
    subject: Optional[str] = None,
    dry_run: bool = False,
    sequential: bool = False,
    skip_ext: Optional[List[str]] = None,
) -> Dict[str, Any]:
    logger.info("--- STARTING INGESTION PROCESS ---")
    import unicodedata
    from collections import defaultdict # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£ import defaultdict ‡πÉ‡∏ô core/ingest.py

    tenant_clean = unicodedata.normalize('NFKC', tenant.lower().replace(" ", "_"))

    files_to_process: List[Dict[str, Any]] = []
    context_to_files: Dict[Tuple[str, Optional[str], Optional[int]], List[Dict]] = defaultdict(list)

    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì context ‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏∏‡∏Å doc_type ‚Üê ‡∏à‡∏∏‡∏î‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
    for dt in doc_types:
        dt_lower = dt.lower()

        resolved_year, resolved_enabler = get_normalized_metadata(
            doc_type=dt_lower,
            year_input=year,
            enabler_input=enabler,
            default_enabler=DEFAULT_ENABLER,
        )

        # Evidence ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ year ‡πÄ‡∏™‡∏°‡∏≠
        if dt_lower == EVIDENCE_DOC_TYPES.lower() and resolved_year is None:
            logger.error("Evidence requires --year. Skipping evidence ingestion.")
            continue

        root_path = get_document_source_dir(tenant_clean, resolved_year, resolved_enabler, dt_lower)
        collection_name = get_doc_type_collection_key(dt_lower, resolved_enabler)

        logger.info(f" [SCAN] '{dt_lower}' ‚Üí Collection: {collection_name} | Path: {root_path}")

        if not os.path.exists(root_path):
            logger.warning(f"Directory not found: {root_path}")
            continue

        for root, dirs, files in os.walk(root_path):
            dirs[:] = [d for d in dirs if d not in ['.DS_Store', '__pycache__', 'backup']]
            for f in files:
                ext = os.path.splitext(f)[1].lower()
                if f.startswith('.') or ext not in SUPPORTED_TYPES:
                    continue
                if skip_ext and ext in skip_ext:
                    continue

                file_path_abs = os.path.join(root, f)

                # ‡∏™‡∏£‡πâ‡∏≤‡∏á UUID ‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡∏î‡πâ‡∏ß‡∏¢ NFKC + relative path
                # try:
                #     rel_path = os.path.relpath(file_path_abs, DATA_STORE_ROOT)
                # except ValueError:
                #     rel_path = file_path_abs
                # normalized_path = unicodedata.normalize('NFKC', rel_path)
                # stable_doc_uuid = create_stable_uuid_from_path(normalized_path)
    
                stable_doc_uuid = create_stable_uuid_from_path(
                    file_path_abs, 
                    tenant=tenant_clean, 
                    year=resolved_year, 
                    enabler=resolved_enabler
                )

                info = {
                    "file_path": file_path_abs,
                    "file_name": f,
                    "doc_type": dt_lower,
                    "enabler": resolved_enabler,
                    "year": resolved_year,
                    "tenant": tenant_clean,
                    "stable_doc_uuid": stable_doc_uuid,
                    "collection_name": collection_name,
                }

                files_to_process.append(info)
                context_to_files[(dt_lower, resolved_enabler, resolved_year)].append(info)

    if not files_to_process:
        logger.warning("--- NO FILES FOUND ---")
        return {"updated_count": 0}

    # ‡πÇ‡∏´‡∏•‡∏î mapping ‡∏ó‡∏∏‡∏Å context
    full_mapping: Dict[str, Dict] = {}
    for ctx in context_to_files:
        dt, ena, yr = ctx
        try:
            full_mapping.update(load_doc_id_mapping(dt, tenant_clean, yr, ena))
        except FileNotFoundError:
            pass

    # ‡∏Å‡∏£‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á ingest
    files_to_ingest = [
        f for f in files_to_process
        if f["stable_doc_uuid"] not in full_mapping
        or full_mapping[f["stable_doc_uuid"]].get("chunk_count", 0) == 0
    ]

    logger.info(f"Will ingest {len(files_to_ingest)} files.")

    # Process + Index (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°)
    chunks_by_collection: Dict[str, List[Document]] = defaultdict(list)
    results: List[Dict] = []

    def process_one(info: Dict):
        # NOTE: process_document ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏Å load_and_chunk_document ‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏•‡πâ‡∏ß
        chunks, doc_uuid, _ = process_document(
            file_path=info["file_path"],
            file_name=info["file_name"],
            stable_doc_uuid=info["stable_doc_uuid"],
            doc_type=info["doc_type"],
            enabler=info["enabler"],
            tenant=tenant_clean,
            year=info["year"],
            subject=subject,
        )
        if chunks:
            for c in chunks:
                c.metadata.update({"tenant": tenant_clean, "year": info["year"]})
            chunks_by_collection[info["collection_name"]].extend(chunks)
            results.append({"file": info["file_name"], "doc_id": doc_uuid, "chunks": len(chunks)})

    if dry_run:
        return {"updated_count": 0}
    elif sequential:
        for info in files_to_ingest:
            process_one(info)
    else:
        with ThreadPoolExecutor() as ex:
            list(ex.map(process_one, files_to_ingest))

    # Index
    for coll, chunks in chunks_by_collection.items():
        if not chunks:
            continue
        # 1. Initialize Vector Store
        # ‡∏î‡∏∂‡∏á‡∏õ‡∏µ‡∏à‡∏≤‡∏Å Chunk ‡πÅ‡∏£‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Vector Store (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
        vs = get_vectorstore(coll, tenant_clean, chunks[0].metadata.get("year")) 
        
        total_chunks = len(chunks)
        
        # 2. Batch Indexing
        for i in range(0, total_chunks, 500):
            batch = chunks[i:i+500]
            
            # --- ‚úÖ Log ---
            start_index = i + 1
            end_index = min(i + 500, total_chunks)
            logger.info(f"Indexing batch {start_index}-{end_index}/{total_chunks} chunks into Collection: {coll}")
            # ---------------------------
            
            vs.add_texts(
                texts=[c.page_content for c in batch],
                metadatas=[c.metadata for c in batch],
                # üéØ FIX 1: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏°‡∏≤‡πÉ‡∏ä‡πâ chunk_uuid
                ids=[c.metadata["chunk_uuid"] for c in batch], 
            )
        
        # Log ‡∏™‡∏£‡∏∏‡∏õ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏ö Collection ‡∏ô‡∏±‡πâ‡∏ô
        logger.info(f"‚úÖ Indexed {total_chunks} chunks successfully into Collection: {coll}")

    # Update mapping (‡πÅ‡∏¢‡∏Å context)
    updated = 0
    for ctx, infos in context_to_files.items():
        dt, ena, yr = ctx
        mapping = load_doc_id_mapping(dt, tenant_clean, yr, ena)

        for info in infos:
            chunks = [c for c in chunks_by_collection[info["collection_name"]]
                     if c.metadata.get("doc_id") == info["stable_doc_uuid"]]
            if not chunks:
                continue

            rel_path = unicodedata.normalize('NFKC',
                os.path.relpath(info["file_path"], DATA_STORE_ROOT))

            mapping[info["stable_doc_uuid"]] = {
                "doc_id": info["stable_doc_uuid"],
                "file_name": info["file_name"],
                "filepath": rel_path,
                "doc_type": dt,
                "enabler": ena,
                "tenant": tenant_clean,
                "year": yr,
                "chunk_count": len(chunks),
                "status": "Ingested",
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "size": os.path.getsize(info["file_path"]),
                # üéØ FIX 2: ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å chunk_uuids ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏•‡∏ö (Deletion)
                "chunk_uuids": [c.metadata["chunk_uuid"] for c in chunks], 
            }
            updated += 1

        save_doc_id_mapping(mapping, dt, tenant_clean, yr, enabler=ena)

    logger.info(f"--- INGESTION COMPLETE | Updated mapping: {updated} docs ---")
    return {"updated_count": updated, "results": results}

# -------------------- Wipe Vectorstore / Mapping --------------------
def wipe_vectorstore(
    doc_type_to_wipe: str = "all",
    enabler: Optional[str] = None,
    tenant: str = DEFAULT_TENANT,
    year: Union[int, str] = DEFAULT_YEAR, # ‡∏£‡∏±‡∏ö int ‡∏´‡∏£‡∏∑‡∏≠ str ‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤
    base_path: Optional[str] = None # ‡∏•‡∏ö‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ô‡∏µ‡πâ‡∏≠‡∏≠‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÇ‡∏Ñ‡πâ‡∏î clean ‡∏Ç‡∏∂‡πâ‡∏ô
) -> None:
    """
    Deletes the Vector Store collection(s) and associated mapping files.
    """
    logger.critical(f"‚ö†Ô∏è !!! ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£ WIPE Vectorstore ‡πÅ‡∏•‡∏∞ Mapping Files !!!")
    
    doc_type_to_wipe_lower = doc_type_to_wipe.lower()
    enabler_req = enabler.upper() if enabler else None
    
    # 1. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Doc Types ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö
    doc_types_to_check: List[str] = []
    supported_doc_types_lower = [dt.lower() for dt in SUPPORTED_DOC_TYPES]
    
    if doc_type_to_wipe_lower == 'all':
        doc_types_to_check.extend(supported_doc_types_lower)
    elif doc_type_to_wipe_lower in supported_doc_types_lower:
        doc_types_to_check.append(doc_type_to_wipe_lower)
    else:
        logger.warning(f"‚ö†Ô∏è Invalid Doc Type '{doc_type_to_wipe}'. Skipping wipe.")
        return

    # 1a. ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ó‡∏µ‡πà Normalize ‡πÅ‡∏•‡πâ‡∏ß‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ Collection
    collections_to_delete: Set[Tuple[str, Optional[Union[str, int]], Optional[str], str]] = set() 
    
    year_int: Optional[int]
    try:
        year_int = int(year) if year is not None and str(year).isdigit() else None
    except ValueError:
        year_int = None
        
    for dt in doc_types_to_check:
        
        is_evidence = dt == EVIDENCE_DOC_TYPES.lower()
        
        enablers_to_iterate: List[Optional[str]] = []
        
        if is_evidence:
            if enabler_req:
                enablers_to_iterate.append(enabler_req)
            elif doc_type_to_wipe_lower == 'all' or (doc_type_to_wipe_lower == EVIDENCE_DOC_TYPES.lower() and not enabler_req):
                # ‡∏ñ‡πâ‡∏≤‡∏•‡πâ‡∏≤‡∏á 'all' ‡∏´‡∏£‡∏∑‡∏≠ 'evidence' ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ‡πÉ‡∏´‡πâ‡∏ß‡∏ô‡∏•‡∏π‡∏õ Enablers ‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö
                enablers_to_iterate.extend(SUPPORTED_ENABLERS)
        else:
            # Global Doc Types ‡∏™‡∏ô‡πÉ‡∏à‡πÅ‡∏Ñ‡πà None
            enablers_to_iterate.append(None) 
            
        enablers_to_iterate = list(set(enablers_to_iterate)) # ‡∏•‡∏ö‡∏ã‡πâ‡∏≥

        if not enablers_to_iterate:
             continue
             
        for ena_req in enablers_to_iterate:
            
            final_year_wipe, final_enabler_wipe = get_normalized_metadata(
                doc_type=dt,
                year_input=year_int,
                enabler_input=ena_req, 
                default_enabler=DEFAULT_ENABLER
            )
            
            # ‡∏Å‡∏£‡∏≠‡∏á‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å‡πÉ‡∏ä‡πâ (‡πÄ‡∏ä‡πà‡∏ô Evidence ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ Enabler/Year)
            if is_evidence and (final_year_wipe is None or final_enabler_wipe is None):
                logger.debug(f"Skipping wipe context: {dt}/{final_enabler_wipe}/{final_year_wipe} (Missing year/enabler for evidence).")
                continue
                
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ó‡∏µ‡πà Normalize ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô Set
            col_name = get_doc_type_collection_key(dt, final_enabler_wipe)
            collections_to_delete.add((dt, final_year_wipe, final_enabler_wipe, col_name))


    if not collections_to_delete:
        logger.warning(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö Collection ‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç Tenant='{tenant}', Year='{year}', Doc Type='{doc_type_to_wipe}', Enabler='{enabler}'. Skipping wipe.")
        return

    # 2. ‡∏•‡∏ö Collection Folder ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á Doc ID Mapping DB
    tenant_clean = tenant.lower().replace(" ", "_")
    
    for dt, map_year_to_use, map_enabler_to_use, col_name in collections_to_delete:
        
        # 2a. ‡∏•‡∏ö Vector Store Folder
        try:
            # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà Normalize ‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Path
            collection_path = get_vectorstore_collection_path(tenant_clean, map_year_to_use, dt, map_enabler_to_use)
            
            if os.path.exists(collection_path):
                shutil.rmtree(collection_path)
                logger.info(f"üóëÔ∏è Deleted Collection Folder: {col_name} at {collection_path}")
            else:
                logger.info(f"Collection Folder ‡πÑ‡∏°‡πà‡∏û‡∏ö: {col_name} ({collection_path}).")
        except Exception as e:
            logger.error(f"‚ùå Error deleting vectorstore folder {col_name}: {e}")

        # 2b. ‡∏•‡∏ö Entry ‡∏à‡∏≤‡∏Å Doc ID Mapping
        
        # ‡πÇ‡∏´‡∏•‡∏î Mapping ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡∏ö‡∏£‡∏¥‡∏ö‡∏ó (‡∏ã‡∏∂‡πà‡∏á‡∏≠‡∏≤‡∏à‡∏°‡∏µ Entry ‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏≠‡∏¢‡∏π‡πà)
        try:
            doc_mapping_db = load_doc_id_mapping(dt, tenant_clean, map_year_to_use, map_enabler_to_use) 
        except FileNotFoundError:
             logger.info(f"Mapping file for {dt}/{map_enabler_to_use}/{map_year_to_use} not found. Skipping mapping update.")
             continue
        except Exception as e:
            logger.error(f"‚ùå Error loading mapping file for {dt}/{map_enabler_to_use}/{map_year_to_use}: {e}. Skipping mapping update.")
            continue
            
        uuids_to_keep = {}
        
        # ‡∏Å‡∏£‡∏≠‡∏á Entry ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö Collection ‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏á‡∏ñ‡∏π‡∏Å‡∏•‡∏ö
        for s_uuid, entry in doc_mapping_db.items():
            
            entry_doc_type = entry.get('doc_type', dt).lower()
            entry_enabler = entry.get('enabler')
            entry_tenant = entry.get("tenant", tenant_clean).lower()
            
            # ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏õ‡∏µ‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡∏Å‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà Normalize ‡πÅ‡∏•‡πâ‡∏ß (str/None)
            entry_year_str = str(entry.get("year")) if entry.get("year") is not None else None
            map_year_to_use_str = str(map_year_to_use) if map_year_to_use is not None else None

            # ‡πÉ‡∏ä‡πâ get_doc_type_collection_key ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Entry ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏á Collection ‡∏ô‡∏µ‡πâ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            entry_col_name = get_doc_type_collection_key(entry_doc_type, entry_enabler)
            
            # ‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡πá‡∏ö: 
            # 1. Entry ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏á Tenant ‡∏≠‡∏∑‡πà‡∏ô
            # 2. Entry ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏á Collection ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏•‡∏ö (col_name) 
            # 3. Entry ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏á‡∏õ‡∏µ‡∏≠‡∏∑‡πà‡∏ô (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Evidence)
            
            is_match = (
                entry_tenant == tenant_clean and
                entry_col_name == col_name and
                (entry_year_str == map_year_to_use_str or dt != EVIDENCE_DOC_TYPES.lower()) # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Global Docs ‡∏õ‡∏µ‡πÑ‡∏°‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
            )

            if not is_match:
                 uuids_to_keep[s_uuid] = entry
            else:
                logger.debug(f"Removed mapping entry for {s_uuid} (Collection: {col_name})")

        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏´‡∏£‡∏∑‡∏≠‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå Mapping
        removed_count = len(doc_mapping_db) - len(uuids_to_keep)
        if removed_count > 0:
            if not uuids_to_keep:
                mapping_path = get_mapping_file_path(tenant_clean, map_year_to_use, map_enabler_to_use) 
                if os.path.exists(mapping_path):
                    try:
                        os.remove(mapping_path)
                        logger.info(f"‚úÖ Deleted (empty) Doc ID mapping file: {mapping_path}")
                    except OSError as e:
                        logger.error(f"‚ùå Error deleting mapping file: {e}")
            else:
                # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠
                save_doc_id_mapping(uuids_to_keep, dt, tenant_clean, map_year_to_use, enabler=map_enabler_to_use) 
                logger.info(f"‚úÖ Saved updated mapping file for {dt}/{map_enabler_to_use}/{map_year_to_use}. Entries left: {len(uuids_to_keep)}.")
        
        logger.info(f"üßπ Removed {removed_count} entries from mapping file for deleted collection (Doc Type: {dt}/{map_enabler_to_use}) of {tenant}/{map_year_to_use}.")

        # 2c. ‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå Evidence Mapping ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô Evidence
        if dt == EVIDENCE_DOC_TYPES.lower():
            # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà Normalize ‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏´‡∏≤ Path
            if map_year_to_use is not None and map_enabler_to_use is not None:
                evidence_map_path = get_evidence_mapping_file_path(tenant_clean, map_year_to_use, map_enabler_to_use) 
                if os.path.exists(evidence_map_path):
                     try:
                        os.remove(evidence_map_path)
                        logger.info(f"‚úÖ Deleted Evidence Mapping file: {evidence_map_path}")
                     except OSError as e:
                        logger.error(f"‚ùå Error deleting evidence mapping file: {e}")
            
    # 3. ‡∏•‡∏ö Root Directory ‡∏Ç‡∏≠‡∏á Vector Store ‡πÅ‡∏•‡∏∞ Mapping ‡∏´‡∏≤‡∏Å‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤
    # (‡πÇ‡∏Ñ‡πâ‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏°‡∏≤‡∏î‡∏π‡∏î‡∏µ‡πÅ‡∏•‡πâ‡∏ß)
    try:
        # Vectorstore cleanup
        tenant_root_path = get_vectorstore_tenant_root_path(tenant_clean) 
        if os.path.isdir(tenant_root_path):
            if not any(f for f in os.listdir(tenant_root_path) if not f.startswith('.')):
                try:
                    shutil.rmtree(tenant_root_path) 
                    logger.info(f"‚úÖ Deleted empty Vector Store tenant directory: {tenant_root_path}")
                except OSError as e:
                    logger.debug(f"Vector Store directory {tenant_root_path} not empty or cannot be deleted: {e}")

        # Mapping cleanup
        mapping_dir_tenant = get_mapping_tenant_root_path(tenant_clean)
        if os.path.isdir(mapping_dir_tenant):
            if not any(f for f in os.listdir(mapping_dir_tenant) if not f.startswith('.')):
                try:
                    shutil.rmtree(mapping_dir_tenant)
                    logger.info(f"‚úÖ Deleted empty Doc ID mapping tenant directory: {mapping_dir_tenant}")
                except OSError as e:
                    logger.debug(f"Mapping directory {mapping_dir_tenant} not empty or cannot be deleted: {e}")
            else: 
                logger.info(f"Mapping directory {mapping_dir_tenant} is not completely empty. Keeping.")
                
    except Exception as e:
        logger.warning(f"Error during final mapping directory cleanup: {e}")
        
    logger.critical("‚úÖ !!! ‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£ WIPE Vectorstore ‡πÅ‡∏•‡∏∞ Mapping Files ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô !!!")

# -------------------- [REVISED] Document Management Utilities --------------------
def delete_document_by_uuid(
    stable_doc_uuid: str, 
    tenant: str = "pwa", 
    year: Union[int, str] = DEFAULT_YEAR, 
    collection_name: Optional[str] = None, 
    doc_type: Optional[str] = None, 
    enabler: Optional[str] = None, 
    base_path: Optional[str] = None
) -> bool:
    """Deletes all chunks associated with the given Stable Document UUID (Multi-Tenant/Year)."""
    if not doc_type:
        logger.error(f"Cannot delete {stable_doc_uuid}: doc_type must be provided for mapping file isolation.")
        return False
        
    tenant_clean = tenant.lower().replace(" ", "_")
    
    # üéØ FIX 1: ‡πÉ‡∏ä‡πâ get_normalized_metadata ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
    doc_type_lower = doc_type.lower()
    
    year_int: Optional[int]
    try:
        year_int = int(year) if year is not None and str(year).isdigit() else None
    except ValueError:
        year_int = None
        
    final_year_for_map, final_enabler = get_normalized_metadata(
        doc_type=doc_type_lower,
        year_input=year_int,
        enabler_input=enabler,
        default_enabler=DEFAULT_ENABLER
    )

    # ‡πÇ‡∏´‡∏•‡∏î Mapping (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ try-except)
    try:
        doc_mapping_db = load_doc_id_mapping(doc_type_lower, tenant_clean, final_year_for_map, final_enabler) 
    except FileNotFoundError:
        logger.warning(f"Mapping file not found for context {doc_type_lower}/{final_year_for_map}/{final_enabler}. Cannot delete entry {stable_doc_uuid}.")
        return False
    except Exception as e:
        logger.error(f"‚ùå Error loading mapping file: {e}")
        return False

    entry = doc_mapping_db.get(stable_doc_uuid)
    if not entry:
        logger.warning(f"UUID {stable_doc_uuid} not found in mapping DB for {doc_type_lower}/{final_year_for_map}/{final_enabler}. No action taken.")
        return False

    # üìå ‡πÉ‡∏ä‡πâ metadata ‡∏à‡∏≤‡∏Å Entry ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å Vectorstore/Mapping
    final_doc_type = entry.get("doc_type", doc_type_lower)
    final_enabler_from_entry = entry.get("enabler", final_enabler)
    final_year_from_entry = entry.get("year", final_year_for_map)
    chunk_uuids = entry.get("chunk_uuids", [])
    
    if not chunk_uuids:
        logger.warning(f"No chunks found for UUID {stable_doc_uuid}. Deleting mapping entry only.")
        del doc_mapping_db[stable_doc_uuid]
        
    else:
        # 1. ‡∏•‡∏ö Chunks ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å Vectorstore
        try:
            col_name = get_doc_type_collection_key(final_doc_type, final_enabler_from_entry)
            
            # üí° ‡πÉ‡∏ä‡πâ final_year_from_entry ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å get_vectorstore 
            # Note: final_year_from_entry ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô None ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Global Docs
            vectorstore = get_vectorstore(col_name, tenant_clean, final_year_from_entry) 
            
            # Note: ‡∏Å‡∏≤‡∏£‡∏•‡∏ö‡πÉ‡∏ô ChromaDB ‡∏ó‡∏≥‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ ID
            vectorstore.delete(ids=chunk_uuids) 
            logger.info(f"‚úÖ Deleted {len(chunk_uuids)} chunks for {stable_doc_uuid} from collection '{col_name}'.")
        except Exception as e:
            logger.error(f"‚ùå Failed to delete chunks from Vectorstore for {stable_doc_uuid}: {e}", exc_info=True)
            # ‡πÑ‡∏°‡πà return False ‡∏ñ‡πâ‡∏≤‡∏•‡∏ö‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ‡πÅ‡∏ï‡πà‡πÉ‡∏´‡πâ‡∏•‡∏ö entry ‡πÉ‡∏ô mapping DB ‡∏ï‡πà‡∏≠‡πÑ‡∏õ
            
        # üéØ FIX: ‡∏•‡∏ö Entry ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å Mapping DB ‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î‡∏°‡∏≤
        del doc_mapping_db[stable_doc_uuid]
        
    # üìå FIX: ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Mapping DB ‡∏Ñ‡∏∑‡∏ô
    
    # 1. ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Entry ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏á‡∏ö‡∏£‡∏¥‡∏ö‡∏ó Tenant/Year/Enabler ‡∏ô‡∏µ‡πâ (‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î‡∏°‡∏≤)
    db_to_save: Dict[str, Dict[str, Any]] = {}
    
    # ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏õ‡∏µ‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡∏Å‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà Normalize ‡πÅ‡∏•‡πâ‡∏ß (str/None)
    final_year_for_map_str = str(final_year_for_map) if final_year_for_map is not None else None

    for s_uuid, entry_in_db in doc_mapping_db.items():
        # ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Entry ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå Mapping ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏à‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ñ‡∏∑‡∏ô
        entry_year_str = str(entry_in_db.get("year")) if entry_in_db.get("year") is not None else None
        
        if (entry_in_db.get("doc_type", "").lower() == doc_type_lower and 
            str(entry_in_db.get("tenant")).lower() == tenant_clean and 
            entry_year_str == final_year_for_map_str and 
            entry_in_db.get("enabler") == final_enabler):
            
            db_to_save[s_uuid] = entry_in_db
            
    # 2. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Mapping DB ‡∏Ñ‡∏∑‡∏ô
    if db_to_save:
         save_doc_id_mapping(
            db_to_save, 
            doc_type_lower, 
            tenant_clean, 
            final_year_for_map, 
            enabler=final_enabler
        )
         logger.info(f"‚úÖ Saved updated mapping DB for {doc_type_lower}/{final_enabler}/{final_year_for_map}. Entries remaining: {len(db_to_save)}.")
    else:
        # ‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå mapping ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ entry ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏≠‡∏¢‡∏π‡πà
        mapping_path = get_mapping_file_path(tenant_clean, final_year_for_map, final_enabler) 
        if os.path.exists(mapping_path):
            try:
                os.remove(mapping_path)
                logger.info(f"‚úÖ Deleted (empty) Doc ID mapping file: {mapping_path}")
            except OSError as e:
                logger.error(f"‚ùå Error deleting mapping file: {e}")
        
    return True

# core/ingest.py: ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô list_documents (‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)

def list_documents(
    doc_types: List[str],
    tenant: str = DEFAULT_TENANT,
    year: Optional[Union[str, int]] = None,
    enabler: Optional[str] = None,
    show_results: Literal["all", "missing", "ingested"] = "missing",
    skip_ext: Optional[List[str]] = None,
) -> pd.DataFrame:
    """
    List files and compare them with existing mapping database.
    (‡∏£‡∏ß‡∏°‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç NFKC/NFD Path Matching ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ñ‡∏≤‡∏ß‡∏£)
    """
    logger.info("--- STARTING DOCUMENT LISTING ---")
    
    tenant_clean = unicodedata.normalize('NFKC', tenant.lower().replace(" ", "_"))
    
    load_contexts: Set[Tuple[str, Optional[str], Optional[Union[str, int]]]] = set()
    files_on_disk: List[Dict[str, Any]] = []

    # 1. Determine Contexts (Doc Type, Enabler, Year)
    for dt in doc_types:
        dt_lower = dt.lower()

        resolved_year, resolved_enabler = get_normalized_metadata(
            doc_type=dt_lower,
            year_input=year,
            enabler_input=enabler,
            default_enabler=DEFAULT_ENABLER,
        )
        
        # Evidence ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏õ‡∏µ
        if dt_lower == EVIDENCE_DOC_TYPES.lower() and resolved_year is None:
            logger.error("Evidence requires --year. Skipping evidence listing.")
            continue
            
        load_contexts.add((dt_lower, resolved_enabler, resolved_year))
        
        root_path = get_document_source_dir(tenant_clean, resolved_year, resolved_enabler, dt_lower)
        logger.info(f" [SCAN] '{dt_lower}' Context: {dt_lower} / {resolved_enabler} / {resolved_year} | Path: {root_path}")

        # 2. Scan Files on Disk (Physical Path)
        if not os.path.exists(root_path):
            logger.warning(f"Directory not found: {root_path}")
            continue

        for root, dirs, files in os.walk(root_path):
            dirs[:] = [d for d in dirs if d not in ['.DS_Store', '__pycache__', 'backup']]
            for f in files:
                ext = os.path.splitext(f)[1].lower()
                if f.startswith('.') or ext not in SUPPORTED_TYPES:
                    continue
                if skip_ext and ext in skip_ext:
                    continue

                file_path_abs = os.path.join(root, f)
                
                # üìå NEW: ‡∏™‡∏£‡πâ‡∏≤‡∏á UUID ‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡∏à‡∏≤‡∏Å Physical Path
                stable_doc_uuid = create_stable_uuid_from_path(
                    file_path_abs,
                    tenant=tenant_clean,
                    year=resolved_year,
                    enabler=resolved_enabler
                )
                
                files_on_disk.append({
                    "doc_type": dt_lower,
                    "enabler": resolved_enabler,
                    "year": resolved_year,
                    "file_name": f,
                    "file_path_abs": file_path_abs,
                    "stable_doc_uuid": stable_doc_uuid,
                    "status": "MISSING",
                    "chunk_count": 0
                })

    if not files_on_disk:
        logger.warning("--- NO FILES FOUND ON DISK ---")
        return pd.DataFrame(columns=["Doc Type", "Enabler", "Year", "File Name", "Status", "Chunks"])

    # 3. Load Mappings (Saved Path)
    full_mapping: Dict[str, Dict] = {}
    # Key: Relative Key (tenant/data/...) -> Value: Stable UUID
    filepath_to_stable_uuid: Dict[str, str] = {} 
    
    for ctx in load_contexts:
        dt, ena, yr = ctx
        try:
            doc_mapping_db = load_doc_id_mapping(dt, tenant_clean, yr, ena)
            full_mapping.update(doc_mapping_db)
        except FileNotFoundError:
            continue
        
        for s_uuid, entry in doc_mapping_db.items():
            entry_context = (entry.get("doc_type", dt), entry.get("enabler", ena), entry.get("year", yr))
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ entry ‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏á context ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            if entry_context in load_contexts: 
                 saved_filepath = entry["filepath"] # saved_filepath ‡∏Ñ‡∏∑‡∏≠ relative path ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡πÉ‡∏ô mapping
                 
                 # üü¢ CRITICAL FIX: ‡πÉ‡∏ä‡πâ get_mapping_key_from_physical_path ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á Key ‡∏à‡∏≤‡∏Å Saved Path
                 # Note: saved_filepath ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡πÉ‡∏ô mapping ‡∏°‡∏±‡∏Å‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô relative path ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ NFD/NFKC
                 stable_lookup_key = get_mapping_key_from_physical_path(saved_filepath)
                 
                 if stable_lookup_key:
                     filepath_to_stable_uuid[stable_lookup_key] = s_uuid
                 else:
                     logger.warning(f"Could not create stable lookup key from saved path: {saved_filepath}")


    # 4. Compare Files on Disk with Mappings
    results: List[Dict] = []
    
    for info in files_on_disk:
        file_path_abs = info["file_path_abs"]
        
        # üü¢ CRITICAL FIX REVISED: Prepare lookup key from physical file (‡πÉ‡∏ä‡πâ Absolute Path ‡∏ó‡∏µ‡πà‡∏™‡πÅ‡∏Å‡∏ô‡πÄ‡∏à‡∏≠)
        # get_mapping_key_from_physical_path ‡∏à‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á Path:
        # 1. Absolute Path
        # 2. NFKC Normalize
        # 3. Relative to DATA_STORE_ROOT
        # 4. Forward Slashes
        relative_key_candidate = get_mapping_key_from_physical_path(file_path_abs) 
        
        stable_doc_uuid = None
        if relative_key_candidate:
            # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Stable UUID ‡∏à‡∏≤‡∏Å Key ‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡∏∂‡πâ‡∏ô (‡∏ã‡∏∂‡πà‡∏á‡∏°‡∏≤‡∏à‡∏≤‡∏Å saved_filepath ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å normalize ‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏ô Section 3)
            stable_doc_uuid = filepath_to_stable_uuid.get(relative_key_candidate)
        
        
        entry = None
        if stable_doc_uuid:
            entry = full_mapping.get(stable_doc_uuid)
        elif info["stable_doc_uuid"] in full_mapping:
             # Fallback: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢ UUID ‡∏õ‡∏Å‡∏ï‡∏¥ (‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å normalize ‡∏ï‡∏≠‡∏ô ingest ‡∏£‡∏≠‡∏ö‡πÄ‡∏Å‡πà‡∏≤‡πÜ)
             entry = full_mapping.get(info["stable_doc_uuid"])

        if entry:
            info["status"] = entry.get("status", "Ingested")
            info["chunk_count"] = entry.get("chunk_count", 0)
            if info["chunk_count"] == 0:
                 info["status"] = "PENDING_REINGEST" # Chunk ‡∏´‡∏≤‡∏¢ ‡∏ï‡πâ‡∏≠‡∏á ingest ‡πÉ‡∏´‡∏°‡πà

        if show_results == "all" or (show_results == "missing" and info["status"] in ["MISSING", "PENDING_REINGEST"]) or (show_results == "ingested" and info["status"] == "Ingested"):
            results.append({
                "Doc Type": info["doc_type"].upper(),
                "Enabler": info["enabler"] or "-",
                "Year": info["year"] or "-",
                "File Name": info["file_name"],
                "Status": info["status"],
                "Chunks": info["chunk_count"],
                "UUID": info["stable_doc_uuid"]
            })

    logger.info(f"--- DOCUMENT LISTING COMPLETE | Total files found: {len(files_on_disk)} | Displayed results: {len(results)} ---")
    
    df = pd.DataFrame(results)
    if not df.empty:
        df.sort_values(by=["Doc Type", "Enabler", "Year", "File Name"], inplace=True)
    return df

# -------------------- Main Execution --------------------

if __name__ == "__main__":
    try:
        import argparse
        
        # -------------------- Argument Parser setup --------------------
        parser = argparse.ArgumentParser(description="Multi-Tenant RAG Ingestion and Management Tool (SE-AM Ready)")
        
        # Global Settings
        parser.add_argument("--tenant", type=str, default=DEFAULT_TENANT, help="Tenant code (e.g., 'pea', 'pwa').")
        parser.add_argument("--year", type=str, default=str(DEFAULT_YEAR), help="Assessment year (e.g., '2568').")
        parser.add_argument("--doc-type", nargs='+', default=[EVIDENCE_DOC_TYPES], help="Document type(s) to process ('evidence', 'document', 'all').")
        parser.add_argument("--enabler", type=str, default=DEFAULT_ENABLER, help="Enabler code (e.g., 'KM', 'HCM').")
        parser.add_argument("--subject", type=str, default=None, help="Subject/Topic tag for documents (optional).")

        # Ingest Mode
        parser.add_argument("--ingest", action="store_true", help="Run ingestion mode.")
        parser.add_argument("--dry-run", action="store_true", help="Simulate ingestion without writing to Chroma/DB.")
        parser.add_argument("--sequential", action="store_true", help="Run ingestion in sequential mode (for debugging).")
        parser.add_argument("--skip-wipe", action="store_true", help="Skip the wiping of vector store before ingestion.")

        # List Mode
        parser.add_argument("--list", action="store_true", help="Run document listing mode.")
        parser.add_argument("--show-results", type=str, default="all", choices=["all", "ingested", "pending"], help="Filter results for list mode.")
        
        # Wipe Mode
        parser.add_argument("--wipe", action="store_true", help="Wipe (delete) vector store and mapping files for the specified context.")
        parser.add_argument("--yes", action="store_true", help="Bypass confirmation prompt for wiping (DANGER: use only when sure!).") 

        args = parser.parse_args()
        
        # -------------------- Pre-Command Setup & Validation --------------------
        
        # 1. Normalize doc_type
        # ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡πÅ‡∏£‡∏Å‡∏à‡∏≤‡∏Å list ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ DEFAULT_DOC_TYPES
        doc_type_for_ingest_wipe = args.doc_type[0].lower() if isinstance(args.doc_type, list) and args.doc_type else DEFAULT_DOC_TYPES[0].lower()
        
        # 2. Check Enabler for Evidence
        if doc_type_for_ingest_wipe == EVIDENCE_DOC_TYPES.lower() and (args.ingest or args.wipe or args.list) and not args.enabler:
            logger.error(f"When using '{EVIDENCE_DOC_TYPES.lower()}', you must specify --enabler.")
            sys.exit(1)

        logger.info(f"--- STARTING EXECUTION: Tenant={args.tenant}, Year={args.year}, DocType={args.doc_type}, Enabler={args.enabler} ---")
        
        # --- Handle all modes ---
        
        if args.ingest:
            logger.info("--- INGESTION MODE ACTIVATED ---")
            
            # 1. Prepare Normalized Year (int/None)
            year_to_use_ingest: Optional[Union[int, str]] = None
            try:
                year_to_use_ingest = int(args.year) if args.year and args.year.isdigit() and int(args.year) > 0 else None
            except ValueError:
                year_to_use_ingest = None
                
            # Global Doc Type ‡πÉ‡∏ä‡πâ None ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö year ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏∞‡∏ö‡∏∏‡∏õ‡∏µ
            if doc_type_for_ingest_wipe != EVIDENCE_DOC_TYPES.lower():
                 year_to_use_ingest = None 
            
            # 2. WIPE LOGIC (Optional)
            if not args.skip_wipe and not args.dry_run:
                logger.warning("‚ö†Ô∏è Wiping Vector Store before ingestion!!!")
                wipe_vectorstore(
                    doc_type_to_wipe=doc_type_for_ingest_wipe,
                    enabler=args.enabler, 
                    tenant=args.tenant, 
                    year=year_to_use_ingest # ‡πÉ‡∏ä‡πâ‡∏õ‡∏µ‡∏ó‡∏µ‡πà Normalize ‡πÅ‡∏•‡πâ‡∏ß
                )
            
            # 3. INGEST LOGIC
            # Note: ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤ ingest_all_files ‡∏£‡∏±‡∏ö doc_type ‡πÄ‡∏õ‡πá‡∏ô List[str]
            ingest_all_files(
                tenant=args.tenant,
                year=year_to_use_ingest,
                doc_types=args.doc_type, # ‡∏™‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô List ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ö‡∏°‡∏≤‡∏à‡∏≤‡∏Å Argument
                enabler=args.enabler,
                subject=args.subject, 
                dry_run=args.dry_run,
                sequential=args.sequential
            )
            
        elif args.list:
            logger.info("--- LIST MODE ACTIVATED ---\n")
            
            # 3. LIST LOGIC
            list_documents(
                doc_types=[dt.lower() for dt in args.doc_type], 
                enabler=args.enabler, 
                tenant=args.tenant, 
                year=args.year, # ‡∏™‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô string, list_documents ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á
                show_results=args.show_results 
            )
            
        elif args.wipe:
            logger.info("--- WIPE MODE ACTIVATED ---")
            logger.critical("‚ö†Ô∏è Wiping Vector Store and Mapping Files as requested!!!")
            
            # --- WIPE Confirmation ---
            if not args.yes:
                confirmation = input("Type 'YES' (all caps) to confirm deletion: ")
                if confirmation != "YES":
                    logger.info("Deletion cancelled.")
                    sys.exit(0)

            # 1. Prepare Normalized Year (int/None)
            year_to_use_wipe: Optional[Union[int, str]] = None
            try:
                year_to_use_wipe = int(args.year) if args.year and args.year.isdigit() and int(args.year) > 0 else None
            except ValueError:
                year_to_use_wipe = None
                
            if doc_type_for_ingest_wipe != EVIDENCE_DOC_TYPES.lower():
                year_to_use_wipe = None # Global Doc Type uses None for year
            
            # 2. Execute Vectorstore Wipe (‡∏•‡∏ö Collection ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô Chroma)
            wipe_vectorstore(
                doc_type_to_wipe=doc_type_for_ingest_wipe,
                enabler=args.enabler, 
                tenant=args.tenant, 
                year=year_to_use_wipe # ‡πÉ‡∏ä‡πâ‡∏õ‡∏µ‡∏ó‡∏µ‡πà Normalize ‡πÅ‡∏•‡πâ‡∏ß
            )
            
        else:
            print("\nUsage: Specify --ingest, --list, or --wipe mode.")
            parser.print_help()

        logger.info("Execution finished.")
        
    except ImportError:
         print("--- RUNNING SCRIPT STANDALONE FAILED: Missing necessary imports ---")
         
    except Exception as e:
         import traceback
         logger.critical(f"FATAL ERROR DURING MAIN EXECUTION: {e}", exc_info=True)
         print(f"--- FATAL ERROR: Check ingest.log for details... \n{traceback.format_exc()}")