# core/ingest.py
import os
import re
import logging
import unicodedata
import json
import uuid
from datetime import datetime, timezone
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Optional, Set, Iterable, Dict, Any, Union, Tuple, TypedDict
import pandas as pd
import shutil
import numpy as np
import glob
from pydantic import ValidationError


# LangChain loaders
from langchain_community.document_loaders import (
    PyPDFLoader,
    UnstructuredPDFLoader,
    CSVLoader,
    UnstructuredWordDocumentLoader,
    UnstructuredExcelLoader,
    TextLoader,
    UnstructuredPowerPointLoader
)

import fitz  # PyMuPDF
import hashlib

# üí° ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç #1: ‡∏¢‡πâ‡∏≤‡∏¢ Document ‡∏à‡∏≤‡∏Å langchain.schema
from langchain_core.documents import Document

# üí° ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç #2: ‡∏¢‡πâ‡∏≤‡∏¢ Text Splitter ‡∏à‡∏≤‡∏Å langchain.text_splitter
from langchain_text_splitters import RecursiveCharacterTextSplitter 

# üí° ‡∏Å‡∏≤‡∏£ Import ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠ (‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á chroma ‡πÅ‡∏•‡∏∞ huggingface embeddings ‡πÅ‡∏•‡πâ‡∏ß)
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings

try:
    from langchain_community.vectorstores.utils import filter_complex_metadata as _imported_filter_complex_metadata
except ImportError:
    _imported_filter_complex_metadata = None


# -------------------- Global Config --------------------
from config.global_vars import (
    DATA_DIR,
    VECTORSTORE_DIR,
    MAPPING_FILE_PATH,
    SUPPORTED_TYPES,
    SUPPORTED_DOC_TYPES,
    DEFAULT_ENABLER,
    SUPPORTED_ENABLERS,
    EVIDENCE_DOC_TYPES,
    DEFAULT_DOC_TYPES,
    CHUNK_OVERLAP,
    CHUNK_SIZE
)

# Logging
logging.basicConfig(
    filename="ingest.log",
    level=logging.DEBUG, # ‚úÖ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏à‡∏≤‡∏Å INFO ‡πÄ‡∏õ‡πá‡∏ô DEBUG
    format="%(asctime)s | %(levelname)s | %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger(__name__)

try:
    import pytesseract
    # Hardcode Tesseract executable path ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏î‡πâ‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡πÅ‡∏•‡πâ‡∏ß
    pytesseract.pytesseract.tesseract_cmd = '/opt/homebrew/bin/tesseract'
    logger.info("‚úÖ Pytesseract path set successfully for Unstructured.")
except ImportError:
    logger.warning("Pytesseract not installed. Tesseract OCR may fail.")
except Exception as e:
    logger.error(f"Failed to set pytesseract path: {e}")

# --- Document Info Model ---
class DocInfo(TypedDict):
    """
    ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Ç‡∏≠‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö API
    """
    doc_id: str             # Stable UUID
    doc_id_key: str         # Filename ID Key (normalized name)
    filename: str
    filepath: str
    doc_type: str           # Collection name (e.g., 'document', 'evidence')
    enabler: Optional[str]  # Enabler code (e.g., 'KM')
    upload_date: str        # ISO format
    chunk_count: int
    status: str             # "Ingested" | "Pending" | "Error"
    size: int               # File size in bytes

# -------------------- Unstructured Loader --------------------
# ‡∏•‡∏≠‡∏á Import ‡∏à‡∏≤‡∏Å Partner Package ‡πÉ‡∏´‡∏°‡πà‡∏Å‡πà‡∏≠‡∏ô
try:
    from langchain_unstructured.document_loaders import UnstructuredLoader as UnstructuredFileLoader
# ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏ö ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô Community package (‡∏°‡∏±‡∏Å‡∏à‡∏∞‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤)
except ImportError:
    from langchain_community.document_loaders import UnstructuredFileLoader

try:
    from pythainlp.tokenize import word_tokenize
    THAI_SEGMENTATION_ENABLED = True
except ImportError:
    THAI_SEGMENTATION_ENABLED = False
    logger.warning("PyThaiNLP not installed. Thai segmentation will be skipped.")

# -------------------- Log Noise Suppression (NEW) --------------------
# üìå FIX: ‡∏õ‡∏¥‡∏î‡∏Å‡∏±‡πâ‡∏ô WARNINGs ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏≤‡∏Å‡πÑ‡∏•‡∏ö‡∏£‡∏≤‡∏£‡∏µ‡∏†‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å
#    ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≥‡∏à‡∏±‡∏î Warning ‡πÄ‡∏ä‡πà‡∏ô "Cannot set gray non-stroke color because..."

import warnings

# 1. ‡πÉ‡∏ä‡πâ warnings.filterwarnings ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ã‡πà‡∏≠‡∏ô UserWarning ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô Log Noise
warnings.filterwarnings(
    "ignore", 
    "Cannot set gray non-stroke color because", 
    category=UserWarning,
    module='pdfminer' 
)

# 2. ‡∏•‡∏î‡∏£‡∏∞‡∏î‡∏±‡∏ö Log ‡∏Ç‡∏≠‡∏á‡πÑ‡∏•‡∏ö‡∏£‡∏≤‡∏£‡∏µ‡∏†‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏Å‡∏¥‡∏î Noise ‡∏•‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠ ERROR
logging.getLogger('pdfminer').setLevel(logging.ERROR)
logging.getLogger('pdfminer.pdfinterp').setLevel(logging.ERROR)
logging.getLogger('unstructured').setLevel(logging.ERROR)
logging.getLogger('pypdf').setLevel(logging.ERROR)
# ---------------------------------------------------------------------

# -------------------- [NEW] Path & Collection Utilities --------------------

def get_target_dir(doc_type: str, enabler: Optional[str] = None) -> str:
    """
    Calculates the target directory name / Chroma Collection Name.
    e.g., ("evidence", "KM") -> "evidence_km"
    e.g., ("document", None) -> "document"
    """
    doc_type_norm = doc_type.strip().lower()

    if doc_type_norm == EVIDENCE_DOC_TYPES:
        # 1. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤ enabler ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ (‡πÉ‡∏ä‡πâ enabler ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤ ‡∏´‡∏£‡∏∑‡∏≠ DEFAULT_ENABLER)
        # Note: ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≥‡∏´‡∏ô‡∏î DEFAULT_ENABLER ‡πÉ‡∏ô core/ingest.py
        final_enabler = (enabler or DEFAULT_ENABLER) 
        
        # üü¢ FIX: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏Ç‡∏≠‡∏á enabler ‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
        if not final_enabler or not final_enabler.strip():
             # ‡∏´‡∏≤‡∏Å‡∏°‡∏≤‡∏ñ‡∏∂‡∏á‡∏à‡∏∏‡∏î‡∏ô‡∏µ‡πâ‡πÇ‡∏î‡∏¢‡∏ó‡∏µ‡πà enabler ‡πÄ‡∏õ‡πá‡∏ô None, Empty String, ‡∏´‡∏£‡∏∑‡∏≠ Whitespace ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
             # ‡πÉ‡∏´‡πâ‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ó‡∏±‡∏ô‡∏ó‡∏µ ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏à‡∏∞‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏ä‡∏∑‡πà‡∏≠ Collection ‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î
             raise ValueError(
                 "CRITICAL: Evidence document chunk cannot be indexed: Final enabler is empty or missing. "
                 "Check DEFAULT_ENABLER definition or ensure enabler is provided."
             )

        # 2. ‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ä‡∏∑‡πà‡∏≠ Collection
        enabler_norm = final_enabler.strip().lower()
        # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô evidence_km
        return f"{doc_type_norm}_{enabler_norm}"
        
    # Default logic (for document, faq, policy, etc.)
    return doc_type_norm

def _parse_collection_name(collection_name: str) -> Tuple[str, Optional[str]]:
    """
    Parses a collection/folder name back into doc_type and enabler.
    e.g., "evidence_km" -> ("evidence", "KM")
    e.g., "document"    -> ("document", None)
    """
    collection_name_lower = collection_name.lower()
    
    if collection_name_lower.startswith(f"{EVIDENCE_DOC_TYPES}_"):
        parts = collection_name_lower.split("_", 1)
        if len(parts) == 2:
            return EVIDENCE_DOC_TYPES, parts[1].upper()
    
    # Check if the collection name is a simple doc_type
    if collection_name_lower in SUPPORTED_DOC_TYPES:
        return collection_name_lower, None
        
    return collection_name_lower, None # Fallback

def _get_source_dir(
    doc_type: str, 
    enabler: Optional[str] = None, 
    base_data_dir: str = DATA_DIR
) -> str:
    """
    Calculates the specific source directory path based on doc_type and enabler.
    e.g., ("evidence", "KM") -> "data/evidence_km"
    e.g., ("document", None) -> "data/document"
    """
    collection_name = get_target_dir(doc_type, enabler)
    return os.path.join(base_data_dir, collection_name)

# -------------------- Helper: safe metadata filter --------------------
def _safe_filter_complex_metadata(meta: Any) -> Dict[str, Any]:
    """
    Ensure metadata is serializable and safe for Chroma / storage.
    Adds a fix to convert single-item lists (like ['eng']) to a single primitive value.
    """
    
    if not isinstance(meta, dict):
        # Attempt to convert to dict if possible
        if hasattr(meta, "items"):
            meta_dict = dict(meta.items())
        else:
            return {} # If it's not a dict, we can't process it safely
    else:
        meta_dict = meta
        
    clean = {}

    for k, v in meta_dict.items():
        if v is None:
            continue
            
        # 1. Handle primitive types (str, int, float, bool)
        if isinstance(v, (str, int, float, bool)):
            clean[k] = v
        # 2. Handle nested dictionary (Chroma does not support, so stringify)
        elif isinstance(v, dict):
            try:
                # ‡πÅ‡∏õ‡∏•‡∏á Dict ‡πÄ‡∏õ‡πá‡∏ô JSON string
                clean[k] = json.dumps(v)
            except TypeError:
                clean[k] = str(v)
                
        # 3. Handle list or tuple
        elif isinstance(v, (list, tuple)):
            # üìå FIX: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á List ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô Primitive Type
            if len(v) == 1:
                item = v[0]
                # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô String/Int/Float/Bool ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡πÇ‡∏î‡∏î‡πÜ
                if isinstance(item, (str, int, float, bool)):
                    clean[k] = item # ‡πÄ‡∏ä‡πà‡∏ô ['eng'] -> 'eng'
                    continue 
                # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô Numpy type ‡πÉ‡∏´‡πâ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô Python native type
                elif isinstance(item, (np.floating, np.integer)):
                    clean[k] = item.item() 
                    continue 
            
            # Fallback ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö List ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ ‡∏´‡∏£‡∏∑‡∏≠‡∏°‡∏µ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô
            try:
                # Stringify ‡∏ó‡∏∏‡∏Å element ‡πÉ‡∏ô List ‡πÅ‡∏•‡πâ‡∏ß dump ‡πÄ‡∏õ‡πá‡∏ô JSON string ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
                clean[k] = json.dumps([str(x) for x in v]) 
            except Exception:
                clean[k] = str(v)
                
        # 4. Handle other complex objects (e.g., numpy types)
        elif isinstance(v, (np.floating, np.integer)):
            clean[k] = v.item()
        else:
            try:
                clean[k] = str(v)
            except Exception:
                continue
                
    # Use LangChain's filter utility as a final safety layer if available
    if _imported_filter_complex_metadata:
        try:
            return _imported_filter_complex_metadata(clean)
        except Exception as e:
            logger.debug(f"LangChain filter failed after local cleanup: {e}")
            pass

    return clean

# -------------------- Normalization utility --------------------
def _normalize_doc_id(raw_id: str, file_content: bytes = None) -> str:
    normalized = re.sub(r'[^a-zA-Z0-9]', '', raw_id).lower()
    if len(normalized) > 28:
        normalized = normalized[:28]
    hash_suffix = '000000'
    if file_content:
        hash_suffix = hashlib.sha1(file_content).hexdigest()[:6]
    final_id = (normalized + hash_suffix).ljust(34, '0')
    return final_id


# -------------------- Text Cleaning --------------------
def _segment_thai_text(text: str) -> str:
    """
    Performs Thai word segmentation and uses a special separator (|) 
    instead of space to preserve word boundaries without adding excessive space.
    """
    if not THAI_SEGMENTATION_ENABLED:
        return text

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 10 ‡∏ï‡∏±‡∏ß‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏Å‡∏≤‡∏£ Segment ‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©)
    thai_char_count = len(re.findall(r'[‡∏Å-‡πô]', text))
    if thai_char_count < 10: 
        return text

    try:
        # ‡πÉ‡∏ä‡πâ 'THAI_SENTIMENT' ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤ default
        # (‡∏´‡∏£‡∏∑‡∏≠‡∏à‡∏∞‡πÉ‡∏ä‡πâ 'newmm' ‡∏Å‡πá‡πÑ‡∏î‡πâ ‡∏ã‡∏∂‡πà‡∏á 'THAI_SENTIMENT' ‡∏Å‡πá‡πÉ‡∏ä‡πâ 'newmm' ‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô)
        words = word_tokenize(text, engine="newmm") 
        # ‡πÉ‡∏ä‡πâ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢ | ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÅ‡∏ö‡πà‡∏á‡∏Ñ‡∏≥‡πÅ‡∏ó‡∏ô space ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ Chunking ‡πÉ‡∏ô‡∏†‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏á
        # ‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡πÉ‡∏´‡πâ Text Splitter ‡πÄ‡∏´‡πá‡∏ô '‡∏Ñ‡∏≥' ‡πÅ‡∏ó‡∏ô '‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏¢‡∏≤‡∏ß‡πÜ ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ß‡∏£‡∏£‡∏Ñ'
        return "|".join(words) 
    except Exception as e:
        logger.warning(f"PyThaiNLP segmentation failed: {e}")
        return text

def clean_text(text: str) -> str:
    """
    Basic text cleaning utility.
    """
    if not text: return ""
    
    # üìå NEW: 1. ‡∏ó‡∏≥ Word Segmentation ‡∏Å‡πà‡∏≠‡∏ô (‡πÉ‡∏ä‡πâ | ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÅ‡∏ö‡πà‡∏á)
    text = _segment_thai_text(text)
    
    # 2. ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏ï‡∏≤‡∏°‡∏õ‡∏Å‡∏ï‡∏¥
    text = text.replace('\xa0', ' ').replace('\u200b', '').replace('\u00ad', '')
    text = re.sub(r'[\uFFFD\u2000-\u200F\u2028-\u202F\u2060-\u206F\uFEFF]', '', text)
    # Remove excessive spaces between Thai characters (‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏≤‡∏Å‡∏ñ‡πâ‡∏≤‡∏ó‡∏≥ Segmentation ‡πÅ‡∏•‡πâ‡∏ß)
    text = re.sub(r'([‡∏Å-‡πô])\s{1,3}(?=[‡∏Å-‡πô])', r'\1', text) 
    ocr_replacements = {"‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏ô": "‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô", "‡∏Ñ‡∏ì‡∏∞‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏£": "‡∏Ñ‡∏ì‡∏∞‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£"}
    for bad, good in ocr_replacements.items(): text = text.replace(bad, good)
    # Filter out non-printable ASCII except standard ones and Thai characters
    text = re.sub(r'[^\x09\x0A\x0D\x20-\x7E\u0E00-\u0E7F|]', '', text) # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏° | ‡πÉ‡∏ô Regex
    text = re.sub(r'\(\s+', '(', text); text = re.sub(r'\s+\)', ')', text)
    text = re.sub(r'\r\n', '\n', text); text = re.sub(r'\n{3,}', '\n\n', text)
    text = re.sub(r'[ \t]{2,}', ' ', text); text = re.sub(r'\s{2,}', ' ', text)
    
    # üìå NEW: 3. ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏ß‡πÅ‡∏ö‡πà‡∏á | ‡∏î‡πâ‡∏ß‡∏¢ space
    # (‡∏´‡∏£‡∏∑‡∏≠‡∏ñ‡πâ‡∏≤‡∏≠‡∏¢‡∏≤‡∏Å‡πÉ‡∏´‡πâ Chunking ‡πÉ‡∏ä‡πâ | ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÅ‡∏ö‡πà‡∏á‡∏Å‡πà‡∏≠‡∏ô ‡∏Å‡πá‡∏Ç‡πâ‡∏≤‡∏°‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÑ‡∏õ)
    # ‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ô‡∏µ‡πâ ‡∏ú‡∏°‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ | ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÅ‡∏ö‡πà‡∏á‡πÉ‡∏ô Text Splitter ‡πÄ‡∏•‡∏¢ 
    return text.strip()


def _is_pdf_image_only(file_path: str) -> bool:
# (‡πÇ‡∏Ñ‡πâ‡∏î‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô _is_pdf_image_only ... ‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°)
    """
    ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ PDF ‡πÄ‡∏õ‡πá‡∏ô image-only ‡∏´‡∏£‡∏∑‡∏≠‡∏°‡∏µ text layer
    ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ True ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô image-only
    """
    try:
        doc = fitz.open(file_path)
        for page in doc:
            text = page.get_text().strip()
            if text:
                return False  # ‡∏°‡∏µ text layer
        return True  # ‡πÑ‡∏°‡πà‡∏°‡∏µ text layer ‚Üí image-only
    except Exception as e:
        logger.warning(f"Cannot check PDF text layer for {file_path}: {e}")
        return True  # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô fail ‚Üí treat as image-only


def _load_document_with_loader(file_path: str, loader_class: Any) -> List[Document]:
    """
    Helper function to load a document using a specific LangChain loader class.
    Includes special handling for PDF (text layer vs. image-only) and image files.
    """
    raw_docs: List[Any] = [] # Initialize raw_docs
    try:
        # üü¢ FIX: ‡∏î‡∏∂‡∏á‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏à‡∏∏‡∏î‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏™‡∏°‡∏≠ (e.g., .pdf, .jpg)
        ext = "." + file_path.lower().split('.')[-1]
        
        # -------------------- 1. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Loader ‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå --------------------
        if loader_class is CSVLoader:
            loader = loader_class(file_path, encoding='utf-8')
            raw_docs = loader.load()
        
        elif ext == ".pdf":
            # Logic ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö PDF: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Text Layer
            if _is_pdf_image_only(file_path):
                # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö PDF ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏•‡πâ‡∏ß‡∏ô (‡πÉ‡∏ä‡πâ Unstructured ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö OCR)
                logger.info(f"PDF is image-only, using OCR loader: {file_path}")
                loader = UnstructuredFileLoader(file_path, mode="elements", languages=['tha','eng'])
            else:
                # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö PDF ‡∏ó‡∏µ‡πà‡∏°‡∏µ Text Layer
                logger.info(f"PDF has text layer, using PyPDFLoader: {file_path}")
                loader = PyPDFLoader(file_path)
            raw_docs = loader.load()
        
        # üü¢ FINAL FIX: ‡πÉ‡∏ä‡πâ UnstructuredFileLoader ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
        elif ext in [".jpg", ".jpeg", ".png"]:
            logger.info(f"Reading image file using UnstructuredFileLoader for robust OCR: {file_path} ...")
            # UnstructuredFileLoader ‡∏à‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ Tesseract ‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡πÄ‡∏≠‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
            loader = UnstructuredFileLoader(file_path, mode="elements", languages=['tha','eng'])
            raw_docs = loader.load()
            
        else:
            # Loader ‡∏≠‡∏∑‡πà‡∏ô‡πÜ (Word, Excel, PowerPoint, TextLoader)
            loader = loader_class(file_path)
            raw_docs = loader.load()
        
        # -------------------- 2. ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤ --------------------
        if raw_docs:
            original_count = len(raw_docs)
            
            # üü¢ FIX: ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ (None/Empty String)
            filtered_docs = [
                doc for doc in raw_docs 
                if isinstance(doc, Document) and doc.page_content is not None and doc.page_content.strip()
            ]
            
            if len(filtered_docs) < original_count:
                logger.warning(
                    f"‚ö†Ô∏è Loader returned {original_count - len(filtered_docs)} empty/None documents "
                    f"for {os.path.basename(file_path)}. Filtered to {len(filtered_docs)} valid documents."
                )
            
            # üü¢ NEW: ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á‡∏à‡∏ô‡∏´‡∏°‡∏î (‡πÄ‡∏´‡∏•‡∏∑‡∏≠ 0) ‡πÅ‡∏•‡∏∞‡∏°‡∏µ ValidationError ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô
            if not filtered_docs and original_count > 0:
                 logger.warning(f"‚ö†Ô∏è Loader returned documents but all were empty/invalid for {os.path.basename(file_path)}. Returning 0 valid documents.")
                 return []
                 
            return filtered_docs
    
    # -------------------- 3. ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î (Exception Handling) --------------------
    except (ValidationError, TypeError) as e:
        # üü¢ FINAL FIX: ‡∏î‡∏±‡∏Å‡∏à‡∏±‡∏ö ValidationError ‡πÅ‡∏•‡∏∞ TypeError (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏£‡∏ì‡∏µ page_content=None)
        # ‡∏´‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏ô‡∏µ‡πâ ‡πÉ‡∏´‡πâ Log ‡πÅ‡∏•‡∏∞‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ
        logger.error(f"‚ùå LOADER FAILED: {os.path.basename(file_path)} raised: {type(e).__name__} ({e}). Treating as 0 documents.")
        return []
        
    except Exception as e:
        loader_name = getattr(loader_class, '__name__', 'UnknownLoader')
        logger.error(f"‚ùå LOADER FAILED: {os.path.basename(file_path)} - {loader_name} raised: {type(e).__name__} ({e})")
        return []
        
    return [] # ‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà raw_docs ‡πÄ‡∏õ‡πá‡∏ô List ‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤

    
# -------------------- Loaders --------------------

# üìå [FIXED] Full implementation using imported loaders and helper
FILE_LOADER_MAP = {
    # üü¢ FIX: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏°‡∏≤‡πÉ‡∏ä‡πâ UnstructuredFileLoader ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö OCR ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏ô‡∏ó‡∏≤‡∏ô
    ".pdf": lambda p: _load_document_with_loader(p, UnstructuredFileLoader), 
    
    ".docx": lambda p: _load_document_with_loader(p, UnstructuredWordDocumentLoader),
    ".txt": lambda p: _load_document_with_loader(p, TextLoader),
    ".xlsx": lambda p: _load_document_with_loader(p, UnstructuredExcelLoader),
    ".pptx": lambda p: _load_document_with_loader(p, UnstructuredPowerPointLoader),
    ".md": lambda p: _load_document_with_loader(p, TextLoader), 
    ".csv": lambda p: _load_document_with_loader(p, CSVLoader),
    
    # # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û (‡πÉ‡∏ä‡πâ UnstructuredFileLoader ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡∏¥‡∏î OCR ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢/‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡πÅ‡∏•‡πâ‡∏ß)
    # ".jpg": lambda p: _load_document_with_loader(p, UnstructuredFileLoader),
    # ".jpeg": lambda p: _load_document_with_loader(p, UnstructuredFileLoader),
    # ".png": lambda p: _load_document_with_loader(p, UnstructuredFileLoader),
    ".jpg": lambda p: _load_document_with_loader(p, TextLoader), 
    ".jpeg": lambda p: _load_document_with_loader(p, TextLoader),
    ".png": lambda p: _load_document_with_loader(p, TextLoader),
}

# -------------------- Normalization utility --------------------
def normalize_loaded_documents(raw_docs: List[Any], source_path: Optional[str] = None) -> List[Document]:
    """
    Converts raw loaded documents into clean LangChain Document objects.
    """
    normalized: List[Document] = []
    for idx, item in enumerate(raw_docs):
        try:
            if isinstance(item, Document): doc = item
            else: doc = Document(page_content=str(item), metadata={})
            
            # üü¢ CRITICAL FIX: ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ None, Normalize (NFKC), ‡πÅ‡∏•‡∏∞ Strip
            # (‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡∏Å‡πà‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô AttributeError: 'NoneType' object has no attribute 'strip')
            doc.page_content = unicodedata.normalize("NFKC", doc.page_content or "").strip() 
            
            # üìå REVISED CHECK: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÅ‡∏•‡πâ‡∏ß
            if not doc.page_content:
                logger.warning(f"‚ö†Ô∏è Doc #{idx} from loader has no content (Empty/None). Skipping normalization for this document.")
                continue # <-- ‡∏Ç‡πâ‡∏≤‡∏°‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
            
            # ... (‡πÇ‡∏Ñ‡πâ‡∏î‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏î‡∏¥‡∏°)
            if not isinstance(doc.metadata, dict): doc.metadata = {"_raw_meta": str(doc.metadata)}
            if source_path: doc.metadata.setdefault("source_file", os.path.basename(source_path))
            try: doc.metadata = _safe_filter_complex_metadata(doc.metadata)
            except Exception: doc.metadata = {"source_file": os.path.basename(source_path)} if source_path else {}
            normalized.append(doc)
            
        except Exception as e:
            logger.warning(f"normalize_loaded_documents: skipping item #{idx} due to error: {e}")
            continue
    return normalized

# üìå Global Text Splitter Configuration (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡πÉ‡∏ô Load & Chunk)
TEXT_SPLITTER = RecursiveCharacterTextSplitter(
    chunk_size=CHUNK_SIZE,              # ‡∏•‡∏î‡∏•‡∏á‡∏à‡∏≤‡∏Å‡∏Ñ‡πà‡∏≤ default (‡πÄ‡∏ä‡πà‡∏ô 1500‚Äì2000) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ granular ‡∏Ç‡∏∂‡πâ‡∏ô
    chunk_overlap=CHUNK_OVERLAP,            # overlap ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 10‚Äì15% ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢
    separators=[
        "\n\n",                   # ‡πÅ‡∏ö‡πà‡∏á‡∏¢‡πà‡∏≠‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏´‡∏ç‡πà
        "\n- ",                   # ‡πÅ‡∏ö‡πà‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£
        "\n‚Ä¢ ",                   # ‡πÅ‡∏ö‡πà‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£
        " ",                      # ‡∏ï‡∏±‡∏ß‡πÅ‡∏ö‡πà‡∏á‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ (Space)
        ""
    ]   ,
    length_function=len,
    is_separator_regex=False
)

# -------------------- Load & Chunk Document --------------------
def load_and_chunk_document(
    file_path: str,
    doc_id_key: str, 
    stable_doc_uuid: str, # ‚¨ÖÔ∏è ‡πÄ‡∏û‡∏¥‡πà‡∏° Stable UUID ‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤
    year: Optional[int] = None,
    version: str = "v1",
    metadata: Optional[Dict[str, Any]] = None,
    ocr_pages: Optional[Iterable[int]] = None
) -> List[Document]:
    """
    Load document, inject metadata, clean, and split into chunks.
    """
    file_extension = os.path.splitext(file_path)[1].lower()
    loader_func = FILE_LOADER_MAP.get(file_extension)
    
    if not loader_func:
        logger.error(f"No loader found for extension: {file_extension} at {file_path}")
        return []
        
    raw_docs = [] # <-- ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
    try:
        # üü¢ [Step 1] ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ Loader ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ (‡∏ã‡∏∂‡πà‡∏á‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß)
        raw_docs = loader_func(file_path)
        
    except ValidationError as e:
        loader_name = str(loader_func)
        if 'Unstructured' in loader_name or 'Unstructured' in str(loader_func):
             logger.warning(f"‚ö†Ô∏è OCR Crash Handled: {os.path.basename(file_path)} - Loader raised Pydantic ValidationError. Treating as 0 documents loaded.")
             raw_docs = [] 
        else:
             raise e 
             
    except Exception as e:
        logger.error(f"‚ùå Critical error during file loading: {file_path}. Error: {e}")
        raw_docs = []

    if not raw_docs:
        logger.warning(f"Loader returned 0 documents for {os.path.basename(file_path)}. Skipping chunking.")
        return []

    # ---------------------------------------------------------------------
    # üü¢ CRITICAL FIX: ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Document object ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å raw_docs (‡∏™‡∏≥‡∏£‡∏≠‡∏á)
    #    ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏Ñ‡∏ß‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÉ‡∏ô _load_document_with_loader ‡πÅ‡∏•‡πâ‡∏ß ‡πÅ‡∏ï‡πà‡∏ó‡∏≥‡∏ã‡πâ‡∏≥‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
    # ---------------------------------------------------------------------
    pre_cleaned_raw_docs = []
    for doc in raw_docs:
        if isinstance(doc, Document):
            pre_cleaned_raw_docs.append(doc)
        else:
            doc_type_str = str(type(doc)).split("'")[-2]
            logger.warning(f"‚ö†Ô∏è Loader for '{os.path.basename(file_path)}' returned non-Document object (Type: {doc_type_str}). Skipping normalization.")

    # 2. Normalize ‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î
    docs = normalize_loaded_documents(pre_cleaned_raw_docs, source_path=file_path)

    # Inject metadata before splitting
    for d in docs:
        if metadata:
            try:
                new_meta = d.metadata
                new_meta.update(metadata) 
                d.metadata = _safe_filter_complex_metadata(new_meta)
            except Exception:
                d.metadata["injected_metadata"] = str(metadata)
        
        if year: d.metadata["year"] = year
        d.metadata["version"] = version
        # d.metadata["doc_id"] = stable_doc_uuid 
        d.metadata["stable_doc_uuid"] = stable_doc_uuid
        d.metadata["source"] = d.metadata.get("source_file", os.path.basename(file_path))
        d.metadata = _safe_filter_complex_metadata(d.metadata) # Final filter

    # 3. Split into Chunks
    try:
        chunks = TEXT_SPLITTER.split_documents(docs) 
    except Exception as e:
        logger.error(f"Error during document splitting for {os.path.basename(file_path)}: {e}")
        chunks = docs # Fallback to using raw docs as chunks

    # ---------------------------------------------------------------------
    # üü¢ FINAL CRITICAL CLEANUP (NEW): ‡∏Å‡∏£‡∏≠‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ CHUNKS ‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà Log/Index
    #    ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏´‡∏¢‡∏∏‡∏î Log Noise ‡∏à‡∏≤‡∏Å LangChain Cleaner ‡πÑ‡∏î‡πâ 100%
    # ---------------------------------------------------------------------
    final_cleaned_chunks = []
    for c in chunks:
        if isinstance(c, Document):
             # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏° metadata ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ
             c.page_content = clean_text(c.page_content) 
             final_cleaned_chunks.append(c)
        else:
             logger.error(f"FATAL: Non-Document object found in 'chunks' list after splitting! Type: {type(c)}. Skipping.")
             
    chunks = final_cleaned_chunks # ‡πÉ‡∏ä‡πâ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÅ‡∏•‡πâ‡∏ß

    for idx, c in enumerate(chunks, start=1):
        # c.page_content = clean_text(c.page_content) # üö® ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ‡∏ñ‡∏π‡∏Å‡∏¢‡πâ‡∏≤‡∏¢‡πÑ‡∏õ‡∏ó‡∏≥‡πÉ‡∏ô loop ‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô‡πÅ‡∏•‡πâ‡∏ß
        c.metadata["chunk_index"] = idx
        c.metadata = _safe_filter_complex_metadata(c.metadata) 
        
    logger.info(f"Loaded and chunked {os.path.basename(file_path)} -> {len(chunks)} chunks.")
    return chunks


# -------------------- [REVISED] Process single document --------------------
def process_document(
    file_path: str,
    file_name: str,
    stable_doc_uuid: str, # ‚¨ÖÔ∏è (Warning: This is the 64-char SHA256 Hash)
    doc_type: Optional[str] = None,
    enabler: Optional[str] = None, 
    base_path: str = VECTORSTORE_DIR, # Assuming default value
    year: Optional[int] = None,
    version: str = "v1",
    metadata: dict = None,
    source_name_for_display: Optional[str] = None,
    ocr_pages: Optional[Iterable[int]] = None
) -> Tuple[List[Document], str, str]: 
    
    raw_doc_id_input = os.path.splitext(file_name)[0]
    filename_doc_id_key = _normalize_doc_id(raw_doc_id_input) # ‚¨ÖÔ∏è ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠ ID 34-char ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£!
            
    doc_type = doc_type or DEFAULT_DOC_TYPES
    
    # üìå Determine resolved enabler
    resolved_enabler = None
    if doc_type.lower() == EVIDENCE_DOC_TYPES:
        # Ensure DEFAULT_ENABLER is defined or handle its absence
        # DEFAULT_ENABLER = "KM" # Mocking if not defined
        resolved_enabler = (enabler or DEFAULT_ENABLER).upper()

    # üìå Inject enabler into metadata if it exists
    injected_metadata = metadata or {}
    if resolved_enabler:
        injected_metadata["enabler"] = resolved_enabler
        
    # --- DEBUG LOGS ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô ID ---
    filter_id_value = filename_doc_id_key # ‡∏Ñ‡πà‡∏≤ 34-char ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÄ‡∏Å‡πá‡∏ö
    logger.critical(f"================== START DEBUG INGESTION: {file_name} ==================")
    logger.critical(f"üîç DEBUG ID (stable_doc_uuid, 64-char Hash): {len(stable_doc_uuid)}-char: {stable_doc_uuid[:34]}...")
    logger.critical(f"‚úÖ FINAL ID TO STORE (34-char Ref ID): {len(filter_id_value)}-char: {filter_id_value[:34]}...")
    # ---------------------------------

    chunks = load_and_chunk_document(
        file_path=file_path,
        doc_id_key=filename_doc_id_key, 
        stable_doc_uuid=stable_doc_uuid,
        year=year,
        version=version,
        metadata=injected_metadata, 
        ocr_pages=ocr_pages
    )
    
    for c in chunks:
        # [2] ‡πÄ‡∏Å‡πá‡∏ö ID 64-char (‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô Ingestion ‡πÄ‡∏î‡∏¥‡∏°)
        # c.metadata["doc_id"] = stable_doc_uuid          
        c.metadata["stable_doc_uuid"] = stable_doc_uuid 
        
        # ‚úÖ ‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏µ‡∏¢‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ID 32-char (‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏î‡∏¥‡∏°)
        c.metadata["original_stable_id"] = filename_doc_id_key[:32].lower()  
        
        c.metadata["doc_type"] = doc_type 
        if resolved_enabler:
             c.metadata["enabler"] = resolved_enabler

        # üéØ FINAL FIX: ‡πÉ‡∏ä‡πâ filename_doc_id_key (34-char) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Hard Filter
        # c.metadata["assessment_filter_id"] = filter_id_value # <--- ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß
        
        c.metadata = _safe_filter_complex_metadata(c.metadata)
        logger.debug(f"Chunk metadata preview: {c.metadata}")
        
    return chunks, stable_doc_uuid, doc_type

# -------------------- Vectorstore / Mapping Utilities --------------------

_VECTORSTORE_SERVICE_CACHE = {} 

def get_vectorstore(collection_name: str = "default", base_path: str = VECTORSTORE_DIR) -> Chroma:
    """
    Loads or creates the Chroma vector store instance for a given collection.
    """
    cache_key = f"{collection_name}_{base_path}"
    if cache_key in _VECTORSTORE_SERVICE_CACHE:
        return _VECTORSTORE_SERVICE_CACHE[cache_key]
    if len(collection_name) < 3:
        raise ValueError(f"Collection name '{collection_name}' is too short. Chroma DB requires at least 3 characters.")
    
    # Load embeddings model once
    embeddings = _VECTORSTORE_SERVICE_CACHE.get("embeddings_model")
    if not embeddings:
        embeddings = HuggingFaceEmbeddings(model_name="intfloat/multilingual-e5-large")
        _VECTORSTORE_SERVICE_CACHE["embeddings_model"] = embeddings
        
    persist_directory = os.path.join(base_path, collection_name)
    os.makedirs(persist_directory, exist_ok=True)
    try:
        vectorstore = Chroma(
            collection_name=collection_name, 
            persist_directory=persist_directory, 
            embedding_function=embeddings
        )
        _VECTORSTORE_SERVICE_CACHE[cache_key] = vectorstore
        logger.info(f"‚úÖ Loaded Vector Store service for collection: {collection_name}")
        return vectorstore
    except Exception as e:
        logger.critical(f"‚ùå Failed to load Chroma for collection '{collection_name}': {e}")
        raise RuntimeError(f"Failed to initialize Chroma DB: {e}")
    

def create_vectorstore_from_documents(
    chunks: List[Document], 
    collection_name: str, 
    doc_mapping_db: Dict[str, Dict[str, Any]],
    base_path: str = VECTORSTORE_DIR
) -> Chroma:
    """
    Adds documents (chunks) to a vector store collection and manages the document ID mapping.
    """
    vectorstore = get_vectorstore(collection_name, base_path)
    if not chunks:
        logger.warning(f"No chunks provided for collection {collection_name}. Skipping indexing.")
        return vectorstore
        
    texts = [c.page_content for c in chunks]; metadatas = [c.metadata for c in chunks]
    
    try:
        ids = [str(uuid.uuid4()) for _ in texts]
        stable_doc_uuids_to_delete = set(); old_chunk_ids_to_delete = []
        
        # 1. Prepare to delete existing chunks for documents being updated
        for m in metadatas:
            s_uuid = m.get("stable_doc_uuid")
            if s_uuid and s_uuid in doc_mapping_db:
                existing_chunk_uuids = doc_mapping_db[s_uuid].get("chunk_uuids")
                if existing_chunk_uuids and isinstance(existing_chunk_uuids, list):
                    stable_doc_uuids_to_delete.add(s_uuid)
                    old_chunk_ids_to_delete.extend(existing_chunk_uuids)
                    doc_mapping_db[s_uuid]["chunk_uuids"] = [] # Clear mapping entry temporarily

        if old_chunk_ids_to_delete:
            vectorstore.delete(ids=old_chunk_ids_to_delete)
            logger.info(f"üßπ Deleted {len(old_chunk_ids_to_delete)} old chunks from Chroma.")
            
        # 2. Add new chunks
        vectorstore.add_texts(texts=texts, metadatas=metadatas, ids=ids)
        
        # 3. Update mapping with new chunk IDs
        for i, chunk_uuid in enumerate(ids):
            stable_doc_uuid = metadatas[i].get("stable_doc_uuid") 
            if stable_doc_uuid:
                if stable_doc_uuid not in doc_mapping_db:
                    # Should not happen if logic in ingest_all_files is correct, but safe check
                    doc_mapping_db[stable_doc_uuid] = {"chunk_uuids": []} 
                if chunk_uuid not in doc_mapping_db[stable_doc_uuid]["chunk_uuids"]:
                    doc_mapping_db[stable_doc_uuid]["chunk_uuids"].append(chunk_uuid)
                metadatas[i]["chunk_uuid"] = chunk_uuid 
                
        logger.info(f"‚úÖ Indexed {len(ids)} chunks and updated mapping for {collection_name}. Persist finished.")
    except Exception as e:
        logger.error(f"‚ùå Error during Chroma indexing or persisting for {collection_name}: {e}")
    return vectorstore


def save_doc_id_mapping(mapping_data: Dict[str, Dict[str, Any]], path: str):
    """Saves the document ID mapping file."""
    try:
        os.makedirs(os.path.dirname(path), exist_ok=True)
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(mapping_data, f, indent=4, ensure_ascii=False)
        logger.info(f"‚úÖ Successfully saved Doc ID Mapping to: {path}")
    except Exception as e:
        logger.error(f"‚ùå Failed to save Doc ID Mapping: {e}")

def load_doc_id_mapping(path: str) -> Dict[str, Dict[str, Any]]:
    """Loads the document ID mapping file."""
    if not os.path.exists(path): return {} 
    try:
        with open(path, 'r', encoding='utf-8') as f: return json.load(f)
    except Exception as e:
        logger.error(f"‚ùå Failed to load Doc ID Mapping from {path}: {e}")
        return {}
        
# -------------------- API Helper: Get UUIDs for RAG Filtering --------------------
def get_stable_uuids_by_doc_type(doc_types: List[str]) -> List[str]:
    """
    Retrieves Stable UUIDs for RAG filtering based on document types.
    """
    if not doc_types: return []
    doc_mapping_db = load_doc_id_mapping(MAPPING_FILE_PATH)
    target_uuids = []
    doc_type_set = {dt.lower() for dt in doc_types}
    for s_uuid, entry in doc_mapping_db.items():
        doc_type = entry.get("doc_type", "default")
        if doc_type.lower() in doc_type_set:
            target_uuids.append(s_uuid)
    return target_uuids
        

def create_stable_uuid_from_path(filepath: str, ref_id_key: Optional[str] = None) -> str:
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á UUID ‡∏ó‡∏µ‡πà‡∏°‡∏±‡πà‡∏ô‡∏Ñ‡∏á‡∏à‡∏≤‡∏Å‡∏û‡∏≤‡∏ò‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏ï‡πá‡∏°, ‡∏Ç‡∏ô‡∏≤‡∏î‡πÑ‡∏ü‡∏•‡πå, ‡πÅ‡∏•‡∏∞ Ref ID Key ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤
    ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏°‡∏µ ID ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏ó‡πâ‡∏à‡∏£‡∏¥‡∏á
    """
    try:
        full_path = os.path.abspath(filepath)
        file_size = os.path.getsize(filepath)
    except FileNotFoundError:
        # Fallback ‡∏ó‡∏µ‡πà‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏Å‡∏ß‡πà‡∏≤: ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡∏∞ Ref ID Key
        full_path = os.path.abspath(filepath)
        file_size = 0
    
    # üí° FIX: ‡∏£‡∏ß‡∏° Ref ID Key ‡πÉ‡∏ô unique_string ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏¢‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô
    unique_string = f"{full_path}-{file_size}-{ref_id_key or 'NO_REF'}" 
    
    # ‡πÉ‡∏ä‡πâ SHA-256 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á Hash ‡∏ó‡∏µ‡πà‡∏¢‡∏≤‡∏ß‡πÅ‡∏•‡∏∞‡∏°‡∏±‡πà‡∏ô‡∏Ñ‡∏á‡∏Å‡∏ß‡πà‡∏≤ SHA-1
    hash_object = hashlib.sha256(unique_string.encode('utf-8'))
    
    # ‡πÅ‡∏õ‡∏•‡∏á Hash ‡πÄ‡∏õ‡πá‡∏ô UUID-like string
    return hash_object.hexdigest()

# -------------------- [REVISED] ingest_all_files --------------------
def ingest_all_files(
    data_dir: str = DATA_DIR,
    doc_type: Optional[str] = None,
    enabler: Optional[str] = None, 
    base_path: str = VECTORSTORE_DIR,
    exclude_dirs: Set[str] = set(),
    year: Optional[int] = None,
    version: str = "v1",
    sequential: bool = True,
    skip_ext: Optional[List[str]] = None,
    log_every: int = 50,
    batch_size: int = 500
) -> List[Dict[str, Any]]:
    """
    Ingest documents into vectorstore based on doc_type and enabler filters.
    Handles filenames with spaces, (), [], and unicode characters.
    """
    skip_ext = skip_ext or []
    os.makedirs(data_dir, exist_ok=True)
    os.makedirs(base_path, exist_ok=True)
    files_to_process = []
    
    doc_type_req = (doc_type or "all").lower()
    enabler_req = (enabler or (DEFAULT_ENABLER if doc_type_req == EVIDENCE_DOC_TYPES else None))
    if enabler_req:
        enabler_req = enabler_req.upper()

    logger.info(f"Starting ingest_all_files: doc_type_req='{doc_type_req}', enabler_req='{enabler_req}'")

    # üìå [FIXED: Flexible Source Dir] ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Root Path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡πÅ‡∏Å‡∏ô
    scan_roots: List[str] = []
    
    if doc_type_req == "all":
        # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô 'all' ‡πÉ‡∏´‡πâ‡∏™‡πÅ‡∏Å‡∏ô‡∏ó‡∏∏‡∏Å‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö
        for dt in SUPPORTED_DOC_TYPES:
             if dt == EVIDENCE_DOC_TYPES:
                for ena in SUPPORTED_ENABLERS:
                    scan_roots.append(_get_source_dir(dt, ena, data_dir))
             else:
                scan_roots.append(_get_source_dir(dt, None, data_dir))
    elif doc_type_req in SUPPORTED_DOC_TYPES:
        # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô Specific Doc Type ‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏û‡∏≤‡∏ò‡πÄ‡∏â‡∏û‡∏≤‡∏∞
        if doc_type_req == EVIDENCE_DOC_TYPES and not enabler_req:
             # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô evidence ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏ enabler ‡πÉ‡∏´‡πâ‡∏™‡πÅ‡∏Å‡∏ô‡∏ó‡∏∏‡∏Å enabler
             for ena in SUPPORTED_ENABLERS:
                 scan_roots.append(_get_source_dir(doc_type_req, ena, data_dir))
        else:
             scan_roots = [_get_source_dir(doc_type_req, enabler_req, data_dir)]
    else:
        logger.error(f"Invalid doc_type_req for ingestion: {doc_type_req}")
        return []

    # 1. ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î based on scanning calculated source dir(s)
    for root_to_scan in set(scan_roots):
        if not os.path.isdir(root_to_scan):
             logger.warning(f"‚ö†Ô∏è Source directory not found: {root_to_scan}. Skipping scan.")
             continue
             
        # Collection name ‡∏Ñ‡∏∑‡∏≠‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏™‡πÅ‡∏Å‡∏ô (e.g., 'document' ‡∏´‡∏£‡∏∑‡∏≠ 'evidence_km')
        current_collection_name = os.path.basename(root_to_scan) 
        original_doc_type, resolved_enabler = _parse_collection_name(current_collection_name)
        
        logger.info(f"Scanning source directory: {root_to_scan} (Maps to: {current_collection_name})")

        for root, dirs, filenames in os.walk(root_to_scan):
            dirs[:] = [d for d in dirs if d not in exclude_dirs]
            # ‡∏™‡πÅ‡∏Å‡∏ô‡πÅ‡∏Ñ‡πà‡∏ä‡∏±‡πâ‡∏ô‡πÅ‡∏£‡∏Å‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ source_dir
            if root != root_to_scan: continue 

            for f in filenames:
                # Skip hidden files
                if f.startswith('.'):
                    continue

                file_path = os.path.join(root, f)
                file_extension = os.path.splitext(f)[1].lower()

                # Skip unsupported types
                if file_extension not in SUPPORTED_TYPES:
                    logger.info(f"‚ö†Ô∏è Skipping unsupported file type {file_extension}: {f}")
                    continue

                # Skip extensions in skip_ext
                if skip_ext and file_extension in skip_ext:
                    logger.info(f"‚ö†Ô∏è Skipping excluded extension {file_extension}: {f}")
                    continue

                # ‚úÖ Append file 
                files_to_process.append({
                    "file_path": file_path,
                    "file_name": f,
                    "doc_type": original_doc_type,
                    "enabler": resolved_enabler,
                    "collection_name": current_collection_name
                })

    if not files_to_process:
        logger.warning("‚ö†Ô∏è No files found to ingest!")
        return []

    # 2. Load Mapping DB ‡πÅ‡∏•‡∏∞‡∏Å‡∏≥‡∏´‡∏ô‡∏î Stable IDs (***REVISED LOGIC***)
    doc_mapping_db = load_doc_id_mapping(MAPPING_FILE_PATH)
    
    # üìå REVISED: ‡πÉ‡∏ä‡πâ Stable UUID ‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏à‡∏≤‡∏Å File Path ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏µ‡∏¢‡πå‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
    uuid_from_path_lookup: Dict[str, str] = {
        entry["filepath"]: s_uuid 
        for s_uuid, entry in doc_mapping_db.items() 
        if "filepath" in entry # ‡πÉ‡∏ä‡πâ filepath ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏¥‡∏°
    } 

    # üìå REVISED: ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Stable UUID ‡∏ó‡∏µ‡∏•‡∏∞‡πÑ‡∏ü‡∏•‡πå‡πÇ‡∏î‡∏¢‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å Path
    for file_info in files_to_process:
        # 0. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì filename_doc_id_key (‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô REF ID)
        filename_doc_id_key = _normalize_doc_id(os.path.splitext(file_info["file_name"])[0])
        file_info["doc_id_key"] = filename_doc_id_key # ‡πÄ‡∏Å‡πá‡∏ö Ref ID ‡πÑ‡∏ß‡πâ‡πÉ‡∏ô file_info

        # üü¢ A. ‡∏™‡∏£‡πâ‡∏≤‡∏á UUID ‡∏ó‡∏µ‡πà‡∏°‡∏±‡πà‡∏ô‡∏Ñ‡∏á‡∏à‡∏≤‡∏Å File Path (‡πÉ‡∏´‡∏°‡πà)
        # üí° FIX: ‡∏™‡πà‡∏á filename_doc_id_key ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÄ‡∏õ‡πá‡∏ô Ref ID Key
        stable_uuid_from_path = create_stable_uuid_from_path(
            file_info["file_path"], 
            ref_id_key=filename_doc_id_key # <--- ‡∏™‡πà‡∏á Ref ID Key ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ
        ) 
        
        # üü¢ B. ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ ID ‡πÄ‡∏î‡∏¥‡∏°‡∏à‡∏≤‡∏Å Mapping ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ File Path ‡πÄ‡∏õ‡πá‡∏ô Key
        stable_doc_uuid = uuid_from_path_lookup.get(file_info["file_path"])
        
        if stable_doc_uuid and stable_doc_uuid in doc_mapping_db:
            # ‡πÉ‡∏ä‡πâ UUID ‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏µ‡πà‡∏û‡∏ö
            file_info["stable_doc_uuid"] = stable_doc_uuid
            # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏õ
            doc_mapping_db[stable_doc_uuid].update({
                "filename": file_info["file_name"],
                "doc_type": file_info["doc_type"],
                "enabler": file_info["enabler"],
                "doc_id_key": filename_doc_id_key, # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï Ref ID Key
            })
        else:
            # üü¢ C. ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ ID ‡πÄ‡∏î‡∏¥‡∏° 
            new_uuid = stable_uuid_from_path # ‡πÉ‡∏ä‡πâ Stable ID ‡πÉ‡∏´‡∏°‡πà
            file_info["stable_doc_uuid"] = new_uuid
            
            # üü¢ D. ‡∏™‡∏£‡πâ‡∏≤‡∏á Mapping entry ‡πÉ‡∏´‡∏°‡πà
            doc_mapping_db[new_uuid] = {
                "file_name": file_info["file_name"],
                "doc_type": file_info["doc_type"],
                "enabler": file_info["enabler"],
                "doc_id_key": filename_doc_id_key, # ‡πÄ‡∏Å‡πá‡∏ö Ref ID Key
                "filepath": file_info["file_path"], # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å File Path
                "notes": "",
                "statement_id": "",
                "chunk_uuids": []
            }
            uuid_from_path_lookup[file_info["file_path"]] = new_uuid # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï Lookup

    # 3. Load & Chunk (sequential or threaded)
    all_chunks: List[Document] = []
    results: List[Dict[str, Any]] = []

    def _process_file_task(file_info: Dict[str, str]):
        return process_document(
            file_path=file_info["file_path"],
            file_name=file_info["file_name"],
            stable_doc_uuid=file_info["stable_doc_uuid"],
            doc_type=file_info["doc_type"],
            enabler=file_info["enabler"],
            base_path=base_path,
            year=year,
            version=version
        )

    if sequential:
        for idx, file_info in enumerate(files_to_process, 1):
            f = file_info["file_name"]
            stable_doc_uuid = file_info["stable_doc_uuid"]
            try:
                chunks, doc_id, dt = _process_file_task(file_info)
                all_chunks.extend(chunks)
                results.append({"file": f, "doc_id": doc_id, "doc_type": dt, "status": "chunked", "chunks": len(chunks)})
                if idx % log_every == 0:
                    logger.info(f"Processed {idx}/{len(files_to_process)} files...")
            except Exception as e:
                results.append({"file": f, "doc_id": stable_doc_uuid, "doc_type": file_info["doc_type"], "status": "failed_chunk", "error": str(e)})
                # üìå [FIXED: Detailed Process/Chunk Failure Logging]
                logger.error(f"‚ùå CHUNK/PROCESS FAILED: {f} (ID: {stable_doc_uuid}) - {type(e).__name__} ({e})")
    else:
        max_workers = os.cpu_count() or 4
        # max_workers = 1
        logger.info(f"Using ThreadPoolExecutor with max_workers={max_workers}")
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_file = {executor.submit(_process_file_task, fi): fi for fi in files_to_process}
            for idx, future in enumerate(as_completed(future_to_file), 1):
                fi = future_to_file[future]
                f = fi["file_name"]
                stable_doc_uuid = fi["stable_doc_uuid"]
                try:
                    chunks, doc_id, dt = future.result()
                    all_chunks.extend(chunks)
                    results.append({"file": f, "doc_id": doc_id, "doc_type": dt, "status": "chunked", "chunks": len(chunks)})
                except Exception as e:
                    results.append({"file": f, "doc_id": stable_doc_uuid, "doc_type": fi["doc_type"], "status": "failed_chunk", "error": str(e)})
                    # üìå [FIXED: Detailed Process/Chunk Failure Logging]
                    logger.error(f"‚ùå CHUNK/PROCESS FAILED: {f} (ID: {stable_doc_uuid}) - {type(e).__name__} ({e})")
                if idx % log_every == 0:
                    logger.info(f"Processed {idx}/{len(files_to_process)} files...")

    # 4. Group & Index Chunks
    chunks_by_collection: Dict[str, List[Document]] = {}
    for chunk in all_chunks:
        dt = chunk.metadata.get("doc_type", "default")
        ena = chunk.metadata.get("enabler")
        coll_name = get_target_dir(dt, ena)
        chunks_by_collection.setdefault(coll_name, []).append(chunk)

    for coll_name, coll_chunks in chunks_by_collection.items():
        if not coll_chunks:
            logger.warning(f"Skipping collection '{coll_name}' - 0 chunks found.")
            continue
        logger.info(f"--- Indexing collection '{coll_name}' ({len(coll_chunks)} chunks) ---")
        for i in range(0, len(coll_chunks), batch_size):
            batch = coll_chunks[i:i+batch_size]
            logger.info(f"Indexing chunks {i+1} to {i+len(batch)} of {len(coll_chunks)}")
            try:
                create_vectorstore_from_documents(
                    chunks=batch,
                    collection_name=coll_name,
                    doc_mapping_db=doc_mapping_db,
                    base_path=base_path
                )
            except Exception as e:
                logger.error(f"Error indexing chunks {i+1}-{i+len(batch)}: {e}")

    save_doc_id_mapping(doc_mapping_db, MAPPING_FILE_PATH)
    logger.info("‚úÖ Batch ingestion process finished.")
    return results


# -------------------- [REVISED] wipe_vectorstore --------------------
def wipe_vectorstore(
    doc_type_to_wipe: str = 'all', 
    enabler: Optional[str] = None, 
    base_path: str = VECTORSTORE_DIR
):
    """Wipes the vector store directory/collection(s) and updates the doc_id_mapping file."""
    
    doc_type_to_wipe = doc_type_to_wipe.lower()
    
    collections_to_delete: List[str] = []
    
    if doc_type_to_wipe == 'all':
        logger.warning(f"Wiping ALL collections in {base_path}")
        if os.path.exists(base_path):
            # Only list directories that are likely collections (avoid .gitkeep, etc.)
            collections_to_delete = [f for f in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, f))]
    elif doc_type_to_wipe == 'evidence':
        if enabler:
            # Wipe specific evidence enabler
            collection_name = get_target_dir(EVIDENCE_DOC_TYPES, enabler)
            collections_to_delete.append(collection_name)
        else:
            # Wipe ALL evidence enablers
            logger.warning("Wiping ALL evidence_* collections.")
            # evidence_paths = glob.glob(os.path.join(base_path, "evidence_*"))
            evidence_paths = glob.glob(os.path.join(base_path, f"{EVIDENCE_DOC_TYPES}_*"))
            collections_to_delete = [os.path.basename(p) for p in evidence_paths]
    elif doc_type_to_wipe in SUPPORTED_DOC_TYPES:
        collection_name = get_target_dir(doc_type_to_wipe, None)
        collections_to_delete.append(collection_name)
    else:
        logger.error(f"Invalid doc_type_to_wipe: {doc_type_to_wipe}")
        return

    # 1. Delete collection folders
    for col_name in collections_to_delete:
        col_path = os.path.join(base_path, col_name)
        if os.path.exists(col_path):
            try:
                shutil.rmtree(col_path)
                logger.info(f"‚úÖ Deleted vector store collection: {col_path}")
            except OSError as e:
                logger.error(f"‚ùå Error deleting collection {col_name}: {e}")

    # 2. Update Mapping file
    doc_mapping_db = load_doc_id_mapping(MAPPING_FILE_PATH)
    uuids_to_keep = {}
    deletion_count = 0
    
    for s_uuid, entry in doc_mapping_db.items():
        entry_doc_type = entry.get("doc_type")
        entry_enabler = entry.get("enabler")
        entry_collection = get_target_dir(entry_doc_type, entry_enabler)
        
        if entry_collection not in collections_to_delete:
            uuids_to_keep[s_uuid] = entry
        else:
            deletion_count += 1
            
    if deletion_count > 0:
        save_doc_id_mapping(uuids_to_keep, MAPPING_FILE_PATH)
        logger.info(f"üßπ Removed {deletion_count} entries from mapping file for deleted collections.")
        
    if doc_type_to_wipe == 'all' and os.path.exists(MAPPING_FILE_PATH):
         # Also delete the mapping file itself if wiping 'all'
         try:
            os.remove(MAPPING_FILE_PATH)
            logger.info(f"‚úÖ Deleted Doc ID mapping file: {MAPPING_FILE_PATH}")
         except OSError as e:
            logger.error(f"‚ùå Error deleting mapping file: {e}")


# -------------------- [REVISED] Document Management Utilities --------------------

def delete_document_by_uuid(
    stable_doc_uuid: str, 
    collection_name: Optional[str] = None, 
    doc_type: Optional[str] = None, 
    enabler: Optional[str] = None, 
    base_path: str = VECTORSTORE_DIR
) -> bool:
    """
    Deletes all chunks associated with the given Stable Document UUID.
    """
    
    doc_mapping_db = load_doc_id_mapping(MAPPING_FILE_PATH)
    
    if stable_doc_uuid not in doc_mapping_db:
        logger.warning(f"Deletion skipped: Stable Doc UUID {stable_doc_uuid} not found in mapping.")
        return False

    doc_entry = doc_mapping_db[stable_doc_uuid]
    all_chunk_uuids = doc_entry.get("chunk_uuids", [])
    
    # üìå Determine collection name from mapping
    doc_type_from_map = doc_entry.get("doc_type")
    enabler_from_map = doc_entry.get("enabler")
    
    # Use provided doc_type/enabler as override, otherwise use mapping
    final_doc_type = doc_type or doc_type_from_map
    final_enabler = enabler or enabler_from_map
    
    if not final_doc_type:
         logger.error(f"Cannot delete {stable_doc_uuid}: doc_type not found in mapping or provided.")
         return False
         
    final_collection_name = get_target_dir(final_doc_type, final_enabler)
    
    if not all_chunk_uuids:
        logger.warning(f"Deletion skipped: Stable Doc UUID {stable_doc_uuid} has no chunk UUIDs recorded.")
        del doc_mapping_db[stable_doc_uuid]
        save_doc_id_mapping(doc_mapping_db, MAPPING_FILE_PATH)
        return True

    persist_directory = os.path.join(base_path, final_collection_name)
    if not os.path.isdir(persist_directory):
        logger.warning(f"Vectorstore directory not found for collection '{final_collection_name}'. Skipping Chroma deletion.")
    else:
        try:
            vectorstore = get_vectorstore(final_collection_name, base_path)
            vectorstore.delete(ids=all_chunk_uuids)
            logger.info(f"‚úÖ Successfully deleted {len(all_chunk_uuids)} chunks from collection '{final_collection_name}' for UUID: {stable_doc_uuid}")
        except Exception as e:
            logger.error(f"‚ùå Error during Chroma deletion for collection '{final_collection_name}' (UUID: {stable_doc_uuid}): {e}")
            
    # Delete from mapping regardless of vectorstore success
    del doc_mapping_db[stable_doc_uuid]
    save_doc_id_mapping(doc_mapping_db, MAPPING_FILE_PATH)
        
    return True

def list_documents(
    doc_types: Optional[List[str]] = None,
    enabler: Optional[str] = None, 
    show_results: str = "ingested" 
) -> Dict[str, Any]: 
    
    doc_mapping_db = load_doc_id_mapping(MAPPING_FILE_PATH)
    all_docs: Dict[str, Any] = {}
    
    # üü¢ [FIX: ‡πÉ‡∏ä‡πâ Filepath ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô Unique ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Stable UUID ‡πÅ‡∏ó‡∏ô doc_id_key]
    filepath_to_stable_uuid: Dict[str, str] = {
        entry["filepath"]: s_uuid 
        for s_uuid, entry in doc_mapping_db.items() 
        if "filepath" in entry # ‡πÉ‡∏ä‡πâ filepath ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏¥‡∏°
    } 
            
    # Filtering setup
    doc_type_reqs = {dt.lower() for dt in doc_types} if doc_types and doc_types[0] and doc_types[0].lower() != "all" else set()
    enabler_req = (enabler or "").upper()
    
    # üìå [FIXED: Flexible Source Dir] ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Root Path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡πÅ‡∏Å‡∏ô
    source_dirs_to_scan: List[str] = []

    if not doc_type_reqs: # 'all'
        # Include all supported doc types and enablers
        for dt in SUPPORTED_DOC_TYPES:
            if dt == EVIDENCE_DOC_TYPES:
                if enabler_req:
                     source_dirs_to_scan.append(_get_source_dir(dt, enabler_req))
                else:
                     for ena in SUPPORTED_ENABLERS:
                         source_dirs_to_scan.append(_get_source_dir(dt, ena))
            else:
                source_dirs_to_scan.append(_get_source_dir(dt, None))
    else:
        # Include only requested doc types and enablers
        for dt in doc_type_reqs:
            if dt == EVIDENCE_DOC_TYPES:
                if enabler_req:
                    source_dirs_to_scan.append(_get_source_dir(dt, enabler_req))
                else:
                    for ena in SUPPORTED_ENABLERS:
                        source_dirs_to_scan.append(_get_source_dir(dt, ena))
            elif dt in SUPPORTED_DOC_TYPES:
                source_dirs_to_scan.append(_get_source_dir(dt, None))
                
    # 1. Scan files and build DocInfo list (all_docs)
    for root_to_scan in set(source_dirs_to_scan): # ‡πÉ‡∏ä‡πâ set ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡πÅ‡∏Å‡∏ô‡∏ã‡πâ‡∏≥
        if not os.path.isdir(root_to_scan):
            logger.debug(f"Source directory not found: {root_to_scan}. Skipping scan.")
            continue
            
        current_collection_name = os.path.basename(root_to_scan)
        original_doc_type, resolved_enabler = _parse_collection_name(current_collection_name)
        
        for root, _, filenames in os.walk(root_to_scan):
            if root != root_to_scan: continue # ‡∏™‡πÅ‡∏Å‡∏ô‡πÅ‡∏Ñ‡πà‡∏ä‡∏±‡πâ‡∏ô‡πÅ‡∏£‡∏Å
            
            for f in filenames:
                file_extension = os.path.splitext(f)[1].lower()
                if f.startswith('.') or file_extension not in SUPPORTED_TYPES:
                    continue
                    
                file_path = os.path.join(root, f)
                file_name_no_ext = os.path.splitext(f)[0]
                filename_doc_id_key = _normalize_doc_id(file_name_no_ext)
                
                # üü¢ [FIX: ‡πÉ‡∏ä‡πâ filepath ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ lookup Stable UUID]
                stable_doc_uuid = filepath_to_stable_uuid.get(file_path)
                
                doc_entry = doc_mapping_db.get(stable_doc_uuid) if stable_doc_uuid else None
                
                chunk_uuids = doc_entry.get("chunk_uuids", []) if doc_entry else []
                chunk_count = len(chunk_uuids)
                is_ingested = chunk_count > 0
                
                # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏ö Stable UUID ‡πÉ‡∏ô mapping (‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà) ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ TEMP_ID
                final_doc_id = stable_doc_uuid or f"TEMP_ID__{filename_doc_id_key}"
                
                try:
                    upload_date = datetime.fromtimestamp(os.path.getmtime(file_path), timezone.utc).isoformat()
                    file_size = os.path.getsize(file_path)
                except FileNotFoundError:
                    upload_date = datetime.now(timezone.utc).isoformat()
                    file_size = 0

                doc_info: Any = {
                    "doc_id": final_doc_id,
                    "doc_id_key": filename_doc_id_key,
                    "filename": f,
                    "filepath": file_path,
                    "doc_type": original_doc_type,
                    "enabler": resolved_enabler, 
                    "upload_date": upload_date,
                    "chunk_count": chunk_count,
                    "status": "Ingested" if is_ingested else "Pending", 
                    "size": file_size,
                }
                all_docs[final_doc_id] = doc_info

    # NEW DEBUG LOG: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å Initial Filter
    logger.info(f"DEBUG: Total physical files found (len(all_docs)): {len(all_docs)}")

    # START OF COUNTING LOGIC 
    total_supported_files = len(all_docs) 
    
    # 2. Apply show_results filtering and calculate X
    show_results_lower = show_results.lower()
    
    filtered_docs_dict: Dict[str, Any] = {}
    
    if total_supported_files == 0:
        doc_types_str = doc_types[0] if doc_types and doc_types[0] else "all"
        logger.warning(f"‚ö†Ô∏è No documents found in DATA_DIR matching the requested type '{doc_types_str}' (Enabler: {enabler_req or 'ALL'}).")
        return filtered_docs_dict 
    
    if show_results_lower == "ingested":
        filtered_docs_dict = {
            k: d for k, d in all_docs.items() 
            if d.get('status', '').lower() == 'ingested' and not k.startswith('TEMP_ID__')
        }
        filter_name = "INGESTED (Successful / Unique Doc IDs)"
        display_count_x = len(filtered_docs_dict)
        
    elif show_results_lower == "failed":
        filtered_docs_dict = {
            k: d for k, d in all_docs.items() 
            if d.get('status', '').lower() == 'failed'
        }
        display_count_x = len(filtered_docs_dict) 
        filter_name = "FAILED/SKIPPED (Physical Files)"
        
    elif show_results_lower == "full":
        filtered_docs_dict = all_docs
        display_count_x = len(all_docs) 
        filter_name = "FULL (All Supported Files)"
        
    else:
        # Default: ingested
        filtered_docs_dict = {
            k: d for k, d in all_docs.items() 
            if d.get('status', '').lower() == 'ingested' and not k.startswith('TEMP_ID__')
        }
        display_count_x = len(filtered_docs_dict)
        filter_name = "INGESTED (Default / Unique Doc IDs)"


    # 3. Format for console output and printing
    
    display_list = []
    for doc_info in filtered_docs_dict.values(): 
        file_size_mb = doc_info['size'] / (1024 * 1024)
        enabler_display = doc_info['enabler'] if doc_info['enabler'] is not None else '-'
        
        display_list.append({
            "doc_id": doc_info["doc_id"],
            "file_name": doc_info["filename"],
            "doc_type": doc_info["doc_type"],
            "enabler": enabler_display,
            "size_mb": file_size_mb,
            "status": doc_info["status"],
            "chunk_count": doc_info["chunk_count"],
            "ref_doc_id": doc_info["doc_id_key"]
        })
        
    display_list.sort(key=lambda x: (x["doc_type"], x["file_name"]))
    
    doc_types_str = doc_types[0] if doc_types and doc_types[0] else "all"
    
    print(f"\nFound {display_count_x}/{total_supported_files} supported documents for type '{doc_types_str}' (Filter: {filter_name}):\n")

    if not display_list:
        print("--- No documents found matching the filter criteria to display ---")
        return filtered_docs_dict 

    # üü¢ FIX: ‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå UUID ‡πÄ‡∏õ‡πá‡∏ô 65 ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô 182
    UUID_COL_WIDTH = 65
    NEW_TABLE_WIDTH = 182 
    
    print("-" * NEW_TABLE_WIDTH)
    print(f"{'DOC ID (Stable/Temp)':<{UUID_COL_WIDTH}} | {'FILENAME':<35} | {'EXT':<5} | {'TYPE':<10} | {'ENB':<5} | {'SIZE(MB)':<9} | {'STATUS':<10} | {'CHUNKS':<8} | {'REF ID (Old Key)'}")
    print("-" * NEW_TABLE_WIDTH)
    
    for info in display_list:
        # üü¢ FIX: ‡πÉ‡∏ä‡πâ UUID ‡πÄ‡∏ï‡πá‡∏° ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏ó‡∏≠‡∏ô
        full_doc_id = info['doc_id'] 
        file_name, file_ext = os.path.splitext(info['file_name'])
        short_filename = file_name[:33] if len(file_name) > 33 else file_name 
        file_ext = file_ext[1:].upper() if file_ext else '-' 
        size_str = f"{info['size_mb']:.2f}"
        short_ref_doc_id = info['ref_doc_id'][:20] if len(info['ref_doc_id']) > 20 else info['ref_doc_id']
        enabler_display = info['enabler'] 
        
        print(
            f"{full_doc_id:<{UUID_COL_WIDTH}} | " # üü¢ FIX: ‡πÉ‡∏ä‡πâ full_doc_id ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏ß‡πâ‡∏≤‡∏á 65
            f"{short_filename:<35} | " 
            f"{file_ext:<5} | "
            f"{info['doc_type']:<10} | "
            f"{enabler_display:<5} | " 
            f"{size_str:<9} | "
            f"{info['status']:<10} | "
            f"{info['chunk_count']:<8} | "
            f"{short_ref_doc_id}"
        )
    print("-" * NEW_TABLE_WIDTH)
    print(f"\nFound {display_count_x}/{total_supported_files} supported documents for type '{doc_types_str}' (Filter: {filter_name}):\n")

    
    # 4. Return the filtered list
    return filtered_docs_dict
