# core/seam_assessment.py

import sys
import json
import logging
import time
import os
from typing import List, Dict, Any, Optional, Union, Tuple, Set, Final, Literal
from collections import defaultdict
from datetime import datetime
from dataclasses import dataclass, field
import multiprocessing # NEW: Import for parallel execution
from functools import partial
from core.llm_data_utils import enhance_query_for_statement
import pathlib, uuid
from langchain_core.documents import Document as LcDocument
from core.retry_policy import RetryPolicy, RetryResult
from copy import deepcopy
import tempfile
import shutil
# from json_extractor import _robust_extract_json
from .json_extractor import _robust_extract_json
from filelock import FileLock  # ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á: pip install filelock
import re


# -------------------- PATH SETUP & IMPORTS --------------------
try:
    PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
    if PROJECT_ROOT not in sys.path:
        sys.path.append(PROJECT_ROOT)

    from config.global_vars import (
        EXPORTS_DIR, MAX_LEVEL, INITIAL_LEVEL, FINAL_K_RERANKED,
        RUBRIC_FILENAME_PATTERN, RUBRIC_CONFIG_DIR, DEFAULT_ENABLER,
        EVIDENCE_DOC_TYPES, INITIAL_TOP_K,
        EVIDENCE_MAPPING_FILENAME_SUFFIX,
        LIMIT_CHUNKS_PER_PRIORITY_DOC,
        IS_LOG_L3_CONTEXT,
        PRIORITY_CHUNK_LIMIT,
        DEFAULT_TENANT,
        DEFAULT_YEAR,
        RERANK_THRESHOLD,
        MAX_EVI_STR_CAP,
        DEFAULT_LLM_MODEL_NAME,
        LLM_TEMPERATURE

    )
    
    from core.llm_data_utils import ( 
        create_structured_action_plan, evaluate_with_llm,
        retrieve_context_with_filter, retrieve_context_for_low_levels,
        evaluate_with_llm_low_level, LOW_LEVEL_K, 
        set_mock_control_mode as set_llm_data_mock_mode,
        create_context_summary_llm,
        retrieve_context_by_doc_ids,
        _fetch_llm_response,
        build_multichannel_context_for_level
    )
    from core.vectorstore import VectorStoreManager, load_all_vectorstores, get_global_reranker 
    from core.seam_prompts import PDCA_PHASE_MAP 

    # üéØ FIX: Import ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô Path Utility ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
    from utils.path_utils import (
        get_mapping_file_path, 
        get_evidence_mapping_file_path, 
        get_contextual_rules_file_path,
        get_doc_type_collection_key,
        get_rubric_file_path # <--- ‡∏ï‡πâ‡∏≠‡∏á Import ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏î‡πâ‡∏ß‡∏¢
    )
        
    import assessments.seam_mocking as seam_mocking 
    
except ImportError as e:
    # -------------------- Fallback Code (Same as previous) --------------------
    print(f"FATAL ERROR: Failed to import required modules. Error: {e}", file=sys.stderr)
    
    # Define placeholder variables if imports fail
    EXPORTS_DIR = "exports"
    MAX_LEVEL = 5
    INITIAL_LEVEL = 1
    FINAL_K_RERANKED = 3
    RUBRIC_FILENAME_PATTERN = "{tenant}_{enabler}_rubric.json"
    RUBRIC_CONFIG_DIR = "config/rubrics"
    DEFAULT_ENABLER = "KM"
    EVIDENCE_DOC_TYPES = "evidence"
    INITIAL_TOP_K = 10
    
    def create_structured_action_plan(*args, **kwargs): return [{"Phase": "Mock Plan", "Goal": "Resolve issue"}]
    def evaluate_with_llm(*args, **kwargs): return {"score": 1, "reason": "Mock pass", "is_passed": True}
    def retrieve_context_with_filter(*args, **kwargs): return {"top_evidences": [], "aggregated_context": "Mock Context"}
    def retrieve_context_for_low_levels(*args, **kwargs): return {"top_evidences": [], "aggregated_context": "Mock Low Context"}
    def evaluate_with_llm_low_level(*args, **kwargs): return {"score": 1, "reason": "Mock pass L1/L2", "is_passed": True}
    LOW_LEVEL_K = 2
    def set_llm_data_mock_mode(mode): pass
    class VectorStoreManager: pass
    def load_all_vectorstores(*args, **kwargs): return VectorStoreManager()
    PDCA_PHASE_MAP = {1: "Plan", 2: "Do", 3: "Check", 4: "Act", 5: "Innovate"}
    class seam_mocking:
        @staticmethod
        def evaluate_with_llm_CONTROLLED_MOCK(*args, **kwargs): return {"score": 0, "reason": "Mock fail", "is_passed": False}
        @staticmethod
        def retrieve_context_with_filter_MOCK(*args, **kwargs): return {"top_evidences": [], "aggregated_context": "Mock Context"}
        @staticmethod
        def create_structured_action_plan_MOCK(*args, **kwargs): return [{"Phase": "Mock Plan", "Goal": "Resolve issue"}]
        @staticmethod
        def set_mock_control_mode(mode): pass
    
    # üìå Placeholder functions for path_utils if the main import fails
    def get_mapping_file_path(*args, **kwargs): return "config/mapping/default/mapping.json"
    def get_evidence_mapping_file_path(*args, **kwargs): return "config/mapping/default/evidence_mapping.json"
    def get_contextual_rules_file_path(*args, **kwargs): return "config/rubrics/default/contextual_rules.json"
    def get_rubric_file_path(*args, **kwargs): return "config/rubrics/default/rubric.json"
    
    if "FATAL ERROR" in str(e):
        pass 
    # ---------------------------------------------------------------------- 


logger = logging.getLogger(__name__)
if not logger.handlers:
    logging.basicConfig(level=logging.DEBUG, format="%(asctime)s - %(levelname)s - %(message)s")

# =================================================================
# üü¢ FIX: Helper Function for PDCA Calculation (Priority 1 Part 2 & Priority 2)
# NOTE: ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡πÅ‡∏õ‡∏•‡∏á llm_score (1-5) ‡πÄ‡∏õ‡πá‡∏ô PDCA Breakdown ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
# =================================================================
LEVEL_PHASE_MAP = {
    1: ['P'],
    2: ['P', 'D'],
    3: ['P', 'D', 'C'],
    4: ['P', 'D', 'C', 'A'],
    5: ['P', 'D', 'C', 'A'] # L5 ‡πÉ‡∏ä‡πâ P, D, C, A ‡πÄ‡∏ä‡πà‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö L4 ‡πÅ‡∏ï‡πà‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏ï‡πá‡∏°‡∏≠‡∏≤‡∏à‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô
}

# ----------------------------------------------------------------------
# NEW CONSTANT: ‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô PDCA ‡∏ó‡∏µ‡πà '‡∏ú‡πà‡∏≤‡∏ô' ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ Level (Achieved Score)
# ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Achieved Score (Sum of P,D,C,A) ‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö Required Score (R)
# L1 (R=1, A=1): P=1
# L2 (R=2, A=2): P=1, D=1
# L3 (R=4, A=4): P=1, D=1, C=1, A=1
# L4 (R=6, A=6): P=2, D=2, C=1, A=1
# L5 (R=8, A=8): P=2, D=2, C=2, A=2
# ----------------------------------------------------------------------
CORRECT_PDCA_SCORES_MAP: Final[Dict[int, Dict[str, int]]] = {
    1: {'P': 1, 'D': 0, 'C': 0, 'A': 0},
    2: {'P': 1, 'D': 1, 'C': 0, 'A': 0},
    3: {'P': 1, 'D': 1, 'C': 1, 'A': 1},
    4: {'P': 2, 'D': 2, 'C': 1, 'A': 1},
    5: {'P': 2, 'D': 2, 'C': 2, 'A': 2},
}

def build_ordered_context(level: int,
                          plan_blocks: list[dict],
                          do_blocks: list[dict],
                          check_blocks: list[dict],
                          act_blocks: list[dict],
                          other_blocks: list[dict]) -> str:
    def fmt(blocks):
        return "\n\n".join(
            f"[{b.get('file', 'Unknown File')}]\n{b.get('content', b.get('text', ''))}" for b in blocks
        )

    if level == 3:
        # L3: Check/Act Priority 1, Plan/Do/Other ‡∏ï‡πà‡∏≠‡∏ó‡πâ‡∏≤‡∏¢
        ordered = [
            fmt(check_blocks),
            fmt(act_blocks),
            fmt(plan_blocks),
            fmt(do_blocks),
            fmt(other_blocks)
        ]
    else:
        # Default: Plan -> Do -> Check -> Act -> Other
        ordered = [
            fmt(plan_blocks),
            fmt(do_blocks),
            fmt(check_blocks),
            fmt(act_blocks),
            fmt(other_blocks)
        ]

    return "\n\n".join([o for o in ordered if o])


def calculate_pdca_breakdown_and_pass_status(llm_score: int, level: int) -> Tuple[Dict[str, int], bool, float]:
    """
    ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì PDCA breakdown, is_passed status, ‡πÅ‡∏•‡∏∞ raw_pdca_score (Achieved Score) 
    ‡πÇ‡∏î‡∏¢‡πÅ‡∏õ‡∏•‡∏á‡∏à‡∏≤‡∏Å llm_score (1-5) ‡πÅ‡∏•‡∏∞ Level ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô 
    
    ‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£:
    - L1 ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ llm_score >= 1
    - L2 ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ llm_score >= 2
    - L3 ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ llm_score >= 3
    - L4 ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ llm_score >= 4
    - L5 ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ llm_score >= 4 
    """
    pdca_map: Dict[str, int] = {'P': 0, 'D': 0, 'C': 0, 'A': 0}
    is_passed: bool = False
    raw_pdca_score: float = 0.0
    
    # 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ PASS (‡πÉ‡∏ä‡πâ Logic ‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏Å‡∏≥‡∏´‡∏ô‡∏î)
    if level == 5:
        if llm_score >= 4:
            is_passed = True
    elif level == 4:
        if llm_score >= 4:
            is_passed = True
    elif level == 3:
        if llm_score >= 3:
            is_passed = True
    elif level == 2:
        if llm_score >= 2:
            is_passed = True
    elif level == 1:
        if llm_score >= 1:
            is_passed = True

    # 2. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì PDCA Breakdown ‡πÅ‡∏•‡∏∞ raw_pdca_score (Achieved Score)
    if is_passed:
        # *** REVISED LOGIC: ‡πÉ‡∏ä‡πâ CORRECT_PDCA_SCORES_MAP ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô P, D, C, A ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ***
        correct_scores = CORRECT_PDCA_SCORES_MAP.get(level, pdca_map) 
        pdca_map.update(correct_scores)
        
        # raw_pdca_score (Achieved Score) ‡∏à‡∏∞‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö Required Score (R) ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ú‡πà‡∏≤‡∏ô
        raw_pdca_score = float(sum(pdca_map.values()))
    
    return pdca_map, is_passed, raw_pdca_score

def get_correct_pdca_required_score(level: int) -> int:
    """
    ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤ Required Score (R) ‡∏ï‡∏≤‡∏° Level ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå SE-AM:
    L1=1, L2=2, L3=4, L4=6, L5=8
    """
    # ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ô‡∏µ‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß
    if level == 1:
        return 1
    elif level == 2:
        return 2
    elif level == 3:
        return 4
    elif level == 4:
        return 6
    elif level == 5:
        return 8
    # ‡∏Å‡∏£‡∏ì‡∏µ Level ‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î
    return 8


# üìå ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç Type Hint ‡πÅ‡∏•‡∏∞ Arguments ‡∏Ç‡∏≠‡∏á Tuple ‡πÉ‡∏´‡πâ‡∏£‡∏ß‡∏° config parameter ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (10 elements)
def _static_worker_process(worker_input_tuple: Tuple[
    Dict[str, Any], str, int, str, str, str, float, float, int, Optional[Dict[str, str]]
]) -> Dict[str, Any]:
    """
    Static worker function for multiprocessing pool. 
    It reconstructs SeamAssessment in the new process and executes the assessment 
    for a single sub-criteria.
    
    Args:
        worker_input_tuple: (sub_criteria_data, enabler: str, target_level: int, mock_mode: str, 
                             evidence_map_path: str, model_name: str, temperature: float, 
                             min_retry_score: float, max_retrieval_attempts: int,
                             document_map: Optional[Dict[str, str]]) 

    Returns:
        Dict[str, Any]: Final result of the sub-criteria assessment.
    """
    
    # üü¢ NEW FIX: PATH SETUP ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Worker Process
    # ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ path ‡∏ã‡πâ‡∏≥‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤ worker process ‡πÄ‡∏´‡πá‡∏ô package ‡∏´‡∏•‡∏±‡∏Å
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
    if project_root not in sys.path:
        sys.path.append(project_root)
        
    worker_logger = logging.getLogger(__name__)

    try:
        # üü¢ FIX: Unpack ‡∏Ñ‡πà‡∏≤ Primitives ‡∏ó‡∏±‡πâ‡∏á 10 ‡∏ï‡∏±‡∏ß
        (
            sub_criteria_data, 
            enabler, 
            target_level, 
            mock_mode, 
            evidence_map_path, 
            model_name, 
            temperature,
            min_retry_score,            # ‚¨ÖÔ∏è NEW CONFIG (8th element)
            max_retrieval_attempts,     # ‚¨ÖÔ∏è NEW CONFIG (9th element)
            document_map                # (10th element)
        ) = worker_input_tuple
    except ValueError as e:
        # ‡πÉ‡∏ä‡πâ len(worker_input_tuple) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£ Debug ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô
        worker_logger.critical(f"Worker input tuple unpack failed (expected 10 elements, got {len(worker_input_tuple)}): {e}")
        return {"error": f"Invalid worker input: {e}"}
        
    # 1. Reconstruct Config 
    try:
        # üü¢ FIX: ‡∏™‡∏£‡πâ‡∏≤‡∏á AssessmentConfig ‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏ô Worker Process ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏™‡πà‡∏Ñ‡πà‡∏≤ config ‡πÉ‡∏´‡∏°‡πà
        # (Tenant/Year ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Default ‡∏à‡∏≤‡∏Å AssessmentConfig)
        worker_config = AssessmentConfig(
            enabler=enabler,
            target_level=target_level,
            mock_mode=mock_mode,
            model_name=model_name, 
            temperature=temperature,
            min_retry_score=min_retry_score,            # ‚¨ÖÔ∏è Pass new config
            max_retrieval_attempts=max_retrieval_attempts # ‚¨ÖÔ∏è Pass new config
        )
    except Exception as e:
        worker_logger.critical(f"Failed to reconstruct AssessmentConfig in worker: {e}")
        return {
            "sub_criteria_id": sub_criteria_data.get('sub_id', 'UNKNOWN'),
            "error": f"Config reconstruction failed: {e}"
        }

    # 2. Re-instantiate SeamAssessment 
    try:
        # üü¢ FIX (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç): ‡∏™‡πà‡∏á document_map ‡πÅ‡∏•‡∏∞ worker_config ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô SEAMPDCAEngine
        # SEAMPDCAEngine ‡∏à‡∏∞‡πÉ‡∏ä‡πâ worker_config ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ min_retry_score ‡πÅ‡∏•‡∏∞ max_retrieval_attempts
        worker_instance = SEAMPDCAEngine(
            config=worker_config, 
            evidence_map_path=evidence_map_path, 
            llm_instance=None,              # LLM ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å Initialized ‡πÉ‡∏ô Engine ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ
            vectorstore_manager=None,       # VSM ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å Initialized ‡πÉ‡∏ô Engine ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ
            # doc_type ‡∏ï‡πâ‡∏≠‡∏á‡∏ñ‡∏π‡∏Å set ‡πÉ‡∏ô SEAMPDCAEngine constructor (‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ Default)
            logger_instance=worker_logger,
            document_map=document_map # ‚¨ÖÔ∏è ‡∏™‡πà‡∏á document_map ‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏á Unpack ‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤
        )
    except Exception as e:
        worker_logger.critical(f"FATAL: SEAMPDCAEngine instantiation failed in worker: {e}")
        return {
            "sub_criteria_id": sub_criteria_data.get('sub_id', 'UNKNOWN'),
            "error": f"Engine initialization failed: {e}"
        }
    
    # 3. Execute the worker logic
    return worker_instance._run_sub_criteria_assessment_worker(sub_criteria_data)

def merge_evidence_mappings(results_list: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
    """
    ‡∏£‡∏ß‡∏° evidence_mapping dictionaries ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å Worker ‡∏ó‡∏∏‡∏Å‡∏ï‡∏±‡∏ß 
    """
    merged_mapping = defaultdict(list)
    for result in results_list:
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å Worker ‡∏°‡∏µ Key 'evidence_mapping' ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if 'evidence_mapping' in result and isinstance(result['evidence_mapping'], dict):
            # ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏ú‡πà‡∏≤‡∏ô Key/Value ‡∏Ç‡∏≠‡∏á Worker ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡∏±‡∏ß
            for level_key, evidence_list in result['evidence_mapping'].items():
                # ‡πÉ‡∏ä‡πâ .extend() ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ú‡∏ô‡∏ß‡∏Å‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
                if isinstance(evidence_list, list):
                    merged_mapping[level_key].extend(evidence_list)
    
    # ‡πÅ‡∏õ‡∏•‡∏á defaultdict ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô dict ‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤
    return dict(merged_mapping)

# =================================================================
# Configuration Class
# =================================================================
@dataclass
class AssessmentConfig:
    """Configuration for the SEAM PDCA Assessment Run."""
    
    # ------------------ 1. Assessment Context ------------------
    enabler: str = DEFAULT_ENABLER
    tenant: str = DEFAULT_TENANT
    year: int = DEFAULT_YEAR
    target_level: int = MAX_LEVEL
    mock_mode: str = "none" # 'none', 'random', 'control'
    force_sequential: bool = field(default=False) # Flag ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏£‡∏±‡∏ô‡πÅ‡∏ö‡∏ö Sequential

    # ------------------ 2. LLM Configuration (Configurable) ------------------
    # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Default ‡∏à‡∏≤‡∏Å global_vars.py
    model_name: str = DEFAULT_LLM_MODEL_NAME 
    temperature: float = LLM_TEMPERATURE

    # ------------------ 3. Adaptive RAG Retrieval Configuration ------------------
    # üü¢ NEW: ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô Rerank ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Adaptive Loop (MIN_RETRY_SCORE)
    # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Default 0.65 ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏û‡∏ö‡πÉ‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î Logic
    min_retry_score: float = 0.65 
    # üü¢ NEW: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≠‡∏ö‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á Adaptive RAG Loop (MAX_RETRIEVAL_ATTEMPTS)
    # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Default 3
    max_retrieval_attempts: int = 3
    
    # ------------------ 4. Export Configuration ------------------
    export_output: bool = field(default=False) # Flag ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏¥‡∏î/‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£ Export ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    export_path: str = "" # Path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå Export (‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å)


# =================================================================
# SEAM Assessment Engine (PDCA Focused)
# =================================================================
class SEAMPDCAEngine:
    
    # üéØ Mapping for RAG Query Augmentation at Level 1 (Plan)
    ENABLER_L1_AUGMENTATION = {
        "KM": "‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢ ‡∏ß‡∏¥‡∏™‡∏±‡∏¢‡∏ó‡∏±‡∏®‡∏ô‡πå ‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå ‡πÅ‡∏ú‡∏ô‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏∏‡πà‡∏á‡∏°‡∏±‡πà‡∏ô",
        "HCM": "‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏• ‡πÅ‡∏ú‡∏ô‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡∏ô ‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏• ‡∏Å‡∏≤‡∏£‡∏°‡∏µ‡∏™‡πà‡∏ß‡∏ô‡∏£‡πà‡∏ß‡∏°‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£",
        "DT": "‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏î‡∏¥‡∏à‡∏¥‡∏ó‡∏±‡∏• ‡πÅ‡∏ú‡∏ô‡πÅ‡∏°‡πà‡∏ö‡∏ó‡∏î‡∏¥‡∏à‡∏¥‡∏ó‡∏±‡∏• ‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á IT ‡πÅ‡∏ú‡∏ô‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ",
        "SP": "‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£ ‡πÅ‡∏ú‡∏ô‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£ ‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå ‡πÅ‡∏ú‡∏ô‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå",
        "DEFAULT": "‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢ ‡∏ß‡∏¥‡∏™‡∏±‡∏¢‡∏ó‡∏±‡∏®‡∏ô‡πå ‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå ‡πÅ‡∏ú‡∏ô‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏∏‡πà‡∏á‡∏°‡∏±‡πà‡∏ô" 
    }
    
    L1_INITIAL_TOP_K_RAG: int = 50 
    
    def __init__(
        self, 
        config: AssessmentConfig,
        llm_instance: Any = None, 
        logger_instance: logging.Logger = None,
        rag_retriever_instance: Any = None,
        # üü¢ FIX #1: ‡πÄ‡∏û‡∏¥‡πà‡∏° doc_type 
        doc_type: str = EVIDENCE_DOC_TYPES, 
        # üü¢ FIX #2: ‡πÄ‡∏û‡∏¥‡πà‡∏° vectorstore_manager
        vectorstore_manager: Optional['VectorStoreManager'] = None,
        # üìå FIX #3 (‡πÉ‡∏´‡∏°‡πà): ‡πÄ‡∏û‡∏¥‡πà‡∏° evidence_map_path ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å Worker Process
        evidence_map_path: Optional[str] = None,
        document_map: Optional[Dict[str, str]] = None 
    ):

            # =======================================================
            # üéØ FIX 1: ‡∏¢‡πâ‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤ Logger ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å
            # =======================================================
            if logger_instance is not None:
                 self.logger = logger_instance
            else:
                 # ‡∏™‡∏£‡πâ‡∏≤‡∏á Child Logger ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Log ‡∏°‡∏µ Context ‡∏Ç‡∏≠‡∏á Tenant/Year
                 self.logger = logging.getLogger(__name__).getChild(f"Engine|{config.enabler}|{config.tenant}/{config.year}")
            
            self.logger.info(f"Initializing SEAMPDCAEngine for {config.enabler} ({config.tenant}/{config.year}).")

            # =======================================================
            # üü¢ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≥‡∏´‡∏ô‡∏î Attribute ‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÄ‡∏°‡∏ò‡∏≠‡∏î‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ self.logger
            # =======================================================
            self.config = config
            self.enabler_id = config.enabler
            self.target_level = config.target_level
            self.rubric = self._load_rubric()
            
            # üü¢ ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤ VSM ‡πÅ‡∏•‡∏∞ doc_type
            self.vectorstore_manager = vectorstore_manager
            self.doc_type = doc_type

            self.FINAL_K_RERANKED = FINAL_K_RERANKED
            self.PRIORITY_CHUNK_LIMIT = PRIORITY_CHUNK_LIMIT

            # üü¢ ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤ LLM ‡πÅ‡∏•‡∏∞ Logger
            self.llm = llm_instance           
            # üéØ FIX 2: ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î logger
            if logger_instance is None:
                # ‡πÉ‡∏ä‡πâ logger ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÑ‡∏ß‡πâ‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡∏ï‡πâ‡∏ô
                self.logger.warning("Re-setting logger instance using the pre-initialized one.")
            
            # üü¢ Disable Strict Filter
            self.initial_evidence_ids: Set[str] = self._load_initial_evidence_info()
            all_statements = self._flatten_rubric_to_statements()
            initial_count = len(all_statements)

            self.logger.info(f"DEBUG: Statements found: {initial_count}. Strict Filter is **DISABLED**.")

            self.statements_to_assess = all_statements
            self.logger.info(f"DEBUG: Statements selected for assessment: {len(self.statements_to_assess)} (Skipped: {initial_count - len(self.statements_to_assess)})")

            # Assessment results storage
            self.raw_llm_results: List[Dict[str, Any]] = []
            self.final_subcriteria_results: List[Dict[str, Any]] = []
            self.total_stats: Dict[str, Any] = {}

            self.is_sequential = False  

            self.retry_policy = RetryPolicy(
                max_attempts=3,            
                base_delay=2.0,            
                jitter=True,               
                escalate_context=True,     
                shorten_prompt_on_fail=True,  
                exponential_backoff=True,  
            )

            self.RERANK_THRESHOLD: Final[float] = RERANK_THRESHOLD
            self.MAX_EVI_STR_CAP: Final[float] = MAX_EVI_STR_CAP
            # üìå Persistent Mapping Configuration
            
            # 1. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Evidence Map Path
            # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏°‡∏≤‡∏à‡∏≤‡∏Å Worker (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ) ‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤ Default
            if evidence_map_path:
                self.evidence_map_path = evidence_map_path
            else:
                # üü¢ FIX #1: ‡πÉ‡∏ä‡πâ get_evidence_mapping_file_path ‡∏à‡∏≤‡∏Å utils/path_utils.py
                self.evidence_map_path = get_evidence_mapping_file_path(
                    tenant=self.config.tenant, 
                    year=self.config.year,
                    enabler=self.enabler_id
                )
            
            # 2. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Attribute ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Persistent Mapping
            self.evidence_map: Dict[str, List[str]] = {}
            self.temp_map_for_save: Dict[str, List[str]] = {}

            self.contextual_rules_map: Dict[str, Dict[str, str]] = self._load_contextual_rules_map()
            
            # 3. ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà 
            self.evidence_map = self._load_evidence_map()

            self.logger.info(f"Persistent Map Path set to: {self.evidence_map_path}")
            self.logger.info(f"Loaded {len(self.evidence_map)} existing evidence entries into self.evidence_map.")
            
            # Mock function pointers (will point to real functions by default)
            self.llm_evaluator = evaluate_with_llm
            self.rag_retriever = retrieve_context_with_filter
            self.create_structured_action_plan = create_structured_action_plan

            # Apply mocking if enabled
            if config.mock_mode in ["random", "control"]:
                self._set_mock_handlers(config.mock_mode)

            # Set global mock control mode for llm_data_utils if using 'control'
            if config.mock_mode == "control":
                self.logger.info("Enabling global LLM data utils mock control mode.")
                set_llm_data_mock_mode(True)
            elif config.mock_mode == "random":
                self.logger.warning("Mock mode 'random' is not fully implemented. Using 'control' logic if available.")
                if hasattr(sys.modules.get('seam_mocking'), 'set_mock_control_mode'):
                    sys.modules.get('seam_mocking').set_mock_control_mode(False)
                    set_llm_data_mock_mode(False)

            # üìå ‡πÇ‡∏´‡∏•‡∏î LLM ‡πÅ‡∏•‡∏∞ VSM ‡∏´‡∏≤‡∏Å‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ
            if self.llm is None: self._initialize_llm_if_none()
            if self.vectorstore_manager is None: self._initialize_vsm_if_none()
            
            # =======================================================
            # üéØ FIX #4: ‡πÇ‡∏´‡∏•‡∏î Document Map ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤ ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡πá‡∏ô Dictionary ‡∏ß‡πà‡∏≤‡∏á
            # (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Filename Resolution Failed)
            # =======================================================
            map_to_use: Dict[str, str] = document_map if document_map is not None else {}

            if not map_to_use:
                # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Path ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå Doc ID Mapping ‡∏ó‡∏µ‡πà VSM ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
                mapping_path = get_mapping_file_path(
                    tenant=self.config.tenant, 
                    year=self.config.year,
                    enabler=self.enabler_id # ‡πÉ‡∏ä‡πâ enabler ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÉ‡∏´‡∏°‡πà (Priority 1)
                )
                
                self.logger.info(f"Attempting to load document_map from file: {mapping_path}")

                try:
                    with open(mapping_path, 'r', encoding='utf-8') as f:
                        doc_map_raw = json.load(f)
                    
                    # ‡πÅ‡∏õ‡∏•‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Doc ID -> {file_name: X, ...} ‡πÄ‡∏õ‡πá‡∏ô Doc ID -> file_name
                    map_to_use = {
                        doc_id: data.get("file_name", doc_id) # ‡πÉ‡∏ä‡πâ doc_id ‡πÄ‡∏õ‡πá‡∏ô fallback
                        for doc_id, data in doc_map_raw.items()
                    }
                    self.logger.info(f"Successfully loaded {len(map_to_use)} document mappings from file.")
                    
                except FileNotFoundError:
                    self.logger.warning(f"Document mapping file not found at: {mapping_path}. Using empty map.")
                except Exception as e:
                    self.logger.error(f"Error loading document map from file: {e}")

            # ‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö Document Map
            self.doc_id_to_filename_map: Dict[str, str] = map_to_use
            self.document_map: Dict[str, str] = self.doc_id_to_filename_map # ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ô‡πÑ‡∏î‡πâ
            
            self.logger.info(f"Loaded {len(self.doc_id_to_filename_map)} document mappings.")
            if not self.doc_id_to_filename_map:
                self.logger.warning("Document ID to Filename Map is empty. Filename resolution might be limited.")

            self.logger.info(f"Engine initialized for Enabler: {self.enabler_id}, Mock Mode: {config.mock_mode}")

    def _initialize_llm_if_none(self):
        """Initializes LLM instance if self.llm is None."""
        if self.llm is None:
            self.logger.warning("‚ö†Ô∏è Initializing LLM: model=%s, temperature=%s", 
                                self.config.model_name, self.config.temperature)
            try:
                # üü¢ FIX: Import ‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ create_llm_instance
                from models.llm import create_llm_instance 
                self.llm = create_llm_instance( 
                    model_name=self.config.model_name,
                    temperature=self.config.temperature
                )
                self.logger.info("‚úÖ LLM Instance created successfully: %s (Temp: %s)", 
                                 self.config.model_name, self.config.temperature)
            except Exception as e:
                self.logger.error(f"FATAL: Could not initialize LLM: {e}")
                raise


    def _initialize_vsm_if_none(self):
        """
        Initializes VectorStoreManager if self.vectorstore_manager is None.
        Handles multi-tenant/multi-year vector store loading.
        """
        if self.vectorstore_manager is None:
            self.logger.info("Loading central evidence vectorstore(s)...")
            try:
                # üéØ FIX: ‡∏™‡πà‡∏á tenant ‡πÅ‡∏•‡∏∞ year ‡∏à‡∏≤‡∏Å config ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô load_all_vectorstores()
                self.vectorstore_manager = load_all_vectorstores(
                    doc_types=[EVIDENCE_DOC_TYPES], 
                    evidence_enabler=self.enabler_id,
                    tenant=self.config.tenant,  # <--- NEW: ‡πÄ‡∏û‡∏¥‡πà‡∏° Argument ‡∏ô‡∏µ‡πâ
                    year=self.config.year       # <--- NEW: ‡πÄ‡∏û‡∏¥‡πà‡∏° Argument ‡∏ô‡∏µ‡πâ
                )
                
                # üìå FINAL FIX: ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á MultiDocRetriever (Private Attribute) 
                # ‡πÅ‡∏•‡∏∞‡∏ï‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ _all_retrievers (Private Attribute)
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ VSM ‡∏ñ‡∏π‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏Å‡πà‡∏≠‡∏ô
                if self.vectorstore_manager and hasattr(self.vectorstore_manager, '_multi_doc_retriever'):
                     len_retrievers = len(
                        self.vectorstore_manager._multi_doc_retriever._all_retrievers
                    )
                     self.logger.info("‚úÖ MultiDocRetriever loaded with %s collections and cached in VSM.", 
                                 len_retrievers) 
                else:
                    self.logger.warning("VectorStoreManager loaded but _multi_doc_retriever structure is missing or unexpected.")

            except Exception as e:
                self.logger.error(f"FATAL: Could not initialize VectorStoreManager: {e}")
                raise # Re-raise the exception to ‡∏´‡∏¢‡∏∏‡∏î‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°
        
    def _resolve_evidence_filenames(self, evidence_entries: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
            """
            ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á
            1. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà doc_id ‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ 'UNKNOWN-' (‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏†‡∏≤‡∏¢‡πÉ‡∏ô/‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£)
            2. ‡πÅ‡∏õ‡∏•‡∏á doc_id (‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô Hash/UUID) ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏î‡πâ ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ doc_id_to_filename_map
            """
            # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤ re ‡πÅ‡∏•‡∏∞ deepcopy ‡∏ñ‡∏π‡∏Å import ‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏±‡∏ß‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡πâ‡∏ß
            from copy import deepcopy
            import re
            
            resolved_entries = []
            for entry in evidence_entries:
                # ‡πÉ‡∏ä‡πâ deepcopy ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
                resolved_entry = deepcopy(entry)
                # ‡πÉ‡∏ä‡πâ doc_id ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏´‡∏•‡∏±‡∏Å‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå
                doc_id = resolved_entry.get("doc_id", "")
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô (‡∏à‡∏≤‡∏Å metadata ‡∏Ç‡∏≠‡∏á vectorstore ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
                current_filename = resolved_entry.get("filename", "")
                
                # --- 1. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏ì‡∏µ UNKNOWN- (AI-GENERATED or Lost Source) ---
                if doc_id.startswith("UNKNOWN-"):
                    # ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏à‡∏£‡∏¥‡∏á
                    # ‡πÄ‡∏ä‡πà‡∏ô "UNKNOWN-2fac2f11" --> "AI-GENERATED-REF-2fac2f11"
                    resolved_entry["filename"] = f"AI-GENERATED-REF-{doc_id.split('-')[-1]}"
                    resolved_entries.append(resolved_entry)
                    continue

                # --- 2. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏ì‡∏µ Doc ID (Hash/UUID) ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ---
                if doc_id:
                    # A. ‡∏•‡∏≠‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å Map
                    if doc_id in self.doc_id_to_filename_map:
                        resolved_entry["filename"] = self.doc_id_to_filename_map[doc_id]
                        # ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß
                        resolved_entries.append(resolved_entry)
                        continue

                    # B. ‡∏ñ‡πâ‡∏≤‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ (Map Fail)
                    else:
                        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° (‡πÄ‡∏ä‡πà‡∏ô "Unknown" ‡∏´‡∏£‡∏∑‡∏≠ Hash.pdf)
                        is_generic_name = (
                            current_filename.lower() == "unknown" or
                            # ‚úÖ ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Hash/UUID 64 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•
                            re.match(r"^[0-9a-f]{64}(\.pdf|\.txt)?$", current_filename, re.IGNORECASE)
                        )
                        
                        if is_generic_name:
                            # ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå Fallback ‡∏ó‡∏µ‡πà‡∏™‡∏∑‡πà‡∏≠‡∏ß‡πà‡∏≤ Map ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ‡πÅ‡∏ï‡πà‡∏°‡∏µ ID
                            resolved_entry["filename"] = f"MAPPING-FAILED-{doc_id[:8]}..."
                            self.logger.warning(f"Failed to map doc_id {doc_id[:8]}... to filename. Using fallback.")
                        # else: ‡∏´‡∏≤‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏à‡∏≤‡∏Å metadata ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà Generic Name 
                        # (‡πÄ‡∏ä‡πà‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß) ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡πÇ‡∏î‡∏¢‡∏õ‡∏£‡∏¥‡∏¢‡∏≤‡∏¢

                # --- 3. ‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏°‡πà‡∏°‡∏µ Doc ID ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ (‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÄ‡∏õ‡πá‡∏ô Unknown) ---
                elif not doc_id:
                    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ doc_id ‡πÅ‡∏•‡∏∞ filename ‡πÄ‡∏õ‡πá‡∏ô Unknown
                    if current_filename.lower() == "unknown":
                        resolved_entry["filename"] = "MISSING-SOURCE-METADATA"
                        self.logger.error("Evidence found with no doc_id and generic filename.")
                
                # ‡πÄ‡∏û‡∏¥‡πà‡∏° entry ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ (‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà)
                resolved_entries.append(resolved_entry)

            return resolved_entries

    
    # -------------------- Contextual Rules Handlers (FIXED) --------------------
    def _load_contextual_rules_map(self) -> Dict[str, Dict[str, str]]:
        """
        Loads the contextual rules JSON file using the path generated by 
        utils.path_utils.get_contextual_rules_file_path.
        """
        
        try:
            # üéØ ‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏à‡∏≤‡∏Å path_utils ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Path ‡πÄ‡∏≠‡∏á
            filepath = get_contextual_rules_file_path(
                tenant=self.config.tenant,
                enabler=self.enabler_id
            )
        except ImportError:
            self.logger.error("‚ùå FATAL: Cannot import get_contextual_rules_file_path. Check utils/path_utils.py import.")
            return {}

        # 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if not os.path.exists(filepath):
            # ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ get_contextual_rules_file_path ‡∏ó‡∏≥‡πÅ‡∏ó‡∏ô
            self.logger.info(f"‚ö†Ô∏è Contextual Rules file not found at: {filepath}. Using empty map.")
            return {}

        self.logger.info(f"‚úÖ Contextual Rules loaded from: {filepath}")
        
        # 2. ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
                self.logger.info(f"‚úÖ Loaded Contextual Rules: {len(data)} sub-criteria rules.")
                return data
        except json.JSONDecodeError as e:
            self.logger.error(f"‚ùå Failed to parse Contextual Rules JSON from {filepath}: {e}")
            return {}
        except Exception as e:
            self.logger.error(f"‚ùå Failed to load Contextual Rules from {filepath}: {e}")
            return {}

    def _collect_previous_level_evidences(self, sub_id: str, current_level: int) -> Dict[str, List[Dict]]:
        """
        ‡∏î‡∏∂‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô (Metadata + Text) ‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏à‡∏≤‡∏Å Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î 
        ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô Baseline Context ‡πÉ‡∏ô Level ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô (Sequential Mode)
        
        ‚úÖ FIX: ‡πÄ‡∏û‡∏¥‡πà‡∏° Logic ‡∏Å‡∏≤‡∏£ Mapping Stable Doc ID (64-char) ‡πÄ‡∏õ‡πá‡∏ô Chunk UUIDs (64-char_index) 
               ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Hydration Fail ‡∏à‡∏≤‡∏Å ChromaDB
        """
        collected = {}
        
        source_map = self.evidence_map
        source_name = "evidence_map (SEQ/PAR Main)"

        # 1. ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏° Metadata ‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏à‡∏≤‡∏Å Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤
        for key, evidence_list in source_map.items():
            if key.startswith(f"{sub_id}.L") and isinstance(evidence_list, list):
                try:
                    level_num = int(key.split(".L")[-1])
                    if level_num < current_level:
                        collected[key] = evidence_list
                except (ValueError, IndexError):
                    continue

        # 2. HYDRATION: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÅ‡∏•‡∏∞‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏° ID
        vectorstore_manager = self.vectorstore_manager
        is_hydration_needed = vectorstore_manager is not None and collected

        if is_hydration_needed:
            # ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏° IDs ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÄ‡∏•‡∏∑‡∏≠‡∏Å (‡∏ó‡∏±‡πâ‡∏á Stable Doc ID ‡πÅ‡∏•‡∏∞ Chunk UUIDs)
            all_uuids_raw = [
                str(
                    ev.get('chunk_uuid') or ev.get('stable_doc_uuid') or ev.get('doc_id')
                ).strip()
                for ev_list in collected.values()
                for ev in ev_list
                # ‡∏Å‡∏£‡∏≠‡∏á ID ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ
                if (ev.get('chunk_uuid') or ev.get('stable_doc_uuid') or ev.get('doc_id')) 
                and len(str(ev.get('chunk_uuid') or ev.get('stable_doc_uuid') or ev.get('doc_id')).strip()) >= 10
                and not str(ev.get('chunk_uuid') or ev.get('stable_doc_uuid') or ev.get('doc_id')).startswith(("TEMP-", "HASH-", "Unknown"))
            ]
            
            # ‡πÅ‡∏ö‡πà‡∏á ID
            all_uuids_stable_doc_only = list(set([uid for uid in all_uuids_raw if len(uid) == 64]))
            all_uuids_non_stable_doc = list(set([uid for uid in all_uuids_raw if len(uid) > 64])) # Chunk ID ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á

            # 3. ‡πÅ‡∏õ‡∏•‡∏á Stable Doc ID (64-char) ‡πÄ‡∏õ‡πá‡∏ô Chunk UUIDs
            chunk_uuids_for_chroma = []
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ VSM ‡∏°‡∏µ Doc ID Map ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏õ‡∏•‡∏á ID ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            has_doc_id_map = (
                hasattr(vectorstore_manager, 'doc_id_map') and 
                isinstance(getattr(vectorstore_manager, 'doc_id_map'), dict) and 
                getattr(vectorstore_manager, 'doc_id_map')
            )

            if not has_doc_id_map:
                self.logger.warning("VSM Doc ID Map is missing! Using raw IDs for ChromaDB (may fail).")
                chunk_uuids_for_chroma = all_uuids_raw # Fallback
            else:
                mapped_count = 0
                # 3.1 ‡πÅ‡∏õ‡∏•‡∏á Stable Doc IDs
                doc_id_map = vectorstore_manager.doc_id_map
                for input_id in all_uuids_stable_doc_only:
                    mapped_info = doc_id_map.get(input_id, {})
                    full_chunk_list = mapped_info.get('chunk_uuids', [])
                    chunk_uuids_for_chroma.extend(full_chunk_list)
                    if full_chunk_list:
                         mapped_count += 1
                
                # 3.2 ‡πÄ‡∏û‡∏¥‡πà‡∏° Chunk IDs ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏°‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
                chunk_uuids_for_chroma.extend(all_uuids_non_stable_doc)

                if mapped_count > 0:
                    self.logger.info(f"VSM: Successfully mapped {mapped_count}/{len(all_uuids_stable_doc_only)} Stable Doc IDs to {len(chunk_uuids_for_chroma)} potential Chunk UUIDs.")
                
                # 3.3 ‡∏•‡∏ö‡∏ã‡πâ‡∏≥‡πÅ‡∏•‡∏∞‡∏Å‡∏£‡∏≠‡∏á
                chunk_uuids_for_chroma = list(set([uid for uid in chunk_uuids_for_chroma if len(uid) > 10]))
            
            # 4. ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Retrieve)
            total_metadata_chunks = sum(len(v) for v in collected.values())
            self.logger.info(f"DEBUG HYDRATION: Total evidence entries found in metadata (evidence_map): {total_metadata_chunks} items. Unique Chroma IDs to retrieve: {len(chunk_uuids_for_chroma)}")
            
            full_chunks = []
            if chunk_uuids_for_chroma:
                # üéØ ‡πÉ‡∏ä‡πâ path_utils ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ä‡∏∑‡πà‡∏≠ Collection
                collection_name = get_doc_type_collection_key(self.doc_type, self.enabler_id)

                try:
                    self.logger.info(
                        f"üö® DEBUG: Attempting to HYDRATE {len(chunk_uuids_for_chroma)} unique chunks from Collection: '{collection_name}'. "
                    )
                    # üö® ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ ChromaDB ‡∏î‡πâ‡∏ß‡∏¢ Chunk UUIDs ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
                    retrieved_lc_docs = vectorstore_manager.retrieve_by_chunk_uuids(chunk_uuids_for_chroma, collection_name)

                    for doc in retrieved_lc_docs:
                        chunk_dict = doc.metadata.copy()
                        chunk_dict["text"] = doc.page_content
                        # ‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô ID ‡∏´‡∏•‡∏±‡∏Å
                        chunk_dict["chunk_uuid"] = doc.metadata.get("chunk_uuid") or doc.metadata.get("id")
                        chunk_dict["stable_doc_uuid"] = doc.metadata.get("stable_doc_uuid") or doc.metadata.get("doc_id")
                        
                        full_chunks.append(chunk_dict)

                    self.logger.info(f"Successfully hydrated {len(full_chunks)} chunks from previous levels")
                    
                    # Debug: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö ID ‡∏ó‡∏µ‡πà‡∏´‡∏≤‡∏¢‡πÑ‡∏õ
                    retrieved_uuids = {c.get('chunk_uuid') for c in full_chunks if c.get('chunk_uuid')}
                    missing_uuids = set(chunk_uuids_for_chroma) - retrieved_uuids
                    if missing_uuids:
                        self.logger.error(
                            f"‚ùå FATAL HYDRATION ISSUE: {len(missing_uuids)} chunks were requested but NOT FOUND in ChromaDB. "
                        )
                        
                except Exception as e:
                    self.logger.error(f"Failed to retrieve full chunks for baseline: {e}")
                    full_chunks = []

            # 5. ‡∏™‡∏£‡πâ‡∏≤‡∏á Map ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏ß‡∏° Text ‡πÄ‡∏ï‡πá‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ö Metadata ‡πÄ‡∏î‡∏¥‡∏°
            
            # Map retrieved full chunks by their full chunk ID (Primary Key)
            full_chunk_map_by_chunk_uuid = {c.get('chunk_uuid'): c for c in full_chunks if c.get('chunk_uuid') and len(c.get('chunk_uuid')) > 64}
            # Map retrieved full chunks by their Stable Doc ID (Hash 64)
            full_chunk_map_by_stable_doc_id = {}
            for c in full_chunks:
                s_doc_id = c.get('stable_doc_uuid') or c.get('doc_id')
                if s_doc_id and len(s_doc_id) == 64 and s_doc_id not in full_chunk_map_by_stable_doc_id:
                    full_chunk_map_by_stable_doc_id[s_doc_id] = c 

            hydrated_collected = {}
            for key, ev_list in collected.items():
                hydrated_list = []
                for ev_metadata in ev_list:
                    uuid_key = ev_metadata.get('chunk_uuid') or ev_metadata.get('stable_doc_uuid') or ev_metadata.get('doc_id')
                    
                    full_chunk = None

                    # 5.1 ‡∏•‡∏≠‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏î‡πâ‡∏ß‡∏¢ Full Chunk UUID (‡∏´‡∏≤‡∏Å L1 ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏°‡∏≤‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á)
                    if uuid_key and len(uuid_key) > 64:
                        full_chunk = full_chunk_map_by_chunk_uuid.get(uuid_key)
                    
                    # 5.2 ‡∏ñ‡πâ‡∏≤‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ ‡∏´‡∏£‡∏∑‡∏≠ ID ‡∏ó‡∏µ‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô Stable Doc ID (64 ‡∏ï‡∏±‡∏ß) ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ Stable Doc ID ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
                    if full_chunk is None and uuid_key and len(uuid_key) == 64:
                        full_chunk = full_chunk_map_by_stable_doc_id.get(uuid_key)

                    if full_chunk and full_chunk.get('text'):
                        combined = full_chunk.copy()
                        # üü¢ FIX: ‡πÉ‡∏ä‡πâ content/text ‡∏Ç‡∏≠‡∏á chunk ‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏°‡∏≤ (Full Text)
                        combined['text'] = full_chunk['text'] 
                        
                        # ‡∏ô‡∏≥ metadata ‡∏™‡πà‡∏ß‡∏ô‡∏≠‡∏∑‡πà‡∏ô ‡πÜ ‡∏à‡∏≤‡∏Å ev_metadata ‡∏°‡∏≤‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ï (‡πÄ‡∏ä‡πà‡∏ô score, reason, filename)
                        metadata_to_update = {k:v for k,v in ev_metadata.items() if k not in ['text', 'page_content']}
                        combined.update(metadata_to_update)
                        
                        hydrated_list.append(combined)
                    else:
                        # ‡∏ñ‡πâ‡∏≤‡∏î‡∏∂‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡πÉ‡∏ä‡πâ metadata ‡πÄ‡∏î‡∏¥‡∏°
                        hydrated_list.append(ev_metadata)

                if hydrated_list:
                    hydrated_collected[key] = hydrated_list

            collected = hydrated_collected

        else:
            if collected:
                self.logger.info("Hydration skipped: vectorstore_manager not ready")

        # 6. Debug Log
        total_files = sum(len(v) for v in collected.values())
        self.logger.info(
            f"BASELINE LOADED ‚Üí Mode: {'SEQ' if self.is_sequential else 'PAR'} | "
            f"Source: {source_name} | "
            f"Found {len(collected)} levels | "
            f"Total files: {total_files}"
        )

        return collected


    def _get_contextual_rules_prompt(self, sub_id: str, level: int) -> str:
        """
        Retrieves the specific Contextual Rule prompt for a given Sub-Criteria and Level,
        ‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á‡∏Å‡∏≤‡∏£ Inject ‡∏Å‡∏é L5 ‡∏û‡∏¥‡πÄ‡∏®‡∏© ‡∏´‡∏≤‡∏Å Level == 5
        """
        sub_id_rules = self.contextual_rules_map.get(sub_id)
        rule_text = ""
        
        # 1. ‡∏î‡∏∂‡∏á‡∏Å‡∏é‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏¢‡πà‡∏≠‡∏¢ (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
        if sub_id_rules:
            level_key = f"L{level}"
            specific_rule = sub_id_rules.get(level_key)
            if specific_rule:
                rule_text += f"\n--- ‡∏Å‡∏é‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏¢‡πà‡∏≠‡∏¢ ({sub_id} L{level}) ---\n‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Ñ‡∏ß‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ: {specific_rule}\n"
        
        # 2. **INJECT L5 SPECIAL RULE (Safe Injection)**
        # ‡πÉ‡∏™‡πà‡∏Å‡∏é Bonus 2.0 ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô L5 ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏ö‡∏Å‡∏ß‡∏ô L3/L4
        if level == 5:
            l5_bonus_rule = """
            \n--- L5 SPECIAL RULE (Innovation & Sustainability) ---
            * **L5 PASS Condition (‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö):** ‡∏´‡∏≤‡∏Å Level ‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠ **L5** ‡∏ó‡πà‡∏≤‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏° PDCA (P+D+C+A) ‡∏ï‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô L3/L4 ‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏ô‡∏û‡∏ö‡∏Å‡πà‡∏≠‡∏ô (‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î 8.0)
            * **‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç Bonus 2.0:** ‡∏´‡∏≤‡∏Å‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏° PDCA ‡πÑ‡∏î‡πâ **‚â• 7.0** **‡πÅ‡∏•‡∏∞** ‡∏ó‡πà‡∏≤‡∏ô‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠ **‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 1 ‡∏ä‡∏¥‡πâ‡∏ô** ‡πÉ‡∏ô Context ‡∏ó‡∏µ‡πà‡πÅ‡∏™‡∏î‡∏á‡∏ñ‡∏∂‡∏á:
                * (a) ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏• KM / ‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏° (Innovation Award)
                * (b) ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡∏ó‡∏µ‡πà‡∏ß‡∏±‡∏î‡πÑ‡∏î‡πâ (ROI, Productivity, Cost Saving)
                * (c) ‡∏Å‡∏≤‡∏£‡πÄ‡∏ú‡∏¢‡πÅ‡∏û‡∏£‡πà/‡∏Å‡∏≤‡∏£‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å (External Recognition/Publication)
            * **‡πÉ‡∏´‡πâ‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡πÉ‡∏´‡πâ Bonus Score 2.0 ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ **Score ‚â• 9.0** ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡πâ‡∏á **is_passed=true** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏∞‡∏ó‡πâ‡∏≠‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏•‡∏¥‡∏® (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£ Reset ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô)
            """
            rule_text += l5_bonus_rule
            
        return rule_text
    

    def _load_rubric(self) -> Dict[str, Any]:
        """ Loads the SEAM rubric JSON file using path_utils. """
        
        # üéØ FIX: ‡πÉ‡∏ä‡πâ get_rubric_file_path ‡∏à‡∏≤‡∏Å path_utils ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Path ‡πÄ‡∏≠‡∏á
        try:
            filepath = get_rubric_file_path(
                tenant=self.config.tenant,
                enabler=self.enabler_id
            )
        except Exception:
            self.logger.error("‚ùå FATAL: get_rubric_file_path is not available.")
            return {} 

        if not os.path.exists(filepath):
            self.logger.error(f"‚ö†Ô∏è Rubric file not found at: {filepath}")
            return {}

        self.logger.info(f"‚úÖ Rubric loaded from: {filepath}")
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
            return data
        except Exception as e:
            self.logger.error(f"Error loading/decoding Rubric JSON from {filepath}: {e}")
            return {}
    
    # -------------------- Helper Function for Map Processing --------------------
    def _get_level_order_value(self, level_str: str) -> int:
        """Converts Level string ('L1', 'L5') to an integer for comparison."""
        try:
            return int(level_str.upper().replace('L', ''))
        except:
            return 0

    # -------------------- Persistent Mapping Handlers (FIXED) --------------------
    def _process_temp_map_to_final_map(self, temp_map: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Converts the temporary map into the final map format for saving, 
        and filters out temporary/unresolvable evidence IDs.
        """
        working_map = temp_map or self.temp_map_for_save or {}
        final_map_for_save = {}
        total_cleaned_items = 0

        for sub_level_key, evidence_list in working_map.items():
            if isinstance(evidence_list, dict):
                evidence_list = [evidence_list]
            elif not isinstance(evidence_list, list):
                logger.warning(f"[EVIDENCE] Skipping {sub_level_key}: not a list or dict")
                continue

            clean_list = []
            seen_ids = set()
            for ev in evidence_list:
                doc_id = ev.get("doc_id")
                
                if not doc_id:
                    continue
                
                # 1. FIX: ‡∏Å‡∏£‡∏≠‡∏á ID ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß (TEMP-) ‡∏≠‡∏≠‡∏Å
                if doc_id.startswith("TEMP-"):
                    # ID ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏õ‡∏•‡∏á‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô Stable Document ID ‡πÑ‡∏î‡πâ ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÉ‡∏ä‡πâ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ
                    logger.debug(f"[EVIDENCE] Filtering out unresolvable TEMP- ID: {doc_id} for {sub_level_key}.")
                    continue 
                
                # 2. Logic ‡πÄ‡∏î‡∏¥‡∏°: ‡∏Å‡∏£‡∏≠‡∏á HASH- (Placeholder) ‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ã‡πâ‡∏≥
                if doc_id.startswith("HASH-") or doc_id in seen_ids:
                    continue
                    
                seen_ids.add(doc_id)
                clean_list.append(ev)
                total_cleaned_items += 1 

            if clean_list:
                final_map_for_save[sub_level_key] = clean_list

        logger.info(f"[EVIDENCE] Processed {len(final_map_for_save)} sub-level keys with total {total_cleaned_items} evidence items")
        return final_map_for_save

    def _clean_map_for_json(self, data: Union[Dict, List, Set, Any]) -> Union[Dict, List, Any]:
        """Recursively converts objects that cannot be serialized (like sets) into lists."""
        if isinstance(data, dict):
            return {k: self._clean_map_for_json(v) for k, v in data.items()}
        elif isinstance(data, list):
            return [self._clean_map_for_json(v) for v in data]
        elif isinstance(data, set):
            return [self._clean_map_for_json(v) for v in data]
        return data

    def _clean_temp_entries(self, evidence_map: Dict[str, List[Dict]]) -> Dict[str, List[Dict]]:
        """
        ‡∏Å‡∏£‡∏≠‡∏á TEMP-, HASH-, ‡πÅ‡∏•‡∏∞ Unknown ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å evidence map ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        ‡πÉ‡∏ä‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏ï‡∏≠‡∏ô merge ‡πÅ‡∏•‡∏∞‡∏Å‡πà‡∏≠‡∏ô save ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î 100%
        """
        if not evidence_map:
            return {}

        cleaned_map = {}
        total_removed = 0
        total_unknown_fixed = 0

        for key, entries in evidence_map.items():
            valid_entries = []
            for entry in entries:
                doc_id = entry.get("doc_id", "")

                # 1. ‡∏Å‡∏£‡∏≠‡∏á TEMP- ‡πÅ‡∏•‡∏∞ HASH- ‡∏≠‡∏≠‡∏Å‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≤‡∏î
                if str(doc_id).startswith("TEMP-") or str(doc_id).startswith("HASH-"):
                    total_removed += 1
                    continue

                # 2. ‡∏ñ‡πâ‡∏≤ doc_id ‡∏ß‡πà‡∏≤‡∏á ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏•‡∏¢ ‚Üí ‡∏ó‡∏¥‡πâ‡∏á
                if not doc_id or doc_id == "Unknown":
                    total_removed += 1
                    continue

                # 3. ‡πÅ‡∏Å‡πâ filename ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô Unknown / None / ‡∏ß‡πà‡∏≤‡∏á
                filename = entry.get("filename", "").strip()
                if not filename or filename == "Unknown" or filename.lower() == "unknown_file.pdf":
                    # ‡πÉ‡∏ä‡πâ doc_id ‡∏™‡∏±‡πâ‡∏ô ‡πÜ ‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß (‡∏î‡∏π‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤)
                    short_id = doc_id[:8]
                    entry["filename"] = f"‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á_{short_id}.pdf"
                    total_unknown_fixed += 1
                else:
                    # ‡πÄ‡∏≠‡∏≤ path ‡∏≠‡∏≠‡∏Å ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÅ‡∏Ñ‡πà‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå
                    entry["filename"] = os.path.basename(filename)

                valid_entries.append(entry)

            if valid_entries:
                cleaned_map[key] = valid_entries
            else:
                logger.debug(f"[CLEAN] Key {key} ‡∏Å‡∏•‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏ß‡πà‡∏≤‡∏á‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏£‡∏≠‡∏á ‚Üí ‡∏ñ‡∏π‡∏Å‡∏•‡∏ö‡∏≠‡∏≠‡∏Å")

        logger.info(f"[CLEANUP] ‡∏•‡∏ö TEMP-/HASH- ‡∏≠‡∏≠‡∏Å {total_removed} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | "
                    f"‡πÅ‡∏Å‡πâ Unknown filename {total_unknown_fixed} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | "
                    f"‡πÄ‡∏´‡∏•‡∏∑‡∏≠ {len(cleaned_map)} keys")

        return cleaned_map

    def _save_evidence_map(self, map_to_save: Optional[Dict[str, List[Dict[str, Any]]]] = None):
        """
        ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å evidence map ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢ 100% - Atomic + Lock + Clean + Sort + Score
        ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ get_evidence_mapping_file_path ‡∏à‡∏≤‡∏Å path_utils.py ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î Path
        
        Args:
            map_to_save: ‡∏´‡∏≤‡∏Å‡∏£‡∏∞‡∏ö‡∏∏ ‡∏à‡∏∞‡πÉ‡∏ä‡πâ Dict ‡∏ô‡∏µ‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á (‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£ Merge)
        """
        
        # üéØ FIX 1: ‡∏Å‡∏≥‡∏´‡∏ô‡∏î path ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ path_utils
        try:
            map_file_path = get_evidence_mapping_file_path(
                tenant=self.config.tenant,
                year=self.config.year,
                enabler=self.enabler_id
            )
        except Exception as e:
            self.logger.critical(f"[EVIDENCE] ‚ùå FATAL: ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Evidence Map Path ‡πÑ‡∏î‡πâ: {e}")
            raise
            
        lock_path = map_file_path + ".lock"
        tmp_path = None

        self.logger.info(f"[EVIDENCE] Saving evidence map ‚Üí {map_file_path}")

        # 1. Acquire Lock (‡πÉ‡∏ä‡πâ with FileLock ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏•‡πà‡∏≠‡∏¢ Lock ‡πÄ‡∏™‡∏°‡∏≠)
        try:
            # üö® NOTE: FileLock ‡∏ï‡πâ‡∏≠‡∏á‡∏ñ‡∏π‡∏Å Import ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå seam_assessment.py
            with FileLock(lock_path, timeout=60):
                self.logger.debug("[EVIDENCE] Lock acquired.")

                if map_to_save is not None:
                    final_map_to_write = map_to_save
                else:
                    # 1. ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡∏≠‡∏á‡πÄ‡∏Å‡πà‡∏≤‡∏à‡∏≤‡∏Å‡∏î‡∏¥‡∏™‡∏Å‡πå
                    existing_map = self._load_evidence_map(is_for_merge=True) or {}
                    runtime_map = deepcopy(self.evidence_map)

                    # 2. Merge & Update: ‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á‡πÄ‡∏Å‡πà‡∏≤ + ‡∏Ç‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà ‡πÇ‡∏î‡∏¢‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
                    final_map_to_write = existing_map
                    
                    for key, new_entries in runtime_map.items():
                        
                        # 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á Map ‡∏Ç‡∏≠‡∏á ID -> Entry ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏£‡∏ß‡∏î‡πÄ‡∏£‡πá‡∏ß (‡∏î‡∏∂‡∏á‡∏Ç‡∏≠‡∏á‡πÄ‡∏Å‡πà‡∏≤‡∏°‡∏≤)
                        entry_map = {
                            e.get("chunk_uuid", e.get("doc_id", "N/A")): e
                            for e in final_map_to_write.setdefault(key, [])
                        }

                        # 2. ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• entries ‡πÉ‡∏´‡∏°‡πà ‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô
                        for new_entry in new_entries:
                            entry_id = new_entry.get("chunk_uuid", new_entry.get("doc_id", "N/A"))
                            if entry_id == "N/A" or not entry_id:
                                continue 

                            new_score = new_entry.get("relevance_score", 0.0) 
                            
                            if entry_id not in entry_map:
                                entry_map[entry_id] = new_entry
                            else:
                                existing_score = entry_map[entry_id].get("relevance_score", 0.0)
                                if new_score > existing_score:
                                    self.logger.debug(f"[EVIDENCE] Updated score for {entry_id} key={key}: {existing_score:.4f} -> {new_score:.4f}")
                                    entry_map[entry_id] = new_entry
                        
                        final_map_to_write[key] = list(entry_map.values())


                    # 3. ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ (TEMP-, HASH-, Unknown)
                    final_map_to_write = self._clean_temp_entries(final_map_to_write)

                    # 4. ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ key ‡∏à‡∏≤‡∏Å relevance_score ‡∏™‡∏π‡∏á ‚Üí ‡∏ï‡πà‡∏≥
                    for key, entries in final_map_to_write.items():
                        if entries and "relevance_score" in entries[0]:
                            entries.sort(
                                key=lambda x: x.get("relevance_score", 0.0),
                                reverse=True
                            )

                if not final_map_to_write:
                    self.logger.warning("[EVIDENCE] Nothing to save.")
                    return

                # 5. Atomic Write
                os.makedirs(os.path.dirname(map_file_path), exist_ok=True)

                # ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏•‡∏á‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß
                with tempfile.NamedTemporaryFile(
                    mode='w', delete=False, encoding="utf-8", dir=os.path.dirname(map_file_path)
                ) as tmp_file:
                    cleaned_for_json = self._clean_map_for_json(final_map_to_write)
                    json.dump(cleaned_for_json, tmp_file, indent=4, ensure_ascii=False)
                    tmp_path = tmp_file.name

                # ‡∏¢‡πâ‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß‡πÑ‡∏õ‡∏ó‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏£‡∏¥‡∏á (Atomic Operation)
                shutil.move(tmp_path, map_file_path)
                tmp_path = None 

                # 6. ‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
                total_keys = len(final_map_to_write)
                total_items = sum(len(v) for v in final_map_to_write.values())
                file_size_kb = os.path.getsize(map_file_path) / 1024

                self.logger.info(f"[EVIDENCE] Evidence map saved successfully!")
                self.logger.info(f"   Keys: {total_keys} | Items: {total_items} | Size: ~{file_size_kb:.1f} KB")

                # ‡πÇ‡∏ä‡∏ß‡πå Top 1 ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ sub-criteria
                preview = []
                for key in sorted(final_map_to_write.keys())[:5]: 
                    entries = final_map_to_write[key]
                    if entries:
                        top = entries[0]
                        score = top.get("relevance_score", "-")
                        filename_preview = top.get('filename', 'Unknown')[:50] 
                        preview.append(f"{key}: {filename_preview} ({score})")
                if preview:
                    self.logger.info(f"   Top evidence preview ‚Üí {', '.join(preview[:3])}{'...' if len(preview)>3 else ''}")

        except TimeoutError:
            self.logger.critical(f"[EVIDENCE] Lock timeout! Another process may be stuck: {lock_path}")
            raise
        except Exception as e:
            self.logger.critical("[EVIDENCE] FATAL SAVE ERROR")
            self.logger.exception(e)
            raise
        finally:
            # 7. Cleanup
            if tmp_path and os.path.exists(tmp_path):
                try: os.unlink(tmp_path)
                except: pass
            self.logger.debug(f"[EVIDENCE] Lock released by context manager.")

    def _load_evidence_map(self, is_for_merge: bool = False):
        """
        Safe load of persistent evidence map. Always returns dict.
        ‡πÉ‡∏ä‡πâ get_evidence_mapping_file_path ‡∏à‡∏≤‡∏Å path_utils.py ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î Path
        is_for_merge: If True, suppresses "No existing evidence map" INFO log.
        """
        # üéØ FIX 2: ‡πÉ‡∏ä‡πâ get_evidence_mapping_file_path ‡πÅ‡∏ó‡∏ô self.evidence_map_path
        try:
            path = get_evidence_mapping_file_path(
                tenant=self.config.tenant,
                year=self.config.year,
                enabler=self.enabler_id
            )
        except Exception as e:
            self.logger.error(f"[EVIDENCE] ‚ùå FATAL: ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Evidence Map Path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏î‡πâ: {e}")
            return {}

        if not os.path.exists(path):
            if not is_for_merge:
                self.logger.info("[EVIDENCE] No existing evidence map ‚Äì starting empty.")
            return {}

        try:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
            if not is_for_merge:
                self.logger.info(f"[EVIDENCE] Loaded evidence map: {len(data)} entries from {path}")
            return data

        except Exception as e:
            self.logger.error(f"[EVIDENCE] Failed to load evidence map from {path}: {e}")
            return {}


    def _set_mock_handlers(self, mode: str):
        """Replaces real LLM/RAG functions with mock versions."""
        if mode == "control" or mode == "random":
            if hasattr(seam_mocking, 'evaluate_with_llm_CONTROLLED_MOCK'):
                self.llm_evaluator = seam_mocking.evaluate_with_llm_CONTROLLED_MOCK
            if hasattr(seam_mocking, 'retrieve_context_with_filter_MOCK'):
                self.rag_retriever = seam_mocking.retrieve_context_with_filter_MOCK
            if hasattr(seam_mocking, 'create_structured_action_plan_MOCK'):
                self.action_plan_generator = seam_mocking.create_structured_action_plan_MOCK
            if hasattr(seam_mocking, 'set_mock_control_mode'):
                seam_mocking.set_mock_control_mode(mode == "control") 

        logger.warning(f"Engine is running in MOCK mode: {mode}")

    def _get_pdca_phase(self, level: int) -> str:
        """Helper to get the PDCA phase string from the map."""
        return PDCA_PHASE_MAP.get(level, f"Level {level} Requirement")
    

    def _get_level_constraint_prompt(self, level: int) -> str:
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á Prompt Constraint ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Å‡∏±‡∏ö‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ß‡∏∏‡∏í‡∏¥‡∏†‡∏≤‡∏ß‡∏∞‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô
        """
        if level == 1:
            return "‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏ñ‡∏∂‡∏á '‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢/‡∏ß‡∏¥‡∏™‡∏±‡∏¢‡∏ó‡∏±‡∏®‡∏ô‡πå', '‡∏Å‡∏≤‡∏£‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå', '‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏ó‡∏≥‡∏Å‡∏£‡∏≠‡∏ö‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á', ‡∏´‡∏£‡∏∑‡∏≠ '‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£' ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (L1-Focus)"
        elif level == 2:
            return "‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ô‡πâ‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞ '‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô', '‡∏Å‡∏≤‡∏£‡∏Ç‡∏±‡∏ö‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô', '‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡∏ò‡∏£‡∏£‡∏°', ‡∏´‡∏£‡∏∑‡∏≠ '‡∏Å‡∏≤‡∏£‡∏°‡∏µ‡∏™‡πà‡∏ß‡∏ô‡∏£‡πà‡∏ß‡∏°' ‡∏ï‡∏≤‡∏°‡πÅ‡∏ú‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (L2-Focus)"
        elif level == 3:
            # üö® HARD RULE: ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏ä‡πâ L3 Logic (Check/Act Focus) ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏° Context ‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡∏°‡πà
            return """
‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î (HARD RULE: L3 CHECK/ACT FOCUS):
1. ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô L3 ‡∏ô‡∏µ‡πâ **‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô '‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö (Check)' ‡πÅ‡∏•‡∏∞ '‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á (Act)' ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô**
2. ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÅ‡∏•‡πâ‡∏ß: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏ï‡πâ‡∏ô‡∏Ç‡∏≠‡∏á Context ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î (Priority 1)
3. ‡∏´‡∏≤‡∏Å‡∏û‡∏ö‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ **[L3_SUMMARY_EVIDENCE]** ‡πÉ‡∏´‡πâ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô **‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö** ‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠‡πÑ‡∏î‡πâ‡∏ã‡∏∂‡πà‡∏á‡∏ñ‡∏π‡∏Å‡∏™‡∏£‡∏∏‡∏õ‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô Check/Act ‡∏à‡∏£‡∏¥‡∏á (‡∏à‡∏±‡∏î‡πÄ‡∏õ‡πá‡∏ô Priority 1)
4. ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô Plan ‡πÅ‡∏•‡∏∞ Do ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏ï‡∏≠‡∏ô‡∏ó‡πâ‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á Context **‡∏´‡πâ‡∏≤‡∏°‡∏ô‡∏≥‡∏°‡∏≤‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤** ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à **FAIL** ‡∏´‡∏≤‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô Check/Act ‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô
5. ‡∏´‡∏≤‡∏Å‡∏Ç‡∏≤‡∏î‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô **Check** ‡∏´‡∏£‡∏∑‡∏≠ **Act** ‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠ (‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏à‡∏≤‡∏Å Summary Evidence ‡∏´‡∏£‡∏∑‡∏≠‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á) ‡πÉ‡∏´‡πâ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÄ‡∏õ‡πá‡∏ô **‚ùå FAIL** ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô L3 ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡∏à‡∏£‡∏¥‡∏á
(L3-Focus: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏° ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏• ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á)
"""
        elif level == 4:
            return "‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ñ‡∏∂‡∏á '‡∏Å‡∏≤‡∏£‡∏ö‡∏π‡∏£‡∏ì‡∏≤‡∏Å‡∏≤‡∏£', '‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á', ‡∏´‡∏£‡∏∑‡∏≠ '‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå' ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (L4-Focus)"
        elif level == 5:
            return "‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ñ‡∏∂‡∏á '‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°', '‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡πà‡∏≤‡∏ó‡∏≤‡∏á‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à', ‡∏´‡∏£‡∏∑‡∏≠ '‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß' ‡πÇ‡∏î‡∏¢‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô (L5-Focus)"
        else:
            return ""
        

    def _classify_pdca_phase_for_chunk(
        self, 
        chunk_text: str
    ) -> Literal["Plan", "Do", "Check", "Act", "Other"]:
        """
        ‡πÉ‡∏ä‡πâ LLM ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏∞‡∏¢‡∏∞‡πÉ‡∏î‡∏£‡∏∞‡∏¢‡∏∞‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏Ç‡∏≠‡∏á PDCA ‡∏´‡∏£‡∏∑‡∏≠ 'Other'
        """
        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏ü‡∏™ PDCA
        pdca_phases_th = ["‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô", "‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥", "‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö", "‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á"]
        
        # 1. System Prompt ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢: ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö JSON 100%
        system_prompt = (
            "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô PDCA Cycle\n"
            "‡∏†‡∏≤‡∏£‡∏Å‡∏¥‡∏à‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô ‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏±‡∏î‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ß‡πà‡∏≤‡πÄ‡∏ô‡πâ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡πÉ‡∏î‡∏Ç‡∏≠‡∏á PDCA\n"
            f"‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏´‡∏ô‡∏∂‡πà‡∏á‡πÉ‡∏ô: {', '.join(pdca_phases_th)} ‡∏´‡∏£‡∏∑‡∏≠ '‡∏≠‡∏∑‡πà‡∏ô‡πÜ'\n\n"
            "‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡∏î‡πâ‡∏ß‡∏¢ **JSON Object ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô** ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö:\n"
            "{\"phase\": \"‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô\"}\n"
            "‡∏´‡∏£‡∏∑‡∏≠ {\"phase\": \"‡∏≠‡∏∑‡πà‡∏ô‡πÜ\"}\n"
            "‡∏´‡πâ‡∏≤‡∏°‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏≠‡∏Å JSON ‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≤‡∏î"
        )

        # 2. User Prompt: ‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô + ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
        user_prompt = (
            f"‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô:\n\"\"\"\n{chunk_text.strip()}\n\"\"\"\n\n"
            "‡∏Ñ‡∏≥‡∏ô‡∏¥‡∏¢‡∏≤‡∏°‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏ü‡∏™:\n"
            "- ‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô: ‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢, ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå, ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢, ‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô, ‡∏Ñ‡∏ì‡∏∞‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£\n"
            "- ‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥: ‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£, ‡∏ù‡∏∂‡∏Å‡∏≠‡∏ö‡∏£‡∏°, ‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£, ‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏£‡∏∞‡∏ö‡∏ö, ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á\n"
            "- ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö: ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏ú‡∏•, ‡∏ß‡∏±‡∏î‡∏ú‡∏•, ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô, ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏†‡∏≤‡∏¢‡πÉ‡∏ô, ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n"
            "- ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á: ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç, ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á, ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡πÉ‡∏´‡∏°‡πà, Lesson Learned, ‡∏õ‡∏¥‡∏î‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á\n\n"
            "‡∏ï‡∏≠‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞ JSON:"
        )
        
        try:
            raw_response = _fetch_llm_response(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                temperature=0.0,  # ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å! ‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô
                max_retries=2,
                llm_executor=self.llm
            )

            if not raw_response:
                return "Other"

            # ‡∏î‡∏∂‡∏á JSON ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏Ç‡πá‡∏á‡πÅ‡∏Å‡∏£‡πà‡∏á (‡πÉ‡∏ä‡πâ _robust_extract_json ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß!)
            parsed = _robust_extract_json(raw_response)
            
            # ‡∏ñ‡πâ‡∏≤ _robust_extract_json ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ fallback ‡∏î‡πâ‡∏ß‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ‡πÄ‡∏ö‡∏™‡∏¥‡∏Å
            if not parsed or not isinstance(parsed, dict):
                # ‡∏•‡∏≠‡∏á‡∏î‡∏∂‡∏á‡∏î‡πâ‡∏ß‡∏¢ regex ‡∏á‡πà‡∏≤‡∏¢ ‡πÜ
                import re
                match = re.search(r'"phase"\s*:\s*"([^"]+)"', raw_response, re.IGNORECASE)
                if match:
                    phase_th = match.group(1).strip()
                else:
                    phase_th = "‡∏≠‡∏∑‡πà‡∏ô‡πÜ"
            else:
                phase_th = parsed.get("phase", parsed.get("classification", "‡∏≠‡∏∑‡πà‡∏ô‡πÜ"))
                phase_th = str(phase_th).strip()

            # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô Literal ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
            mapping = {
                "‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô": "Plan",
                "‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥": "Do",
                "‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö": "Check",
                "‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á": "Act",
                "‡∏≠‡∏∑‡πà‡∏ô‡πÜ": "Other",
                "‡∏≠‡∏∑‡πà‡∏ô": "Other",
                "other": "Other"
            }
            result = mapping.get(phase_th, "Other")
            
            self.logger.debug(f"PDCA Classification: '{phase_th}' ‚Üí {result}")
            return result

        except Exception as e:
            self.logger.error(f"PDCA Classification failed: {e}\nRaw: {raw_response[:200]}")
            return "Other"

    # -------------------- Statement Preparation & Filtering Helpers --------------------
    def _flatten_rubric_to_statements(self) -> List[Dict[str, Any]]:
        """
        Transforms the hierarchical rubric structure loaded in self.rubric
        into a flat list of statements ready for assessment.
        """
        if not self.rubric:
            self.logger.warning("Cannot flatten rubric: self.rubric is empty.")
            return []
            
        data = deepcopy(self.rubric)
        extracted_list = []
        
        if not isinstance(data, dict):
             self.logger.error("Rubric data structure is invalid (expected dict of criteria).")
             return []
             
        for criteria_id, criteria_data in data.items():
            # üéØ FIX 1: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Criteria Data
            if not isinstance(criteria_data, dict):
                self.logger.warning(f"Skipping malformed criteria entry: {criteria_id} (not a dict).")
                continue
                
            sub_criteria_map = criteria_data.get('subcriteria', {})
            criteria_name = criteria_data.get('name')
            
            # üéØ FIX 2: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Sub-criteria Map
            if not isinstance(sub_criteria_map, dict):
                 self.logger.warning(f"Skipping criteria {criteria_id}: 'subcriteria' is not a dictionary.")
                 continue

            for sub_id, sub_data in sub_criteria_map.items():
                
                # üéØ FIX 3: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ sub_data ‡πÄ‡∏õ‡πá‡∏ô Dictionary ‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô TypeError)
                if not isinstance(sub_data, dict):
                    self.logger.warning(
                        f"Skipping malformed sub-criteria entry: {criteria_id}.{sub_id} "
                        f"is not a dictionary (found type: {type(sub_data).__name__})."
                    )
                    continue
                
                # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤ Metadata ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
                sub_data['criteria_id'] = criteria_id
                sub_data['criteria_name'] = criteria_name
                sub_data['sub_id'] = sub_id 
                sub_data['sub_criteria_name'] = sub_data.get('name', criteria_name + ' sub')
                if 'weight' not in sub_data:
                    sub_data['weight'] = criteria_data.get('weight', 0)
                extracted_list.append(sub_data)

        # Re-check and re-sort levels
        final_list = []
        for sub_criteria in extracted_list: 
            
            # üéØ FIX 4: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á Level ‡∏à‡∏≤‡∏Å Dict ‡∏ó‡∏µ‡πà‡∏°‡∏µ Key ‡πÄ‡∏õ‡πá‡∏ô String ‡πÄ‡∏õ‡πá‡∏ô List
            if "levels" in sub_criteria and isinstance(sub_criteria["levels"], dict):
                levels_list = []
                for level_str, statement in sub_criteria["levels"].items():
                    try:
                        # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÅ‡∏õ‡∏•‡∏á Level Key ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô Integer
                        level_int = int(level_str)
                        if isinstance(statement, str):
                            levels_list.append({"level": level_int, "statement": statement})
                        else:
                            self.logger.warning(f"Level {level_str} statement in {sub_criteria.get('sub_id')} is not a string.")
                    except ValueError:
                        self.logger.error(f"Invalid level key '{level_str}' found in {sub_criteria.get('sub_id', 'UNKNOWN_ID')}. Skipping.")
                        continue
                        
                sub_criteria["levels"] = levels_list
            
            # ‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏£‡∏∞‡∏î‡∏±‡∏ö
            if "levels" in sub_criteria and isinstance(sub_criteria["levels"], list):
                 sub_criteria["levels"].sort(key=lambda x: x.get("level", 0))
                 final_list.append(sub_criteria)
            else:
                 self.logger.warning(f"Sub-criteria {sub_criteria.get('sub_id', 'UNKNOWN_ID')} missing 'levels' list.")


        return final_list

    def _load_initial_evidence_info(self) -> Set[str]:
        """Retrieves the set of all available stable document IDs in the VectorStore."""
        if self.config.mock_mode != "none":
            return {"00000000-0000-0000-0000-000000000001"}
        return set() 

    def _apply_strict_filter(self, statements: List[Dict[str, Any]], available_evidence_ids: Set[str]) -> List[Dict[str, Any]]:
        """
        Filters out statements that have no specified evidence_doc_ids 
        that match any ID found in the available_evidence_ids set.
        """
        if not available_evidence_ids:
            logger.warning("Strict Filter bypassed: No available evidence IDs loaded.")
            return statements

        filtered_statements = []
        for stmt in statements:
            required_ids = set(stmt.get('evidence_doc_ids', []))
            
            if not required_ids or required_ids.isdisjoint(available_evidence_ids):
                 logger.debug(f"Strict Filter: Skipping {stmt['sub_id']} L{stmt['level']} (No evidence match)")
                 continue
            
            filtered_statements.append(stmt)
            
        return filtered_statements
    

    # -------------------- Evidence Classification Helper --------------------

    def _get_pdca_blocks_from_evidences(
        self, 
        top_evidences: List[Dict[str, Any]], 
        level: int # level ‡∏¢‡∏±‡∏á‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ logging/context ‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï
    ) -> Tuple[str, str, str, str, str]:
        """
        Groups retrieved evidence chunks into PDCA phases based on the 'pdca_tag' 
        generated by the LLM classifier. This replaces the old index-based heuristic.

        Args:
            top_evidences: List of retrieved evidence dictionaries, each containing 'text' and 'pdca_tag'.

        Returns:
            A tuple of aggregated context strings: (plan_blocks, do_blocks, check_blocks, act_blocks, other_blocks)
        """
        logger = logging.getLogger(__name__)

        # 1. Initialize groupings
        pdca_groups = defaultdict(list)
        
        # 2. Group chunks based on the 'pdca_tag'
        for i, doc in enumerate(top_evidences):
            # üìå Use the classified tag directly. Fallback to 'Other' if tag is missing.
            tag = doc.get('pdca_tag', 'Other')
            
            # üìå Format the chunk before appending to the group list
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÄ‡∏î‡∏¥‡∏° (i+1) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ trace ‡∏´‡∏≤‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
            formatted_chunk = f"--- [Chunk {i+1} | Tag: {tag}] ---\n{doc.get('text', '')}\n"
            pdca_groups[tag].append(formatted_chunk)

        # 3. Aggregate groups into single strings
        plan_blocks = "\n\n".join(pdca_groups.get('Plan', []))
        do_blocks = "\n\n".join(pdca_groups.get('Do', []))
        check_blocks = "\n\n".join(pdca_groups.get('Check', []))
        act_blocks = "\n\n".join(pdca_groups.get('Act', []))
        other_blocks = "\n\n".join(pdca_groups.get('Other', []))

        logger.debug(
            f"  > PDCA Blocks Grouped (L{level}): "
            f"P={len(pdca_groups.get('Plan', []))}, D={len(pdca_groups.get('Do', []))}, "
            f"C={len(pdca_groups.get('Check', []))}, A={len(pdca_groups.get('Act', []))}, "
            f"Other={len(pdca_groups.get('Other', []))}"
        )

        return plan_blocks, do_blocks, check_blocks, act_blocks, other_blocks

    def _get_mapped_uuids_and_priority_chunks(
                self, 
                sub_id: str, 
                level: int, 
                statement_text: str, 
                level_constraint: str,
                vectorstore_manager: Optional['VectorStoreManager']
            ) -> Tuple[List[str], List[Dict[str, Any]]]:
        """
        1. Gathers all PASSED Stable Chunk UUIDs (doc_id) from L1 up to L[level-1]. 
        2. Fetches limited priority RAG chunks (Hybrid Retrieval) 
        based on the gathered Chunk UUIDs.
        
        Returns: (mapped_chunk_uuids: list[str], priority_docs: list[dict])
        """
        
        all_priority_items: List[Dict[str, Any]] = [] 
        
        # üìå DEBUG: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Ç‡∏≠‡∏á evidence_map ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡∏∂‡∏á
        logger.info(f"DEBUG: EVIDENCE MAP KEYS BEFORE PRIORITY SEARCH (L{level}): {sorted(self.evidence_map.keys())}")
        
        # 1. ‡∏ß‡∏ô‡∏ã‡πâ‡∏≥‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà PASS ‡∏à‡∏≤‡∏Å Level 1 ‡∏à‡∏ô‡∏ñ‡∏∂‡∏á Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ (L1 -> L[level - 1])
        for prev_level in range(1, level): 
            prev_map_key = f"{sub_id}.L{prev_level}"
            
            # üéØ ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å self.evidence_map (‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÉ‡∏ô Sequential)
            items_from_map = self.evidence_map.get(prev_map_key, [])
            all_priority_items.extend(items_from_map)

        
        # 2. ‡πÅ‡∏õ‡∏•‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô Chunk UUID (String) ‡πÅ‡∏•‡∏∞ Dedup
        # üü¢ FIX 2: ‡πÄ‡∏ô‡πâ‡∏ô‡πÉ‡∏ä‡πâ Chunk UUID ‡∏´‡∏£‡∏∑‡∏≠ Stable Doc UUID ‡πÄ‡∏õ‡πá‡∏ô ID ‡∏´‡∏•‡∏±‡∏Å
        doc_ids_for_dedup: List[str] = [
            (
                item.get('chunk_uuid') 
                or item.get('stable_doc_uuid') # <-- ‡πÄ‡∏ô‡πâ‡∏ô Stable Doc UUID
                or item.get('doc_id')
            )
            for item in all_priority_items
            if isinstance(item, dict) and (
                item.get('chunk_uuid') or item.get('stable_doc_uuid') or item.get('doc_id')
            )
        ]

        mapped_chunk_uuids: List[str] = list(set([uid for uid in doc_ids_for_dedup if uid is not None])) # ‡∏Å‡∏£‡∏≠‡∏á None ‡∏≠‡∏≠‡∏Å
        num_historical_chunks = len(mapped_chunk_uuids)

        priority_docs = [] 
        
        if num_historical_chunks > 0:
            levels_logged = f"L1-L{level-1}" if level > 1 else "L0 (Should not happen)"
            logger.critical(f"üß≠ DEBUG: Priority Search initiated with {num_historical_chunks} historical Chunk UUIDs ({levels_logged}).") 
            logger.info(f"‚úÖ Hybrid Mapping: Found {num_historical_chunks} pre-mapped Chunk UUIDs from {levels_logged} for {sub_id}. Prioritizing these.")
            
            if vectorstore_manager:
                try:
                    # Assuming enhance_query_for_statement is available
                    rag_queries_for_vsm = enhance_query_for_statement(
                        statement_text=statement_text,
                        sub_id=sub_id, 
                        statement_id=sub_id, 
                        level=level, 
                        enabler_id=self.enabler_id,
                        focus_hint=level_constraint 
                    )
                    
                    doc_type = self.doc_type 
                    
                    # 3.1 ‡∏î‡∏∂‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ï‡∏≤‡∏° Chunk UUIDs ‡∏ó‡∏µ‡πà‡∏û‡∏ö
                    retrieved_docs_result = retrieve_context_by_doc_ids(
                        doc_uuids=mapped_chunk_uuids, # <-- ‡πÉ‡∏ä‡πâ Chunk/Stable Doc UUIDs
                        doc_type=doc_type,
                        enabler=self.enabler_id,
                        vectorstore_manager=vectorstore_manager
                    )
                    
                    initial_priority_chunks: List[Dict[str, Any]] = retrieved_docs_result.get("top_evidences", [])
                    
                    if initial_priority_chunks:
                        # Rerank ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Chunk ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
                        reranker = get_global_reranker() 
                        rerank_query = rag_queries_for_vsm[0] 
                        
                        # ‡∏™‡∏£‡πâ‡∏≤‡∏á LcDocument list ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Rerank (‡∏ï‡πâ‡∏≠‡∏á‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤ LcDocument)
                        lc_docs_for_rerank = [
                            LcDocument(
                                page_content=d.get('content') or d.get('text', ''), 
                                metadata={
                                    **d, 
                                    'relevance_score': 1.0 
                                }
                            ) 
                            for d in initial_priority_chunks
                        ]
                        
                        if reranker and hasattr(reranker, 'compress_documents'):
                            reranked_docs = reranker.compress_documents(
                                query=rerank_query,
                                documents=lc_docs_for_rerank,
                                top_n=self.PRIORITY_CHUNK_LIMIT 
                            )

                            # === ‡∏ß‡∏¥‡∏ä‡∏≤‡∏°‡∏≤‡∏£‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏Ü‡πà‡∏≤ 0.0000 ‡∏ï‡∏•‡∏≠‡∏î‡∏Å‡∏≤‡∏• ===
                            # ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô relevance_score ‡∏Å‡∏•‡∏±‡∏ö‡∏•‡∏á metadata ‡∏Å‡πà‡∏≠‡∏ô

                            # üõë [‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç V3] ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏û‡∏∂‡πà‡∏á reranker.scores ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏ú‡πà‡∏≤‡∏ô reranked_docs 
                            # ‡πÅ‡∏•‡∏∞‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏î‡∏∂‡∏á score ‡∏à‡∏≤‡∏Å metadata ‡∏Ç‡∏≠‡∏á‡∏°‡∏±‡∏ô
                            
                            scores = []
                            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Reranker ‡πÑ‡∏î‡πâ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô score ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô Document.metadata ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                            for doc in reranked_docs:
                                # score ‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏à‡∏≤‡∏Å reranker ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô metadata ‡∏†‡∏≤‡∏¢‡πÉ‡∏ï‡πâ key 'relevance_score' ‡∏´‡∏£‡∏∑‡∏≠ 'score'
                                score = doc.metadata.get('relevance_score') or doc.metadata.get('score', 0.0)
                                scores.append(float(score))
                                # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô relevance_score ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô metadata
                                doc.metadata["relevance_score"] = float(score)


                            if scores: # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ Scores ‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡πÑ‡∏î‡πâ
                                
                                # üü¢ [‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç] 1. Log ‡∏™‡∏£‡∏∏‡∏õ‡∏î‡πâ‡∏ß‡∏¢ INFO ‡πÅ‡∏•‡∏∞ CRITICAL
                                num_reranked = len(reranked_docs)
                                logger.info(f"‚ú® Reranking success ({sub_id} L{level}) ‚Üí Prioritized {num_reranked} chunks. Logging top scores:")
                                # üéØ Log 2 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏ó‡πà‡∏≤‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£!
                                logger.critical(f"‚ú® RERANK SCORE LOG (PRIORITY CHUNKS) ({sub_id} L{level}) ‚Üí Logging top {min(5, num_reranked)} scores:")
                                
                                for i in range(len(reranked_docs)):
                                    doc = reranked_docs[i]
                                    score = scores[i] # ‡πÉ‡∏ä‡πâ score ‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß
                                    
                                    # üü¢ [‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç] 2. ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡πÅ‡∏•‡∏∞ Log ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏î‡πâ‡∏ß‡∏¢ CRITICAL
                                    if i < 5: 
                                        filename = doc.metadata.get('filename', doc.metadata.get('source_filename', 'N/A'))
                                        doc_id_full = doc.metadata.get('doc_id', doc.metadata.get('chunk_uuid', 'N/A'))
                                        
                                        # ‡∏ï‡∏±‡∏î Chunk ID ‡πÉ‡∏´‡πâ‡∏™‡∏±‡πâ‡∏ô‡∏•‡∏á
                                        if len(doc_id_full) > 8 and '_' in doc_id_full:
                                            doc_id_short = doc_id_full.split('_')[0][:8]
                                        else:
                                            doc_id_short = doc_id_full[:8]
                                            
                                        logger.critical(f"  > Rerank #{i+1}: {doc_id_short} ({filename}) | Score: {float(score):.4f}")

                            # ‡πÅ‡∏õ‡∏•‡∏á‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô dict ‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ 'score' ‡∏ó‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡πÄ‡∏Å‡πà‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô
                            priority_docs = []
                            for d in reranked_docs:
                                # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å metadata ‡πÄ‡∏î‡∏¥‡∏°
                                item = dict(d.metadata)
                                # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
                                item.update({
                                    'content': d.page_content,
                                    'text': d.page_content,
                                    # ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î: score ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏≤‡∏ó‡πâ‡∏≤‡∏¢‡∏™‡∏∏‡∏î ‡πÅ‡∏•‡∏∞‡∏ó‡∏±‡∏ö‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô
                                    'score': float(d.metadata.get('relevance_score', 0.0)),
                                    'relevance_score': float(d.metadata.get('relevance_score', 0.0))
                                })
                                priority_docs.append(item)
                            # ========================================

                            logger.critical(f"DEBUG: Limited and prioritized {len(priority_docs)} chunks from {num_historical_chunks} mapped UUIDs.")
                        else:
                            # fallback ‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏°‡πà‡∏°‡∏µ reranker
                            priority_docs = initial_priority_chunks[:self.PRIORITY_CHUNK_LIMIT]
                            # ‡πÅ‡∏°‡πâ fallback ‡∏Å‡πá‡∏¢‡∏±‡∏á‡πÉ‡∏™‡πà score ‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö
                            for item in priority_docs:
                                if 'score' not in item:
                                    item['score'] = 0.0

                except Exception as e:
                    logger.error(f"Error fetching/reranking priority chunks for {sub_id}: {e}")
                    priority_docs = [] 
        
        # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ Chunk UUIDs ‡πÅ‡∏•‡∏∞ Chunks ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏î‡∏∂‡∏á‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÅ‡∏•‡πâ‡∏ß
        return mapped_chunk_uuids, priority_docs


    # -------------------- Calculation Helpers (ADDED) --------------------
    def _calculate_weighted_score(self, highest_full_level: int, weight: int) -> float:
        """
        Calculates the weighted score based on the highest full level achieved.
        Score is calculated by: (Level / 5) * Weight
        """
        MAX_LEVEL_CALC = 5  
        
        if highest_full_level <= 0:
            return 0.0
        
        level_for_calc = min(highest_full_level, MAX_LEVEL_CALC)
        score = (level_for_calc / MAX_LEVEL_CALC) * weight
        return score

    def _calculate_overall_stats(self, target_sub_id: str = "all"):
            """
            Calculates the total weighted score, total possible weight, and overall maturity score/level.
            """
            total_weighted_score = 0.0
            total_possible_weight = 0.0
            assessed_count = 0
            
            for result in self.final_subcriteria_results:
                if target_sub_id.lower() != "all" and result.get('sub_criteria_id') != target_sub_id:
                    continue

                weighted_score = result.get('weighted_score', 0.0)
                weight = result.get('weight', 0)
                
                total_weighted_score += weighted_score
                total_possible_weight += weight
                assessed_count += 1
                
            overall_maturity_score_avg = 0.0
            overall_maturity_level = "N/A"
            overall_progress_percent = 0.0 # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏ó‡∏µ‡πà 0.0
            
            if total_possible_weight > 0:
                overall_progress_percent = total_weighted_score / total_possible_weight
                
                MAX_LEVEL_STATS = 5 
                overall_maturity_score_avg = overall_progress_percent * MAX_LEVEL_STATS 

                # üü¢ FIX: Completed Logic for Maturity Level Determination
                if overall_maturity_score_avg >= 4.5:
                    overall_maturity_level = "L5"
                elif overall_maturity_score_avg >= 3.5:
                    overall_maturity_level = "L4"
                elif overall_maturity_score_avg >= 2.5:
                    overall_maturity_level = "L3"
                elif overall_maturity_score_avg >= 1.5:
                    overall_maturity_level = "L2"
                elif overall_maturity_score_avg >= 0.5:
                    overall_maturity_level = "L1"
                else:
                    overall_maturity_level = "L0"
            
            self.total_stats = {
                "Overall Maturity Score (Avg.)": overall_maturity_score_avg,
                "Overall Maturity Level (Weighted)": overall_maturity_level,
                "Number of Sub-Criteria Assessed": assessed_count,
                "Total Weighted Score Achieved": total_weighted_score,
                "Total Possible Weight": total_possible_weight,
                "Overall Progress Percentage (0.0 - 1.0)": overall_progress_percent,
                "percentage_achieved_run": overall_progress_percent * 100,
                "total_subcriteria": len(self.rubric),
                "target_level": self.config.target_level
            }
            return self.total_stats
            
    def _export_results(self, results: dict, sub_criteria_id: str, **kwargs) -> str:
        """
        Exports the assessment results (for a specific sub-criteria or the final run) 
        to a JSON file, using utils/path_utils.py for full path determination.
        
        Args:
            results: The dictionary containing the assessment summary and results.
            sub_criteria_id: The specific sub-criteria ID being exported (e.g., 2.2).
            
        Returns:
            The path to the saved JSON file, or an empty string on failure.
        """
        
        # **kwargs ‡∏£‡∏±‡∏ö Argument ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡∏°‡∏≤ (‡πÄ‡∏ä‡πà‡∏ô enabler, target_level) ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
        
        # ‡πÇ‡∏Ñ‡πâ‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡πÉ‡∏ä‡πâ self.enabler_id ‡πÅ‡∏•‡∏∞ self.config.target_level ‡∏ï‡∏≤‡∏°‡πÄ‡∏î‡∏¥‡∏°
        enabler = self.enabler_id
        target_level = self.config.target_level
        
        try:
            # 1. Import ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ os, json, datetime ‡∏ñ‡∏π‡∏Å import ‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà‡∏™‡πà‡∏ß‡∏ô‡∏ö‡∏ô‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå)
            from utils.path_utils import get_export_dir, get_assessment_export_file_path
            from datetime import datetime # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£ import datetime

            # 2. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Path Utility
            tenant = self.config.tenant
            year = self.config.year
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á Suffix ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå (‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏° tenant/year/enabler)
            # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á Suffix: "assessment_results_2.2_20251209_100000"
            suffix = f"assessment_results_{sub_criteria_id}_{timestamp}"
            
            # 4. ‡πÉ‡∏ä‡πâ Path Utility ‡∏™‡∏£‡πâ‡∏≤‡∏á Full Path (‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á Directory ‡πÅ‡∏•‡∏∞ Filename)
            # *‡∏à‡∏∞‡πÉ‡∏ä‡πâ self.config.export_path ‡∏Å‡πá‡∏ï‡πà‡∏≠‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ override directory ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô*
            if self.config.export_path:
                # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î export_path (Override) ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ directory ‡∏ô‡∏±‡πâ‡∏ô‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏ö‡∏ö‡πÄ‡∏î‡∏¥‡∏°
                export_dir = self.config.export_path
                file_name = f"assessment_results_{enabler}_{sub_criteria_id}_{timestamp}.json"
                full_path = os.path.join(export_dir, file_name)
            else:
                # üéØ ‡πÉ‡∏ä‡πâ get_assessment_export_file_path ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á Full Path ‡∏ï‡∏≤‡∏°‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô
                full_path = get_assessment_export_file_path(
                    tenant=tenant,
                    year=year,
                    enabler=enabler,
                    suffix=suffix,
                    extension="json"
                )
                export_dir = get_export_dir(tenant, year, enabler)

        except ImportError:
            self.logger.error("‚ùå FATAL: Cannot import path_utils. Falling back to manual path.")
            # üìå ASSUMPTION: EXPORTS_DIR ‡∏ñ‡∏π‡∏Å import ‡∏à‡∏≤‡∏Å config.global_vars
            export_dir = self.config.export_path or EXPORTS_DIR
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            file_name = f"assessment_results_{enabler}_{sub_criteria_id}_{timestamp}.json"
            full_path = os.path.join(export_dir, file_name)
            
        
        # 5. ‡∏™‡∏£‡πâ‡∏≤‡∏á Directory ‡∏´‡∏≤‡∏Å‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ (‡πÉ‡∏ä‡πâ export_dir ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å path utility ‡∏´‡∏£‡∏∑‡∏≠ fallback)
        if not os.path.exists(export_dir):
            try:
                os.makedirs(export_dir)
                self.logger.info(f"Created export directory: {export_dir}")
            except OSError as e:
                self.logger.error(f"‚ùå Failed to create export directory {export_dir}: {e}")
                return ""

        # 6. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°/‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ï Summary Field
        if 'summary' not in results:
            results['summary'] = {}
            
        results['summary']['enabler'] = enabler
        results['summary']['sub_criteria_id'] = sub_criteria_id
        results['summary']['target_level'] = target_level
        
        if 'sub_criteria_results' in results:
            results['summary']['Number of Sub-Criteria Assessed'] = len(results['sub_criteria_results'])
        else:
             results['summary']['Number of Sub-Criteria Assessed'] = 1 

        # 7. Export ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏õ‡∏ó‡∏µ‡πà JSON File
        try:
            # üìå ASSUMPTION: json ‡∏ñ‡∏π‡∏Å import ‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà‡∏™‡πà‡∏ß‡∏ô‡∏ö‡∏ô‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå
            with open(full_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=4)
            
            self.logger.info(f"üíæ Successfully exported results for {sub_criteria_id} to: {full_path}")
            return full_path
        
        except Exception as e:
            self.logger.error(f"‚ùå Failed to export results for {sub_criteria_id} to {full_path}: {e}")
            return ""
        
    def rephrase_query_for_retry(self, original_query: str, level: int, sub_id: str) -> str:
        """
        Helper method to slightly rephrase the query for the next retrieval attempt.
        """
        self.logger.info(f"Rephrasing query for L{level} retry: {original_query[:50]}...")
        # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö query: ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏ï‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á‡∏≠‡∏≠‡∏Å ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ LLM ‡∏ä‡πà‡∏ß‡∏¢ rephrase 
        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÅ‡∏Ñ‡πà‡∏Ñ‡∏∑‡∏ô query ‡πÄ‡∏î‡∏¥‡∏° ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ '‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î'
        if level >= 3:
            # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Level ‡∏™‡∏π‡∏á‡πÜ ‡∏•‡∏≠‡∏á‡πÄ‡∏ô‡πâ‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ó‡∏µ‡πà‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡∏Ç‡∏∂‡πâ‡∏ô
            return f"‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö {original_query}"
        return original_query

    def _run_sub_criteria_assessment_worker(
            self,
            sub_criteria: Dict[str, Any],
        ) -> Tuple[Dict[str, Any], Dict[str, List[Dict[str, Any]]]]:
            """
            ‡∏£‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô L1-L5 ‡πÅ‡∏ö‡∏ö sequential ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö sub-criteria ‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏ï‡∏±‡∏ß
            ‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á evidence map ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡πÉ‡∏´‡πâ main process ‡∏£‡∏ß‡∏° (‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Action Plan)
            """
            sub_id = sub_criteria['sub_id']
            sub_criteria_name = sub_criteria['sub_criteria_name']
            sub_weight = sub_criteria.get('weight', 0)

            MAX_L1_ATTEMPTS = 2
            highest_full_level = 0
            is_passed_current_level = True
            raw_results_for_sub_seq: List[Dict[str, Any]] = []
            start_ts = time.time() # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô

            self.logger.info(f"[WORKER START] Assessing Sub-Criteria: {sub_id} - {sub_criteria_name} (Weight: {sub_weight})")

            # ‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï temp_map_for_save ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ worker ‡∏ô‡∏µ‡πâ (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Parallel/Async!)
            self.temp_map_for_save = {}

            # -----------------------------------------------------------
            # 1. LOOP THROUGH LEVELS (L1 ‚Üí L5)
            # -----------------------------------------------------------
            for statement_data in sub_criteria.get('levels', []):
                level = statement_data.get('level')
                if level is None or level > self.config.target_level:
                    continue

                # üéØ FIX: ‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å Dependency Check
                # ‡∏ó‡∏≥‡πÉ‡∏´‡πâ Level ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å Capped ‡πÅ‡∏•‡∏∞ LLM ‡∏à‡∏∞‡∏£‡∏±‡∏ô‡πÄ‡∏™‡∏°‡∏≠ (‡∏à‡∏ô‡∏ñ‡∏∂‡∏á Target Level)
                dependency_failed = False
                
                previous_level = level - 1
                persistence_key = f"{sub_id}.L{previous_level}"
                # ‡∏î‡∏∂‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏à‡∏≤‡∏Å Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
                sequential_chunk_uuids = self.evidence_map.get(persistence_key, []) 

                level_result = {}
                level_temp_map: List[Dict[str, Any]] = []

                # --- 1.1 CALL _run_single_assessment (with Retry/Attempt Logic) ---
                if dependency_failed:
                    # **‡πÇ‡∏Ñ‡πâ‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡πÅ‡∏•‡πâ‡∏ß**
                    # ‡∏ñ‡πâ‡∏≤ Dependency Failed ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô LLM ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå CAPPED
                    error_msg = f"Assessment capped: L{previous_level} did not pass fully."
                    level_result = self._create_error_result(
                        level=level, 
                        error_message=error_msg, 
                        start_time=start_ts, 
                        sub_id=sub_id, 
                        statement_id=statement_data.get('statement_id', sub_id), 
                        statement_text=statement_data['statement']
                    )
                    level_result['is_capped'] = True
                    level_result['status'] = "CAPPED"
                    self.logger.info(f"  > üõë CAPPED L{level}: Due to L{previous_level} failure.")

                elif level >= 3:
                    # L3-L5: ‡πÉ‡∏ä‡πâ RetryPolicy
                    wrapper = self.retry_policy.run(
                        fn=lambda attempt: self._run_single_assessment(
                            sub_criteria=sub_criteria,
                            statement_data=statement_data,
                            vectorstore_manager=self.vectorstore_manager,
                            sequential_chunk_uuids=sequential_chunk_uuids
                        ),
                        level=level,
                        statement=statement_data.get('statement', ''),
                        context_blocks={"sequential_chunk_uuids": sequential_chunk_uuids},
                        logger=self.logger
                    )
                    level_result = wrapper.result if isinstance(wrapper, RetryResult) and wrapper.result is not None else {}
                    level_temp_map = level_result.get("temp_map_for_level", []) 

                else:
                    # L1-L2: ‡∏•‡∏≠‡∏á‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î 2 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á
                    for attempt in range(MAX_L1_ATTEMPTS):
                        level_result = self._run_single_assessment(
                            sub_criteria=sub_criteria,
                            statement_data=statement_data,
                            vectorstore_manager=self.vectorstore_manager,
                            sequential_chunk_uuids=sequential_chunk_uuids
                        )
                        level_temp_map = level_result.get("temp_map_for_level", []) 
                        if level_result.get('is_passed', False):
                            break

                # --- 1.2 PROCESS RESULT AND HANDLE EVIDENCE ---
                result_to_process = level_result or {}
                result_to_process.setdefault("used_chunk_uuids", [])

                # ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô pass/fail ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ (LLM result AND NOT dependency cap)
                is_passed_llm = result_to_process.get('is_passed', False)
                # is_passed_final ‡∏à‡∏∞‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö is_passed_llm ‡πÄ‡∏™‡∏°‡∏≠ ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å dependency_failed ‡πÄ‡∏õ‡πá‡∏ô False
                is_passed_final = is_passed_llm and not dependency_failed

                result_to_process['is_passed'] = is_passed_final
                # is_capped ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô False ‡πÄ‡∏™‡∏°‡∏≠
                result_to_process['is_capped'] = is_passed_llm and not is_passed_final
                # NOTE: Assuming get_correct_pdca_required_score is defined elsewhere
                result_to_process['pdca_score_required'] = get_correct_pdca_required_score(level) 

                # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å evidence ‡∏•‡∏á temp_map_for_save ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠ PASS ‡∏à‡∏£‡∏¥‡∏á
                if is_passed_final and level_temp_map and isinstance(level_temp_map, list):
                    
                    # üü¢ FIX: Resolve Filename ‡∏à‡∏≤‡∏Å Doc ID Map (‡πÉ‡∏ä‡πâ self.doc_id_to_filename_map)
                    resolved_temp_map = []
                    for ev in level_temp_map:
                        filename = ev.get("filename")
                        doc_id = ev.get("doc_id")
                        
                        if not filename or filename == "Unknown":
                            resolved_filename = self.doc_id_to_filename_map.get(doc_id) 
                            if resolved_filename:
                                ev['filename'] = resolved_filename
                                self.logger.debug(f"Resolved 'Unknown' filename for {doc_id} to {resolved_filename}")
                        
                        resolved_temp_map.append(ev)
                        
                    current_key = f"{sub_id}.L{level}"
                    self.temp_map_for_save[current_key] = resolved_temp_map 
                    self.logger.info(f"[EVIDENCE SAVED] {current_key} ‚Üí {len(resolved_temp_map)} chunks")

                    # üéØ FIX SEQUENTIAL DEPENDENCY: ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï self.evidence_map ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
                    if self.is_sequential:
                        self.evidence_map[current_key] = resolved_temp_map
                        self.logger.info(f"[SEQUENTIAL UPDATE] {current_key} added to engine's main evidence_map for L{level+1} dependency.")
                    
                # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö level ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
                is_passed_current_level = is_passed_final

                # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏•‡∏á raw results
                result_to_process.setdefault("level", level)
                result_to_process["execution_index"] = len(raw_results_for_sub_seq)
                raw_results_for_sub_seq.append(result_to_process)

                # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï highest level
                if is_passed_final:
                    highest_full_level = level
                # ‡πÑ‡∏°‡πà break ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ ‡πÅ‡∏ï‡πà‡∏õ‡∏•‡πà‡∏≠‡∏¢‡πÉ‡∏´‡πâ loop ‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡πÑ‡∏õ‡∏à‡∏ô‡∏à‡∏ö‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
            
            # -----------------------------------------------------------
            # 2. CALCULATE SUMMARY
            # -----------------------------------------------------------
            weighted_score = self._calculate_weighted_score(highest_full_level, sub_weight)
            num_passed = sum(1 for r in raw_results_for_sub_seq if r.get("is_passed", False))

            sub_summary = {
                "num_statements": len(raw_results_for_sub_seq),
                "num_passed": num_passed,
                "num_failed": len(raw_results_for_sub_seq) - num_passed,
                "pass_rate": round(num_passed / len(raw_results_for_sub_seq), 4) if raw_results_for_sub_seq else 0.0
            }

        
            # -----------------------------------------------------------
            # 3. GENERATE ACTION PLAN (POST-PROCESSING) üöÄ
            # -----------------------------------------------------------

            target_next_level = highest_full_level + 1 if highest_full_level < 5 else 5
            
            # üü¢ NEW: ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏Å‡∏ì‡∏ë‡πå Evidence Strength ‡∏ï‡πà‡∏≥
            WEAK_EVIDENCE_THRESHOLD = 5.0 
            
            statements_for_action_plan = []
            
            for r in raw_results_for_sub_seq:
                is_passed = r.get('is_passed', False)
                is_capped = r.get('is_evidence_strength_capped', False)
                # ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤ evidence_strength (‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô float 0.0 - 10.0)
                evidence_strength = r.get('evidence_strength', 10.0)

                # 1. Statements ‡∏ó‡∏µ‡πà FAIL ‡∏à‡∏£‡∏¥‡∏á‡πÜ (‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ç‡πâ‡∏≠‡∏ö‡∏Å‡∏û‡∏£‡πà‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å)
                if not is_passed and not is_capped:
                    r['recommendation_type'] = 'FAILED'
                    statements_for_action_plan.append(r)
                    continue

                # 2. Statements ‡∏ó‡∏µ‡πà PASS ‡πÅ‡∏ï‡πà‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏≠‡πà‡∏≠‡∏ô‡πÅ‡∏≠ (‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô)
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö: ‡∏ñ‡πâ‡∏≤‡∏ú‡πà‡∏≤‡∏ô AND Strength ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏Å‡∏ì‡∏ë‡πå
                if is_passed and evidence_strength < WEAK_EVIDENCE_THRESHOLD:
                    # üí° Mark Statement ‡∏ô‡∏µ‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏Å‡∏≤‡∏£‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô
                    r['recommendation_type'] = 'WEAK_EVIDENCE' 
                    statements_for_action_plan.append(r)

            action_plan_result = []
            try:
                # üü¢ ‡πÉ‡∏ä‡πâ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏° Weak Evidence ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢
                action_plan_result = self.create_structured_action_plan( 
                    # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å failed_statements ‡πÄ‡∏õ‡πá‡∏ô statements_for_action_plan
                    failed_statements=statements_for_action_plan, 
                    sub_id=sub_id,
                    target_level=target_next_level,
                    llm_executor=self.llm
                )
            except Exception as e:
                self.logger.error(f"Failed to generate Action Plan for {sub_id}: {e}")
                action_plan_result = [{
                    "Phase": "Error", 
                    "Goal": "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á Action Plan ‡πÑ‡∏î‡πâ", 
                    "Actions": [{
                        "Statement_ID": "ERROR", 
                        "Recommendation": f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ LLM ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Action Plan: {str(e)}"
                    }]
                }]


            # -----------------------------------------------------------
            # 4. FINAL RESULT
            # -----------------------------------------------------------
            
            # ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏° Evidence Map ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô
            final_temp_map = {}
            if self.is_sequential:
                for key in self.evidence_map:
                    if key.startswith(sub_criteria['sub_id'] + "."):
                        final_temp_map[key] = self.evidence_map[key]
            else:
                final_temp_map = self.temp_map_for_save.copy()

            final_sub_result = {
                "sub_criteria_id": sub_id,
                "sub_criteria_name": sub_criteria_name,
                "highest_full_level": highest_full_level,
                "weight": sub_weight,
                "target_level_achieved": highest_full_level >= self.config.target_level,
                "weighted_score": weighted_score,
                "action_plan": action_plan_result, # üü¢ ‡πÅ‡∏ô‡∏ö Action Plan
                "raw_results_ref": raw_results_for_sub_seq,
                "sub_summary": sub_summary,
                "worker_duration_s": round(time.time() - start_ts, 2) # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏≥‡∏á‡∏≤‡∏ô
            }


            self.logger.info(f"[WORKER END] {sub_id} | Highest: L{highest_full_level} | Action Plans: {len(action_plan_result)} phase(s) | Duration: {final_sub_result['worker_duration_s']:.2f}s")

            return final_sub_result, final_temp_map

    def _calculate_evidence_strength_cap(
        self,
        top_evidences: List[Union[Dict[str, Any], Any]],  # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á dict ‡πÅ‡∏•‡∏∞ LcDocument
        level: int,
        # üü¢ FIX: ‡πÄ‡∏û‡∏¥‡πà‡∏° Argument ‡∏ï‡∏±‡∏ß‡∏ô‡∏µ‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ TypeError ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏à‡∏≤‡∏Å _run_single_assessment ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ
        highest_rerank_score: Optional[float] = None 
    ) -> Dict[str, Any]:
        """
        Relevant Score Gate ‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô DEBUG FINAL: ‡∏î‡∏∂‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏à‡∏≤‡∏Å metadata, top-level key/attribute, ‡πÅ‡∏•‡∏∞ Regex fallback ‡∏ó‡∏µ‡πà‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°
        """

        # ‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ô‡∏µ‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
        max_score_found = 0.0 
        max_score_source = "N/A"

        score_keys = [
            "relevance_score", "rerank_score", "score", 
            "_rerank_score_force", "_rerank_score", 
            "Score", "RelevanceScore"
        ]
        
        # üí° ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤ Threshold ‡πÅ‡∏•‡∏∞ Cap ‡∏à‡∏≤‡∏Å Attribute ‡∏Ç‡∏≠‡∏á Class
        threshold = getattr(self, "RERANK_THRESHOLD", 0.5) 
        cap_value = getattr(self, "MAX_EVI_STR_CAP", 3.0)
        
        # üí° Fallback: ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Attribute ‡πÉ‡∏´‡πâ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å config/global_vars
        if not isinstance(threshold, (int, float)):
            try:
                from config.global_vars import RERANK_THRESHOLD as G_RERANK_THRESHOLD
                from config.global_vars import MAX_EVI_STR_CAP as G_MAX_EVI_STR_CAP
                threshold = G_RERANK_THRESHOLD
                cap_value = G_MAX_EVI_STR_CAP
            except ImportError:
                # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Default ‡∏´‡∏≤‡∏Å Config ‡∏´‡∏≤‡∏¢‡πÑ‡∏õ
                threshold = 0.5
                cap_value = 3.0

        # üí° ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å Adaptive Loop (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ) ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
        if highest_rerank_score is not None and highest_rerank_score > max_score_found:
             max_score_found = highest_rerank_score
             max_score_source = "Adaptive_RAG_Loop"


        for doc in top_evidences:
            
            # -------------------- DEBUGGING BLOCK (START) --------------------
            if doc is top_evidences[0]:
                self.logger.critical(f"DEBUG L{level}: Inspecting first document (Type: {type(doc)})")
                
                if isinstance(doc, dict):
                    content = doc.get("text", "")
                    tail_content = content[-200:] if len(content) > 200 else content
                    self.logger.critical(f"DEBUG L{level}: Dict keys: {list(doc.keys())}")
                    self.logger.critical(f"DEBUG L{level}: END OF 'text' content (last 200 chars): \n***\n{tail_content}\n***")
                else:
                    try:
                        doc_attrs = [attr for attr in dir(doc) if not attr.startswith('_') and not callable(getattr(doc, attr))]
                        self.logger.critical(f"DEBUG L{level}: Doc public attributes (potential score location): {doc_attrs}")
                    except:
                        self.logger.critical(f"DEBUG L{level}: Cannot inspect attributes of this object type.")
            # -------------------- DEBUGGING BLOCK (END) --------------------
            
            page_content = ""
            metadata = {}
            
            # ‚îÄ‚îÄ‚îÄ 1. ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô metadata + content ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏∏‡∏Å‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á) ‚îÄ‚îÄ‚îÄ
            if isinstance(doc, dict):
                metadata = doc.get("metadata", {}) 
                page_content = doc.get("page_content", "") or doc.get("text", "") or doc.get("content", "")
            else:
                metadata = getattr(doc, "metadata", {})
                page_content = getattr(doc, "page_content", "") or getattr(doc, "text", "")

            # ‚îÄ‚îÄ‚îÄ 2. ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô (‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö top-level key/attribute ‡πÅ‡∏•‡∏∞ metadata) ‚îÄ‚îÄ‚îÄ
            current_score = 0.0
            
            for key in score_keys:
                score_val = None
                
                if key in metadata:
                    score_val = metadata[key]
                
                if score_val is None:
                    if isinstance(doc, dict):
                        score_val = doc.get(key)
                    else:
                        score_val = getattr(doc, key, None)

                if score_val is not None:
                    try:
                        current_score = float(score_val)
                        if current_score > 0:
                            break
                    except (ValueError, TypeError):
                        continue
            
            # ‚îÄ‚îÄ‚îÄ 3. Fallback: ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å‡∏ó‡πâ‡∏≤‡∏¢ content (Aggressive Regex) ‚îÄ‚îÄ‚îÄ
            if current_score == 0.0 and page_content and isinstance(page_content, str):
                try:
                    import re
                    tail = page_content[-1000:]
                    patterns = [
                        r"Relevance[ :]+([0-9]*\.?[0-9]+)",
                        r"Score[ :]+([0-9]*\.?[0-9]+)",
                        r"Re:[ ]*([0-9]*\.?[0-9]+)",
                        r"\[Relevance: ([0-9]*\.?[0-9]+)\]",
                        r"\[Score: ([0-9]*\.?[0-9]+)\]",
                        r"rerank_score['\"]?\s*:\s*([0-9]*\.?[0-9]+)",
                        r"\|\s*([0-9]*\.?[0-9]+)\s*\|",
                        r"\s+([0-9]\.[0-9]+)$",
                    ]
                    for pat in patterns:
                        m = re.search(pat, tail, re.IGNORECASE)
                        if m:
                            try:
                                current_score = float(m.group(1))
                                break
                            except:
                                continue
                except ImportError:
                    pass

            # üî¥ FIX: ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô (Score Clamp) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Reranker Score
            # ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏Å‡∏¥‡∏ô 1.0 (‡∏ã‡∏∂‡πà‡∏á‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡∏Å‡∏±‡∏ö Cross-Encoder Reranker)
            # ‡πÉ‡∏´‡πâ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡∏ñ‡∏π‡∏Å‡∏î‡∏∂‡∏á‡∏°‡∏≤‡∏ú‡∏¥‡∏î‡πÅ‡∏•‡∏∞‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï‡πÄ‡∏õ‡πá‡∏ô 0.0 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ Evi Str ‡πÄ‡∏ï‡πá‡∏°‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ï‡∏±‡πâ‡∏á‡πÉ‡∏à
            if current_score > 1.0:
                self.logger.warning(f"üö® Score Clamp L{level}: Resetting invalid score {current_score:.4f} > 1.0 from source 'Fallback Regex' to 0.0")
                current_score = 0.0


            # ‚îÄ‚îÄ‚îÄ 4. ‡∏î‡∏∂‡∏á source ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‚îÄ‚îÄ‚îÄ
            source = (
                metadata.get("source_filename") or metadata.get("filename") or
                doc.get("source_filename") or doc.get("filename") or 
                doc.get("source") or doc.get("doc_id") or
                "N/A"
            )

            # ‚îÄ‚îÄ‚îÄ 5. ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î ‚îÄ‚îÄ‚îÄ
            if current_score > max_score_found:
                max_score_found = current_score
                max_score_source = source

        # ‚îÄ‚îÄ‚îÄ 6. Relevant Score Gate + Log ‚îÄ‚îÄ‚îÄ
        
        # NOTE: ‡πÉ‡∏ä‡πâ threshold ‡πÅ‡∏•‡∏∞ cap_value ‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏°‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
        if max_score_found < threshold: 
            max_evi_str_for_prompt = cap_value
            is_capped = True
            self.logger.warning(
                f"üö® Evi Str CAPPED L{level}: "
                f"Rerank {max_score_found:.4f} (‡∏à‡∏≤‡∏Å '{max_score_source}') "
                f"< {threshold} ‚Üí ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏ó‡∏µ‡πà {cap_value}"
            )
        else:
            max_evi_str_for_prompt = 10.0
            is_capped = False
            self.logger.info(
                f"‚úÖ Evi Str FULL L{level}: "
                f"Rerank {max_score_found:.4f} (‡∏à‡∏≤‡∏Å '{max_score_source}') "
                f">= {threshold} ‚Üí ‡∏õ‡∏•‡πà‡∏≠‡∏¢‡πÄ‡∏ï‡πá‡∏° 10.0"
            )

        return {
            "is_capped": is_capped,
            "max_evi_str_for_prompt": max_evi_str_for_prompt,
            "highest_rerank_score": round(float(max_score_found), 4), 
            "max_score_source": max_score_source,
        }    
        

    def run_assessment(
            self,
            target_sub_id: str = "all",
            export: bool = False,
            vectorstore_manager: Optional['VectorStoreManager'] = None,
            sequential: bool = False,
            document_map: Optional[Dict[str, str]] = None, # üü¢ FIX: ‡∏£‡∏±‡∏ö document_map
        ) -> Dict[str, Any]:
        """
        Main runner ‡∏Ç‡∏≠‡∏á Assessment Engine
        ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á Parallel ‡πÅ‡∏•‡∏∞ Sequential 100%
        ‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏ß‡πà‡∏≤ evidence_map ‡∏Ñ‡∏£‡∏ö‡∏ó‡∏∏‡∏Å‡∏Å‡∏£‡∏ì‡∏µ
        """

        start_ts = time.time()
        self.is_sequential = sequential

        # ============================== 1. Filter Rubric ==============================
        if target_sub_id.lower() == "all":
            sub_criteria_list = self._flatten_rubric_to_statements() # üü¢ NOTE: ‡πÉ‡∏ä‡πâ _flatten_rubric_to_statements ‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏•‡πâ‡∏ß
        else:
            # üü¢ NOTE: ‡πÉ‡∏ä‡πâ _flatten_rubric_to_statements ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á List ‡∏Å‡πà‡∏≠‡∏ô
            all_statements = self._flatten_rubric_to_statements()
            sub_criteria_list = [
                s for s in all_statements if s.get('sub_id') == target_sub_id
            ]
            if not sub_criteria_list:
                self.logger.error(f"Sub-Criteria ID '{target_sub_id}' not found in rubric.")
                return {"error": f"Sub-Criteria ID '{target_sub_id}' not found."}

        # Reset states
        self.raw_llm_results = []
        self.final_subcriteria_results = []

        # ‡πÇ‡∏´‡∏•‡∏î evidence map ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß (‡πÑ‡∏°‡πà clear!)
        if os.path.exists(self.evidence_map_path):
            loaded = self._load_evidence_map()
            if loaded:
                self.evidence_map = loaded
                self.logger.info(f"Resumed from existing evidence map: {len(self.evidence_map)} keys")
            else:
                self.evidence_map = {}
        else:
            self.evidence_map = {}

        if not sequential:
            self.logger.info("[PARALLEL MODE] Starting parallel assessment...")

        run_parallel = (target_sub_id.lower() == "all") and not (sequential or export)

        # ============================== 2. Run Assessment ==============================
        if run_parallel:
            # --------------------- PARALLEL MODE ---------------------
            self.logger.info("Starting Parallel Assessment with Multiprocessing...")
            worker_args = [(
                sub_data,
                self.config.enabler,
                self.config.target_level,
                self.config.mock_mode,
                self.evidence_map_path,
                self.config.model_name,
                self.config.temperature
            ) for sub_data in sub_criteria_list]

            try:
                # ‡πÉ‡∏ä‡πâ self.logger ‡πÅ‡∏ó‡∏ô logger (‡∏ñ‡πâ‡∏≤ logger ‡πÄ‡∏õ‡πá‡∏ô global)
                self.logger.info(f"Using {max(1, os.cpu_count() - 1)} processes...")
                pool_ctx = multiprocessing.get_context('spawn')
                with pool_ctx.Pool(processes=max(1, os.cpu_count() - 1)) as pool:
                    # NOTE: _static_worker_process ‡∏à‡∏∞‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ (sub_result, temp_map_from_worker)
                    results_list = pool.map(_static_worker_process, worker_args)
            except Exception as e:
                self.logger.critical(f"Multiprocessing failed: {e}")
                raise

            # ‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å worker
            for result_tuple in results_list:
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å worker ‡∏Å‡πà‡∏≠‡∏ô unpack
                if not isinstance(result_tuple, tuple) or len(result_tuple) != 2:
                    self.logger.error(f"Worker returned invalid result structure: {result_tuple}")
                    continue
                
                sub_result, temp_map_from_worker = result_tuple

                if isinstance(temp_map_from_worker, dict):
                    # ‡∏£‡∏ß‡∏° Evidence Map ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å Worker
                    for level_key, evidence_list in temp_map_from_worker.items():
                        if isinstance(evidence_list, list) and evidence_list:
                            current_list = self.evidence_map.setdefault(level_key, [])
                            current_list.extend(evidence_list)
                            self.logger.info(f"AGGREGATED: +{len(evidence_list)} ‚Üí {level_key} "
                                        f"(total: {len(current_list)})")

                # ‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô
                raw_refs = sub_result.get("raw_results_ref", [])
                self.raw_llm_results.extend(raw_refs if isinstance(raw_refs, list) else [])
                self.final_subcriteria_results.append(sub_result)


        else:
            # --------------------- SEQUENTIAL MODE ---------------------
            mode_desc = target_sub_id if target_sub_id != "all" else "All Sub-Criteria (Sequential)"
            self.logger.info(f"Starting Sequential Assessment: {mode_desc}")

            # üéØ FIX: ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç load_all_vectorstores ‡πÉ‡∏´‡πâ‡∏°‡∏µ tenant ‡πÅ‡∏•‡∏∞ year
            local_vsm = vectorstore_manager or (
                load_all_vectorstores(
                    doc_types=[EVIDENCE_DOC_TYPES], 
                    evidence_enabler=self.config.enabler,
                    tenant=self.config.tenant,  # <--- NEW: Argument ‡∏ó‡∏µ‡πà‡∏´‡∏≤‡∏¢‡πÑ‡∏õ
                    year=self.config.year       # <--- NEW: Argument ‡∏ó‡∏µ‡πà‡∏´‡∏≤‡∏¢‡πÑ‡∏õ
                )
                if self.config.mock_mode == "none" else None
            )
            self.vectorstore_manager = local_vsm

            for sub_criteria in sub_criteria_list:
                sub_result, final_temp_map = self._run_sub_criteria_assessment_worker(sub_criteria)
                self.raw_llm_results.extend(sub_result.get("raw_results_ref", []))
                self.final_subcriteria_results.append(sub_result)

        # ============================== 3. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Evidence Map ==============================
        if self.evidence_map:
            self._save_evidence_map(map_to_save=self.evidence_map)
            total_items = sum(len(v) for v in self.evidence_map.values())
            self.logger.info(f"Persisted final evidence map | Keys: {len(self.evidence_map)} | "
                            f"Items: {total_items} | Size: ~{total_items * 0.35:.1f} KB")

        # ============================== 4. ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏• & Export ==============================
        self._calculate_overall_stats(target_sub_id)

        final_results = {
            "summary": self.total_stats,
            "sub_criteria_results": self.final_subcriteria_results,
            "raw_llm_results": self.raw_llm_results,
            "run_time_seconds": round(time.time() - start_ts, 2),
            "timestamp": datetime.now().isoformat(),
        }

        if export:
            export_path = self._export_results(
                results=final_results,
                enabler=self.config.enabler,
                sub_criteria_id=target_sub_id if target_sub_id != "all" else "ALL",
                target_level=self.config.target_level
            )
            final_results["export_path_used"] = export_path
            final_results["evidence_map"] = deepcopy(self.evidence_map)
            self.logger.info(f"Exported full results ‚Üí {export_path}")

        return final_results

    def _create_error_result(
        self,
        level: int,
        error_message: str,
        start_time: float,
        sub_id: str = "N/A",
        statement_id: str = "N/A",
        statement_text: str = "N/A",
        retrieval_duration: float = 0.0,
        llm_duration: float = 0.0,
    ) -> Dict[str, Any]:
        """Creates a standardized error result dictionary."""
        end_time = time.time()
        
        try:
            pdca_phase = self._get_pdca_phase(level)
        except Exception:
            pdca_phase = "N/A"

        return {
            "sub_criteria_id": sub_id,
            "statement_id": statement_id,
            "level": level,
            "statement": statement_text,
            "pdca_phase": pdca_phase,
            "llm_score": 0,
            "pdca_breakdown": {},
            "is_passed": False,
            "status": "ERROR",
            "score": 0,
            "llm_result_full": {"error": error_message, "duration_s": round(end_time - start_time, 2)},
            "retrieval_duration_s": round(retrieval_duration, 2),
            "llm_duration_s": round(llm_duration, 2),
            "top_evidences_ref": [],
            "temp_map_for_level": [],
            "evidence_strength": 0.0,
            "ai_confidence": "LOW",
            "evidence_count": 0,
            "pdca_coverage": 0.0,
            "direct_evidence_count": 0,
            "rag_query": "N/A",
            "full_context_meta": {"error": error_message},
            "max_relevant_score": 0.0,
            "max_relevant_source": "Error",
            "is_evidence_strength_capped": True,
            "max_evidence_strength_used": 0.0,
        }

    def _run_single_assessment(
        self,
        sub_criteria: Dict[str, Any],
        statement_data: Dict[str, Any],
        vectorstore_manager: Optional['VectorStoreManager'],
        sequential_chunk_uuids: Optional[List[str]] = None,
        attempt: int = 1 # ‡πÄ‡∏û‡∏¥‡πà‡∏° attempt ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö RetryPolicy ‡πÉ‡∏ô L3-L5
    ) -> Dict[str, Any]:
        """
        ‡∏£‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô Level ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (L1-L5) ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå ‡∏û‡∏£‡πâ‡∏≠‡∏° implement Adaptive Retrieval Loop
        
        Args:
            sub_criteria: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏¢‡πà‡∏≠‡∏¢
            statement_data: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Statement ‡∏Ç‡∏≠‡∏á Level ‡∏ô‡∏±‡πâ‡∏ô
            vectorstore_manager: ‡∏ï‡∏±‡∏ß‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Vector Store
            sequential_chunk_uuids: List ‡∏Ç‡∏≠‡∏á chunk IDs ‡∏à‡∏≤‡∏Å Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Sequential Mode)
            attempt: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏° (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö RetryPolicy)
            
        Returns:
            Dict[str, Any]: ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô Level ‡∏ô‡∏±‡πâ‡∏ô
        """
        # üéØ Assumed Constants for Adaptive Loop (‡∏ó‡πà‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô self.config)
        # ‡πÉ‡∏ä‡πâ getattr ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ñ‡πà‡∏≤ Default ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô self.config
        MIN_RETRY_SCORE = getattr(self.config, 'MIN_RETRY_SCORE', 0.65)
        MAX_RETRIEVAL_ATTEMPTS = getattr(self.config, 'MAX_RETRIEVAL_ATTEMPTS', 3)

        start_time = time.time()
        sub_id = sub_criteria['sub_id']
        level = statement_data['level']
        statement_text = statement_data['statement']
        sub_criteria_name = sub_criteria['sub_criteria_name']
        statement_id = statement_data.get('statement_id', sub_id)
        
        retrieval_duration = 0.0 
        llm_duration = 0.0
        rag_query = statement_text # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô

        self.logger.info(f"  > Starting assessment for {sub_id} L{level} (Attempt: {attempt})...")

        # ==================== 1. PDCA & Level Prompt ====================
        pdca_phase = self._get_pdca_phase(level)
        level_constraint = self._get_level_constraint_prompt(level)
        contextual_rules_prompt = self._get_contextual_rules_prompt(sub_id, level)
        full_focus_hint = level_constraint + contextual_rules_prompt

        # ==================== 2. Hybrid Retrieval Setup ====================
        mapped_stable_doc_ids, priority_docs = self._get_mapped_uuids_and_priority_chunks(
            sub_id=sub_id,
            level=level,
            statement_text=statement_text,
            level_constraint=level_constraint,
            vectorstore_manager=vectorstore_manager
        )

        # ==================== 3. Enhance Query ====================
        rag_query_list = enhance_query_for_statement(
            statement_text=statement_text,
            sub_id=sub_id,
            statement_id=statement_id,
            level=level,
            enabler_id=self.config.enabler,
            focus_hint=full_focus_hint,
            llm_executor=self.llm
        )
        rag_query = rag_query_list[0] if rag_query_list else statement_text # Query ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô

        # ==================== 4. LLM Evaluator Setup ====================
        llm_evaluator_to_use = self.llm_evaluator
        if level <= 2:
            llm_evaluator_to_use = evaluate_with_llm_low_level

        # ==================== 5. ADAPTIVE RAG RETRIEVAL LOOP ====================
        
        current_rag_query = rag_query # Query ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ñ‡∏π‡∏Å rephrase ‡πÉ‡∏ô‡∏•‡∏π‡∏õ
        
        highest_rerank_score = 0.0
        final_top_evidences = []
        final_aggregated_context = ""
        used_chunk_uuids = []

        retrieval_start = time.time()
        
        loop_attempt = 1
        while loop_attempt <= MAX_RETRIEVAL_ATTEMPTS:
            self.logger.info(
                f"  > RAG Retrieval {sub_id} L{level} (Attempt: {loop_attempt}/{MAX_RETRIEVAL_ATTEMPTS}). Max Score so far: {highest_rerank_score:.4f}"
            )
            
            # --- ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Query Input (‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô List ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ rag_retriever ‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á List) ---
            query_input = [current_rag_query]
            if loop_attempt == 1 and rag_query_list:
                query_input = rag_query_list # ‡πÉ‡∏ä‡πâ Query List ‡∏ó‡∏µ‡πà Enhance ‡∏°‡∏≤‡πÉ‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å

            try:
                retrieval_result = self.rag_retriever(
                    query=query_input, 
                    doc_type=EVIDENCE_DOC_TYPES,
                    enabler=self.config.enabler,
                    sub_id=sub_id,
                    level=level,
                    vectorstore_manager=vectorstore_manager,
                    mapped_uuids=mapped_stable_doc_ids,
                    priority_docs_input=priority_docs,
                    sequential_chunk_uuids=sequential_chunk_uuids,
                    # ‚ùå [FIXED 1]: ‡∏•‡∏ö top_k ‡∏≠‡∏≠‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ TypeError
                )
            except Exception as e:
                self.logger.error(f"RAG retrieval failed for {sub_id} L{level} at Attempt {loop_attempt}: {e}")
                if loop_attempt == 1:
                    retrieval_duration = time.time() - retrieval_start
                    # ‚ùå [FIXED 2]: ‡∏™‡πà‡∏á Argument ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Error
                    return self._create_error_result(
                        level=level, 
                        error_message=f'RAG Retrieval Error: {str(e)}', 
                        start_time=start_time, 
                        retrieval_duration=retrieval_duration,
                        sub_id=sub_id, 
                        statement_id=statement_id, 
                        statement_text=statement_text
                    )
                break 

            top_evidences_current = retrieval_result.get("top_evidences", [])
            
            # --- Check Max Score in current batch ---
            current_max_score = 0.0
            if top_evidences_current:
                for ev in top_evidences_current:
                    score = ev.get("rerank_score", ev.get("score", 0.0))
                    try:
                        current_max_score = max(current_max_score, float(score))
                    except (ValueError, TypeError):
                        pass

            self.logger.debug(f"  > Attempt {loop_attempt} Max Score: {current_max_score:.4f}")

            # --- Update Best Result ---
            if current_max_score >= highest_rerank_score:
                highest_rerank_score = current_max_score
                final_top_evidences = top_evidences_current 
                final_aggregated_context = retrieval_result.get("aggregated_context", "")
                used_chunk_uuids = retrieval_result.get("used_chunk_uuids", [])
                
                if loop_attempt > 1:
                    self.logger.info(f"  > ‚ú® L{level} Retrieval improved: New max score {highest_rerank_score:.4f} found.")
            
            # --- Check Stop Condition ---
            if highest_rerank_score >= MIN_RETRY_SCORE:
                self.logger.info(f"  > ‚úÖ Adaptive Retrieval L{level}: Max score {highest_rerank_score:.4f} >= {MIN_RETRY_SCORE}. Stopping loop.")
                break
                
            # --- Prepare for next attempt ---
            if loop_attempt < MAX_RETRIEVAL_ATTEMPTS:
                # Rephrase Query (‡πÉ‡∏ä‡πâ Helper ‡∏ó‡∏µ‡πà‡∏ó‡πà‡∏≤‡∏ô‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏ß‡πà‡∏≤‡∏Ñ‡∏á‡πÑ‡∏ß‡πâ)
                current_rag_query = self.rephrase_query_for_retry(current_rag_query, level, sub_id) 
                # ‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏¥‡πà‡∏° Top-K ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ rag_retriever ‡πÑ‡∏°‡πà‡∏£‡∏±‡∏ö parameter ‡∏ï‡∏£‡∏á‡πÜ
                
            loop_attempt += 1

        # ‡πÉ‡∏ä‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏•‡∏π‡∏õ
        top_evidences = final_top_evidences
        retrieval_duration = time.time() - retrieval_start
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if not top_evidences:
             self.logger.warning(f"  > Retrieval loop finished but NO evidences found for {sub_id} L{level}. Continuing with empty context.")
        
        # =====================================================================================
        
        # ==================== 6. ‡∏î‡∏∂‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏à‡∏≤‡∏Å Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ ====================
        try:
            previous_levels_raw = self._collect_previous_level_evidences(sub_id, current_level=level)
        except Exception as e:
            self.logger.error(f"Failed to collect previous evidences: {e}")
            previous_levels_raw = {}

        previous_levels_evidence_full = []
        for ev_list in previous_levels_raw.values():
            for ev in ev_list:
                doc_id = ev.get("doc_id") or ev.get("chunk_uuid")
                if not doc_id or str(doc_id).startswith("HASH-"):
                    continue
                previous_levels_evidence_full.append(ev)

        # ==================== 6a. Sequential fallback ====================
        if level > 1 and self.is_sequential:
            current_ids = {d.get("doc_id") or d.get("chunk_uuid") for d in top_evidences}
            for ev in previous_levels_evidence_full:
                ev_id = ev.get("doc_id") or ev.get("chunk_uuid")
                if ev_id not in current_ids:
                    fallback_ev = ev.copy()
                    fallback_ev["pdca_tag"] = "Baseline"
                    top_evidences.append(fallback_ev)

        # ==================== 7. ‡∏™‡∏£‡πâ‡∏≤‡∏á Multi-Channel Context ====================
        channels = build_multichannel_context_for_level(
            level=level,
            top_evidences=top_evidences,
            previous_levels_evidence=previous_levels_evidence_full,
            max_main_context_tokens=3000,
            max_summary_sentences=4
        )

        debug = channels.get("debug_meta", {})
        self.logger.info(
            f"  > Context built ‚Üí Direct: {debug.get('direct_count',0)}, "
            f"Aux: {debug.get('aux_count',0)}, "
            f"Baseline: {len(previous_levels_evidence_full)} files "
            f"from {len(previous_levels_raw)} previous levels"
        )

        # ==================== 8. LLM Evaluation ====================
        
        # üü¢ NEW: 8.1. Relevant Score Gate - Calculate Max Evidence Strength
        try:
            if top_evidences and isinstance(top_evidences[0], dict):
                raw_doc_data = top_evidences[0]
            else:
                raw_doc_data = {'page_content': 'N/A', 'metadata': {}, 'score': 'N/A'}
        except Exception as e:
             self.logger.critical(f"FINAL DEBUG L{level}: FAILED TO DUMP RAW DOC: {e}")

        max_score_to_cap = highest_rerank_score
        
        evi_cap_data = self._calculate_evidence_strength_cap(top_evidences, level, 
                                                               highest_rerank_score=max_score_to_cap)
        max_evi_str_for_prompt = evi_cap_data['max_evi_str_for_prompt']
        
        context_parts = [
            f"--- DIRECT EVIDENCE (L{level})---\n{channels.get('direct_context','')}",
            f"--- AUXILIARY EVIDENCE ---\n{channels.get('aux_summary','')}",
            f"--- BASELINE FROM PREVIOUS LEVELS ---\n{channels.get('baseline_summary','‡πÑ‡∏°‡πà‡∏°‡∏µ')}"
        ]
        final_llm_context = "\n\n".join([p for p in context_parts if p.strip()])

        llm_start = time.time()
        try:
            llm_result = llm_evaluator_to_use(
                context=final_llm_context,
                sub_criteria_name=sub_criteria_name,
                level=level,
                statement_text=statement_text,
                sub_id=sub_id,
                pdca_phase=pdca_phase,
                level_constraint=level_constraint,
                contextual_rules=contextual_rules_prompt,
                llm_executor=self.llm,
                max_evidence_strength=max_evi_str_for_prompt
            )
        except Exception as e:
            self.logger.error(f"LLM Call failed for {sub_id} L{level}: {e}")
            llm_duration = time.time() - llm_start
            # ‚ùå [FIXED 2]: ‡∏™‡πà‡∏á Argument ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Error
            return self._create_error_result(
                level=level, 
                error_message=f'LLM Call Error: {str(e)}', 
                start_time=start_time, 
                retrieval_duration=retrieval_duration,
                llm_duration=llm_duration,
                sub_id=sub_id, 
                statement_id=statement_id, 
                statement_text=statement_text
            )
        
        llm_duration = time.time() - llm_start

        # =====================================================================================
        # üéØ FINAL FIX: ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏±‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç (int) ‡∏Å‡πà‡∏≠‡∏ô RETURN 
        # =====================================================================================
        
        is_numeric_result = isinstance(llm_result, (int, float)) or \
                            (isinstance(llm_result, str) and str(llm_result).strip().isdigit())
                            
        if is_numeric_result:
            level_num = int(llm_result)
            self.logger.warning(
                f"üö® L{level} LLM returned ONLY number {level_num} (Type: {type(llm_result).__name__}). "
                f"Converting to standardized dict format to prevent RetryPolicy crash."
            )
            
            try:
                pdca_breakdown_data, is_passed_num, _ = calculate_pdca_breakdown_and_pass_status(level_num, level) 
            except NameError:
                # ‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤ calculate_pdca_breakdown_and_pass_status ‡∏ñ‡∏π‡∏Å import
                self.logger.error("Required helper function is missing from scope.")
                is_passed_num = level_num >= level
                pdca_breakdown_data = {}

            status_num = "PASS" if is_passed_num else "FAIL"

            return {
                "sub_criteria_id": sub_id,
                "statement_id": statement_id,
                "level": level,
                "statement": statement_text,
                "pdca_phase": pdca_phase,
                "llm_score": level_num,
                "pdca_breakdown": pdca_breakdown_data, 
                "is_passed": is_passed_num,             
                "status": status_num,                   
                "score": level_num,
                "llm_result_full": {"raw_number": level_num, "raw_type": type(llm_result).__name__},
                "retrieval_duration_s": round(retrieval_duration, 2),
                "llm_duration_s": round(llm_duration, 2),
                "top_evidences_ref": [],
                "temp_map_for_level": [],
                "evidence_strength": self.config.MAX_EVI_STR_CAP if is_passed_num else 0.0,
                "ai_confidence": "HIGH" if is_passed_num else "LOW",
                "evidence_count": 0,
                "pdca_coverage": 0.0,
                "direct_evidence_count": 0,
                "rag_query": rag_query,
                "full_context_meta": debug,
                # üü¢ NEW: Relevant Score Gate Metadata
                "max_relevant_score": evi_cap_data['highest_rerank_score'],
                "max_relevant_source": evi_cap_data['max_score_source'],
                "is_evidence_strength_capped": evi_cap_data['is_capped'],
                "max_evidence_strength_used": max_evi_str_for_prompt,
            }
        # =====================================================================================

        # ==================== 9-10. Scoring & Pass/Fail ====================
        
        if not isinstance(llm_result, dict):
            self.logger.error(
                f"üö® LLM parsing failed for {statement_id} L{level}. Received unexpected type: {type(llm_result).__name__}. Setting FAIL defaults."
            )
            llm_result = {}
        
        llm_score = llm_result.get('score', 0)
        # ‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤ calculate_pdca_breakdown_and_pass_status ‡∏ñ‡∏π‡∏Å import
        pdca_breakdown, is_passed, _ = calculate_pdca_breakdown_and_pass_status(llm_score, level)
        status = "PASS" if is_passed else "FAIL"

        # -------------------- 11. SAVE EVIDENCE MAP (PASS ONLY) --------------------
        temp_map_for_level = None
        evidence_entries = []
        
        if is_passed and top_evidences:
            seen = set()
            
            def safe_float(val, default=0.0):
                """Convert val to float safely, fallback to default if fails"""
                try:
                    return float(val)
                except (TypeError, ValueError):
                    return default

            for ev in top_evidences:
                doc_id = ev.get("doc_id") or ev.get("chunk_uuid")
                
                if not doc_id or str(doc_id).startswith(("TEMP-", "HASH-")) or doc_id in seen:
                    continue

                # --- START: SCORE EXTRACTION REVISED (Unchanged Logic) ---
                score = 0.0
                metadata = ev.get("metadata", {}) or {}
                filename_to_use = ev.get("source_filename") or metadata.get("source_filename") or ""

                score_sources = [
                    ev.get("relevance_score"), ev.get("score"),
                    metadata.get("relevance_score"), metadata.get("score"),
                    ev.get("rerank_score"), metadata.get("rerank_score"),
                    metadata.get("_rerank_score_force")
                ]
                
                for s in score_sources:
                    score = max(score, safe_float(s))
                    
                distance = metadata.get("distance") or ev.get("distance")
                if distance is not None:
                    try:
                        distance_val = safe_float(distance)
                        similarity = round(1.0 - distance_val, 4)
                        score = max(score, similarity)
                    except (TypeError, ValueError):
                        pass

                if "|SCORE:" in filename_to_use:
                    try:
                        score_str = filename_to_use.split("|SCORE:")[1].split("|")[0]
                        filename_score = safe_float(score_str)
                        score = max(score, filename_score)
                        filename_to_use = filename_to_use.split("|SCORE:")[0] 
                    except Exception:
                        pass
                
                if score == 0.0 and (ev.get("pdca_tag") != "Baseline"):
                    score = 0.5 
                    
                score = round(score, 4)
                # --- END: SCORE EXTRACTION REVISED ---
                
                if not filename_to_use:
                    filename_to_use = ev.get("source_filename") or ev.get("source") or ev.get("filename") or metadata.get("source_filename") or metadata.get("filename") or "UNKNOWN_FILE"

                # üü¢ FIX: ‡∏î‡∏∂‡∏á‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö Page Number ‡πÉ‡∏´‡πâ‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö Log Output
                page_number = str(metadata.get('page') or metadata.get('page_number') or metadata.get('Page') or '')
                if page_number and page_number.isdigit():
                    page_number = f"P{page_number}" 
                    
                # üü¢ NEW: ‡∏Å‡∏≥‡∏´‡∏ô‡∏î File Reference ID
                file_ref_id = doc_id 

                evidence_entries.append({
                    "doc_id": doc_id,
                    "filename": filename_to_use,
                    "mapper_type": "AI_GENERATED", 
                    "timestamp": datetime.now().isoformat(), 
                    "relevance_score": score, 
                    "chunk_uuid": doc_id,
                    "page": page_number, # ‚¨ÖÔ∏è [FIXED]: ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ page_number ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å metadata key ‡∏ï‡πà‡∏≤‡∏á‡πÜ
                    "file_reference_id": file_ref_id, # ‚¨ÖÔ∏è [NEW]: ‡πÉ‡∏ä‡πâ doc_id ‡πÄ‡∏õ‡πá‡∏ô reference
                })
                seen.add(doc_id)
            
            # -------------------- 12. Calculate PDCA Coverage & Strength --------------------
            direct_count = channels.get("debug_meta", {}).get("direct_count", 0)
            
            avg_score = sum(entry.get("relevance_score", 0.0) for entry in evidence_entries) / len(evidence_entries) if evidence_entries else 0.0
            
            pdca_coverage = sum(1 for score in pdca_breakdown.values() if score > 0) / 4.0

            evidence_strength_raw = (avg_score * 10.0) * (pdca_coverage * 2.0)
            
            evidence_strength = min(
                max_evi_str_for_prompt, 
                evidence_strength_raw
            )

            ai_confidence = "HIGH" if evidence_strength >= 8.0 and is_passed else \
                            "MEDIUM" if evidence_strength >= 5.5 else "LOW"

            evidence_count_for_level = len(evidence_entries)
            
            # -------------------- 13. Prepare temp_map_for_level and Finalize Evidence --------------------
            evidence_entries.sort(key=lambda x: x.get("relevance_score", 0.0), reverse=True)

            final_k_reranked = self.config.final_k_reranked if hasattr(self.config, 'final_k_reranked') else 5
            evidence_entries = evidence_entries[:final_k_reranked]

            temp_map_for_level = [
                {
                    "doc_id": entry["doc_id"],
                    "filename": entry["filename"],
                    "mapper_type": entry["mapper_type"],
                    "timestamp": entry["timestamp"],
                    "relevance_score": entry["relevance_score"],
                    "chunk_uuid": entry["chunk_uuid"],
                    "page": entry.get("page", ""), # ‚¨ÖÔ∏è [FIXED]: ‡πÄ‡∏û‡∏¥‡πà‡∏° key 'page'
                    "file_reference_id": entry.get("file_reference_id", ""), # ‚¨ÖÔ∏è [NEW]: ‡πÄ‡∏û‡∏¥‡πà‡∏° key 'file_reference_id'
                }
                for entry in evidence_entries
            ]
        
        else:
            evidence_entries = []
            temp_map_for_level = []
            evidence_strength = 0.0
            ai_confidence = "LOW"
            pdca_coverage = 0.0
            direct_count = 0
            evidence_count_for_level = 0
        
        # ==================== 14. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ ====================
        final_result = {
            "sub_criteria_id": sub_id,
            "statement_id": statement_id,
            "level": level,
            "statement": statement_text,
            "pdca_phase": pdca_phase,
            "llm_score": llm_score,
            "pdca_breakdown": pdca_breakdown,
            "is_passed": is_passed,
            "status": status,
            "score": llm_score,
            "llm_result_full": llm_result,
            "retrieval_duration_s": round(retrieval_duration, 2),
            "llm_duration_s": round(llm_duration, 2),
            "top_evidences_ref": self._resolve_evidence_filenames(evidence_entries), 
            "temp_map_for_level": temp_map_for_level,
            "evidence_strength": round(evidence_strength, 1),
            "ai_confidence": ai_confidence,
            "evidence_count": evidence_count_for_level,
            "pdca_coverage": round(pdca_coverage, 4), 
            "direct_evidence_count": direct_count,
            "rag_query": rag_query,
            "full_context_meta": debug,
            
            # üü¢ NEW: Relevant Score Gate Metadata
            "max_relevant_score": evi_cap_data['highest_rerank_score'],
            "max_relevant_source": evi_cap_data['max_score_source'],
            "is_evidence_strength_capped": evi_cap_data['is_capped'],
            "max_evidence_strength_used": max_evi_str_for_prompt,
        }

        icon_status = "‚úÖ" if status == "PASS" else "‚ùå"

        self.logger.info(f"  > Assessment {sub_id} L{level} completed ‚Üí {icon_status} {status} (Score: {llm_score:.1f} | Evi Str: {final_result['evidence_strength']:.1f} | Conf: {ai_confidence})")

        return final_result