# core/seam_assessment.py

import sys
import json
import logging
import time
import os
from typing import List, Dict, Any, Optional, Union, Tuple, Set, Final, Literal
from collections import defaultdict
from datetime import datetime
from dataclasses import dataclass, field
import multiprocessing # NEW: Import for parallel execution
from functools import partial
import pathlib, uuid
from langchain_core.documents import Document as LcDocument
from core.retry_policy import RetryPolicy, RetryResult
from copy import deepcopy
import tempfile
import shutil
# from json_extractor import _robust_extract_json
from .json_extractor import _robust_extract_json
from filelock import FileLock  # ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á: pip install filelock
import re
import hashlib
import copy


# -------------------- PATH SETUP & IMPORTS --------------------
try:
    PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
    if PROJECT_ROOT not in sys.path:
        sys.path.insert(0, PROJECT_ROOT)

    # 1. Import Constants ‡∏à‡∏≤‡∏Å global_vars
    from config.global_vars import (
        EXPORTS_DIR, MAX_LEVEL, INITIAL_LEVEL, QA_FINAL_K,
        RUBRIC_FILENAME_PATTERN, DEFAULT_ENABLER,
        EVIDENCE_DOC_TYPES, INITIAL_TOP_K,
        EVIDENCE_MAPPING_FILENAME_SUFFIX,
        LIMIT_CHUNKS_PER_PRIORITY_DOC,
        IS_LOG_L3_CONTEXT,
        PRIORITY_CHUNK_LIMIT,
        DEFAULT_TENANT,
        DEFAULT_YEAR,
        RERANK_THRESHOLD,
        MAX_EVI_STR_CAP,
        DEFAULT_LLM_MODEL_NAME,
        LLM_TEMPERATURE,
        MAX_PARALLEL_WORKERS,
        MIN_RERANK_SCORE_TO_KEEP,
        MIN_RETRY_SCORE,
        MIN_RELEVANCE_THRESHOLD,
        OLLAMA_MAX_RETRIES,
        CONTEXT_CAP_L3_PLUS,
        CRITICAL_CA_THRESHOLD,
        MAX_RETRIEVAL_ATTEMPTS,
        HYBRID_VECTOR_WEIGHT,
        HYBRID_BM25_WEIGHT,
        CHUNK_SIZE,
        CHUNK_OVERLAP,
        REQUIRED_PDCA,
        CORRECT_PDCA_SCORES_MAP,
        PDCA_PHASE_MAP,        # ‚úÖ ‡∏¢‡πâ‡∏≤‡∏¢‡∏°‡∏≤‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÉ‡∏ô global_vars.py
        PDCA_PRIORITY_ORDER,
        BASE_PDCA_KEYWORDS,
        PDCA_LEVEL_SYNONYMS,
        ENABLE_HARD_FAIL_LOGIC,
        ENABLE_CONTEXTUAL_RULE_OVERRIDE,
        TARGET_SCORE_THRESHOLD_MAP
    )
    
    # 2. Import Logic Functions
    from core.llm_data_utils import ( 
        create_structured_action_plan, evaluate_with_llm,
        retrieve_context_with_filter, retrieve_context_for_low_levels,
        evaluate_with_llm_low_level, LOW_LEVEL_K, 
        set_mock_control_mode as set_llm_data_mock_mode,
        create_context_summary_llm,
        retrieve_context_by_doc_ids,
        _fetch_llm_response,
        build_multichannel_context_for_level
    )
    from core.vectorstore import VectorStoreManager, load_all_vectorstores, get_global_reranker 
    
    # ‚ùå ‡∏•‡∏ö‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏‡∏Ç‡∏≠‡∏á ImportError ‡∏≠‡∏≠‡∏Å:
    # from core.seam_prompts import PDCA_PHASE_MAP 
    
    from core.action_plan_schema import ActionPlanActions

    # 3. üéØ Import Path Utilities
    from utils.path_utils import (
        get_mapping_file_path, 
        get_evidence_mapping_file_path, 
        get_contextual_rules_file_path,
        get_doc_type_collection_key,
        get_assessment_export_file_path,
        get_export_dir,
        get_rubric_file_path,
        load_evidence_mapping,
        _n
    )

    import assessments.seam_mocking as seam_mocking 
    
except ImportError as e:
    # -------------------- Modernized Fallback Code --------------------
    print(f"‚ö†Ô∏è WARNING: Import failed, using dynamic fallback for Mac. Error: {e}", file=sys.stderr)
    
    # Fallback Constants
    EXPORTS_DIR = "exports"
    MAX_LEVEL = 5
    INITIAL_LEVEL = 1
    QA_FINAL_K = 3
    RUBRIC_FILENAME_PATTERN = "{tenant}_{enabler}_rubric.json"
    DEFAULT_ENABLER = "KM"
    EVIDENCE_DOC_TYPES = "evidence"
    INITIAL_TOP_K = 10

    # üìå Placeholder functions for path_utils (‡∏ä‡∏µ‡πâ‡πÄ‡∏Ç‡πâ‡∏≤ data_store ‡∏ï‡∏£‡∏á‡πÜ)
    def _n(s): return str(s).lower().strip()

    def get_mapping_file_path(doc_type, tenant, year=None, enabler=None):
        t = _n(tenant)
        if _n(doc_type) == "evidence":
            return f"data_store/{t}/mapping/{year}/{t}_{year}_{_n(enabler)}_doc_id_mapping.json"
        return f"data_store/{t}/mapping/{t}_{_n(doc_type)}_doc_id_mapping.json"

    def get_evidence_mapping_file_path(tenant, year, enabler):
        t = _n(tenant)
        return f"data_store/{t}/mapping/{year}/{t}_{year}_{_n(enabler)}_evidence_mapping.json"

    def get_contextual_rules_file_path(tenant, enabler):
        t = _n(tenant)
        return f"data_store/{t}/config/{t}_{_n(enabler)}_contextual_rules.json"

    def get_rubric_file_path(tenant, enabler):
        t = _n(tenant)
        return f"data_store/{t}/config/{t}_{_n(enabler)}_rubric.json"

    def load_evidence_mapping(tenant="pea", year=2568, enabler="KM"):
        path = get_evidence_mapping_file_path(tenant, year, enabler)
        if os.path.exists(path):
            try:
                with open(path, "r", encoding="utf-8") as f: return json.load(f)
            except: return {}
        return {}

    # Mock Logic Functions
    def create_structured_action_plan(*args, **kwargs): return []
    def evaluate_with_llm(*args, **kwargs): return {"score": 0, "reason": "Import Error Fallback", "is_passed": False}
    def retrieve_context_with_filter(*args, **kwargs): return {"top_evidences": [], "aggregated_context": ""}
    def retrieve_context_for_low_levels(*args, **kwargs): return {"top_evidences": [], "aggregated_context": ""}
    def evaluate_with_llm_low_level(*args, **kwargs): return {"score": 0, "is_passed": False}
    def set_llm_data_mock_mode(mode): pass
    def build_multichannel_context_for_level(*args, **kwargs): return ""
    
    class VectorStoreManager: pass
    def load_all_vectorstores(*args, **kwargs): return None
    
    PDCA_PHASE_MAP = {1: "Plan", 2: "Do", 3: "Check", 4: "Act", 5: "Sustainability"}

    class seam_mocking:
        @staticmethod
        def set_mock_control_mode(mode): pass

    if "FATAL ERROR" in str(e):
        pass 
# ----------------------------------------------------------------------

logger = logging.getLogger(__name__)
if not logger.handlers:
    logging.basicConfig(level=logging.DEBUG, format="%(asctime)s - %(levelname)s - %(message)s")

def classify_by_keyword(
    text: str, 
    sub_id: str = None, 
    level: int = None,
    contextual_rules_map: dict = None
) -> str:
    """
    Heuristic PDCA Classification v18 (Supports Array/List from JSON)
    --------------------------------------------------
    - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Keywords ‡∏ó‡∏±‡πâ‡∏á‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö String ‡πÅ‡∏•‡∏∞ List
    - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç Error: 'list' object has no attribute 'split'
    - ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á Defaults ‡πÉ‡∏´‡πâ‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Ç‡∏∂‡πâ‡∏ô
    """
    if not text or not contextual_rules_map:
        return 'Other'

    text_lower = text.lower()

    def keyword_match(text_to_search: str, keywords_input) -> bool:
        """
        ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏ä‡πá‡∏Ñ‡∏Å‡∏≤‡∏£ Match ‡∏Ñ‡∏≥ ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á String ‡πÅ‡∏•‡∏∞ List
        """
        # ‡πÅ‡∏õ‡∏•‡∏á input ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô list ‡πÄ‡∏™‡∏°‡∏≠
        keywords_list = []
        if isinstance(keywords_input, list):
            keywords_list = keywords_input
        elif isinstance(keywords_input, str):
            keywords_list = [k.strip() for k in keywords_input.split(",") if k.strip()]
        
        for kw in keywords_list:
            kw_clean = str(kw).strip().lower()
            if not kw_clean:
                continue
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            is_thai = any("\u0e00" <= c <= "\u0e7f" for c in kw_clean)
            
            if is_thai:
                pattern = re.escape(kw_clean)
            else:
                pattern = r'\b{}\b'.format(re.escape(kw_clean))
                
            if re.search(pattern, text_to_search, re.IGNORECASE):
                return True
        return False

    def check_level_keywords(l_rules: dict) -> str:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏´‡∏≤ P, D, C, A ‡∏à‡∏≤‡∏Å‡∏ä‡∏∏‡∏î‡∏Å‡∏é ‡πÇ‡∏î‡∏¢‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á List ‡πÅ‡∏•‡∏∞ String"""
        mapping = {
            "plan_keywords": "P",
            "do_keywords": "D",
            "check_keywords": "C",
            "act_keywords": "A"
        }
        for json_key, tag in mapping.items():
            kw_data = l_rules.get(json_key)
            if kw_data:
                # üéØ ‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: ‡πÉ‡∏ä‡πâ keyword_match ‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö List ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£ split ‡πÄ‡∏≠‡∏á
                if keyword_match(text_lower, kw_data):
                    return tag
        return None

    # --- Step 1: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ï‡∏≤‡∏° Level ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô ---
    if sub_id and level:
        rules = contextual_rules_map.get(sub_id, {})
        current_level_rules = rules.get(f"L{level}", {})
        if isinstance(current_level_rules, dict):
            tag = check_level_keywords(current_level_rules)
            if tag:
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö
                must_include = rules.get("must_include_keywords", [])
                avoid_kw = rules.get("avoid_keywords", [])
                if must_include and not keyword_match(text_lower, must_include):
                    return 'Other'
                if avoid_kw and keyword_match(text_lower, avoid_kw):
                    return 'Other'
                return tag

    # --- Step 2: ‡∏ß‡∏ô‡∏´‡∏≤‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å Level ‡πÉ‡∏ô Sub-ID ---
    if sub_id:
        rules = contextual_rules_map.get(sub_id, {})
        for l_key, l_rules in rules.items():
            if l_key.startswith("L") and isinstance(l_rules, dict):
                tag = check_level_keywords(l_rules)
                if tag:
                    must_include = rules.get("must_include_keywords", [])
                    avoid_kw = rules.get("avoid_keywords", [])
                    if must_include and not keyword_match(text_lower, must_include):
                        continue
                    if avoid_kw and keyword_match(text_lower, avoid_kw):
                        continue
                    return tag

    # --- Step 3: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô (_enabler_defaults) ---
    defaults = contextual_rules_map.get("_enabler_defaults", {})
    mapping_defaults = {
        "plan_keywords": "P", # ‡πÅ‡∏Å‡πâ‡∏à‡∏≤‡∏Å plann_keywords
        "do_keywords": "D", 
        "check_keywords": "C", 
        "act_keywords": "A"
    }
    for json_key, tag in mapping_defaults.items():
        kw_data = defaults.get(json_key)
        if kw_data:
            if keyword_match(text_lower, kw_data):
                return tag

    # --- Step 4: Fallback ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ ---
    try:
        from config.global_vars import PDCA_PRIORITY_ORDER, BASE_PDCA_KEYWORDS
        tag_map = {"Plan": "P", "Do": "D", "Check": "C", "Act": "A"}
        for full_tag in PDCA_PRIORITY_ORDER: 
            patterns = BASE_PDCA_KEYWORDS.get(full_tag, [])
            if patterns and keyword_match(text_lower, patterns):
                return tag_map.get(full_tag, 'Other')
    except Exception as e:
        if 'logger' in globals():
            logger.error(f"Error in classify_by_keyword fallback: {e}")

    return 'Other'


def get_actual_score(ev: dict) -> float:
    """
    Unified score resolver (ENGINE SOURCE OF TRUTH)
    Priority:
    1) relevance_score
    2) rerank_score
    3) score
    (fallback to metadata)
    """
    if not ev:
        return 0.0

    score = ev.get("relevance_score") or ev.get("rerank_score") or ev.get("score")
    if score is not None:
        return float(score)

    meta = ev.get("metadata", {}) or {}
    return float(
        meta.get("relevance_score")
        or meta.get("rerank_score")
        or meta.get("score")
        or 0.0
    )

def get_correct_pdca_required_score(level: int) -> int:
    """‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡πÑ‡∏î‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡∏ú‡πà‡∏≤‡∏ô Level ‡∏ô‡∏±‡πâ‡∏ô ‡πÜ"""
    if level == 1:
        return 1
    elif level == 2:
        return 2
    # ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: L3, L4, L5 ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏° PDCA ‡∏Ñ‡∏£‡∏ö‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÉ‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå 
    # (P, D, C, A = 2 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ï‡πà‡∏≠‡πÅ‡∏Å‡∏ô)
    elif level == 3: # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ P, D, C ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 1.0/2.0 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏° 4 (‡∏ñ‡πâ‡∏≤ L3 ‡πÄ‡∏ô‡πâ‡∏ô C ‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏î‡πâ P, D, C)
        return 4
    elif level == 4: # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ P, D, C, A ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏° 6 (‡∏ñ‡πâ‡∏≤ L4 ‡πÄ‡∏ô‡πâ‡∏ô A ‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏î‡πâ P, D, C, A)
        return 6
    elif level == 5: # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏ï‡πá‡∏°‡∏ó‡∏∏‡∏Å‡πÅ‡∏Å‡∏ô ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏° 8
        return 8
    return 8


# üìå ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç Type Hint ‡πÅ‡∏•‡∏∞ Arguments ‡∏Ç‡∏≠‡∏á Tuple ‡πÉ‡∏´‡πâ‡∏£‡∏ß‡∏° config parameter ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (10 elements)
def _static_worker_process(worker_input_tuple: Tuple[
    Dict[str, Any], str, int, str, str, str, float, float, int, Optional[Dict[str, str]]
]) -> Dict[str, Any]:
    """
    Static worker function for multiprocessing pool. 
    It reconstructs SeamAssessment in the new process and executes the assessment 
    for a single sub-criteria.
    
    Args:
        worker_input_tuple: (sub_criteria_data, enabler: str, target_level: int, mock_mode: str, 
                             evidence_map_path: str, model_name: str, temperature: float, 
                             min_retry_score: float, max_retrieval_attempts: int,
                             document_map: Optional[Dict[str, str]]) 

    Returns:
        Dict[str, Any]: Final result of the sub-criteria assessment.
    """
    
    # üü¢ NEW FIX: PATH SETUP ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Worker Process
    # ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ path ‡∏ã‡πâ‡∏≥‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤ worker process ‡πÄ‡∏´‡πá‡∏ô package ‡∏´‡∏•‡∏±‡∏Å
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
    if project_root not in sys.path:
        sys.path.append(project_root)
        
    worker_logger = logging.getLogger(__name__)

    try:
        # üü¢ FIX: Unpack ‡∏Ñ‡πà‡∏≤ Primitives ‡∏ó‡∏±‡πâ‡∏á 10 ‡∏ï‡∏±‡∏ß
        (
            sub_criteria_data, 
            enabler, 
            target_level, 
            mock_mode, 
            evidence_map_path, 
            model_name, 
            temperature,
            min_retry_score,            # ‚¨ÖÔ∏è NEW CONFIG (8th element)
            max_retrieval_attempts,     # ‚¨ÖÔ∏è NEW CONFIG (9th element)
            document_map,                # (10th element)
            action_plan_model
        ) = worker_input_tuple
    except ValueError as e:
        # ‡πÉ‡∏ä‡πâ len(worker_input_tuple) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£ Debug ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô
        worker_logger.critical(f"Worker input tuple unpack failed (expected 10 elements, got {len(worker_input_tuple)}): {e}")
        return {"error": f"Invalid worker input: {e}"}
        
    # 1. Reconstruct Config 
    try:
        # üü¢ FIX: ‡∏™‡∏£‡πâ‡∏≤‡∏á AssessmentConfig ‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏ô Worker Process ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏™‡πà‡∏Ñ‡πà‡∏≤ config ‡πÉ‡∏´‡∏°‡πà
        # (Tenant/Year ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Default ‡∏à‡∏≤‡∏Å AssessmentConfig)
        worker_config = AssessmentConfig(
            enabler=enabler,
            target_level=target_level,
            mock_mode=mock_mode,
            model_name=model_name, 
            temperature=temperature,
            min_retry_score=min_retry_score,            # ‚¨ÖÔ∏è Pass new config
            max_retrieval_attempts=max_retrieval_attempts # ‚¨ÖÔ∏è Pass new config
        )
    except Exception as e:
        worker_logger.critical(f"Failed to reconstruct AssessmentConfig in worker: {e}")
        return {
            "sub_criteria_id": sub_criteria_data.get('sub_id', 'UNKNOWN'),
            "error": f"Config reconstruction failed: {e}"
        }

    # 2. Re-instantiate SeamAssessment 
    try:
        # üü¢ FIX (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç): ‡∏™‡πà‡∏á document_map ‡πÅ‡∏•‡∏∞ worker_config ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô SEAMPDCAEngine
        # SEAMPDCAEngine ‡∏à‡∏∞‡πÉ‡∏ä‡πâ worker_config ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ min_retry_score ‡πÅ‡∏•‡∏∞ max_retrieval_attempts
        worker_instance = SEAMPDCAEngine(
            config=worker_config, 
            evidence_map_path=evidence_map_path, 
            llm_instance=None,              # LLM ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å Initialized ‡πÉ‡∏ô Engine ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ
            vectorstore_manager=None,       # VSM ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å Initialized ‡πÉ‡∏ô Engine ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ
            # doc_type ‡∏ï‡πâ‡∏≠‡∏á‡∏ñ‡∏π‡∏Å set ‡πÉ‡∏ô SEAMPDCAEngine constructor (‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ Default)
            logger_instance=worker_logger,
            document_map=document_map, # ‚¨ÖÔ∏è ‡∏™‡πà‡∏á document_map ‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏á Unpack ‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤
            ActionPlanActions=action_plan_model
        )
    except Exception as e:
        worker_logger.critical(f"FATAL: SEAMPDCAEngine instantiation failed in worker: {e}")
        return {
            "sub_criteria_id": sub_criteria_data.get('sub_id', 'UNKNOWN'),
            "error": f"Engine initialization failed: {e}"
        }
    
    # 3. Execute the worker logic
    return worker_instance._run_sub_criteria_assessment_worker(sub_criteria_data)

def merge_evidence_mappings(results_list: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
    """
    ‡∏£‡∏ß‡∏° evidence_mapping dictionaries ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å Worker ‡∏ó‡∏∏‡∏Å‡∏ï‡∏±‡∏ß 
    """
    merged_mapping = defaultdict(list)
    for result in results_list:
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å Worker ‡∏°‡∏µ Key 'evidence_mapping' ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if 'evidence_mapping' in result and isinstance(result['evidence_mapping'], dict):
            # ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏ú‡πà‡∏≤‡∏ô Key/Value ‡∏Ç‡∏≠‡∏á Worker ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡∏±‡∏ß
            for level_key, evidence_list in result['evidence_mapping'].items():
                # ‡πÉ‡∏ä‡πâ .extend() ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ú‡∏ô‡∏ß‡∏Å‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
                if isinstance(evidence_list, list):
                    merged_mapping[level_key].extend(evidence_list)
    
    # ‡πÅ‡∏õ‡∏•‡∏á defaultdict ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô dict ‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤
    return dict(merged_mapping)

def get_pdca_keywords_str(phase: str) -> str:
    """
    ‡∏î‡∏∂‡∏á Keywords ‡∏à‡∏≤‡∏Å Global Vars ‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î Regex 
    ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ LLM ‡∏ô‡∏≥‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Extraction
    """
    # ‡∏î‡∏∂‡∏á list ‡∏ï‡∏≤‡∏° phase (Plan, Do, Check, Act)
    raw_keywords = BASE_PDCA_KEYWORDS.get(phase, [])
    
    # ‡∏•‡πâ‡∏≤‡∏á‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏Ç‡∏≠‡∏á Regex ‡∏≠‡∏≠‡∏Å (‡πÄ‡∏ä‡πà‡∏ô r"", \, ^, $)
    clean_keywords = []
    for kw in raw_keywords:
        # ‡∏•‡∏ö escape characters ‡πÅ‡∏•‡∏∞‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå regex ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        k = re.sub(r'[\\^$r"\']', '', kw)
        if k not in clean_keywords:
            clean_keywords.append(k)
            
    # ‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô‡πÄ‡∏õ‡πá‡∏ô string ‡∏Ç‡∏±‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏à‡∏∏‡∏•‡∏†‡∏≤‡∏Ñ (‡πÄ‡∏≠‡∏≤‡πÅ‡∏Ñ‡πà 10 ‡∏Ñ‡∏≥‡πÅ‡∏£‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î Token)
    return ", ".join(clean_keywords[:10])


def post_process_llm_result(llm_output: Dict[str, Any], level: int) -> Dict[str, Any]:
    """
    POST-PROCESSOR v21.9.3 ‚Äî REGEX FALLBACK & PDCA SAFEGUARD
    """

    extraction_map = {"Extraction_P": "P_Plan_Score", "Extraction_D": "D_Do_Score", 
                      "Extraction_C": "C_Check_Score", "Extraction_A": "A_Act_Score"}
    phase_map = {"Extraction_P": "Plan", "Extraction_D": "Do", "Extraction_C": "Check", "Extraction_A": "Act"}

    reason_text = llm_output.get("reason", "") or ""

    for ext_key, score_key in extraction_map.items():
        val = llm_output.get(ext_key)
        
        # 1. Fallback: ‡∏ñ‡πâ‡∏≤‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ Regex ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å Reason (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á "‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô P:" ‡πÅ‡∏•‡∏∞ "‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô Plan:")
        if val is None or str(val).strip() in ["", "-"]:
            p_full = phase_map[ext_key]
            p_char = p_full[0]
            
            # Regex ‡∏Å‡∏ß‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏à‡∏≠ Marker ‡∏ï‡∏±‡∏ß‡∏ñ‡∏±‡∏î‡πÑ‡∏õ ‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏≥‡∏™‡∏£‡∏∏‡∏õ
            pattern = rf"‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô\s*({p_full}|{p_char})\s*:\s*(.*?)(?=‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô|‡∏™‡∏£‡∏∏‡∏õ‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°|is_passed|score|\Z)"
            match = re.search(pattern, reason_text, re.DOTALL | re.IGNORECASE)
            if match:
                val = match.group(2).strip()
                logger.info(f" üõ°Ô∏è [Regex Match] Recovered {p_full} from reason text")

        raw_val = str(val or "-").strip()
        current_score = float(llm_output.get(score_key, 0))
        
        logger.info(f" üì¶ [Content Check] Phase: {phase_map[ext_key]} | Score: {current_score} | Raw: '{raw_val[:80]}...'")

        # 2. Validation Logic
        has_file_ref = any(x in raw_val.lower() for x in [".pdf", ".docx", "source", "["])
        content_stripped = re.sub(r'[\[\]\(\)\-\|\:\s\_\#\*\"]', '', raw_val)
        is_too_short = len(content_stripped) < 10

        # 3. Smart Penalty: ‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏ï‡πà‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏™‡∏±‡πâ‡∏ô‡πÑ‡∏õ (‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏´‡∏•‡∏∑‡∏≠ 0.5)
        if has_file_ref and is_too_short and current_score > 0.5:
            llm_output[score_key] = 0.5
            current_score = 0.5

        # 4. Safeguard: ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÄ‡∏•‡∏¢‡πÅ‡∏ï‡πà‡πÑ‡∏î‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô -> ‡∏£‡∏¥‡∏ö‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏õ‡πá‡∏ô 0
        if not has_file_ref and is_too_short and current_score > 0:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Keyword ‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÉ‡∏ô reason (Heuristic Rescue)
            if not any(kw.lower() in reason_text.lower() for kw in ["‡∏û‡∏ö", "‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á", "‡∏°‡∏µ‡πÅ‡∏ú‡∏ô"]):
                logger.warning(f" üö® [Safeguard] L{level}: {score_key} revoked (No evidence found)")
                llm_output[score_key] = 0.0

    # 5. Final Score Calculation (SE-AM Logic)
    scores = { k: round(min(max(float(llm_output.get(k, 0)), 0.0), 2.0), 1) 
              for k in ["P_Plan_Score", "D_Do_Score", "C_Check_Score", "A_Act_Score"] }
    
    total_score = sum(scores.values())
    threshold = {1: 1, 2: 2, 3: 4, 4: 6, 5: 8}.get(level, 2)
    is_passed = total_score >= threshold

    # Hard-fail rules
    if is_passed:
        if level == 3 and scores["C_Check_Score"] <= 0: is_passed = False
        elif level == 4 and scores["A_Act_Score"] <= 0: is_passed = False

    llm_output.update({
        "score": total_score, **scores, "is_passed": is_passed, "normalized": True
    })
    
    return llm_output

# =================================================================
# Configuration Class
# =================================================================
@dataclass
class AssessmentConfig:
    """Configuration for the SEAM PDCA Assessment Run."""
    
    # ------------------ 1. Assessment Context ------------------
    enabler: str = DEFAULT_ENABLER
    tenant: str = DEFAULT_TENANT
    year: int = DEFAULT_YEAR
    target_level: int = MAX_LEVEL
    mock_mode: str = "none" # 'none', 'random', 'control'
    force_sequential: bool = field(default=False) # Flag ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏£‡∏±‡∏ô‡πÅ‡∏ö‡∏ö Sequential

    # ------------------ 2. LLM Configuration (Configurable) ------------------
    # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Default ‡∏à‡∏≤‡∏Å global_vars.py
    model_name: str = DEFAULT_LLM_MODEL_NAME 
    temperature: float = LLM_TEMPERATURE

    # ------------------ 3. Adaptive RAG Retrieval Configuration ------------------
    # üü¢ NEW: ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô Rerank ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Adaptive Loop (MIN_RETRY_SCORE)
    # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Default 0.65 ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏û‡∏ö‡πÉ‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î Logic
    min_retry_score: float = MIN_RETRY_SCORE
    # üü¢ NEW: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≠‡∏ö‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á Adaptive RAG Loop (MAX_RETRIEVAL_ATTEMPTS)
    # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Default 3
    max_retrieval_attempts: int = 3
    
    # ------------------ 4. Export Configuration ------------------
    export_output: bool = field(default=False) # Flag ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏¥‡∏î/‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£ Export ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    export_path: str = "" # Path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå Export (‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å)


# =================================================================
# SEAM Assessment Engine (PDCA Focused)
# =================================================================
class SEAMPDCAEngine:
        
    def __init__(
        self, 
        config: AssessmentConfig,
        llm_instance: Any = None, 
        logger_instance: logging.Logger = None,
        rag_retriever_instance: Any = None,
        doc_type: str = None, 
        vectorstore_manager: Optional['VectorStoreManager'] = None,
        evidence_map_path: Optional[str] = None,
        document_map: Optional[Dict[str, str]] = None,
        is_parallel_all_mode: bool = False,
        sub_id: str = 'all',
        **kwargs  
    ):
        # =======================================================
        # 1. Logger & ActionPlan Setup
        # =======================================================
        # ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤ doc_type ‡∏°‡∏≤‡πÄ‡∏ä‡πá‡∏Ñ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠ Logger
        clean_dt = str(doc_type or getattr(config, 'doc_type', EVIDENCE_DOC_TYPES)).strip().lower()
        log_year = config.year if clean_dt == EVIDENCE_DOC_TYPES.lower() else "general"

        if logger_instance is not None:
            self.logger = logger_instance
        else:
            # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô config.year ‡πÄ‡∏õ‡πá‡∏ô log_year ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
            self.logger = logging.getLogger(__name__).getChild(
                f"Engine|{config.enabler}|{config.tenant}/{log_year}"
            )

        # ‡∏õ‡∏£‡∏±‡∏ö Log ‡∏ï‡∏≠‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢
        self.logger.info(f"Initializing SEAMPDCAEngine for {config.enabler} ({config.tenant}/{log_year})")

        # =======================================================
        # 2. Core Configuration & Safety First
        # =======================================================
        self.config = config
        self.enabler_id = config.enabler
        self.target_level = config.target_level
        self.sub_id = sub_id
        self.llm = llm_instance
        self.vectorstore_manager = vectorstore_manager

        # ‚úÖ [CRITICAL FIX] ‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏® doc_type ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Worker ‡∏°‡∏≠‡∏á‡πÄ‡∏´‡πá‡∏ô‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô
        # ‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏£‡∏±‡∏ô Parallel ‡∏´‡∏£‡∏∑‡∏≠‡∏°‡∏µ document_map ‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏Å‡πá‡∏ï‡∏≤‡∏°
        self.doc_type = doc_type or getattr(config, 'doc_type', EVIDENCE_DOC_TYPES)

        # --- [CRITICAL LOADING] ---
        self.rubric = self._load_rubric()
        self.retry_policy = RetryPolicy(
            max_attempts=3,
            base_delay=2.0,
            jitter=True,
            escalate_context=True,
            shorten_prompt_on_fail=True,
            exponential_backoff=True,
        )

        self.is_sequential = getattr(config, 'force_sequential', True)
        self.is_parallel_all_mode = is_parallel_all_mode
        self.required_pdca_map = REQUIRED_PDCA
        self.base_pdca_keywords = BASE_PDCA_KEYWORDS
        self.RERANK_THRESHOLD: float = RERANK_THRESHOLD
        self.MAX_EVI_STR_CAP: float = MAX_EVI_STR_CAP

        # =======================================================
        # 3. Persistent Evidence Mapping (‡∏â‡∏ö‡∏±‡∏ö‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á)
        # =======================================================
        
        clean_dt = str(self.doc_type).strip().lower()
        self.evidence_map = {}
        self.evidence_map_path = None # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Default ‡πÄ‡∏õ‡πá‡∏ô None ‡πÑ‡∏ß‡πâ‡∏Å‡πà‡∏≠‡∏ô

        if clean_dt == EVIDENCE_DOC_TYPES.lower():
            # üéØ ‡∏™‡∏£‡πâ‡∏≤‡∏á Path ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô Evidence Mode ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
            self.evidence_map_path = evidence_map_path or get_evidence_mapping_file_path(
                tenant=self.config.tenant, 
                year=self.config.year, 
                enabler=self.enabler_id
            )
            self.evidence_map = self._load_evidence_map()
            self.logger.info(f"üìä Evidence mode: Loaded {len(self.evidence_map)} mapping keys.")
        else:
            # üéØ ‡πÇ‡∏´‡∏°‡∏î Document ‡∏à‡∏∞‡πÑ‡∏°‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏∏‡∏î‡∏•‡∏á‡πÑ‡∏õ‡πÉ‡∏ô folder ‡∏õ‡∏µ
            self.logger.info(f"üìÑ Document mode: Skipping heavy evidence mapping load (Speed Optimized).")

        self.contextual_rules_map = self._load_contextual_rules_map()
        self.temp_map_for_save = {}

        # =======================================================
        # 4. Document Map Loading (Dynamic Logic)
        # =======================================================
        map_to_use: Dict[str, str] = document_map or {}

        if not map_to_use:
            # ‡πÉ‡∏ä‡πâ self.doc_type ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡πÑ‡∏ß‡πâ‡∏ä‡∏±‡∏ß‡∏£‡πå‡πÜ ‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô
            clean_dt = str(self.doc_type).strip().lower()
            
            if clean_dt == EVIDENCE_DOC_TYPES.lower():
                mapping_path = get_mapping_file_path(self.doc_type, tenant=self.config.tenant, year=self.config.year, enabler=self.enabler_id)
            else:
                mapping_path = get_mapping_file_path(self.doc_type, tenant=self.config.tenant)

            self.logger.info(f"üéØ Loading {clean_dt} mapping from: {mapping_path}")

            try:
                if os.path.exists(mapping_path):
                    with open(mapping_path, 'r', encoding='utf-8') as f:
                        doc_map_raw = json.load(f)
                    map_to_use = {doc_id: data.get("file_name", doc_id) for doc_id, data in doc_map_raw.items()}
                    self.logger.info(f"Loaded {len(map_to_use)} mappings.")
                else:
                    self.logger.warning(f"File not found: {mapping_path}")
            except Exception as e:
                self.logger.error(f"Failed to load document map: {e}")

        self.doc_id_to_filename_map = map_to_use
        self.document_map = map_to_use

        # =======================================================
        # 5. Lazy Initialization (VSM & LLM)
        # =======================================================
        # ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ _initialize_vsm_if_none ‡∏à‡∏∞‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏£‡∏≤‡∏ö‡∏£‡∏∑‡πà‡∏ô‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏°‡∏µ self.doc_type ‡πÅ‡∏•‡πâ‡∏ß
        if self.llm is None: self._initialize_llm_if_none()
        if self.vectorstore_manager is None: self._initialize_vsm_if_none()

        if self.vectorstore_manager and not getattr(self.vectorstore_manager, '_doc_id_mapping', None):
            self.vectorstore_manager._load_doc_id_mapping()

        # =======================================================
        # 6. Function Pointers
        # =======================================================
        self.llm_evaluator = evaluate_with_llm
        self.rag_retriever = retrieve_context_with_filter
        self.create_structured_action_plan = create_structured_action_plan
        self.ActionPlanActions = ActionPlanActions

        self.logger.info(f"‚úÖ Engine initialized: Enabler={self.enabler_id}, DocType={self.doc_type}")

    def get_rule_content(self, sub_id: str, level: int, key_type: str):
        """
        [NEW] ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Contextual Rules ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Nested L1-L5
        key_type: 'plan_keywords', 'do_keywords', 'specific_contextual_rule', 'must_include_keywords'
        """
        rule = self.contextual_rules_map.get(sub_id, {})
        level_key = f"L{level}"
        
        # 1. ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô Level ‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á‡∏Å‡πà‡∏≠‡∏ô (L1, L2...)
        level_data = rule.get(level_key, {})
        if key_type in level_data:
            return level_data[key_type]
        
        # 2. Fallback ‡πÑ‡∏õ‡∏ó‡∏µ‡πà Root ‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏ô‡∏±‡πâ‡∏ô‡πÜ
        if key_type in rule:
            return rule[key_type]
        
        # 3. Fallback ‡πÑ‡∏õ‡∏ó‡∏µ‡πà Defaults ‡∏Å‡∏•‡∏≤‡∏á (‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å _enabler_defaults ‡πÉ‡∏ô JSON)
        if "keywords" in key_type:
            return self.contextual_rules_map.get("_enabler_defaults", {}).get(key_type, [])
            
        return ""

    def enhance_query_for_statement(
        self,
        statement_text: str,
        sub_id: str,
        statement_id: str,
        level: int,
        focus_hint: str,
        additional_context: Dict[str, Any] = None
    ) -> List[str]:
        """
        [REVISED v21.4 - FULL]
        ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ (Search Queries) ‡πÇ‡∏î‡∏¢‡∏î‡∏∂‡∏á‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏î‡∏¥‡∏ö‡∏à‡∏≤‡∏Å Focus Points ‡πÅ‡∏•‡∏∞ Guidelines 
        ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á
        """
        logger = logging.getLogger(__name__)
        enabler_id = self.enabler_id
        additional_context = additional_context or {}
        
        # --- 1. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏î‡∏¥‡∏ö (Keywords & Context) ---
        # ‡∏î‡∏∂‡∏á Focus Points ‡πÅ‡∏•‡∏∞ Guideline ‡∏à‡∏≤‡∏Å Context
        focus_points = additional_context.get('focus_points', [])
        guideline = additional_context.get('guideline', "")
        
        # ‡∏î‡∏∂‡∏á Keywords ‡∏ï‡∏≤‡∏°‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô PDCA (‡∏™‡∏∞‡∏™‡∏°‡∏ï‡∏≤‡∏° Level)
        raw_keywords = []
        # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ (Must include)
        must_list = self.get_rule_content(sub_id, level, "must_include_keywords")
        if isinstance(must_list, list): raw_keywords.extend(must_list)
        
        # ‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ï‡∏≤‡∏°‡∏£‡∏∞‡∏î‡∏±‡∏ö Maturity (Plan/Do/Check/Act)
        if level >= 1: raw_keywords.extend(self.get_rule_content(sub_id, 1, "plan_keywords") or [])
        if level >= 3: raw_keywords.extend(self.get_rule_content(sub_id, 3, "check_keywords") or [])
        
        # ‡∏£‡∏ß‡∏° Focus Points ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô‡∏Å‡πâ‡∏≠‡∏ô Keywords
        if isinstance(focus_points, list):
            raw_keywords.extend(focus_points)

        # Clean & Deduplicate
        clean_keywords = list(dict.fromkeys([str(k).strip() for k in raw_keywords if k]))
        keywords_str = " ".join(clean_keywords[:12]) # ‡πÉ‡∏ä‡πâ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î 12 ‡∏Ñ‡∏≥‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ Query ‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ

        # --- 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î Queries ‡πÅ‡∏ö‡∏ö‡∏°‡∏∏‡πà‡∏á‡πÄ‡∏õ‡πâ‡∏≤ (Targeted Queries) ---
        queries = []

        # Query 1: Focus-Core Query (‡∏ú‡∏™‡∏° Enabler + Sub-ID + Focus Points)
        # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: "KM 2.2 ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏™‡∏£‡∏£‡∏á‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì ‡∏£‡∏∞‡∏ö‡∏ö IT Infrastructure"
        queries.append(f"{enabler_id} {sub_id} {keywords_str}")

        # Query 2: Evidence-Type Query (‡πÄ‡∏ô‡πâ‡∏ô‡∏ï‡∏≤‡∏° Guideline ‡∏´‡∏£‡∏∑‡∏≠ Maturity)
        # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: "‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏° ‡∏Ñ‡∏ì‡∏∞‡∏ó‡∏≥‡∏á‡∏≤‡∏ô KM" ‡∏´‡∏£‡∏∑‡∏≠ "‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥ ‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢"
        if guideline:
            queries.append(f"{guideline} {sub_id} {enabler_id} {keywords_str}")
        else:
            if level <= 2:
                queries.append(f"‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏® ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á ‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥ ‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô ‡∏´‡∏•‡∏±‡∏Å‡πÄ‡∏Å‡∏ì‡∏ë‡πå {sub_id} {keywords_str}")
            else:
                queries.append(f"‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° {sub_id} {keywords_str}")

        # Query 3: Statement-Based (‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á + Focus Hint)
        # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: "‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏´‡∏•‡∏±‡∏Å‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏™‡∏£‡∏£‡∏á‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì L2 KM"
        queries.append(f"{statement_text} {sub_id} {focus_hint}")

        # Query 4: PDCA & Result (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á L4-L5)
        if level >= 4:
            queries.append(f"‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á ‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á ‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏° Best Practice {sub_id} {keywords_str}")

        # --- 3. Finalize & Limit ---
        # ‡∏•‡∏ö Query ‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô‡πÅ‡∏•‡∏∞‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á
        final_queries = []
        seen = set()
        for q in queries:
            q_clean = " ".join(q.split())
            if q_clean and q_clean not in seen:
                final_queries.append(q_clean)
                seen.add(q_clean)

        logger.info(f"üöÄ Enhanced {len(final_queries[:5])} queries for {sub_id} L{level} (Context: {len(clean_keywords)} keywords)")
        return final_queries[:5] # ‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î 5 Queries

    def _initialize_llm_if_none(self):
        """Initializes LLM instance if self.llm is None."""
        if self.llm is None:
            self.logger.warning("‚ö†Ô∏è Initializing LLM: model=%s, temperature=%s", 
                                self.config.model_name, self.config.temperature)
            try:
                # üü¢ FIX: Import ‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ create_llm_instance
                from models.llm import create_llm_instance 
                self.llm = create_llm_instance( 
                    model_name=self.config.model_name,
                    temperature=self.config.temperature
                )
                self.logger.info("‚úÖ LLM Instance created successfully: %s (Temp: %s)", 
                                 self.config.model_name, self.config.temperature)
            except Exception as e:
                self.logger.error(f"FATAL: Could not initialize LLM: {e}")
                raise

    def _initialize_vsm_if_none(self):
        """
        Initializes VectorStoreManager with Smart Year Selection.
        - If Evidence: Priority 1 = Specific Year, Priority 2 = Root Fallback.
        - If Document: Priority 1 = Root (General), Priority 2 = Specific Year Fallback.
        """
        if self.vectorstore_manager is not None:
            return

        # 1. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ç‡∏≠‡∏á DocType
        clean_dt = str(self.doc_type or getattr(self.config, 'doc_type', EVIDENCE_DOC_TYPES)).strip().lower()
        is_evidence = (clean_dt == EVIDENCE_DOC_TYPES.lower())
        
        self.logger.info(f"üöÄ Loading vectorstore(s) for DocType: '{clean_dt}' (Mode: {'Evidence' if is_evidence else 'General'})")

        try:
            target_enabler = str(self.enabler_id).lower() if self.enabler_id else None
            
            # üéØ [SMART SELECTION] ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å
            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô evidence ‡πÉ‡∏´‡πâ‡∏´‡∏≤‡∏ï‡∏≤‡∏°‡∏õ‡∏µ (2568) ‡∏Å‡πà‡∏≠‡∏ô ‡πÅ‡∏ï‡πà‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô document ‡πÉ‡∏´‡πâ‡∏´‡∏≤‡∏ó‡∏µ‡πà Root (None) ‡∏Å‡πà‡∏≠‡∏ô
            primary_year = self.config.year if is_evidence else None
            secondary_year = None if is_evidence else self.config.year

            # 2. First Attempt: ‡πÇ‡∏´‡∏•‡∏î‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
            self.vectorstore_manager = load_all_vectorstores(
                doc_types=[clean_dt], 
                enabler_filter=target_enabler, 
                tenant=self.config.tenant, 
                year=primary_year       
            )
            
            def count_retrievers(vsm):
                if vsm and hasattr(vsm, '_multi_doc_retriever') and vsm._multi_doc_retriever:
                    return len(vsm._multi_doc_retriever._all_retrievers)
                return 0

            len_retrievers = count_retrievers(self.vectorstore_manager)
            
            # 3. Second Attempt (Fallback): ‡∏ñ‡πâ‡∏≤‡∏´‡∏≤‡πÅ‡∏ö‡∏ö‡πÅ‡∏£‡∏Å‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ ‡πÉ‡∏´‡πâ‡∏™‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏´‡∏≤‡∏≠‡∏µ‡∏Å‡πÅ‡∏ö‡∏ö
            if len_retrievers == 0:
                lookup_label = f"year {secondary_year}" if secondary_year else "tenant root"
                self.logger.info(f"‚ö†Ô∏è No collections found in primary path, searching in {lookup_label}...")
                
                self.vectorstore_manager = load_all_vectorstores(
                    doc_types=[clean_dt], 
                    enabler_filter=target_enabler, 
                    tenant=self.config.tenant, 
                    year=secondary_year       
                )
                len_retrievers = count_retrievers(self.vectorstore_manager)

            # 4. Post-Load Process
            if self.vectorstore_manager and len_retrievers > 0:
                self.vectorstore_manager._load_doc_id_mapping() 
                self.logger.info(f"‚úÖ MultiDocRetriever loaded with {len_retrievers} collections.") 
            else:
                # 5. Final Error Handling
                expected_p = f"data_store/{self.config.tenant}/vectorstore/{primary_year or 'root'}"
                self.logger.error(f"‚ùå FATAL: 0 vector store collections loaded. Please check folder: {expected_p}")
                raise ValueError(f"No vector collections found for '{target_enabler}' in {self.config.tenant}")

        except Exception as e:
            self.logger.error(f"‚ùå FATAL: Could not initialize VectorStoreManager: {str(e)}")
            raise

    def _get_applicable_contextual_rule(self, sub_id: str, level: int) -> Optional[Dict[str, Any]]:
        """
        ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Contextual Rule ‡∏ó‡∏µ‡πà‡∏°‡∏µ target_sub_criteria ‡πÅ‡∏•‡∏∞ target_level ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô
        """
        # self.contextual_rules_map ‡∏Ñ‡∏∑‡∏≠ dict ‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î‡∏à‡∏≤‡∏Å pea_km_contextual_rules.json
        for rule_name, rule_data in self.contextual_rules_map.items():
            if (rule_data.get('target_sub_criteria') == sub_id and 
                rule_data.get('target_level') == level):
                
                # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ä‡∏∑‡πà‡∏≠ Rule ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô Data ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏†‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏á
                rule_data['name'] = rule_name 
                return rule_data
        return None

    def _check_contextual_rule_condition(
        self, 
        condition: Dict[str, Any], 
        sub_id: str, 
        level: int, 
        previous_levels_evidence_dict: Dict[str, List[Dict[str, Any]]], 
        top_evidences: List[Dict[str, Any]]
    ) -> bool:
        """
        ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô Rule Condition (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö 'and' ‡πÅ‡∏•‡∏∞‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏¢‡πà‡∏≠‡∏¢)
        """
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç 'and' (‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô Contextual Rule ‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ‡πÅ‡∏Ñ‡πà 'and')
        if 'and' in condition:
            for sub_condition in condition['and']:
                
                # 1. check_passed_levels (‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö L1, L2 ‡∏ï‡πâ‡∏≠‡∏á PASS)
                if 'check_passed_levels' in sub_condition:
                    required_levels = sub_condition['check_passed_levels']
                    for required_level in required_levels:
                        if not self._is_previous_level_passed(sub_id, required_level):
                            self.logger.debug(f"Rule Condition Failed: {required_level} not passed.")
                            return False # ‡∏ñ‡πâ‡∏≤ L1/L2 ‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç Fail
                
                # 2. check_missing_pdca (‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏Å‡∏¥‡∏î Evidence Gap ‡πÉ‡∏ô D/C ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà)
                if 'check_missing_pdca' in sub_condition:
                    # Logic ‡∏ô‡∏µ‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏†‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å‡πÉ‡∏ô _run_single_assessment (missing_tags) 
                    # ‡πÅ‡∏ï‡πà‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ã‡πâ‡∏≥‡πÑ‡∏î‡πâ ‡πÅ‡∏ï‡πà‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ ‡πÉ‡∏´‡πâ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡∏ñ‡πâ‡∏≤‡∏°‡∏≤‡∏ñ‡∏∂‡∏á‡∏Ç‡∏±‡πâ‡∏ô‡∏ô‡∏µ‡πâ‡πÅ‡∏•‡πâ‡∏ß missing_tags ‡∏ñ‡∏π‡∏Å‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡πâ‡∏ß
                    pass
                
                # 3. check_evidence_exists (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô C/A ‡∏ó‡∏µ‡πà‡∏°‡∏µ Rerank Score ‡∏™‡∏π‡∏á)
                if 'check_evidence_exists' in sub_condition:
                    required_phases = sub_condition['check_evidence_exists'].get('phase', [])
                    min_rerank = sub_condition['check_evidence_exists'].get('min_rerank', globals().get('CRITICAL_CA_THRESHOLD', 0.65))
                    min_count = sub_condition['check_evidence_exists'].get('min_count', 1)
                    
                    found_count = 0
                    
                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÉ‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏ô‡∏û‡∏ö‡πÉ‡∏ô Level ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô (top_evidences)
                    for doc in top_evidences:
                        if doc.get('pdca_tag') in required_phases and doc.get('rerank_score', 0.0) >= min_rerank:
                            found_count += 1
                            
                    if found_count < min_count:
                        self.logger.debug(f"Rule Condition Failed: Found only {found_count} of required {min_count} {required_phases} with Rerank >= {min_rerank}.")
                        return False # ‡∏ñ‡πâ‡∏≤‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô C/A Critical ‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç Fail

            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏¢‡πà‡∏≠‡∏¢‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô 'and' ‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏£‡∏¥‡∏á
            return True 
        
        return False # ‡∏ñ‡πâ‡∏≤ Rule Format ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á

    def _is_previous_level_passed(self, sub_id: str, level: int) -> bool:
        """
        Helper: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡∏ú‡πà‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≤‡∏Å self.assessment_results_map
        (‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö check_passed_levels ‡πÉ‡∏ô Contextual Rule)
        """
        # Key ‡πÉ‡∏ô assessment_results_map ‡∏°‡∏±‡∏Å‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô '3.1.L1', '3.1.L2'
        key = f"{sub_id}.L{level}"
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏≠‡∏á Level ‡∏ô‡∏±‡πâ‡∏ô‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà ‡πÅ‡∏•‡∏∞‡∏°‡∏µ is_passed ‡πÄ‡∏õ‡πá‡∏ô True
        result = self.assessment_results_map.get(key)
        
        return result is not None and result.get('is_passed', False)

    def _expand_context_with_neighbor_pages(self, top_evidences: List[Any], collection_name: str) -> List[Any]:
        """
        [NEW] ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô ‡∏´‡∏≤‡∏Å‡∏û‡∏ö Keyword ‡∏Ç‡∏≠‡∏á 'Check' 
        ‡∏à‡∏∞‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏ñ‡∏±‡∏î‡πÑ‡∏õ (Act) ‡∏°‡∏≤‡πÄ‡∏™‡∏£‡∏¥‡∏°‡πÉ‡∏´‡πâ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
        """
        if not self.vectorstore_manager or not top_evidences:
            return top_evidences

        expanded_evidences = list(top_evidences)
        seen_pages = set()
        
        # Keywords ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Trigger (‡∏ñ‡πâ‡∏≤‡πÄ‡∏à‡∏≠‡∏Ñ‡∏≥‡∏û‡∏ß‡∏Å‡∏ô‡∏µ‡πâ ‡πÉ‡∏´‡πâ‡πÑ‡∏õ‡∏î‡∏∂‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ)
        check_triggers = ["‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏∂‡∏á‡∏û‡∏≠‡πÉ‡∏à", "‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô", "‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•", "‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô", "score", "kpi", "3.41"]

        for doc in top_evidences:
            # ‡∏î‡∏∂‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ (Engine ‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ dict ‡∏ó‡∏µ‡πà‡∏°‡∏µ key 'text' ‡∏´‡∏•‡∏±‡∏á‡∏ú‡πà‡∏≤‡∏ô Hydration)
            text = (doc.get('text') or doc.get('page_content') or "").lower()
            
            # ‡∏î‡∏∂‡∏á Metadata
            meta = doc.metadata if hasattr(doc, 'metadata') else doc.get('metadata', {})
            page_label = meta.get("page_label")
            doc_uuid = meta.get("stable_doc_uuid")

            # ‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç: ‡πÄ‡∏à‡∏≠ Keyword + ‡∏°‡∏µ‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤ + ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏Ñ‡∏¢‡∏î‡∏∂‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏ô‡∏µ‡πâ‡∏°‡∏≤‡πÄ‡∏™‡∏£‡∏¥‡∏°‡πÉ‡∏ô‡∏£‡∏≠‡∏ö‡∏ô‡∏µ‡πâ
            if any(k in text for k in check_triggers) and str(page_label).isdigit():
                next_page = str(int(page_label) + 1)
                cache_key = f"{doc_uuid}_{next_page}"

                if cache_key not in seen_pages:
                    self.logger.info(f"üîç Act-Hook: ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Check ‡∏ó‡∏µ‡πà‡∏´‡∏ô‡πâ‡∏≤ {page_label}, ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á‡∏´‡∏ô‡πâ‡∏≤ {next_page}...")
                    
                    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å Method ‡∏à‡∏≤‡∏Å Step 1 (‡πÉ‡∏ô vectorstore.py)
                    neighbor_chunks = self.vectorstore_manager.get_chunks_by_page(
                        collection_name=collection_name,
                        stable_doc_uuid=doc_uuid,
                        page_label=next_page
                    )

                    if neighbor_chunks:
                        for nc in neighbor_chunks:
                            # ‡∏™‡∏£‡πâ‡∏≤‡∏á dict ‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏´‡πâ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ö top_evidences ‡πÄ‡∏î‡∏¥‡∏°
                            new_doc = {
                                "text": f"[Supplemental Context - Next Page {next_page} for Act analysis]:\n{nc.page_content}",
                                "page_content": nc.page_content,
                                "metadata": nc.metadata,
                                "pdca_tag": "Act", # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô Act ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏•‡∏á‡πÉ‡∏ô act_blocks
                                "is_supplemental": True,
                                "rerank_score": doc.get('rerank_score', 0.0) # ‡πÉ‡∏ä‡πâ score ‡πÄ‡∏î‡∏¥‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ú‡πà‡∏≤‡∏ô Filter
                            }
                            expanded_evidences.append(new_doc)
                        seen_pages.add(cache_key)

        return expanded_evidences
    
    
    def _resolve_evidence_filenames(self, evidence_entries: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á
        1. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà doc_id ‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ 'UNKNOWN-' (‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏†‡∏≤‡∏¢‡πÉ‡∏ô/‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£)
        2. ‡πÅ‡∏õ‡∏•‡∏á doc_id (‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô Hash/UUID) ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏î‡πâ ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ doc_id_to_filename_map
        """
        
        resolved_entries = []
        
        for entry in evidence_entries:
            # ‡πÉ‡∏ä‡πâ deepcopy ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
            resolved_entry = deepcopy(entry)
            doc_id = resolved_entry.get("doc_id", "")
            current_filename = resolved_entry.get("filename", "") # ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏î‡∏¥‡∏°‡∏à‡∏≤‡∏Å Metadata (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
            
            # --- 1. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏ì‡∏µ UNKNOWN- (AI-GENERATED or Lost Source) ---
            if doc_id.startswith("UNKNOWN-"):
                resolved_entry["filename"] = f"AI-GENERATED-REF-{doc_id.split('-')[-1]}"
                resolved_entries.append(resolved_entry)
                continue

            # --- 2. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏ì‡∏µ Doc ID (Hash/UUID) ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ---
            if doc_id:
                # A. ‡∏•‡∏≠‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å Map
                if doc_id in self.doc_id_to_filename_map:
                    resolved_entry["filename"] = self.doc_id_to_filename_map[doc_id]
                    resolved_entries.append(resolved_entry)
                    continue

                # B. ‡∏ñ‡πâ‡∏≤‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ (Map Fail)
                else:
                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏Å‡∏±‡∏ö Metadata ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏™‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                    is_generic_name = (
                        not current_filename.strip() or # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô String ‡∏ß‡πà‡∏≤‡∏á
                        current_filename.lower() == "unknown" or
                        # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Hash/UUID 64 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•
                        re.match(r"^[0-9a-f]{64}(\.pdf|\.txt)?$", current_filename, re.IGNORECASE)
                    )
                    
                    if is_generic_name:
                        # ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå Fallback ‡∏ó‡∏µ‡πà‡∏™‡∏∑‡πà‡∏≠‡∏ß‡πà‡∏≤ Map ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
                        resolved_entry["filename"] = f"MAPPING-FAILED-{doc_id[:8]}..."
                        self.logger.warning(f"Failed to map doc_id {doc_id[:8]}... to filename. Using fallback.")
                        
            # --- 3. ‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏°‡πà‡∏°‡∏µ Doc ID ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ (‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÄ‡∏õ‡πá‡∏ô Unknown) ---
            elif not doc_id and (not current_filename.strip() or current_filename.lower() == "unknown"):
                # ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ doc_id ‡πÅ‡∏•‡∏∞ filename ‡πÄ‡∏î‡∏¥‡∏°‡∏Å‡πá‡πÄ‡∏õ‡πá‡∏ô Unknown/Empty
                resolved_entry["filename"] = "MISSING-SOURCE-METADATA"
                self.logger.error("Evidence found with no doc_id and generic filename.")
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏° entry ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ (‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà)
            resolved_entries.append(resolved_entry)

        return resolved_entries
    
    # -------------------- Contextual Rules Handlers (FIXED) --------------------
    def _load_contextual_rules_map(self) -> Dict[str, Dict[str, str]]:
        """
        Loads the contextual rules JSON file using the path generated by 
        utils.path_utils.get_contextual_rules_file_path.
        """
        
        try:
            # üéØ ‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏à‡∏≤‡∏Å path_utils ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Path ‡πÄ‡∏≠‡∏á
            filepath = get_contextual_rules_file_path(
                tenant=self.config.tenant,
                enabler=self.enabler_id
            )
        except ImportError:
            self.logger.error("‚ùå FATAL: Cannot import get_contextual_rules_file_path. Check utils/path_utils.py import.")
            return {}

        # 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if not os.path.exists(filepath):
            # ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ get_contextual_rules_file_path ‡∏ó‡∏≥‡πÅ‡∏ó‡∏ô
            self.logger.info(f"‚ö†Ô∏è Contextual Rules file not found at: {filepath}. Using empty map.")
            return {}

        self.logger.info(f"‚úÖ Contextual Rules loaded from: {filepath}")
        
        # 2. ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
                self.logger.info(f"‚úÖ Loaded Contextual Rules: {len(data)} sub-criteria rules.")
                return data
        except json.JSONDecodeError as e:
            self.logger.error(f"‚ùå Failed to parse Contextual Rules JSON from {filepath}: {e}")
            return {}
        except Exception as e:
            self.logger.error(f"‚ùå Failed to load Contextual Rules from {filepath}: {e}")
            return {}
    

    # ----------------------------------------------------------------------
    # üéØ FINAL FIX 2.3: Manual Map Reload Function (inside SEAMPDCAEngine)
    # ----------------------------------------------------------------------

    def _collect_previous_level_evidences(self, sub_id: str, current_level: int) -> Dict[str, List[Dict]]:
        """
        ‡∏î‡∏∂‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏à‡∏≤‡∏Å Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ (L1 ‚Üí L2, L2 ‚Üí L3 ‡∏Ø‡∏•‡∏Ø) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô Baseline Context

        Final Production Version - 14 ‡∏ò.‡∏Ñ. 2568 (ULTIMATE VICTORY EDITION)
        - ‡πÄ‡∏û‡∏¥‡πà‡∏° is_parallel_all_mode ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ç‡πâ‡∏≤‡∏° hydration ‡πÉ‡∏ô full parallel mode (by design)
        - ‡∏£‡∏±‡∏Å‡∏©‡∏≤ Heuristic Fallback ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ CRITICAL MAPPING FAILURE
        """
        
        # --------------------------------------------------------------
        # 1. ‡∏Ç‡πâ‡∏≤‡∏° Hydration ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô Full Parallel Mode (by design)
        # --------------------------------------------------------------
        if self.is_parallel_all_mode:
            self.logger.info("FULL PARALLEL MODE: Skipping previous level evidence hydration (stateless by design)")
            return {}  # ‡∏Ñ‡∏∑‡∏ô map ‡∏ß‡πà‡∏≤‡∏á ‚Üí ‡πÑ‡∏°‡πà hydrate priority chunks ‡∏à‡∏≤‡∏Å level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤

        # --------------------------------------------------------------
        # 2. ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏° evidence ‡∏à‡∏≤‡∏Å Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏ô Sub-Criteria ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
        # --------------------------------------------------------------
        collected = {}
        for key, ev_list in self.evidence_map.items():
            if (key.startswith(f"{sub_id}.L") and 
                isinstance(ev_list, list) and 
                ev_list):
                try:
                    level_num = int(key.split(".L")[-1])
                    if level_num < current_level:
                        collected[key] = ev_list
                except (ValueError, IndexError):
                    continue

        if not collected:
            self.logger.info("No previous level evidences found for hydration.")
            return {}

        # --------------------------------------------------------------
        # 3. ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏° Stable IDs + Chunk UUIDs (cleaned)
        # --------------------------------------------------------------
        stable_ids = set()
        chunk_uuids_clean = set()

        for ev_list in collected.values():
            for ev in ev_list:
                sid = ev.get("stable_doc_uuid") or ev.get("doc_id")
                if isinstance(sid, str) and len(sid) == 64 and sid.isalnum():
                    stable_ids.add(sid)
                
                cid = ev.get("chunk_uuid")
                if isinstance(cid, str) and len(cid.replace("-", "")) >= 64:
                    chunk_uuids_clean.add(cid.replace("-", ""))

        if not stable_ids and not chunk_uuids_clean:
            self.logger.info("No valid IDs found for hydration.")
            return collected

        # --------------------------------------------------------------
        # 4. ‡πÅ‡∏õ‡∏•‡∏á Stable ‚Üí Chunk UUIDs (‡πÄ‡∏û‡∏∑‡πà‡∏≠ Log ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏Ç‡∏≠‡∏á‡πÇ‡∏Ñ‡πâ‡∏î)
        # --------------------------------------------------------------
        vsm = self.vectorstore_manager
        final_uuids = list(chunk_uuids_clean)
        self.logger.info(f"HYDRATION ‚Üí Resolved {len(final_uuids)} unique chunk UUIDs ‚Üí fetching full text...")

        stable_ids_list = list(stable_ids)
        if not stable_ids_list:
            self.logger.warning("No Stable IDs resolved for VSM hydration call.")
            return collected

        # --------------------------------------------------------------
        # 5. ‡∏î‡∏∂‡∏á full chunks (‡πÉ‡∏ä‡πâ Stable IDs 64-char)
        # --------------------------------------------------------------
        try:
            full_chunks = vsm.get_documents_by_id(stable_ids_list, self.doc_type, self.enabler_id) 
            self.logger.info(f"HYDRATION success: Retrieved {len(full_chunks)} full chunks (via Stable ID search)")
            
        except Exception as e:
            self.logger.error(f"Hydration failed in VSM call (get_documents_by_id): {e}", exc_info=True)
            return collected

        # --------------------------------------------------------------
        # 6. ‡∏™‡∏£‡πâ‡∏≤‡∏á map ‡πÅ‡∏•‡∏∞ hydrate text ‡∏û‡∏£‡πâ‡∏≠‡∏° Fallback Logic (FINAL FIX 27.0)
        # --------------------------------------------------------------
        chunk_map = {}  # Key: Cleaned V4 UUID (without dashes)
        total_retrieved = len(full_chunks)
        
        # 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á Map (Key: V4 UUID Cleaned)
        for idx, chunk in enumerate(full_chunks):
            meta = getattr(chunk, "metadata", {})
            cid_raw = meta.get("chunk_uuid")
            cid = (cid_raw or "").replace("-", "") 
            
            if cid:
                chunk_map[cid] = {
                    "text": chunk.page_content,
                    "metadata": meta
                }
            else:
                self.logger.error(f"CRITICAL HYDRATION ERROR: Retrieved chunk {idx+1}/{total_retrieved} has NO or empty 'chunk_uuid' in metadata. Skipping this chunk.")

        self.logger.info(f"DEBUG: Chunk Map built with {len(chunk_map)}/{total_retrieved} entries.")

        hydrated = {}
        restored = 0
        total = sum(len(v) for v in collected.values())

        for key, ev_list in collected.items():
            new_list = []
            for ev in ev_list:
                new_ev = ev.copy()
                data = None
                
                # ID ‡∏à‡∏≤‡∏Å evidence level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤
                cid_l1 = (ev.get("chunk_uuid") or "").replace("-", "") 
                sid_l1 = ev.get("stable_doc_uuid") or ev.get("doc_id") 
                vsm_mapping_failed = True

                # --- 1. Primary Lookup ---
                if cid_l1:
                    data = chunk_map.get(cid_l1)

                # --- 2. Fallback: VSM Mapping ---
                if not data and sid_l1 and vsm and hasattr(vsm, '_doc_id_mapping') and vsm._doc_id_mapping:
                    if cid_l1 and not data:
                        self.logger.warning(f"Hydration Check: Primary L1 chunk_uuid '{cid_l1[:8]}...' NOT found in map. Starting Stable ID Fallback...")
                    
                    if sid_l1 in vsm._doc_id_mapping:
                        vsm_mapping_failed = False
                        for v4_uuid_raw in vsm._doc_id_mapping[sid_l1].get("chunk_uuids", []):
                            v4_uuid_cleaned = v4_uuid_raw.replace("-", "")
                            if v4_uuid_cleaned in chunk_map:
                                data = chunk_map[v4_uuid_cleaned]
                                self.logger.info(f"‚úÖ Fallback SUCCESS (VSM Map): Matched via V4 UUID '{v4_uuid_cleaned[:8]}...'")
                                break
                    else:
                        self.logger.warning(f"Hydration Check: Stable ID {sid_l1[:8]}... NOT found in VSM Doc ID Mapping.")

                # --- 3. HEURISTIC FALLBACK (Final Bypass) ---
                if not data:
                    if not vsm_mapping_failed:
                        self.logger.warning(f"‚ö†Ô∏è VSM Map exists but failed. Attempting Heuristic Match by Stable ID.")
                    
                    for retrieved_chunk_data in chunk_map.values():
                        retrieved_sid = retrieved_chunk_data["metadata"].get("stable_doc_uuid")
                        if retrieved_sid == sid_l1:
                            data = retrieved_chunk_data
                            new_ev["chunk_uuid"] = retrieved_chunk_data["metadata"].get("chunk_uuid", new_ev["chunk_uuid"])
                            self.logger.info(f"üü¢ Heuristic SUCCESS: Restored using matching Stable ID {sid_l1[:8]}...")
                            break

                # --------------------------------------------------------------------------
                if data:
                    new_ev["text"] = data["text"]
                    new_ev.update({k: v for k, v in data["metadata"].items() 
                                if k not in ["text", "page_content"]})
                    restored += 1
                    
                    # üéØ CRITICAL FIX: ‡πÄ‡∏û‡∏¥‡πà‡∏° Flag is_baseline
                    new_ev["is_baseline"] = True 
                    
                else:
                    sid_l1 = ev.get("stable_doc_uuid") or ev.get("doc_id")
                    self.logger.error(f"‚ùå CRITICAL MAPPING FAILURE: Could not restore chunk (Stable ID: {sid_l1[:8] if sid_l1 else 'N/A'}...) from {len(chunk_map)} retrieved chunks.")
                    # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏î‡∏¥‡∏°‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß ‡πÉ‡∏´‡πâ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡∏¢‡∏±‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡πÅ‡∏°‡πâ‡πÑ‡∏°‡πà‡∏°‡∏µ Text ‡πÄ‡∏ï‡πá‡∏°
                    new_ev["is_baseline"] = False
                    if "page" not in new_ev:
                         new_ev["page"] = ev.get("page") # Fallback ‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡∏Ñ‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏°‡πÉ‡∏ô evidence_map
                
                new_list.append(new_ev)
            hydrated[key] = new_list
                
        self.logger.info(f"BASELINE HYDRATED ‚Üí {restored}/{total} chunks restored with full text (including fallback)")
        return hydrated

    def _get_contextual_rules_prompt(self, sub_id: str, level: int) -> str:
        """
        Retrieves the specific Contextual Rule prompt for a given Sub-Criteria and Level,
        ‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á‡∏Å‡∏≤‡∏£ Inject ‡∏Å‡∏é L5 ‡∏û‡∏¥‡πÄ‡∏®‡∏© ‡∏´‡∏≤‡∏Å Level == 5
        """
        sub_id_rules = self.contextual_rules_map.get(sub_id)
        rule_text = ""
        
        # 1. ‡∏î‡∏∂‡∏á‡∏Å‡∏é‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏¢‡πà‡∏≠‡∏¢ (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
        if sub_id_rules:
            level_key = f"L{level}"
            specific_rule = sub_id_rules.get(level_key)
            if specific_rule:
                rule_text += f"\n--- ‡∏Å‡∏é‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏¢‡πà‡∏≠‡∏¢ ({sub_id} L{level}) ---\n‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Ñ‡∏ß‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ: {specific_rule}\n"
        
        # 2. **INJECT L5 SPECIAL RULE (Safe Injection)**
        # ‡πÉ‡∏™‡πà‡∏Å‡∏é Bonus 2.0 ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô L5 ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏ö‡∏Å‡∏ß‡∏ô L3/L4
        if level == 5:
            l5_bonus_rule = """
            \n--- L5 SPECIAL RULE (Innovation & Sustainability) ---
            * **L5 PASS Condition (‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö):** ‡∏´‡∏≤‡∏Å Level ‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠ **L5** ‡∏ó‡πà‡∏≤‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏° PDCA (P+D+C+A) ‡∏ï‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô L3/L4 ‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏ô‡∏û‡∏ö‡∏Å‡πà‡∏≠‡∏ô (‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î 8.0)
            * **‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç Bonus 2.0:** ‡∏´‡∏≤‡∏Å‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏° PDCA ‡πÑ‡∏î‡πâ **‚â• 7.0** **‡πÅ‡∏•‡∏∞** ‡∏ó‡πà‡∏≤‡∏ô‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠ **‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 1 ‡∏ä‡∏¥‡πâ‡∏ô** ‡πÉ‡∏ô Context ‡∏ó‡∏µ‡πà‡πÅ‡∏™‡∏î‡∏á‡∏ñ‡∏∂‡∏á:
                * (a) ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏• KM / ‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏° (Innovation Award)
                * (b) ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡∏ó‡∏µ‡πà‡∏ß‡∏±‡∏î‡πÑ‡∏î‡πâ (ROI, Productivity, Cost Saving)
                * (c) ‡∏Å‡∏≤‡∏£‡πÄ‡∏ú‡∏¢‡πÅ‡∏û‡∏£‡πà/‡∏Å‡∏≤‡∏£‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å (External Recognition/Publication)
            * **‡πÉ‡∏´‡πâ‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡πÉ‡∏´‡πâ Bonus Score 2.0 ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ **Score ‚â• 9.0** ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡πâ‡∏á **is_passed=true** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏∞‡∏ó‡πâ‡∏≠‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏•‡∏¥‡∏® (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£ Reset ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô)
            """
            rule_text += l5_bonus_rule
            
        return rule_text

    def _load_rubric(self) -> Dict[str, Any]:
        """ Loads the SEAM rubric JSON file using path_utils. """
        
        # üéØ FIX: ‡πÉ‡∏ä‡πâ get_rubric_file_path ‡∏à‡∏≤‡∏Å path_utils ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Path ‡πÄ‡∏≠‡∏á
        filepath = None # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô UnboundLocalError
        
        try:
            # 1. ‡∏£‡∏±‡∏ö Path ‡∏à‡∏≤‡∏Å path_utils ‡∏ã‡∏∂‡πà‡∏á‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏ä‡∏µ‡πâ‡πÑ‡∏õ‡∏ó‡∏µ‡πà 'config/' ‡πÅ‡∏•‡πâ‡∏ß
            filepath = get_rubric_file_path(
                tenant=self.config.tenant,
                enabler=self.enabler_id
            )
        except Exception as e:
            # ‡∏î‡∏±‡∏Å‡∏à‡∏±‡∏ö Exception ‡∏´‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏î‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô path_utils
            self.logger.error(f"‚ùå FATAL: Error calling get_rubric_file_path: {e}")
            return {} 

        # 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if filepath is None or not os.path.exists(filepath):
            self.logger.error(f"‚ö†Ô∏è Rubric file not found at expected path: {filepath}")
            return {}

        # 3. ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå JSON
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
            self.logger.info(f"‚úÖ Rubric loaded successfully from: {filepath}")
            return data
        except json.JSONDecodeError:
            self.logger.error(f"‚ùå Error decoding Rubric JSON. File might be corrupted: {filepath}")
            return {}
        except Exception as e:
            self.logger.error(f"‚ùå Error loading Rubric file from {filepath}: {e}")
            return {}
    
    # -------------------- Helper Function for Map Processing --------------------
    def _get_level_order_value(self, level_str: str) -> int:
        """Converts Level string ('L1', 'L5') to an integer for comparison."""
        try:
            return int(level_str.upper().replace('L', ''))
        except:
            return 0

    # -------------------- Persistent Mapping Handlers (FIXED) --------------------
    def _process_temp_map_to_final_map(self, temp_map: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Converts the temporary map into the final map format for saving, 
        and filters out temporary/unresolvable evidence IDs.
        """
        working_map = temp_map or self.temp_map_for_save or {}
        final_map_for_save = {}
        total_cleaned_items = 0

        for sub_level_key, evidence_list in working_map.items():
            if isinstance(evidence_list, dict):
                evidence_list = [evidence_list]
            elif not isinstance(evidence_list, list):
                logger.warning(f"[EVIDENCE] Skipping {sub_level_key}: not a list or dict")
                continue

            clean_list = []
            seen_ids = set()
            for ev in evidence_list:
                doc_id = ev.get("doc_id")
                
                if not doc_id:
                    continue
                
                # 1. FIX: ‡∏Å‡∏£‡∏≠‡∏á ID ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß (TEMP-) ‡∏≠‡∏≠‡∏Å
                if doc_id.startswith("TEMP-"):
                    # ID ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏õ‡∏•‡∏á‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô Stable Document ID ‡πÑ‡∏î‡πâ ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÉ‡∏ä‡πâ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ
                    logger.debug(f"[EVIDENCE] Filtering out unresolvable TEMP- ID: {doc_id} for {sub_level_key}.")
                    continue 
                
                # 2. Logic ‡πÄ‡∏î‡∏¥‡∏°: ‡∏Å‡∏£‡∏≠‡∏á HASH- (Placeholder) ‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ã‡πâ‡∏≥
                if doc_id.startswith("HASH-") or doc_id in seen_ids:
                    continue
                    
                seen_ids.add(doc_id)
                clean_list.append(ev)
                total_cleaned_items += 1 

            if clean_list:
                final_map_for_save[sub_level_key] = clean_list

        logger.info(f"[EVIDENCE] Processed {len(final_map_for_save)} sub-level keys with total {total_cleaned_items} evidence items")
        return final_map_for_save

    def _clean_map_for_json(self, data: Union[Dict, List, Set, Any]) -> Union[Dict, List, Any]:
        """Recursively converts objects that cannot be serialized (like sets) into lists."""
        if isinstance(data, dict):
            return {k: self._clean_map_for_json(v) for k, v in data.items()}
        elif isinstance(data, list):
            return [self._clean_map_for_json(v) for v in data]
        elif isinstance(data, set):
            return [self._clean_map_for_json(v) for v in data]
        return data

    def _clean_temp_entries(self, evidence_map: Dict[str, List[Dict]]) -> Dict[str, List[Dict]]:
        """
        ‡∏Å‡∏£‡∏≠‡∏á TEMP-, HASH-, ‡πÅ‡∏•‡∏∞ Unknown ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å evidence map ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        ‡πÉ‡∏ä‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏ï‡∏≠‡∏ô merge ‡πÅ‡∏•‡∏∞‡∏Å‡πà‡∏≠‡∏ô save ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î 100%
        """
        if not evidence_map:
            return {}

        cleaned_map = {}
        total_removed = 0
        total_unknown_fixed = 0

        for key, entries in evidence_map.items():
            valid_entries = []
            for entry in entries:
                doc_id = entry.get("doc_id", "")

                # 1. ‡∏Å‡∏£‡∏≠‡∏á TEMP- ‡πÅ‡∏•‡∏∞ HASH- ‡∏≠‡∏≠‡∏Å‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≤‡∏î
                if str(doc_id).startswith("TEMP-") or str(doc_id).startswith("HASH-"):
                    total_removed += 1
                    continue

                # 2. ‡∏ñ‡πâ‡∏≤ doc_id ‡∏ß‡πà‡∏≤‡∏á ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏•‡∏¢ ‚Üí ‡∏ó‡∏¥‡πâ‡∏á
                if not doc_id or doc_id == "Unknown":
                    total_removed += 1
                    continue

                # 3. ‡πÅ‡∏Å‡πâ filename ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô Unknown / None / ‡∏ß‡πà‡∏≤‡∏á
                filename = entry.get("filename", "").strip()
                if not filename or filename == "Unknown" or filename.lower() == "unknown_file.pdf":
                    # ‡πÉ‡∏ä‡πâ doc_id ‡∏™‡∏±‡πâ‡∏ô ‡πÜ ‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß (‡∏î‡∏π‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤)
                    short_id = doc_id[:8]
                    entry["filename"] = f"‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á_{short_id}.pdf"
                    total_unknown_fixed += 1
                else:
                    # ‡πÄ‡∏≠‡∏≤ path ‡∏≠‡∏≠‡∏Å ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÅ‡∏Ñ‡πà‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå
                    entry["filename"] = os.path.basename(filename)

                valid_entries.append(entry)

            if valid_entries:
                cleaned_map[key] = valid_entries
            else:
                logger.debug(f"[CLEAN] Key {key} ‡∏Å‡∏•‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏ß‡πà‡∏≤‡∏á‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏£‡∏≠‡∏á ‚Üí ‡∏ñ‡∏π‡∏Å‡∏•‡∏ö‡∏≠‡∏≠‡∏Å")

        logger.info(f"[CLEANUP] ‡∏•‡∏ö TEMP-/HASH- ‡∏≠‡∏≠‡∏Å {total_removed} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | "
                    f"‡πÅ‡∏Å‡πâ Unknown filename {total_unknown_fixed} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | "
                    f"‡πÄ‡∏´‡∏•‡∏∑‡∏≠ {len(cleaned_map)} keys")

        return cleaned_map
    
    def _save_evidence_map(self, map_to_save: Optional[Dict[str, List[Dict[str, Any]]]] = None):
        """
        ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å evidence map ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢ 100% - Atomic Write + FileLock + Cleanup + Raw LLM Data
        ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å raw_llm_pdca ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÇ‡∏õ‡∏£‡πà‡∏á‡πÉ‡∏™‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏¢‡πâ‡∏≠‡∏ô‡∏Å‡∏•‡∏±‡∏ö
        """
        try:
            map_file_path = get_evidence_mapping_file_path(
                tenant=self.config.tenant,
                year=self.config.year,
                enabler=self.enabler_id
            )
        except Exception as e:
            self.logger.critical(f"[EVIDENCE] FATAL: ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Evidence Map Path ‡πÑ‡∏î‡πâ: {e}")
            raise

        lock_path = map_file_path + ".lock"
        tmp_path = None

        self.logger.info(f"[EVIDENCE] Preparing to save evidence map ‚Üí {map_file_path}")

        try:
            os.makedirs(os.path.dirname(map_file_path), exist_ok=True)

            with FileLock(lock_path, timeout=60):
                self.logger.debug("[EVIDENCE] File lock acquired.")

                # === Merge Logic ===
                if map_to_save is not None:
                    final_map_to_write = map_to_save
                else:
                    existing_map = self._load_evidence_map(is_for_merge=True) or {}
                    runtime_map = deepcopy(self.evidence_map)
                    final_map_to_write = existing_map

                    for key, new_entries in runtime_map.items():
                        entry_map = {
                            e.get("chunk_uuid", e.get("doc_id", "N/A")): e
                            for e in final_map_to_write.setdefault(key, [])
                        }
                        for new_entry in new_entries:
                            entry_id = new_entry.get("chunk_uuid", new_entry.get("doc_id", "N/A"))
                            if entry_id == "N/A" or not entry_id:
                                continue

                            new_score = new_entry.get("relevance_score", 0.0)

                            if entry_id not in entry_map:
                                entry_map[entry_id] = new_entry
                            else:
                                # Update ‡∏ñ‡πâ‡∏≤ score ‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤ ‡∏´‡∏£‡∏∑‡∏≠‡∏ñ‡πâ‡∏≤‡∏°‡∏µ raw_llm_pdca ‡∏ó‡∏µ‡πà‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Å‡∏ß‡πà‡∏≤
                                old_entry = entry_map[entry_id]
                                old_score = old_entry.get("relevance_score", 0.0)

                                if "page" not in new_entry or new_entry["page"] in ["N/A", None]:
                                    if "page" in old_entry:
                                        new_entry["page"] = old_entry["page"]
                                        
                                if new_score > old_score:
                                    entry_map[entry_id] = new_entry

                        final_map_to_write[key] = list(entry_map.values())

                if not final_map_to_write:
                    self.logger.warning("[EVIDENCE] Nothing to save (empty map).")
                    return

                # === Cleanup + Sort ===
                final_map_to_write = self._clean_temp_entries(final_map_to_write)
                for key, entries in final_map_to_write.items():
                    entries.sort(key=lambda x: x.get("relevance_score", 0.0), reverse=True)

                # === Atomic Write ===
                with tempfile.NamedTemporaryFile(
                    mode='w', delete=False, encoding="utf-8", dir=os.path.dirname(map_file_path)
                ) as tmp_file:
                    cleaned_data = self._clean_map_for_json(final_map_to_write)
                    json.dump(cleaned_data, tmp_file, indent=4, ensure_ascii=False)
                    tmp_path = tmp_file.name

                shutil.move(tmp_path, map_file_path)
                tmp_path = None

                # === Stats ===
                total_keys = len(final_map_to_write)
                total_items = sum(len(v) for v in final_map_to_write.values())
                file_size_kb = os.path.getsize(map_file_path) / 1024
                self.logger.info(
                    f"[EVIDENCE] SAVED SUCCESSFULLY! "
                    f"Keys: {total_keys} | Items: {total_items} | Size: ~{file_size_kb:.1f} KB | Path: {map_file_path}"
                )

        except Exception as e:
            self.logger.critical("[EVIDENCE] FATAL ERROR DURING SAVE")
            self.logger.exception(e)
            raise

        finally:
            # === Cleanup lock & temp file (Double Safety) ===
            if os.path.exists(lock_path):
                try:
                    os.unlink(lock_path)
                    self.logger.debug(f"[EVIDENCE] Removed lock file: {lock_path}")
                except Exception as e:
                    self.logger.warning(f"[EVIDENCE] Failed to remove lock: {e}")

            if tmp_path and os.path.exists(tmp_path):
                try:
                    os.unlink(tmp_path)
                except:
                    pass

    def _load_evidence_map(self, is_for_merge: bool = False) -> Dict[str, List[Dict[str, Any]]]:
        """
        ‡πÇ‡∏´‡∏•‡∏î evidence map ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
        is_for_merge = True ‚Üí ‡πÑ‡∏°‡πà log "No existing map" (‡πÉ‡∏ä‡πâ‡πÉ‡∏ô save)
        """
        try:
            path = get_evidence_mapping_file_path(
                tenant=self.config.tenant,
                year=self.config.year,
                enabler=self.enabler_id
            )
        except Exception as e:
            self.logger.error(f"[EVIDENCE] FATAL: ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏î‡πâ: {e}")
            return {}

        if not os.path.exists(path):
            if not is_for_merge:
                self.logger.info("[EVIDENCE] No existing evidence map found ‚Äì starting fresh.")
            return {}

        try:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
            if not is_for_merge:
                total_items = sum(len(v) for v in data.values() if isinstance(v, list))
                self.logger.info(f"[EVIDENCE] Loaded evidence map: {len(data)} keys, {total_items} items from {path}")
            return data
        except Exception as e:
            self.logger.error(f"[EVIDENCE] Failed to load evidence map from {path}: {e}")
            return {}


    def _set_mock_handlers(self, mode: str):
        """Replaces real LLM/RAG functions with mock versions."""
        if mode == "control" or mode == "random":
            if hasattr(seam_mocking, 'evaluate_with_llm_CONTROLLED_MOCK'):
                self.llm_evaluator = seam_mocking.evaluate_with_llm_CONTROLLED_MOCK
            if hasattr(seam_mocking, 'retrieve_context_with_filter_MOCK'):
                self.rag_retriever = seam_mocking.retrieve_context_with_filter_MOCK
            if hasattr(seam_mocking, 'create_structured_action_plan_MOCK'):
                self.action_plan_generator = seam_mocking.create_structured_action_plan_MOCK
            if hasattr(seam_mocking, 'set_mock_control_mode'):
                seam_mocking.set_mock_control_mode(mode == "control") 

        logger.warning(f"Engine is running in MOCK mode: {mode}")

    def _get_pdca_phase(self, level: int) -> str:
        """Helper to get the PDCA phase string from the map."""
        return PDCA_PHASE_MAP.get(level, f"Level {level} Requirement")
    

    def _get_level_constraint_prompt(self, level: int) -> str:
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á Prompt Constraint ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Å‡∏±‡∏ö‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ß‡∏∏‡∏í‡∏¥‡∏†‡∏≤‡∏ß‡∏∞‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô
        """
        if level == 1:
            return "‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏ñ‡∏∂‡∏á '‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢/‡∏ß‡∏¥‡∏™‡∏±‡∏¢‡∏ó‡∏±‡∏®‡∏ô‡πå', '‡∏Å‡∏≤‡∏£‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå', '‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏ó‡∏≥‡∏Å‡∏£‡∏≠‡∏ö‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á', ‡∏´‡∏£‡∏∑‡∏≠ '‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£' ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (L1-Focus)"
        elif level == 2:
            return "‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ô‡πâ‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞ '‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô', '‡∏Å‡∏≤‡∏£‡∏Ç‡∏±‡∏ö‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô', '‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡∏ò‡∏£‡∏£‡∏°', ‡∏´‡∏£‡∏∑‡∏≠ '‡∏Å‡∏≤‡∏£‡∏°‡∏µ‡∏™‡πà‡∏ß‡∏ô‡∏£‡πà‡∏ß‡∏°' ‡∏ï‡∏≤‡∏°‡πÅ‡∏ú‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (L2-Focus)"
        elif level == 3:
            # üö® HARD RULE: ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏ä‡πâ L3 Logic (Check/Act Focus) ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏° Context ‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡∏°‡πà
            return """
‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î (HARD RULE: L3 CHECK/ACT FOCUS):
1. ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô L3 ‡∏ô‡∏µ‡πâ **‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô '‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö (Check)' ‡πÅ‡∏•‡∏∞ '‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á (Act)' ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô**
2. ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÅ‡∏•‡πâ‡∏ß: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏ï‡πâ‡∏ô‡∏Ç‡∏≠‡∏á Context ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î (Priority 1)
3. ‡∏´‡∏≤‡∏Å‡∏û‡∏ö‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ **[L3_SUMMARY_EVIDENCE]** ‡πÉ‡∏´‡πâ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô **‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö** ‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠‡πÑ‡∏î‡πâ‡∏ã‡∏∂‡πà‡∏á‡∏ñ‡∏π‡∏Å‡∏™‡∏£‡∏∏‡∏õ‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô Check/Act ‡∏à‡∏£‡∏¥‡∏á (‡∏à‡∏±‡∏î‡πÄ‡∏õ‡πá‡∏ô Priority 1)
4. ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô Plan ‡πÅ‡∏•‡∏∞ Do ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏ï‡∏≠‡∏ô‡∏ó‡πâ‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á Context **‡∏´‡πâ‡∏≤‡∏°‡∏ô‡∏≥‡∏°‡∏≤‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤** ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à **FAIL** ‡∏´‡∏≤‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô Check/Act ‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô
5. ‡∏´‡∏≤‡∏Å‡∏Ç‡∏≤‡∏î‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô **Check** ‡∏´‡∏£‡∏∑‡∏≠ **Act** ‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠ (‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏à‡∏≤‡∏Å Summary Evidence ‡∏´‡∏£‡∏∑‡∏≠‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á) ‡πÉ‡∏´‡πâ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÄ‡∏õ‡πá‡∏ô **‚ùå FAIL** ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô L3 ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡∏à‡∏£‡∏¥‡∏á
(L3-Focus: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏° ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏• ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á)
"""
        elif level == 4:
            return "‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ñ‡∏∂‡∏á '‡∏Å‡∏≤‡∏£‡∏ö‡∏π‡∏£‡∏ì‡∏≤‡∏Å‡∏≤‡∏£', '‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á', ‡∏´‡∏£‡∏∑‡∏≠ '‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå' ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (L4-Focus)"
        elif level == 5:
            return "‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ñ‡∏∂‡∏á '‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°', '‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡πà‡∏≤‡∏ó‡∏≤‡∏á‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à', ‡∏´‡∏£‡∏∑‡∏≠ '‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß' ‡πÇ‡∏î‡∏¢‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô (L5-Focus)"
        else:
            return ""
        

    def _classify_pdca_phase_for_chunk(
        self, 
        chunk_text: str
    ) -> Literal["Plan", "Do", "Check", "Act", "Other"]:
        """
        ‡πÉ‡∏ä‡πâ LLM ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏∞‡∏¢‡∏∞‡πÉ‡∏î‡∏£‡∏∞‡∏¢‡∏∞‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏Ç‡∏≠‡∏á PDCA ‡∏´‡∏£‡∏∑‡∏≠ 'Other'
        """
        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏ü‡∏™ PDCA
        pdca_phases_th = ["‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô", "‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥", "‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö", "‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á"]
        
        # 1. System Prompt ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢: ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö JSON 100%
        system_prompt = (
            "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô PDCA Cycle\n"
            "‡∏†‡∏≤‡∏£‡∏Å‡∏¥‡∏à‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô ‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏±‡∏î‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ß‡πà‡∏≤‡πÄ‡∏ô‡πâ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡πÉ‡∏î‡∏Ç‡∏≠‡∏á PDCA\n"
            f"‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏´‡∏ô‡∏∂‡πà‡∏á‡πÉ‡∏ô: {', '.join(pdca_phases_th)} ‡∏´‡∏£‡∏∑‡∏≠ '‡∏≠‡∏∑‡πà‡∏ô‡πÜ'\n\n"
            "‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡∏î‡πâ‡∏ß‡∏¢ **JSON Object ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô** ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö:\n"
            "{\"phase\": \"‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô\"}\n"
            "‡∏´‡∏£‡∏∑‡∏≠ {\"phase\": \"‡∏≠‡∏∑‡πà‡∏ô‡πÜ\"}\n"
            "‡∏´‡πâ‡∏≤‡∏°‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏≠‡∏Å JSON ‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≤‡∏î"
        )

        # 2. User Prompt: ‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô + ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
        user_prompt = (
            f"‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô:\n\"\"\"\n{chunk_text.strip()}\n\"\"\"\n\n"
            "‡∏Ñ‡∏≥‡∏ô‡∏¥‡∏¢‡∏≤‡∏°‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏ü‡∏™:\n"
            "- ‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô: ‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢, ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå, ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢, ‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô, ‡∏Ñ‡∏ì‡∏∞‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£\n"
            "- ‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥: ‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£, ‡∏ù‡∏∂‡∏Å‡∏≠‡∏ö‡∏£‡∏°, ‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£, ‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏£‡∏∞‡∏ö‡∏ö, ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á\n"
            "- ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö: ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏ú‡∏•, ‡∏ß‡∏±‡∏î‡∏ú‡∏•, ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô, ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏†‡∏≤‡∏¢‡πÉ‡∏ô, ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n"
            "- ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á: ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç, ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á, ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡πÉ‡∏´‡∏°‡πà, Lesson Learned, ‡∏õ‡∏¥‡∏î‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á\n\n"
            "‡∏ï‡∏≠‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞ JSON:"
        )
        
        try:
            raw_response = _fetch_llm_response(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                temperature=0.0,  # ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å! ‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô
                max_retries=2,
                llm_executor=self.llm
            )

            if not raw_response:
                return "Other"

            # ‡∏î‡∏∂‡∏á JSON ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏Ç‡πá‡∏á‡πÅ‡∏Å‡∏£‡πà‡∏á (‡πÉ‡∏ä‡πâ _robust_extract_json ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß!)
            parsed = _robust_extract_json(raw_response)
            
            # ‡∏ñ‡πâ‡∏≤ _robust_extract_json ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ fallback ‡∏î‡πâ‡∏ß‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ‡πÄ‡∏ö‡∏™‡∏¥‡∏Å
            if not parsed or not isinstance(parsed, dict):
                # ‡∏•‡∏≠‡∏á‡∏î‡∏∂‡∏á‡∏î‡πâ‡∏ß‡∏¢ regex ‡∏á‡πà‡∏≤‡∏¢ ‡πÜ
                match = re.search(r'"phase"\s*:\s*"([^"]+)"', raw_response, re.IGNORECASE)
                if match:
                    phase_th = match.group(1).strip()
                else:
                    phase_th = "‡∏≠‡∏∑‡πà‡∏ô‡πÜ"
            else:
                phase_th = parsed.get("phase", parsed.get("classification", "‡∏≠‡∏∑‡πà‡∏ô‡πÜ"))
                phase_th = str(phase_th).strip()

            # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô Literal ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
            mapping = {
                "‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô": "Plan",
                "‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥": "Do",
                "‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö": "Check",
                "‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á": "Act",
                "‡∏≠‡∏∑‡πà‡∏ô‡πÜ": "Other",
                "‡∏≠‡∏∑‡πà‡∏ô": "Other",
                "other": "Other"
            }
            result = mapping.get(phase_th, "Other")
            
            self.logger.debug(f"PDCA Classification: '{phase_th}' ‚Üí {result}")
            return result

        except Exception as e:
            self.logger.error(f"PDCA Classification failed: {e}\nRaw: {raw_response[:200]}")
            return "Other"

    # -------------------- Statement Preparation & Filtering Helpers --------------------
    def _flatten_rubric_to_statements(self) -> List[Dict[str, Any]]:
        """
        Transforms the hierarchical rubric structure loaded in self.rubric
        into a flat list of statements ready for assessment.
        """
        if not self.rubric:
            self.logger.warning("Cannot flatten rubric: self.rubric is empty.")
            return []
            
        data = deepcopy(self.rubric)
        extracted_list = []
        
        if not isinstance(data, dict):
             self.logger.error("Rubric data structure is invalid (expected dict of criteria).")
             return []
             
        # for criteria_id, criteria_data in data.items():
        for criteria_id, criteria_data in data.get('criteria', {}).items():
            # üéØ FIX 1: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Criteria Data
            if not isinstance(criteria_data, dict):
                self.logger.warning(f"Skipping malformed criteria entry: {criteria_id} (not a dict).")
                continue
                
            sub_criteria_map = criteria_data.get('subcriteria', {})
            criteria_name = criteria_data.get('name')
            
            # üéØ FIX 2: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Sub-criteria Map
            if not isinstance(sub_criteria_map, dict):
                 self.logger.warning(f"Skipping criteria {criteria_id}: 'subcriteria' is not a dictionary.")
                 continue

            for sub_id, sub_data in sub_criteria_map.items():
                
                # üéØ FIX 3: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ sub_data ‡πÄ‡∏õ‡πá‡∏ô Dictionary ‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô TypeError)
                if not isinstance(sub_data, dict):
                    self.logger.warning(
                        f"Skipping malformed sub-criteria entry: {criteria_id}.{sub_id} "
                        f"is not a dictionary (found type: {type(sub_data).__name__})."
                    )
                    continue
                
                # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤ Metadata ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
                sub_data['criteria_id'] = criteria_id
                sub_data['criteria_name'] = criteria_name
                sub_data['sub_id'] = sub_id 
                sub_data['sub_criteria_name'] = sub_data.get('name', criteria_name + ' sub')
                if 'weight' not in sub_data:
                    sub_data['weight'] = criteria_data.get('weight', 0)
                extracted_list.append(sub_data)

        # Re-check and re-sort levels
        final_list = []
        for sub_criteria in extracted_list: 
            
            # üéØ FIX 4: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á Level ‡∏à‡∏≤‡∏Å Dict ‡∏ó‡∏µ‡πà‡∏°‡∏µ Key ‡πÄ‡∏õ‡πá‡∏ô String ‡πÄ‡∏õ‡πá‡∏ô List
            if "levels" in sub_criteria and isinstance(sub_criteria["levels"], dict):
                levels_list = []
                for level_str, statement in sub_criteria["levels"].items():
                    try:
                        # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÅ‡∏õ‡∏•‡∏á Level Key ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô Integer
                        level_int = int(level_str)
                        if isinstance(statement, str):
                            levels_list.append({"level": level_int, "statement": statement})
                        else:
                            self.logger.warning(f"Level {level_str} statement in {sub_criteria.get('sub_id')} is not a string.")
                    except ValueError:
                        self.logger.error(f"Invalid level key '{level_str}' found in {sub_criteria.get('sub_id', 'UNKNOWN_ID')}. Skipping.")
                        continue
                        
                sub_criteria["levels"] = levels_list
            
            # ‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏£‡∏∞‡∏î‡∏±‡∏ö
            if "levels" in sub_criteria and isinstance(sub_criteria["levels"], list):
                 sub_criteria["levels"].sort(key=lambda x: x.get("level", 0))
                 final_list.append(sub_criteria)
            else:
                 self.logger.warning(f"Sub-criteria {sub_criteria.get('sub_id', 'UNKNOWN_ID')} missing 'levels' list.")


        return final_list

    def _load_initial_evidence_info(self) -> Set[str]:
        """Retrieves the set of all available stable document IDs in the VectorStore."""
        if self.config.mock_mode != "none":
            return {"00000000-0000-0000-0000-000000000001"}
        return set() 

    def _apply_strict_filter(self, statements: List[Dict[str, Any]], available_evidence_ids: Set[str]) -> List[Dict[str, Any]]:
        """
        Filters out statements that have no specified evidence_doc_ids 
        that match any ID found in the available_evidence_ids set.
        """
        if not available_evidence_ids:
            logger.warning("Strict Filter bypassed: No available evidence IDs loaded.")
            return statements

        filtered_statements = []
        for stmt in statements:
            required_ids = set(stmt.get('evidence_doc_ids', []))
            
            if not required_ids or required_ids.isdisjoint(available_evidence_ids):
                 logger.debug(f"Strict Filter: Skipping {stmt['sub_id']} L{stmt['level']} (No evidence match)")
                 continue
            
            filtered_statements.append(stmt)
            
        return filtered_statements
    
# -------------------- Evidence Classification Helper (Optimized) --------------------
    def _get_mapped_uuids_and_priority_chunks(
        self,
        sub_id: str,
        level: int,
        statement_text: str,
        level_constraint: str,
        vectorstore_manager: Optional['VectorStoreManager']
    ) -> Tuple[List[str], List[Dict]]:
        """
        ‡∏î‡∏∂‡∏á Priority Chunks ‡∏à‡∏≤‡∏Å Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ + Hydrate ‡πÄ‡∏ï‡πá‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
        üü¢ FIX: ‡πÄ‡∏û‡∏¥‡πà‡∏° Pre-boost ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Score 0.0000
        """
        priority_chunks = []
        mapped_stable_ids = []

        # 1. ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å evidence_map (L1 ‚Üí L2, L2 ‚Üí L3 ‡∏Ø‡∏•‡∏Ø)
        for key, evidences in self.evidence_map.items():
            if key.startswith(f"{sub_id}.L") and evidences:
                try:
                    prev_level = int(key.split(".L")[-1])
                    if prev_level < level:
                        priority_chunks.extend(evidences)
                except:
                    continue

        if not priority_chunks:
            self.logger.info(f"No priority chunks found for {sub_id} L{level}")
            return mapped_stable_ids, []

        # üü¢ FIX 1: PRE-BOOST BEFORE HYDRATION
        # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Score 0.0000 ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏Å‡∏£‡∏≠‡∏á‡∏≠‡∏≠‡∏Å‡∏†‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏á
        for chunk in priority_chunks:
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ Score ‡∏´‡∏£‡∏∑‡∏≠ Score = 0 ‚Üí Set ‡∏Ñ‡πà‡∏≤ Default 0.80
            if "rerank_score" not in chunk or chunk.get("rerank_score", 0.0) == 0.0:
                chunk["rerank_score"] = 0.80
            if "score" not in chunk or chunk.get("score", 0.0) == 0.0:
                chunk["score"] = 0.80
            
            # ‚≠ê CRITICAL: ‡∏ï‡∏±‡πâ‡∏á Flag is_baseline = True ‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ
            chunk["is_baseline"] = True

        self.logger.info(f"Pre-boosted {len(priority_chunks)} priority chunks (Score set to 0.80, is_baseline=True)")

        # 2. ‡∏ó‡∏≥ Robust Hydration ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà‡πÄ‡∏•‡∏¢!
        priority_chunks = self._robust_hydrate_documents_for_priority_chunks(
            chunks_to_hydrate=priority_chunks,
            vsm=vectorstore_manager
        )

        # 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á mapped_stable_ids ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö RAG Retriever
        for chunk in priority_chunks:
            sid = chunk.get("stable_doc_uuid") or chunk.get("doc_id")
            if sid and isinstance(sid, str) and len(sid.replace("-", "")) >= 64:
                mapped_stable_ids.append(sid)

        self.logger.info(f"PRIORITY HYDRATED ‚Üí {len(priority_chunks)} chunks ready for L{level} (with full text + baseline flag)")

        return mapped_stable_ids, priority_chunks

    # -------------------- Calculation Helpers (ADDED) --------------------
    def _calculate_weighted_score(self, highest_full_level: int, weight: int) -> float:
        """
        Calculates the weighted score based on the highest full level achieved.
        Score is calculated by: (Level / MAX_LEVEL) * Weight
        """
        # üéØ ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å MAX_LEVEL_CALC ‡πÄ‡∏õ‡πá‡∏ô MAX_LEVEL ‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏°‡∏≤‡∏à‡∏≤‡∏Å global_vars
        from config.global_vars import MAX_LEVEL  
        
        if highest_full_level <= 0:
            return 0.0
        
        # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô (‡πÄ‡∏ä‡πà‡∏ô ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ data ‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î)
        level_for_calc = min(highest_full_level, MAX_LEVEL)
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ñ‡πà‡∏ß‡∏á‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏ï‡∏≤‡∏°‡πÄ‡∏û‡∏î‡∏≤‡∏ô‡πÄ‡∏•‡πÄ‡∏ß‡∏•‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
        score = (level_for_calc / MAX_LEVEL) * weight
        return score

    def _calculate_overall_stats(self, target_sub_id: str):
        """
        Calculates overall statistics from sub-criteria results 
        and stores them in self.total_stats.
        """
        from config.global_vars import MAX_LEVEL
        
        results = self.final_subcriteria_results
        
        # ---------------------------------------------------------
        # 1. ‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå (Safety Guard)
        # ---------------------------------------------------------
        if not results:
            self.total_stats = {
                "Overall Maturity Score (Avg.)": 0.0,
                "Overall Maturity Level (Weighted)": "L0",
                "Number of Sub-Criteria Assessed": 0,
                "Total Weighted Score Achieved": 0.0,
                "Total Possible Weight": 0.0,
                "Overall Progress Percentage (0.0 - 1.0)": 0.0,
                "percentage_achieved_run": 0.0,
                "total_subcriteria": len(self._flatten_rubric_to_statements()),
                "target_level": self.config.target_level,
                "enabler": self.config.enabler,
                "sub_criteria_id": target_sub_id,
                "status": "No Data"
            }
            return

        # ---------------------------------------------------------
        # 2. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ú‡∏•‡∏£‡∏ß‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô (Summation)
        # ---------------------------------------------------------
        # weighted_score ‡∏Ñ‡∏∑‡∏≠ (Level / 5) * Weight ‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏ô‡∏±‡πâ‡∏ô‡πÜ
        total_weighted_score_achieved = sum(r.get('weighted_score', 0.0) for r in results)
        
        # total_possible_weight ‡∏Ñ‡∏∑‡∏≠ ‡∏ú‡∏•‡∏£‡∏ß‡∏°‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏Ç‡∏≠‡∏á Sub-criteria ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÉ‡∏ô‡∏£‡∏≠‡∏ö‡∏ô‡∏µ‡πâ
        # ‡πÄ‡∏ä‡πà‡∏ô ‡∏ñ‡πâ‡∏≤‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÅ‡∏Ñ‡πà 1.2 ‡∏à‡∏∞‡πÑ‡∏î‡πâ 4.0 ‡πÅ‡∏ï‡πà‡∏ñ‡πâ‡∏≤‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ó‡∏±‡πâ‡∏á Enabler ‡∏à‡∏∞‡πÑ‡∏î‡πâ 40.0
        total_possible_weight = sum(r.get('weight', 0.0) for r in results)

        # ---------------------------------------------------------
        # 3. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Maturity Score ‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ (0.0 - 5.0)
        # ---------------------------------------------------------
        overall_avg_score = 0.0
        if total_possible_weight > 0:
            # ‡∏™‡∏π‡∏ï‡∏£: ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ / ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏£‡∏ß‡∏° = ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡πÄ‡∏•‡πÄ‡∏ß‡∏• (1-5)
            overall_avg_score = (total_weighted_score_achieved / total_possible_weight) * MAX_LEVEL
            overall_avg_score = round(overall_avg_score, 2) 
        
        # ---------------------------------------------------------
        # 4. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Progress (%) ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏ï‡πá‡∏° (Max Possible)
        # ---------------------------------------------------------
        overall_progress_percentage = 0.0
        # ‡πÄ‡∏û‡∏î‡∏≤‡∏ô‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡∏à‡∏∞‡πÑ‡∏î‡πâ (Weight ‡∏£‡∏ß‡∏° * 5)
        max_possible_points = total_possible_weight * MAX_LEVEL
        
        if max_possible_points > 0:
            overall_progress_percentage = total_weighted_score_achieved / max_possible_points
            overall_progress_percentage = round(overall_progress_percentage, 4)

        # ---------------------------------------------------------
        # 5. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Label ‡∏Ç‡∏≠‡∏á Maturity Level (L1 - L5)
        # ---------------------------------------------------------
        # ‡πÉ‡∏ä‡πâ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡∏õ‡∏±‡∏î‡πÄ‡∏®‡∏© (Round) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏Ñ‡πà‡∏≤ Level ‡∏ó‡∏µ‡πà‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
        highest_level_achieved = round(overall_avg_score)
        final_level = min(max(int(highest_level_achieved), 0), MAX_LEVEL)
        overall_level_label = f"L{final_level}"
        
        # ---------------------------------------------------------
        # 6. ‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏õ‡∏≠‡∏£‡πå‡πÄ‡∏ã‡πá‡∏ô‡∏ï‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (0-100%)
        # ---------------------------------------------------------
        percentage_achieved_run = round(overall_progress_percentage * 100, 1)

        # ---------------------------------------------------------
        # 7. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ñ‡πà‡∏≤‡∏•‡∏á‡πÉ‡∏ô stats object
        # ---------------------------------------------------------
        self.total_stats = {
            "Overall Maturity Score (Avg.)": overall_avg_score,
            "Overall Maturity Level (Weighted)": overall_level_label,
            "Number of Sub-Criteria Assessed": len(results),
            "Total Weighted Score Achieved": round(total_weighted_score_achieved, 2),
            "Total Possible Weight": total_possible_weight,
            "Overall Progress Percentage (0.0 - 1.0)": overall_progress_percentage,
            "percentage_achieved_run": percentage_achieved_run,
            "total_subcriteria": len(self._flatten_rubric_to_statements()),
            "target_level": self.config.target_level,
            "enabler": self.config.enabler,
            "sub_criteria_id": target_sub_id,
            "gap_to_full_score": round(total_possible_weight - total_weighted_score_achieved, 2)
        }
        
        self.logger.info(f"--- ASSESSMENT SUMMARY ---")
        self.logger.info(f"Enabler: {self.config.enabler} | Sub: {target_sub_id}")
        self.logger.info(f"Maturity: {overall_level_label} (Avg Score: {overall_avg_score})")
        self.logger.info(f"Score: {total_weighted_score_achieved}/{total_possible_weight} ({percentage_achieved_run}%)")
        self.logger.info(f"---------------------------")

    def _export_results(self, results: dict, sub_criteria_id: str, **kwargs) -> str:
        """
        Exports the assessment results to a JSON file.
        Includes enhanced summary stats and persistence support with record_id.
        """
        # --- 0. ‡∏î‡∏∂‡∏á record_id ‡∏à‡∏≤‡∏Å kwargs (‡∏™‡πà‡∏á‡∏°‡∏≤‡∏à‡∏≤‡∏Å run_assessment) ---
        record_id = kwargs.get("record_id", "no_id")

        enabler = self.enabler_id
        target_level = self.config.target_level
        tenant = self.config.tenant
        year = self.config.year
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # --- 1. ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Path ‡πÅ‡∏•‡∏∞‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå (‡πÄ‡∏û‡∏¥‡πà‡∏° record_id ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Router ‡∏´‡∏≤‡πÄ‡∏à‡∏≠) ---
        # ‡∏õ‡∏£‡∏±‡∏ö Suffix ‡πÉ‡∏´‡πâ‡∏°‡∏µ ID: assessment_results_b0deb65560b3_1.1_20251218...
        suffix = f"results_{record_id}_{sub_criteria_id}_{timestamp}"

        full_path = ""
        export_dir = ""

        try:
            if self.config.export_path:
                export_dir = self.config.export_path
                file_name = f"assessment_{enabler}_{record_id}_{sub_criteria_id}_{timestamp}.json"
                full_path = os.path.join(export_dir, file_name)
            else:
                # ‡πÉ‡∏ä‡πâ utility ‡∏ä‡πà‡∏ß‡∏¢‡∏™‡∏£‡πâ‡∏≤‡∏á path ‡πÇ‡∏î‡∏¢‡πÉ‡∏™‡πà suffix ‡∏ó‡∏µ‡πà‡∏°‡∏µ record_id
                full_path = get_assessment_export_file_path(
                    tenant=tenant,
                    year=year,
                    enabler=enabler,
                    suffix=suffix,
                    ext="json"
                )
                export_dir = os.path.dirname(full_path)

        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Path utility failed, using fallback: {e}")
            export_dir = os.path.join("data_store", tenant, "exports", str(year), enabler)
            file_name = f"assessment_{enabler}_{record_id}_{sub_criteria_id}_{timestamp}.json"
            full_path = os.path.join(export_dir, file_name)

        if not os.path.exists(export_dir):
            os.makedirs(export_dir, exist_ok=True)

        # --- 2. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£/‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Summary Field ---
        if 'summary' not in results:
            results['summary'] = {}
        
        summary = results['summary']
        summary['record_id'] = record_id  # ‡∏ù‡∏±‡∏á ID ‡∏•‡∏á‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå JSON ‡∏î‡πâ‡∏ß‡∏¢
        summary['enabler'] = enabler
        summary['sub_criteria_id'] = sub_criteria_id
        summary['target_level'] = target_level
        summary['tenant'] = tenant
        summary['year'] = year
        summary['export_timestamp'] = timestamp

        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å sub_criteria_results ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏°‡∏≤‡∏ó‡∏≥ summary
        sub_res_list = results.get('sub_criteria_results', [])
        
        if sub_criteria_id.lower() != "all" and len(sub_res_list) > 0:
            # ‡∏Å‡∏£‡∏ì‡∏µ‡∏£‡∏±‡∏ô Single Sub-Criteria
            main_res = sub_res_list[0]
            summary['highest_pass_level'] = main_res.get('highest_full_level', 0)
            summary['achieved_weight'] = main_res.get('weighted_score', 0.0)
            summary['total_weight'] = main_res.get('weight', 0.0)
            summary['is_target_achieved'] = main_res.get('target_level_achieved', False)
            summary['total_subcriteria_assessed'] = 1
        else:
            # ‡∏Å‡∏£‡∏ì‡∏µ‡∏£‡∏±‡∏ô All Sub-Criteria
            all_pass_levels = [r.get('highest_full_level', 0) for r in sub_res_list]
            total_achieved = sum(r.get('weighted_score', 0.0) for r in sub_res_list)
            total_possible = sum(r.get('weight', 0.0) for r in sub_res_list)
            
            summary['highest_pass_level_overall'] = max(all_pass_levels) if all_pass_levels else 0
            summary['total_achieved_weight'] = round(total_achieved, 2)
            summary['total_possible_weight'] = round(total_possible, 2)
            summary['total_subcriteria_assessed'] = len(sub_res_list)
            
            if total_possible > 0:
                summary['overall_percentage'] = round((total_achieved / total_possible) * 100, 2)
            else:
                summary['overall_percentage'] = 0.0

        # --- 3. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Action Plan Status ---
        total_action_plans = 0
        for res in sub_res_list:
            ap = res.get('action_plan', [])
            if isinstance(ap, list):
                total_action_plans += len(ap)
        summary['total_action_plan_phases'] = total_action_plans

        # --- 4. ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÑ‡∏ü‡∏•‡πå JSON ---
        try:
            with open(full_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=4)
            
            self.logger.info(f"üíæ Exported Results to: {full_path}")
            
            # ‡πÅ‡∏™‡∏î‡∏á Log Summary
            final_lvl = summary.get('highest_pass_level', summary.get('highest_pass_level_overall', 0))
            final_score = summary.get('achieved_weight', summary.get('total_achieved_weight', 0.0))
            total_score = summary.get('total_weight', summary.get('total_possible_weight', 0.0))
            
            self.logger.info(
                f"üìä [SUMMARY] ID: {record_id} | Sub: {sub_criteria_id} | "
                f"Level: L{final_lvl} | Score: {final_score}/{total_score}"
            )
            return full_path
        
        except Exception as e:
            self.logger.error(f"‚ùå Export failed: {e}")
            return ""
        
    def rephrase_query_for_retry(self, original_query: str, level: int, sub_id: str) -> str:
        """
        Helper method to slightly rephrase the query for the next retrieval attempt.
        """
        self.logger.info(f"Rephrasing query for L{level} retry: {original_query[:50]}...")
        # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö query: ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏ï‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á‡∏≠‡∏≠‡∏Å ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ LLM ‡∏ä‡πà‡∏ß‡∏¢ rephrase 
        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÅ‡∏Ñ‡πà‡∏Ñ‡∏∑‡∏ô query ‡πÄ‡∏î‡∏¥‡∏° ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ '‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î'
        if level >= 3:
            # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Level ‡∏™‡∏π‡∏á‡πÜ ‡∏•‡∏≠‡∏á‡πÄ‡∏ô‡πâ‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ó‡∏µ‡πà‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡∏Ç‡∏∂‡πâ‡∏ô
            return f"‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö {original_query}"
        return original_query
    
    def _create_error_result(
        self,
        level: int,
        error_message: str,
        start_time: float,
        retrieval_duration: float,
        sub_id: str,
        statement_id: str,
        statement_text: str,
        llm_duration: float = 0.0,
    ) -> Dict[str, Any]:
        """
        Generates a standard error dictionary for when RAG or LLM fails.
        """
        # NOTE: Assumes self._get_pdca_phase is defined
        try:
            pdca_phase = self._get_pdca_phase(level) 
        except Exception:
            pdca_phase = "N/A"
            
        pdca_breakdown = {p: 0 for p in ['P', 'D', 'C', 'A']}
        total_duration = time.time() - start_time

        self.logger.error(f"FATAL ERROR RESULT for {sub_id} L{level}: {error_message}")

        return {
            "sub_criteria_id": sub_id,
            "statement_id": statement_id,
            "level": level,
            "statement": statement_text,
            "pdca_phase": pdca_phase,
            "llm_score": 0.0,
            "pdca_breakdown": pdca_breakdown,
            "is_passed": False,
            "status": "FAIL",
            "score": 0.0,
            "llm_result_full": {"error": error_message, "details": "Assessment skipped due to critical failure."},
            "retrieval_duration_s": round(retrieval_duration, 2),
            "llm_duration_s": round(llm_duration, 2),
            "top_evidences_ref": [],
            "temp_map_for_level": [],
            "evidence_strength": 0.0,
            "ai_confidence": "LOW",
            "evidence_count": 0,
            "pdca_coverage": 0.0,
            "direct_evidence_count": 0,
            "rag_query": statement_text,
            "full_context_meta": {"error_type": "Critical Failure"},
            # üü¢ NEW: Relevant Score Gate Metadata (Set to default error values)
            "max_relevant_score": 0.0,
            "max_relevant_source": "ERROR_HANDLING",
            "is_evidence_strength_capped": False,
            "max_evidence_strength_used": 0.0,
            "total_run_time_s": round(total_duration, 2)
        }

    def _save_level_evidences_and_calculate_strength(
        self, 
        level_temp_map: List[Dict[str, Any]], 
        sub_id: str, 
        level: int, 
        llm_result: Dict[str, Any],
        highest_rerank_score: float = 0.0
    ) -> float:
        """
        [FULL REFINED VERSION 25.1]
        ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Evidence Strength 
        ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Metadata ‡πÅ‡∏ö‡∏ö Robust ‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏à‡∏≤‡∏Å Direct Key ‡πÅ‡∏•‡∏∞ Nested Metadata
        ‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏à‡∏≤‡∏Å ingest.py (deterministic UUIDs)
        """
        map_key = f"{sub_id}.L{level}"
        new_evidence_list: List[Dict[str, Any]] = []
        
        # ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Debug ‡∏´‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏î‡∏õ‡∏±‡∏ç‡∏´‡∏≤
        self.logger.info(f"üíæ [EVI SAVE] Starting save for {map_key} with {len(level_temp_map)} potential chunks.")

        for chunk in level_temp_map:
            # üéØ 1. ‡∏î‡∏∂‡∏á Metadata ‡πÅ‡∏•‡∏∞ Identifiers (Robust Extraction)
            # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á Dictionary, Object ‡∏ó‡∏µ‡πà‡∏°‡∏µ metadata field ‡∏´‡∏£‡∏∑‡∏≠‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏ö‡∏ô‡∏£‡∏≤‡∏ö (Flattened)
            meta = chunk.get("metadata", {}) if isinstance(chunk.get("metadata"), dict) else {}
            
            # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Chunk UUID (‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: Direct -> Metadata -> id)
            chunk_uuid_key = (
                chunk.get("chunk_uuid") or 
                meta.get("chunk_uuid") or 
                chunk.get("id")
            )
            
            # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Stable Doc UUID (‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: Direct -> Metadata -> doc_id)
            stable_doc_uuid_key = (
                chunk.get("stable_doc_uuid") or 
                chunk.get("doc_id") or 
                meta.get("stable_doc_uuid") or 
                meta.get("doc_id")
            )

            # üéØ 2. Fallback Logic: ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡∏´‡∏≤ ID ‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Data Loss)
            if not chunk_uuid_key and stable_doc_uuid_key:
                # ‡πÉ‡∏ä‡πâ Stable Doc ID + fake suffix ‡∏ñ‡πâ‡∏≤‡∏´‡∏≤ chunk uuid ‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡∏à‡∏£‡∏¥‡∏á‡πÜ
                chunk_uuid_key = f"{stable_doc_uuid_key}_missing_uuid"
                self.logger.warning(f"‚ö†Ô∏è [EVI SAVE] Missing chunk_uuid for doc {stable_doc_uuid_key[:8]}. Using Fallback.")

            # üéØ 3. Validation: ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ ID ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÄ‡∏•‡∏¢ ‡πÉ‡∏´‡πâ Skip ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏Ç‡∏≠‡∏á Database
            if not stable_doc_uuid_key or not chunk_uuid_key:
                self.logger.error(f"‚ùå [EVI SAVE] Critical ID Missing! Skipping chunk from source: {chunk.get('source', 'Unknown')}")
                continue

            # üéØ 4. ‡∏™‡∏£‡πâ‡∏≤‡∏á Evidence Entry (‡∏≠‡∏¥‡∏á‡∏ï‡∏≤‡∏°‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô ingest.py)
            # ‡∏î‡∏∂‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏Å‡∏£‡∏∞‡∏î‡∏≤‡∏©‡πÇ‡∏î‡∏¢‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Å‡∏±‡∏ö page_label ‡∏Å‡πà‡∏≠‡∏ô (‡πÄ‡∏û‡∏∑‡πà‡∏≠ UI ‡∏ó‡∏µ‡πà‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°)
            page_val = (
                meta.get("page_label") or 
                chunk.get("page") or 
                meta.get("page") or 
                chunk.get("page_number") or 
                "N/A"
            )

            evidence_entry = {
                "sub_id": sub_id,
                "level": level,
                "relevance_score": float(chunk.get("rerank_score", chunk.get("score", 0.0))),
                "doc_id": str(stable_doc_uuid_key),
                "stable_doc_uuid": str(stable_doc_uuid_key),
                "chunk_uuid": str(chunk_uuid_key),
                "source": chunk.get("source") or meta.get("source") or "N/A",
                "source_filename": chunk.get("filename") or meta.get("source_filename") or "N/A",
                "page": str(page_val),
                "pdca_tag": chunk.get("pdca_tag") or meta.get("pdca_tag") or "Other", 
                "status": "PASS" if llm_result.get("is_passed") else "FAIL", 
                "timestamp": datetime.now().isoformat(),
            }
            new_evidence_list.append(evidence_entry)

        # üéØ 5. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Evidence Strength (Evi Str)
        # ‡πÉ‡∏ä‡πâ logic ‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Cap ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡πÑ‡∏ß‡πâ
        evi_cap_data = self._calculate_evidence_strength_cap(
            top_evidences=new_evidence_list,
            level=level,
            highest_rerank_score=highest_rerank_score
        )
        
        final_evi_str = evi_cap_data.get('max_evi_str_for_prompt', 0.0)

        # üéØ 6. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏Ç‡πâ‡∏≤ Memory Maps (‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏•‡∏±‡∏Å‡πÅ‡∏•‡∏∞‡∏™‡∏≥‡∏£‡∏≠‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Worker)
        self.evidence_map.setdefault(map_key, []).extend(new_evidence_list)
        self.temp_map_for_save.setdefault(map_key, []).extend(new_evidence_list)
        
        # üéØ 7. Log ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•
        self.logger.info(f"‚úÖ [EVIDENCE SAVED] {map_key}: {len(new_evidence_list)} chunks | Strength: {final_evi_str}")
        
        return final_evi_str
        
    def _calculate_evidence_strength_cap(
        self,
        top_evidences: List[Union[Dict[str, Any], Any]],
        level: int,
        highest_rerank_score: Optional[float] = None
    ) -> Dict[str, Any]:
        """
        Relevant Score Gate ‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô FINAL
        
        ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Evidence Strength ‡πÇ‡∏î‡∏¢‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å Rerank Score ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏û‡∏ö
        - ‡∏î‡∏∂‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏à‡∏≤‡∏Å metadata, top-level key, ‡πÅ‡∏•‡∏∞ regex fallback
        - ‡∏¢‡∏∂‡∏î‡∏ï‡∏≤‡∏° global_vars:
            ‚Ä¢ RERANK_THRESHOLD = 0.5
            ‚Ä¢ MAX_EVI_STR_CAP = 10.0
        
        Returns:
            dict ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢ is_capped, max_evi_str_for_prompt, highest_rerank_score, max_score_source
        """

        score_keys = [
            "rerank_score", "score", "relevance_score",
            "_rerank_score_force", "_rerank_score",
            "Score", "RelevanceScore"
        ]

        # ‚îÄ‚îÄ‚îÄ 1. ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤ config ‡∏à‡∏≤‡∏Å class attribute ‡∏Å‡πà‡∏≠‡∏ô ‚Üí fallback ‡πÑ‡∏õ global_vars ‚îÄ‚îÄ‚îÄ
        threshold = getattr(self, "RERANK_THRESHOLD", 0.5)
        cap_value = getattr(self, "MAX_EVI_STR_CAP", 10.0)

        # Fallback ‡∏à‡∏≤‡∏Å global_vars ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á (‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å import global_vars ‡πÅ‡∏•‡πâ‡∏ß)
        threshold = threshold if threshold != 0.5 else RERANK_THRESHOLD
        cap_value = cap_value if cap_value != 10.0 else MAX_EVI_STR_CAP

        # ‚îÄ‚îÄ‚îÄ 2. ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ highest_rerank_score ‡∏à‡∏≤‡∏Å Adaptive Loop (‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î) ‚îÄ‚îÄ‚îÄ
        max_score_found = highest_rerank_score if highest_rerank_score is not None else 0.0
        max_score_source = "Adaptive_RAG_Loop" if highest_rerank_score is not None else "N/A"

        for doc in top_evidences:
            page_content = ""
            metadata = {}
            current_score = 0.0

            # ‚îÄ‚îÄ‚îÄ ‡πÅ‡∏õ‡∏•‡∏á document ‡πÉ‡∏´‡πâ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á dict ‡πÅ‡∏•‡∏∞ Langchain Document ‚îÄ‚îÄ‚îÄ
            if isinstance(doc, dict):
                metadata = doc.get("metadata", {})
                page_content = doc.get("page_content", "") or doc.get("text", "") or doc.get("content", "")
            else:
                metadata = getattr(doc, "metadata", {})
                page_content = getattr(doc, "page_content", "") or getattr(doc, "text", "")

            # ‚îÄ‚îÄ‚îÄ ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏à‡∏≤‡∏Å metadata ‡πÅ‡∏•‡∏∞ top-level keys ‚îÄ‚îÄ‚îÄ
            for key in score_keys:
                score_val = metadata.get(key)
                if score_val is None:
                    if isinstance(doc, dict):
                        score_val = doc.get(key)
                    else:
                        score_val = getattr(doc, key, None)

                if score_val is not None:
                    try:
                        temp_score = float(score_val)
                        if 0.0 < temp_score <= 1.0:
                            if temp_score > current_score:
                                current_score = temp_score
                                break
                    except (ValueError, TypeError):
                        continue

            # ‚îÄ‚îÄ‚îÄ Fallback: ‡∏î‡∏∂‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏à‡∏≤‡∏Å‡∏ó‡πâ‡∏≤‡∏¢ content ‡∏î‡πâ‡∏ß‡∏¢ regex (aggressive) ‚îÄ‚îÄ‚îÄ
            if current_score == 0.0 and page_content and isinstance(page_content, str):
                try:
                    tail = page_content[-1000:]
                    patterns = [
                        r"Relevance[ :]+([0-9]*\.?[0-9]+)",
                        r"Score[ :]+([0-9]*\.?[0-9]+)",
                        r"Re:[ ]*([0-9]*\.?[0-9]+)",
                        r"\[Relevance: ([0-9]*\.?[0-9]+)\]",
                        r"\[Score: ([0-9]*\.?[0-9]+)\]",
                        r"rerank_score['\"]?\s*:\s*([0-9]*\.?[0-9]+)",
                        r"\|\s*([0-9]*\.?[0-9]+)\s*\|",
                        r"\s+([0-9]\.[0-9]+)$",
                    ]
                    for pat in patterns:
                        m = re.search(pat, tail, re.IGNORECASE)
                        if m:
                            try:
                                temp_score = float(m.group(1))
                                if 0.0 < temp_score <= 1.0:
                                    if temp_score > current_score:
                                        current_score = temp_score
                                        break
                            except:
                                continue
                except Exception as e:
                    self.logger.debug(f"Regex fallback failed at L{level}: {e}")

            # ‚îÄ‚îÄ‚îÄ Score Clamp: ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô > 1.0 ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà relevance scale 0-1 ‚Üí ignore ‚îÄ‚îÄ‚îÄ
            if current_score > 1.0:
                source = (
                    metadata.get("source_filename") or metadata.get("filename") or
                    doc.get("source_filename") or doc.get("filename") or
                    doc.get("source") or doc.get("doc_id") or "N/A"
                )
                self.logger.warning(
                    f"üö® Score Clamp L{level}: Score {current_score:.4f} > 1.0 from '{source}'. Ignoring."
                )
                current_score = 0.0

            # ‚îÄ‚îÄ‚îÄ ‡∏î‡∏∂‡∏á source ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö log ‚îÄ‚îÄ‚îÄ
            source = (
                metadata.get("source_filename") or metadata.get("filename") or
                doc.get("source_filename") or doc.get("filename") or
                doc.get("source") or doc.get("doc_id") or "N/A"
            )

            # ‚îÄ‚îÄ‚îÄ ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î ‡∏û‡∏£‡πâ‡∏≠‡∏° log override ‚îÄ‚îÄ‚îÄ
            if current_score > max_score_found:
                if highest_rerank_score is not None and current_score > highest_rerank_score:
                    self.logger.critical(
                        f"‚ö†Ô∏è Score Override L{level}: Hidden score {current_score:.4f} > Loop score {highest_rerank_score:.4f} "
                        f"from source: {source}"
                    )
                max_score_found = current_score
                max_score_source = source

        # ‚îÄ‚îÄ‚îÄ Relevant Score Gate: ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à cap ‡∏´‡∏£‡∏∑‡∏≠ full ‚îÄ‚îÄ‚îÄ
        if max_score_found < threshold:
            max_evi_str_for_prompt = cap_value  # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ cap ‡∏à‡∏≤‡∏Å config (10.0)
            is_capped = True
            self.logger.warning(
                f"üö® Evi Str CAPPED L{level}: Rerank {max_score_found:.4f} (from '{max_score_source}') "
                f"< {threshold} ‚Üí ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏ó‡∏µ‡πà {cap_value}"
            )
        else:
            max_evi_str_for_prompt = 10.0
            is_capped = False
            self.logger.info(
                f"‚úÖ Evi Str FULL L{level}: Rerank {max_score_found:.4f} (from '{max_score_source}') "
                f">= {threshold} ‚Üí ‡∏õ‡∏•‡πà‡∏≠‡∏¢‡πÄ‡∏ï‡πá‡∏° 10.0"
            )

        return {
            "is_capped": is_capped,
            "max_evi_str_for_prompt": max_evi_str_for_prompt,
            "highest_rerank_score": round(float(max_score_found), 4),
            "max_score_source": max_score_source,
        }
        
    def _robust_hydrate_documents_for_priority_chunks(
        self,
        chunks_to_hydrate: List[Dict],
        vsm: Optional['VectorStoreManager'],
        current_sub_id: Optional[str] = None,
        level: Optional[int] = None
    ) -> List[Dict]:

        active_sub_id = current_sub_id or getattr(self, 'sub_id', 'unknown')
        if not chunks_to_hydrate:
            return []

        TAG_ABBREV = {
            "PLAN": "P", "DO": "D", "CHECK": "C", "ACT": "A",
            "P": "P", "D": "D", "C": "C", "A": "A"
        }

        def _safe_classify(text: str) -> str:
            try:
                raw = classify_by_keyword(
                    text=text,
                    sub_id=active_sub_id,
                    level=level,
                    contextual_rules_map=self.contextual_rules_map
                )
                if not raw:
                    return "Other"
                return TAG_ABBREV.get(str(raw).upper(), "Other")
            except Exception as e:
                self.logger.warning(f"PDCA classify failed ‚Üí Other | {e}")
                return "Other"

        def _standardize_chunk(chunk: Dict, score: float):
            chunk.setdefault("is_baseline", True)

            text = chunk.get("text", "").strip()
            if text:
                chunk["pdca_tag"] = _safe_classify(text)

                # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô baseline score inflate
                chunk["rerank_score"] = max(chunk.get("rerank_score", 0.0), score)
                chunk["score"] = max(chunk.get("score", 0.0), score)

            return chunk

        stable_ids = {
            sid for c in chunks_to_hydrate
            if (sid := (c.get("stable_doc_uuid") or c.get("doc_id") or c.get("chunk_uuid")))
        }

        if not stable_ids or not vsm:
            boosted = [_standardize_chunk(c.copy(), 0.9) for c in chunks_to_hydrate]
            return self._guarantee_text_key(boosted)

        stable_id_map = defaultdict(list)

        try:
            retrieved_docs = vsm.get_documents_by_id(
                list(stable_ids),
                doc_type=self.doc_type,
                enabler=self.config.enabler
            )

            for doc in retrieved_docs:
                sid = doc.metadata.get("stable_doc_uuid") or doc.metadata.get("doc_id")
                if sid:
                    stable_id_map[sid].append({
                        "text": doc.page_content,
                        "metadata": doc.metadata
                    })

        except Exception as e:
            self.logger.error(f"VSM Hydration failed: {e}")
            fallback = [_standardize_chunk(c.copy(), 0.9) for c in chunks_to_hydrate]
            return self._guarantee_text_key(fallback)

        hydrated_priority_docs = []
        restored_count = 0
        seen_signatures = set()

        SAFE_META_KEYS = {
            "source", "file_name", "page", "page_label",
            "page_number", "enabler", "tenant", "year"
        }

        for chunk in chunks_to_hydrate:
            new_chunk = chunk.copy()
            sid = new_chunk.get("stable_doc_uuid") or new_chunk.get("doc_id")

            hydrated = False
            if sid and sid in stable_id_map:
                best_match = stable_id_map[sid][0]
                new_chunk["text"] = best_match["text"]

                meta = best_match.get("metadata", {})
                new_chunk.update({k: v for k, v in meta.items() if k in SAFE_META_KEYS})

                hydrated = True
                restored_count += 1

            new_chunk = _standardize_chunk(
                new_chunk,
                score=1.0 if hydrated else 0.85
            )

            signature = (
                sid,
                new_chunk.get("chunk_uuid"),
                new_chunk.get("text", "")[:200]
            )

            if signature in seen_signatures:
                continue

            seen_signatures.add(signature)
            hydrated_priority_docs.append(new_chunk)

        return self._guarantee_text_key(
            hydrated_priority_docs,
            total_count=len(chunks_to_hydrate),
            restored_count=restored_count
        )


    def _guarantee_text_key(
        self,
        chunks: List[Dict],
        total_count: int = 0,
        restored_count: int = 0
    ) -> List[Dict]:

        final_chunks = []

        for chunk in chunks:
            if "text" not in chunk:
                chunk["text"] = ""
                cid = str(chunk.get("chunk_uuid", "N/A"))
                self.logger.debug(f"Guaranteed 'text' key for chunk (ID: {cid[:8]})")
            final_chunks.append(chunk)

        if total_count > 0:
            baseline_count = sum(1 for c in final_chunks if c.get("is_baseline"))
            self.logger.info(
                f"HYDRATION SUMMARY: Restored {restored_count}/{total_count} "
                f"(Baseline={baseline_count})"
            )

        return final_chunks

    
    def _get_keywords_for_phase(self, sub_id: str, level: int, phase: str = "Plan") -> str:
        """
        ‡∏î‡∏∂‡∏á keywords ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö phase (Plan, Do, Check, Act) ‡∏à‡∏≤‡∏Å contextual_rules.json
        
        ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ:
        1. ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö Array [..] ‡πÅ‡∏•‡∏∞ String ".." ‡∏à‡∏≤‡∏Å JSON
        2. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á (Whitespace) ‡πÉ‡∏´‡πâ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
        3. ‡∏°‡∏µ‡∏£‡∏∞‡∏ö‡∏ö Fallback 4 ‡∏ä‡∏±‡πâ‡∏ô (Level -> Sub -> Enabler -> Global)
        4. ‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡∏Ñ‡πà‡∏≤ String ‡πÅ‡∏•‡πâ‡∏ß‡πÇ‡∏î‡∏ô join ‡πÅ‡∏¢‡∏Å‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£
        """
        sub_rules = self.contextual_rules_map.get(sub_id, {})
        phase_key = f"{phase.lower()}_keywords"
        
        raw_keywords = None

        # 1. Level-specific (L1, L2, ...)
        level_key = f"L{level}"
        level_rules = sub_rules.get(level_key, {})
        if phase_key in level_rules:
            raw_keywords = level_rules[phase_key]
        
        # 2. Sub-specific fallback (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô Level ‡∏ô‡∏±‡πâ‡∏ô‡πÜ)
        elif phase_key in sub_rules:
            raw_keywords = sub_rules[phase_key]
        
        # 3. Enabler default (‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏Å‡∏•‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£)
        elif "_enabler_defaults" in self.contextual_rules_map:
            raw_keywords = self.contextual_rules_map["_enabler_defaults"].get(phase_key)

        # ---------------------------------------------------------
        # üéØ ‡∏à‡∏∏‡∏î‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• Keywords (Data Cleaning)
        # ---------------------------------------------------------
        keywords_list = []
        if raw_keywords:
            if isinstance(raw_keywords, list):
                # ‡∏Å‡∏£‡∏ì‡∏µ‡πÄ‡∏õ‡πá‡∏ô Array: ‡∏•‡πâ‡∏≤‡∏á‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≥
                keywords_list = [str(k).strip() for k in raw_keywords if k]
            elif isinstance(raw_keywords, str):
                # ‡∏Å‡∏£‡∏ì‡∏µ‡πÄ‡∏õ‡πá‡∏ô String: ‡πÅ‡∏¢‡∏Å‡∏î‡πâ‡∏ß‡∏¢ comma ‡πÅ‡∏•‡∏∞‡∏•‡πâ‡∏≤‡∏á‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á
                keywords_list = [k.strip() for k in raw_keywords.split(",") if k.strip()]
        
        # 4. Global fallback (‡∏Å‡∏£‡∏ì‡∏µ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏à‡∏£‡∏¥‡∏á‡πÜ ‡∏ñ‡πâ‡∏≤‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå JSON ‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤)
        if not keywords_list:
            fallback_map = {
                "plan": ["‡∏ß‡∏¥‡∏™‡∏±‡∏¢‡∏ó‡∏±‡∏®‡∏ô‡πå", "‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢", "‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á", "‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢", "‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå", "‡πÅ‡∏ú‡∏ô"],
                "do": ["‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£", "‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥", "‡∏à‡∏±‡∏î‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£", "‡∏Ç‡∏±‡∏ö‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô"],
                "check": ["‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö", "‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô", "‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°", "‡∏ß‡∏±‡∏î‡∏ú‡∏•"],
                "act": ["‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á", "‡∏û‡∏±‡∏í‡∏ô‡∏≤", "‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç", "‡∏ô‡∏≥‡πÑ‡∏õ‡πÉ‡∏ä‡πâ", "‡∏ó‡∏ö‡∏ó‡∏ß‡∏ô"]
            }
            keywords_list = fallback_map.get(phase.lower(), [])

        # ---------------------------------------------------------
        # ‚úÖ ‡∏à‡∏∏‡∏î‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢: ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô String ‡∏ó‡∏µ‡πà‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
        # ---------------------------------------------------------
        # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏£‡∏ì‡∏µ keywords_list ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà list (‡∏ã‡∏∂‡πà‡∏á‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡πÅ‡∏ï‡πà‡πÄ‡∏ä‡πá‡∏Ñ‡πÑ‡∏ß‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏ß‡∏£‡πå)
        if isinstance(keywords_list, str):
            return keywords_list
            
        return ", ".join(keywords_list)
    
    def _get_pdca_blocks_from_evidences(
        self,
        evidences: List[Dict],
        baseline_evidences: Dict[str, List[Dict]],
        level: int,
        sub_id: str,
        contextual_rules_map: Dict[str, Any]
    ) -> Tuple[str, str, str, str, str]:
        """
        Build PDCA context blocks from evidences.
        Logic: Re-classify chunks, force L1 Plan, group, and render to text.
        """
        import copy
        from collections import defaultdict

        # ------------------------------------------------------------------
        # 1) ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Chunks ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (‡∏£‡∏ß‡∏° Baseline)
        # ------------------------------------------------------------------
        all_chunks: List[Dict] = []
        evidences = self._guarantee_text_key(evidences or [])
        for c in evidences:
            all_chunks.append(copy.deepcopy(c))

        level_baseline = baseline_evidences.get(str(level), []) or []
        for b in level_baseline:
            b_copy = copy.deepcopy(b)
            b_copy["is_baseline"] = True
            all_chunks.append(b_copy)

        all_chunks = [c for c in all_chunks if isinstance(c, dict) and c.get("text", "").strip()]
        if not all_chunks:
            return "", "", "", "", ""

        # ------------------------------------------------------------------
        # 2) Re-classify PDCA Tags ‡∏ï‡∏≤‡∏° Keyword ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
        # ------------------------------------------------------------------
        for chunk in all_chunks:
            try:
                new_tag = classify_by_keyword(
                    text=chunk["text"],
                    sub_id=sub_id,
                    level=level,
                    contextual_rules_map=contextual_rules_map
                )
                chunk["pdca_tag"] = new_tag if new_tag in {"P", "D", "C", "A"} else "Other"
            except Exception as e:
                self.logger.warning(f"PDCA classify failed ‚Üí fallback Other | {e}")
                chunk["pdca_tag"] = "Other"

        # ------------------------------------------------------------------
        # 3) üî• KM L1 Logic: ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô Level 1 ‡πÉ‡∏´‡πâ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏∑‡∏≠ Plan (P)
        # ------------------------------------------------------------------
        if level == 1:
            forced_count = 0
            for chunk in all_chunks:
                if chunk.get("pdca_tag") == "Other":
                    chunk["pdca_tag"] = "P"
                    forced_count += 1
            if forced_count > 0:
                self.logger.info(f"üí° [L1 Domain Logic] Forced {forced_count} 'Other' chunks to 'P' (Plan) for level 1")

        # ------------------------------------------------------------------
        # 4) ‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ï‡∏≤‡∏° Label ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á Text Blocks
        # ------------------------------------------------------------------
        TAG_FULL = {"P": "Plan", "D": "Do", "C": "Check", "A": "Act", "Other": "Other"}
        pdca_groups_full: Dict[str, List[Dict]] = defaultdict(list)
        for c in all_chunks:
            tag_abbr = c.get("pdca_tag", "Other")
            full_label = TAG_FULL.get(tag_abbr, "Other")
            pdca_groups_full[full_label].append(c)

        # ------------------------------------------------------------------
        # 5) Helpers ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
        # ------------------------------------------------------------------
        def _normalize_meta(c: Dict) -> Tuple[str, str]:
            """
            ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡∏∞‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≤‡∏Å Chunk ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Fallback Logic 
            ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏ö Ingest ‡πÉ‡∏´‡∏°‡πà‡πÅ‡∏•‡∏∞‡πÄ‡∏Å‡πà‡∏≤ (source_filename, page_label)
            ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏£‡∏ì‡∏µ‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô 0 ‡∏´‡∏£‡∏∑‡∏≠ Metadata ‡∏Å‡∏£‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢
            """
            # ‡∏î‡∏∂‡∏á metadata ‡∏°‡∏≤‡πÑ‡∏ß‡πâ‡∏Å‡πà‡∏≠‡∏ô ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏´‡∏≤‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏≠‡∏ö
            meta = c.get("metadata", {}) or {}
            
            # 1. üîç ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå (Source)
            # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏´‡∏≤‡∏à‡∏≤‡∏Å‡∏ä‡∏±‡πâ‡∏ô‡∏ô‡∏≠‡∏Å‡∏™‡∏∏‡∏î‡∏Å‡πà‡∏≠‡∏ô ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏≠‡∏¢‡πÑ‡∏•‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô metadata
            source = (
                c.get("source_filename") or 
                c.get("filename") or 
                meta.get("source_filename") or 
                meta.get("source") or 
                meta.get("file_name") or 
                None  # ‡πÉ‡∏ä‡πâ None ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏õ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ï‡πà‡∏≠‡∏ó‡∏µ‡πà clean_source
            )
            
            # 2. üîç ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤ (Page)
            # ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å: ‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏¢‡∏Å‡πÄ‡∏ä‡πá‡∏Ñ‡πÄ‡∏•‡∏Ç 0 ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ Python ‡∏°‡∏≠‡∏á 0 ‡πÄ‡∏õ‡πá‡∏ô False
            page = None
            page_keys = ["page_label", "page", "page_number"]
            
            # ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡∏à‡∏≤‡∏Å Root ‡∏Ç‡∏≠‡∏á Chunk ‡∏Å‡πà‡∏≠‡∏ô
            for key in page_keys:
                val = c.get(key)
                if val is not None:
                    page = val
                    break
                    
            # ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡πÉ‡∏ô metadata
            if page is None:
                for key in page_keys:
                    val = meta.get(key)
                    if val is not None:
                        page = val
                        break

            # 3. ‚ú® ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Cleaning)
            # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå
            clean_source = str(source).strip() if source is not None else "Unknown"
            
            # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤: ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÄ‡∏•‡∏Ç 0 ‡∏´‡∏≤‡∏¢ ‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏û‡∏ß‡∏Å 'n/a'
            if page is not None:
                page_str = str(page).strip()
                clean_page = page_str if page_str.lower() != "n/a" else "N/A"
            else:
                clean_page = "N/A"
            
            return clean_source, clean_page

        def _create_block(tag: str, chunks: List[Dict]) -> str:
            if not chunks: return ""
            
            # 1. ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô Re-rank ‡∏à‡∏≤‡∏Å‡∏°‡∏≤‡∏Å‡πÑ‡∏õ‡∏ô‡πâ‡∏≠‡∏¢
            chunks = sorted(chunks, key=get_actual_score, reverse=True)
            
            # 2. üõ°Ô∏è ‡∏Ç‡∏µ‡∏î‡πÄ‡∏™‡πâ‡∏ô‡πÉ‡∏ï‡πâ 7 ‡∏ä‡∏¥‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏û‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (‡πÉ‡∏ä‡πâ‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô‡∏ó‡∏±‡πâ‡∏á Mac ‡πÅ‡∏•‡∏∞ Server)
            top_chunks = chunks[:7]
            
            total = len(top_chunks)
            blocks: List[str] = []
            seen_texts = set() # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ã‡πâ‡∏≥ (Deduplication)

            for i, c in enumerate(top_chunks, start=1):
                body = c["text"].strip()
                
                # Check ‡∏ã‡πâ‡∏≥‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô (‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡πÄ‡∏à‡∏≠ Chunk ‡∏ó‡∏µ‡πà‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô‡πÄ‡∏õ‡πä‡∏∞)
                if body[:100] in seen_texts: 
                    continue
                seen_texts.add(body[:100])

                source, page = _normalize_meta(c)
                score = get_actual_score(c)
                baseline_mark = " [üìú BASELINE/REFERENCE]" if c.get("is_baseline") else ""
                
                header = f"### [{tag} Evidence {i}/{total}]{baseline_mark}"
                footer = f"\n[‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á: {source}, ‡∏´‡∏ô‡πâ‡∏≤: {page}, Score: {score:.4f}]"
                blocks.append(f"{header}\n{body}{footer}")
                
            return "\n---\n".join(blocks)

        # ------------------------------------------------------------------
        # 6) ‡∏™‡∏£‡πâ‡∏≤‡∏á Output ‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Log ‡∏™‡∏£‡∏∏‡∏õ
        # ------------------------------------------------------------------
        plan_text = _create_block("Plan", pdca_groups_full.get("Plan", []))
        do_text = _create_block("Do", pdca_groups_full.get("Do", []))
        check_text = _create_block("Check", pdca_groups_full.get("Check", []))
        act_text = _create_block("Act", pdca_groups_full.get("Act", []))
        other_text = _create_block("Other", pdca_groups_full.get("Other", []))

        # ‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ï‡πà‡∏•‡∏∞ Phase
        pdca_status = [f"{t}:{'‚úÖ' if txt else '‚ùå'}" for t, txt in [("P", plan_text), ("D", do_text), ("C", check_text), ("A", act_text)]]
        self.logger.info(f"üìä [PDCA Block Output] {sub_id} L{level} | {' | '.join(pdca_status)} | Other:{'‚úÖ' if other_text else '‚ùå'}")

        return (plan_text, do_text, check_text, act_text, other_text)
    
    def run_assessment(
        self,
        target_sub_id: str = "all",
        export: bool = False,
        vectorstore_manager: Optional['VectorStoreManager'] = None,
        sequential: bool = False,
        document_map: Optional[Dict[str, str]] = None,
        record_id: str = None,
    ) -> Dict[str, Any]:
        """
        [FIXED VERSION] 
        - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç AttributeError ‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ä‡∏∑‡πà‡∏≠ _initialize_vsm_if_none ‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
        - ‡∏Ñ‡∏á‡∏Ñ‡∏ß‡∏≤‡∏° Robust ‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£ Merge Evidence ‡πÅ‡∏•‡∏∞ Persistence ‡πÑ‡∏ß‡πâ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô
        """
        start_ts = time.time()
        self.is_sequential = sequential
        self.current_record_id = record_id 

        # ============================== 1. Filter Rubric ==============================
        all_statements = self._flatten_rubric_to_statements()
        
        if target_sub_id.lower() == "all":
            sub_criteria_list = all_statements
            self.logger.info(f"üìã Assessing ALL criteria ({len(sub_criteria_list)} items)")
        else:
            sub_criteria_list = [
                s for s in all_statements 
                if str(s.get('sub_id')).strip().lower() == str(target_sub_id).strip().lower()
            ]

        if not sub_criteria_list:
            error_msg = f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏£‡∏´‡∏±‡∏™‡πÄ‡∏Å‡∏ì‡∏ë‡πå '{target_sub_id}' ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå Rubric"
            self.logger.error(f"‚ùå {error_msg}")
            return {
                "record_id": record_id, "status": "FAILED", "error_message": error_msg,
                "summary": {"score": 0.0, "level": "L0", "total_weighted_score": 0.0, "max_weight": 0.0},
                "sub_criteria_results": [], "run_time_seconds": round(time.time() - start_ts, 2),
                "timestamp": datetime.now().isoformat()
            }

        # ‡πÇ‡∏´‡∏•‡∏î Evidence Map ‡πÄ‡∏î‡∏¥‡∏°
        if os.path.exists(self.evidence_map_path):
            try:
                loaded = self._load_evidence_map()
                self.evidence_map = loaded if loaded else {}
            except Exception: self.evidence_map = {}
        else:
            self.evidence_map = {}

        self.raw_llm_results = []
        self.final_subcriteria_results = []

        max_workers = globals().get('MAX_PARALLEL_WORKERS', 4)
        run_parallel = (target_sub_id.lower() == "all") and not sequential

        # ============================== 2. Run Assessment ==============================
        if run_parallel:
            # (Parallel logic ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°...)
            self.logger.info(f"üöÄ Starting Parallel Assessment with {max_workers} processes")
            worker_args = [(
                sub_data, self.config.enabler, self.config.target_level, self.config.mock_mode,
                self.evidence_map_path, self.config.model_name, self.config.temperature,
                getattr(self.config, 'MIN_RETRY_SCORE', 0.50), getattr(self.config, 'MAX_RETRIEVAL_ATTEMPTS', 3),
                document_map or self.document_map, self.ActionPlanActions 
            ) for sub_data in sub_criteria_list]

            try:
                with multiprocessing.get_context('spawn').Pool(processes=max_workers) as pool:
                    results_list = pool.map(_static_worker_process, worker_args)
            except Exception as e:
                self.logger.critical(f"Multiprocessing failed: {e}")
                raise

            for result_tuple in results_list:
                if not isinstance(result_tuple, tuple) or len(result_tuple) != 2: continue
                sub_result, temp_map_from_worker = result_tuple
                if isinstance(temp_map_from_worker, dict):
                    for k, v in temp_map_from_worker.items():
                        for ev in v:
                            if "page" not in ev: ev["page"] = ev.get("metadata", {}).get("page", "N/A")
                        current_list = self.evidence_map.setdefault(k, [])
                        current_list.extend(v)
                self.raw_llm_results.extend(sub_result.get("raw_results_ref", []))
                self.final_subcriteria_results.append(sub_result)

        else:
            # --------------------- SEQUENTIAL MODE ---------------------
            self.logger.info(f"Starting Sequential Assessment: {target_sub_id}")
            
            # ‚úÖ [CRITICAL FIX] ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö Class Method ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
            if vectorstore_manager:
                self.vectorstore_manager = vectorstore_manager
            else:
                self._initialize_vsm_if_none() # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á
            
            for sub_criteria in sub_criteria_list:
                sub_result, final_temp_map = self._run_sub_criteria_assessment_worker(sub_criteria)
                
                if final_temp_map:
                    for k, v in final_temp_map.items():
                        for ev in v:
                            if "page" not in ev: ev["page"] = ev.get("metadata", {}).get("page", "N/A")
                        current_list = self.evidence_map.setdefault(k, [])
                        current_list.extend(v)

                self.raw_llm_results.extend(sub_result.get("raw_results_ref", []))
                self.final_subcriteria_results.append(sub_result)

        # ============================== 3. Final Persistence & Summary ==============================
        if self.evidence_map:
            try:
                self._save_evidence_map(map_to_save=self.evidence_map)
                self.logger.info(f"‚úÖ Evidence Map Saved (Total: {len(self.evidence_map)} keys)")
            except Exception as e:
                self.logger.error(f"‚ùå Failed to save evidence map: {e}")

        self._calculate_overall_stats(target_sub_id)

        final_results = {
            "record_id": record_id,
            "summary": self.total_stats,
            "sub_criteria_results": self.final_subcriteria_results,
            "raw_llm_results": self.raw_llm_results,
            "run_time_seconds": round(time.time() - start_ts, 2),
            "timestamp": datetime.now().isoformat(),
        }

        if export:
            export_path = self._export_results(
                results=final_results,
                sub_criteria_id=target_sub_id if target_sub_id != "all" else "ALL",
                record_id=record_id
            )
            final_results["export_path_used"] = export_path

        return final_results
    
    def _run_sub_criteria_assessment_worker(
        self,
        sub_criteria: Dict[str, Any],
    ) -> Tuple[Dict[str, Any], Dict[str, List[Dict[str, Any]]]]:
        """
        [v21.9.15 - STABLE VERSION]
        - ‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Source of Evidence ‡∏´‡∏≤‡∏¢‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤ UI
        - ‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ PDCA Opaque (‡∏™‡∏µ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß‡∏à‡∏≤‡∏á) ‡πÑ‡∏°‡πà‡∏Ç‡∏∂‡πâ‡∏ô‡πÉ‡∏ô‡∏ä‡πà‡∏≠‡∏á GAP_ONLY
        - ‡πÅ‡∏Å‡πâ Bug TypeError (dict + int) ‡πÉ‡∏ô Action Plan
        """
        # üìå ‡πÇ‡∏´‡∏•‡∏î Global Constants
        REQUIRED_PDCA: Final[Dict[int, Set[str]]] = globals().get('REQUIRED_PDCA', {1: {"P"}, 2: {"P", "D"}, 3: {"P", "D", "C"}, 4: {"P", "D", "C", "A"}, 5: {"P", "D", "C", "A"}})
        MAX_L1_ATTEMPTS = globals().get('MAX_L1_ATTEMPTS', 2)
        MIN_KEEP_SC = globals().get('MIN_RERANK_SCORE_TO_KEEP', 0.15)

        sub_id = sub_criteria['sub_id']
        sub_criteria_name = sub_criteria['sub_criteria_name']
        sub_weight = sub_criteria.get('weight', 0)
        
        current_sequential_pass_level = 0 
        first_failed_level_local = None 
        raw_results_for_sub_seq: List[Dict[str, Any]] = []
        start_ts = time.time() 

        self.logger.info(f"[WORKER START] Assessing: {sub_id}")
        self.temp_map_for_save = {}

        # -----------------------------------------------------------
        # 1. LOOP THROUGH LEVELS (L1 ‚Üí L5)
        # -----------------------------------------------------------
        for statement_data in sub_criteria.get('levels', []):
            level = statement_data.get('level')
            if level is None or level > self.config.target_level:
                continue
            
            sequential_chunk_uuids = [] 
            level_result = {}

            # --- ‡∏™‡πà‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô (‡∏Ñ‡∏á‡πÄ‡∏î‡∏¥‡∏°‡∏à‡∏≤‡∏Å Original) ---
            if level >= 3:
                wrapper = self.retry_policy.run(
                    fn=lambda attempt: self._run_single_assessment(
                        sub_criteria=sub_criteria,
                        statement_data=statement_data,
                        vectorstore_manager=self.vectorstore_manager,
                        sequential_chunk_uuids=sequential_chunk_uuids,
                        attempt=attempt
                    ),
                    level=level,
                    statement=statement_data.get('statement', ''),
                    context_blocks={"sequential_chunk_uuids": sequential_chunk_uuids},
                    logger=self.logger
                )
                level_result = wrapper.result if isinstance(wrapper, RetryResult) and wrapper.result is not None else {}
            else:
                for attempt_num in range(1, MAX_L1_ATTEMPTS + 1):
                    level_result = self._run_single_assessment(
                        sub_criteria=sub_criteria,
                        statement_data=statement_data,
                        vectorstore_manager=self.vectorstore_manager,
                        sequential_chunk_uuids=sequential_chunk_uuids,
                        attempt=attempt_num
                    )
                    if level_result.get('is_passed', False): break

            # --- 1.2 PROCESS RESULT AND HANDLE EVIDENCE ---
            result_to_process = level_result or {"level": level, "is_passed": False}
            is_passed_llm = result_to_process.get('is_passed', False)
            
            # üü¢ [REPAIR] PDCA Repair Logic (‡∏â‡∏µ‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏™‡∏µ‡∏Ç‡∏≤‡∏ß)
            pdca_val = result_to_process.get('pdca_breakdown', {})
            if is_passed_llm and (not pdca_val or all(v == 0 for v in pdca_val.values())):
                repaired_pdca = {"P": 1, "D": (1 if level >= 2 else 0), "C": (1 if level >= 3 else 0), "A": (1 if level >= 4 else 0)}
                result_to_process['pdca_breakdown'] = repaired_pdca

            result_to_process.setdefault("is_counted", True)
            result_to_process.setdefault("is_capped", False)

            # üü¢ [SOURCE] ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Evidence ‡∏•‡∏á‡∏£‡∏∞‡∏ö‡∏ö (‡∏â‡∏µ‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Source ‡πÑ‡∏°‡πà‡∏´‡∏≤‡∏¢)
            level_temp_map = result_to_process.get("temp_map_for_level", [])
            if level_temp_map:
                highest_rerank = result_to_process.get('max_relevant_score', 0.0)
                # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö Mapping
                max_evi_str = self._save_level_evidences_and_calculate_strength(
                    level_temp_map=level_temp_map,
                    sub_id=sub_id,
                    level=level,
                    llm_result=result_to_process, 
                    highest_rerank_score=highest_rerank 
                )
                # ‡πÄ‡∏Å‡πá‡∏ö Strength ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô Sequential ‡∏à‡∏£‡∏¥‡∏á
                if is_passed_llm and first_failed_level_local is None:
                    result_to_process['evidence_strength'] = round(min(max_evi_str, 10.0), 1)
                else:
                    result_to_process['evidence_strength'] = 0.0
                
            # --- üü° [SAFE UPDATE] Sequential State (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏à‡∏≤‡∏Å Original ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏Å‡∏©‡∏≤ Metadata) ---
            if first_failed_level_local is not None:
                # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏à‡∏≥‡∏Ñ‡πà‡∏≤ Metadata ‡∏ó‡∏µ‡πà AI ‡πÄ‡∏à‡∏≠‡∏à‡∏£‡∏¥‡∏á‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡πÄ‡∏õ‡πá‡∏ô GAP_ONLY
                act_pdca = result_to_process.get('pdca_breakdown', {"P": 0, "D": 0, "C": 0, "A": 0})
                act_src = result_to_process.get('temp_map_for_level', [])
                
                result_to_process.update({
                    "evaluation_mode": "GAP_ONLY",
                    "is_counted": False,
                    "is_passed": False,
                    "pdca_breakdown": act_pdca,      # <--- ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏™‡∏µ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß‡∏à‡∏≤‡∏á‡∏Ç‡∏∂‡πâ‡∏ô
                    "temp_map_for_level": act_src,   # <--- ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏õ‡∏∏‡πà‡∏° Source ‡∏Ç‡∏∂‡πâ‡∏ô
                    "cap_reason": f"Gap analysis after sequential fail at L{first_failed_level_local}"
                })
            elif not is_passed_llm:
                first_failed_level_local = level
            else:
                if level == current_sequential_pass_level + 1:
                    current_sequential_pass_level = level
                else:
                    if self.is_sequential:
                        first_failed_level_local = current_sequential_pass_level + 1
                        result_to_process["is_counted"] = False
                        result_to_process["is_passed"] = False

            result_to_process["execution_index"] = len(raw_results_for_sub_seq)
            raw_results_for_sub_seq.append(result_to_process)
        
        # -----------------------------------------------------------
        # 2. CALCULATE SUMMARY (‡∏Ñ‡∏á‡πÄ‡∏î‡∏¥‡∏°)
        # -----------------------------------------------------------
        highest_full_level = current_sequential_pass_level
        weighted_score = round(self._calculate_weighted_score(highest_full_level, sub_weight), 2)
        num_passed = sum(1 for r in raw_results_for_sub_seq if r.get("is_passed", False) and r.get("is_counted", True))

        sub_summary = {
            "num_statements": len(raw_results_for_sub_seq),
            "num_passed": num_passed,
            "num_failed": len(raw_results_for_sub_seq) - num_passed,
            "pass_rate": round(num_passed / len(raw_results_for_sub_seq), 4) if raw_results_for_sub_seq else 0.0
        }

        # -----------------------------------------------------------
        # 3. GENERATE ACTION PLAN (‡∏Ñ‡∏á‡πÄ‡∏î‡∏¥‡∏° ‡πÅ‡∏ï‡πà‡πÅ‡∏Å‡πâ Bug Parameter)
        # -----------------------------------------------------------
        roadmap_target_level = self.config.target_level if hasattr(self.config, 'target_level') else 5
        statements_for_ap = []
        level_statements_map = {l.get('level'): l.get('statement', '') for l in sub_criteria.get('levels', [])}
        
        for r in raw_results_for_sub_seq:
            res_item = r.copy()
            res_item['statement_text'] = level_statements_map.get(res_item.get('level'), "")
            if not res_item.get('is_passed', False):
                res_item['recommendation_type'] = 'FAILED' if res_item.get('evaluation_mode') != "GAP_ONLY" else 'GAP_ANALYSIS'
                statements_for_ap.append(res_item)
            else:
                pdca = res_item.get('pdca_breakdown', {})
                if any(v == 0 for v in pdca.values()):
                    res_item['recommendation_type'] = 'PDCA_INCOMPLETE'
                    statements_for_ap.append(res_item)

        # ‚úÖ FIX: ‡∏£‡∏∞‡∏ö‡∏∏‡∏ä‡∏∑‡πà‡∏≠ parameter ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô TypeError (dict + int)
        action_plan_result = create_structured_action_plan(
            recommendation_statements=statements_for_ap,
            sub_id=sub_id,
            sub_criteria_name=sub_criteria_name,
            target_level=roadmap_target_level, 
            llm_executor=self.llm,
            logger=self.logger,
            enabler_rules=self.contextual_rules_map 
        )

        # -----------------------------------------------------------
        # 4. FINAL RETURN
        # -----------------------------------------------------------
        final_temp_map = {}
        # ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏° Source ‡∏Ç‡∏≠‡∏á‡∏ó‡∏∏‡∏∏‡∏Å Level ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ô‡∏ú‡πà‡∏≤‡∏ô‡∏°‡∏≤
        for res in raw_results_for_sub_seq:
            lvl = res.get('level')
            for evi in res.get("temp_map_for_level", []):
                f_id = evi.get("file_id") or evi.get("uuid")
                if f_id:
                    final_temp_map[f"{sub_id}.{lvl}.{f_id}"] = evi

        final_sub_result = {
            "sub_criteria_id": sub_id,
            "sub_criteria_name": sub_criteria_name,
            "highest_full_level": highest_full_level,
            "weight": sub_weight,
            "target_level_achieved": highest_full_level >= roadmap_target_level,
            "weighted_score": weighted_score,
            "action_plan": action_plan_result, 
            "raw_results_ref": raw_results_for_sub_seq,
            "sub_summary": sub_summary,
            "worker_duration_s": round(time.time() - start_ts, 2)
        }

        return final_sub_result, final_temp_map
    
    def _run_expert_re_evaluation(
        self,
        sub_id: str,
        level: int,
        statement_text: str,
        context: str,
        first_attempt_reason: str,
        missing_tags: Set[str],
        highest_rerank_score: float,
        sub_criteria_name: str,
        llm_evaluator_to_use: Any,
        base_kwargs: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        [EXPERT LOOP v21.9.3] Fixed Context Delivery & Expert Hint
        """
        # 1. ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Infinite Loop (Max 2 times)
        if not hasattr(self, "_expert_re_eval_count"):
            self._expert_re_eval_count = 0
        self._expert_re_eval_count += 1
        
        if self._expert_re_eval_count > 2:
            self.logger.warning(f"üõë Expert Re-evaluation limit reached for {sub_id} L{level}")
            return {
                "score": 0.0, "is_passed": False, 
                "reason": f"Limit exceeded. Original error: {first_attempt_reason}",
                "P_Plan_Score": 0.0, "D_Do_Score": 0.0, "C_Check_Score": 0.0, "A_Act_Score": 0.0
            }

        self.logger.info(f"üîç [EXPERT RE-EVAL #{self._expert_re_eval_count}] Starting second pass for {sub_id} L{level}...")

        # 2. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Hint Message ‡πÉ‡∏´‡πâ AI
        hint_msg = f"""
        --- ‚ö†Ô∏è EXPERT RE-EVALUATION MODE (Attempt #{self._expert_re_eval_count}) ---
        ‡∏ó‡πà‡∏≤‡∏ô‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô SE-AM ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á
        
        ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:
        - ‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å (Rerank Score: {highest_rerank_score:.4f}) ‡πÅ‡∏ï‡πà‡∏£‡∏≠‡∏ö‡πÅ‡∏£‡∏Å LLM ‡πÉ‡∏´‡πâ‡∏ï‡∏Å
        - ‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏ó‡∏µ‡πà‡∏ï‡∏Å‡πÉ‡∏ô‡∏£‡∏≠‡∏ö‡πÅ‡∏£‡∏Å: {first_attempt_reason}
        - Tag ‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡πÑ‡∏°‡πà‡∏û‡∏ö: {', '.join(missing_tags) if missing_tags else 'N/A'}

        ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏û‡∏¥‡πÄ‡∏®‡∏©:
        ‡πÇ‡∏õ‡∏£‡∏î‡∏≠‡πà‡∏≤‡∏ô Context ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á ‡∏´‡∏≤‡∏Å‡∏û‡∏ö‡∏£‡πà‡∏≠‡∏á‡∏£‡∏≠‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà "‡∏™‡∏∞‡∏ó‡πâ‡∏≠‡∏ô‡∏ñ‡∏∂‡∏á" ‡∏ß‡∏á‡∏à‡∏£ PDCA 
        ‡πÅ‡∏°‡πâ‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏µ Keyword ‡∏ï‡∏£‡∏á‡∏ï‡∏±‡∏ß (‡πÄ‡∏ä‡πà‡∏ô ‡∏û‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏≠‡∏ô‡∏∏‡∏°‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô) 
        ‡∏à‡∏á‡πÉ‡∏ä‡πâ Expert Judgment ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° ‡∏≠‡∏¢‡πà‡∏≤‡∏ï‡∏¥‡∏î‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÄ‡∏î‡∏¥‡∏°
        """

        # 3. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Context (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á)
        final_context = f"{context}\n\n{hint_msg}" if context else hint_msg
        
        # 4. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Kwargs ‡πÅ‡∏•‡∏∞‡∏•‡∏ö‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô‡∏≠‡∏≠‡∏Å
        expert_kwargs = base_kwargs.copy()
        expert_kwargs.pop("context", None)
        expert_kwargs.pop("context_str", None)
        expert_kwargs.pop("final_llm_context", None)

        # 5. ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å Evaluator ‡πÅ‡∏ö‡∏ö Explicit (‡∏£‡∏∞‡∏ö‡∏∏‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô)
        result = llm_evaluator_to_use(
            context=final_context, # ‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤ positional 'context' ‡∏Ç‡∏≠‡∏á evaluator
            sub_id=sub_id,
            level=level,
            statement_text=statement_text,
            sub_criteria_name=f"{sub_criteria_name} (Expert Mode)",
            max_rerank_score=highest_rerank_score,
            **expert_kwargs
        )
        
        self.logger.info(f"‚úÖ [EXPERT RE-EVAL #{self._expert_re_eval_count}] Completed for {sub_id} L{level}")
        return result

    def _run_single_assessment(
        self,
        sub_criteria: Dict[str, Any],
        statement_data: Dict[str, Any],
        vectorstore_manager: Optional['VectorStoreManager'],
        sequential_chunk_uuids: Optional[List[str]] = None,
        attempt: int = 1
    ) -> Dict[str, Any]:
        """
        [REVISED v21.9.11 - PRODUCTION READY]
        - ‡∏Å‡∏≤‡∏£‡∏±‡∏ô‡∏ï‡∏µ‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á temp_map_for_level ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡πÄ‡∏™‡∏°‡∏≠‡πÅ‡∏°‡πâ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô
        - ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Metadata ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ UI ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏• Source of Evidence ‡πÑ‡∏î‡πâ‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥
        - ‡∏ö‡∏π‡∏£‡∏ì‡∏≤‡∏Å‡∏≤‡∏£ Focus Points ‡πÅ‡∏•‡∏∞ Evidence Guidelines ‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏ö RAG
        """
        start_time = time.time()
        sub_id = sub_criteria['sub_id']
        level = statement_data['level']
        statement_text = statement_data['statement']
        sub_criteria_name = sub_criteria['sub_criteria_name']
        statement_id = statement_data.get('statement_id', sub_id)

        # ‚öôÔ∏è Configuration Setup
        MAX_RETRI_ATTEMPTS = globals().get('MAX_RETRIEVAL_ATTEMPTS', 3)
        MIN_RETRY_SC = globals().get('MIN_RETRY_SCORE', 0.7)
        MIN_KEEP_SC = globals().get('MIN_RERANK_SCORE_TO_KEEP', 0.15)
        TARGET_SCORE_THRESHOLD_MAP = globals().get('TARGET_SCORE_THRESHOLD_MAP', {1:2, 2:2, 3:2, 4:2, 5:2})

        self.logger.info(f"  > Starting assessment for {sub_id} L{level} (Attempt: {attempt})...")

        # ==================== 1. PDCA & Keywords Setup ====================
        pdca_phase = self._get_pdca_phase(level)
        level_constraint = self._get_level_constraint_prompt(level)
        
        must_list = self.get_rule_content(sub_id, level, "must_include_keywords")
        must_include_keywords = ", ".join(must_list) if isinstance(must_list, list) else (must_list or "")
        
        avoid_list = self.get_rule_content(sub_id, level, "avoid_keywords")
        avoid_keywords = ", ".join(avoid_list) if isinstance(avoid_list, list) else (avoid_list or "")
        
        plan_keywords = self.get_rule_content(sub_id, level, "plan_keywords")

        # ==================== 2. Hybrid Retrieval Setup ====================
        mapped_ids, priority_unhydrated = self._get_mapped_uuids_and_priority_chunks(
            sub_id=sub_id, level=level, statement_text=statement_text,
            level_constraint=level_constraint, vectorstore_manager=vectorstore_manager
        )
        priority_docs = self._robust_hydrate_documents_for_priority_chunks(
            chunks_to_hydrate=priority_unhydrated, vsm=vectorstore_manager, current_sub_id=sub_id
        )

        # ==================== 3. Enhanced Query with Focus & Guidelines ====================
        focus_points = sub_criteria.get('focus_points', [])
        guideline = sub_criteria.get('evidence_guidelines', {}).get(f'level_{level}', "")

        rag_query_list = self.enhance_query_for_statement(
            statement_text=statement_text, 
            sub_id=sub_id, 
            statement_id=statement_id,
            level=level, 
            focus_hint=level_constraint,
            additional_context={
                "focus_points": focus_points,
                "guideline": guideline
            }
        )

        # ==================== 4. LLM Evaluator Selection ====================
        llm_evaluator_to_use = evaluate_with_llm_low_level if level <= 2 else self.llm_evaluator

        # ==================== 5. ADAPTIVE RAG LOOP ====================
        highest_rerank_score = -1.0
        final_top_evidences = []

        for loop_attempt in range(1, MAX_RETRI_ATTEMPTS + 1):
            query_input = rag_query_list if loop_attempt == 1 and rag_query_list else [statement_text]
            try:
                retrieval_result = self.rag_retriever(
                    query=query_input, doc_type=self.doc_type, enabler=self.config.enabler,
                    sub_id=sub_id, level=level, vectorstore_manager=vectorstore_manager,
                    mapped_uuids=mapped_ids, priority_docs_input=priority_docs,
                    sequential_chunk_uuids=sequential_chunk_uuids,
                )
                top_evidences_current = retrieval_result.get("top_evidences", [])
                
                # ‡∏´‡∏≤‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
                current_max = max((ev.get('metadata', {}).get('rerank_score', 0.0) for ev in top_evidences_current), default=0.0)
                priority_max = max((doc.get('metadata', {}).get('rerank_score', 0.0) for doc in priority_docs), default=0.0)
                overall_max = max(current_max, priority_max)

                if overall_max >= highest_rerank_score:
                    highest_rerank_score = overall_max
                    final_top_evidences = top_evidences_current + priority_docs

                if highest_rerank_score >= MIN_RETRY_SC:
                    break 
            except Exception as e:
                self.logger.error(f"RAG retrieval failed at loop {loop_attempt}: {e}")
                break

        # Filter & Expand Context
        top_evidences = [doc for doc in final_top_evidences if doc.get('metadata', {}).get('rerank_score', 0) >= MIN_KEEP_SC or doc.get('is_baseline', False)]
        
        if not top_evidences and level <= 2:
            top_evidences = sorted(final_top_evidences, key=lambda x: x.get('metadata', {}).get('rerank_score', 0), reverse=True)[:10]

        if top_evidences and vectorstore_manager:
            top_evidences = self._robust_hydrate_documents_for_priority_chunks(top_evidences, vectorstore_manager)
            top_evidences = self._expand_context_with_neighbor_pages(top_evidences, f"evidence_{self.config.enabler.lower()}")

        # ==================== 6. Context Building ====================
        previous_evidence = self._collect_previous_level_evidences(sub_id, level) if level > 1 else {}
        flat_previous = [item for sublist in previous_evidence.values() for item in sublist]

        plan_blocks, do_blocks, check_blocks, act_blocks, other_blocks = self._get_pdca_blocks_from_evidences(
            top_evidences + flat_previous,
            baseline_evidences=previous_evidence,
            level=level,
            sub_id=sub_id,
            contextual_rules_map=self.contextual_rules_map
        )

        channels = build_multichannel_context_for_level(level, top_evidences, flat_previous)
        final_llm_context = "\n\n".join(filter(None, [
            f"--- DIRECT EVIDENCE (L{level} | PDCA Structured)---\n{plan_blocks}\n{do_blocks}\n{check_blocks}\n{act_blocks}\n{other_blocks}",
            f"--- AUXILIARY SUMMARY ---\n{channels.get('aux_summary')}",
            f"--- BASELINE SUMMARY ---\n{channels.get('baseline_summary')}"
        ]))

        # ==================== 7. Evidence Strength & Tags ====================
        available_tags = {tag for tag, block in zip(['P', 'D', 'C', 'A'], [plan_blocks, do_blocks, check_blocks, act_blocks]) if block.strip()}
        evi_cap_data = self._calculate_evidence_strength_cap(top_evidences, level, highest_rerank_score)
        max_evi_str_for_prompt = evi_cap_data['max_evi_str_for_prompt']

        # ==================== 8. Contextual Rule Logic ====================
        rule_instruction = self.get_rule_content(sub_id, level, "specific_contextual_rule")
        if not rule_instruction:
            rule_instruction = f"‡πÄ‡∏ô‡πâ‡∏ô‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ï‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠: {', '.join(focus_points)}" if focus_points else "‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå SE-AM ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô"

        # ==================== 9. EVALUATION EXECUTION ====================
        llm_kwargs = {
            "context": final_llm_context,
            "sub_criteria_name": sub_criteria_name,
            "level": level,
            "statement_text": statement_text,
            "sub_id": sub_id,
            "pdca_phase": pdca_phase,
            "level_constraint": level_constraint,
            "must_include_keywords": must_include_keywords,
            "avoid_keywords": avoid_keywords,
            "specific_contextual_rule": rule_instruction,
            "max_rerank_score": highest_rerank_score,
            "max_evidence_strength": max_evi_str_for_prompt,
            "llm_executor": self.llm,
            "ai_confidence": "HIGH" if len(available_tags) >= 2 else "MEDIUM",
            "target_score_threshold": TARGET_SCORE_THRESHOLD_MAP.get(level, 2),
            "planning_keywords": plan_keywords if level <= 2 else "N/A"
        }

        llm_result = llm_evaluator_to_use(**llm_kwargs)

        # Expert Re-evaluation Fallback
        if not llm_result.get('is_passed', False) and highest_rerank_score >= 0.6:
            try:
                llm_result = self._run_expert_re_evaluation(
                    sub_id=sub_id, level=level, statement_text=statement_text,
                    context=final_llm_context, first_attempt_reason=llm_result.get('reason', 'N/A'),
                    missing_tags=set(), highest_rerank_score=highest_rerank_score,
                    sub_criteria_name=sub_criteria_name, llm_evaluator_to_use=llm_evaluator_to_use,
                    base_kwargs=llm_kwargs
                )
            except Exception: pass

        # ==================== 10. Metadata Mapping for Save (THE FIX) ====================
        # ‡∏™‡∏Å‡∏±‡∏î‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡∏µ‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á‡∏Å‡∏•‡∏±‡∏ö ‡πÅ‡∏°‡πâ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô
        temp_map_for_level = []
        for doc in top_evidences:
            meta = doc.get('metadata', {})
            chunk_id = meta.get('id') or meta.get('uuid') or meta.get('chunk_id')
            if chunk_id:
                temp_map_for_level.append({
                    "id": chunk_id,
                    "file_id": meta.get('file_id') or meta.get('uuid'),
                    "file_name": meta.get('file_name', 'Unknown'),
                    "page": meta.get('page', 'N/A'),
                    "rerank_score": meta.get('rerank_score', 0.0),
                    "content": doc.page_content if hasattr(doc, 'page_content') else ""
                })

        # ==================== 11. Final Output Mapping ====================
        llm_result = post_process_llm_result(llm_result, level)
        thai_summary_data = create_context_summary_llm(final_llm_context, sub_criteria_name, level, sub_id, self.llm)

        return {
            "sub_criteria_id": sub_id,
            "level": level,
            "is_passed": llm_result.get('is_passed', False),
            "score": llm_result.get('score', 0.0),
            "pdca_breakdown": llm_result.get('pdca_breakdown', {'P': 0, 'D': 0, 'C': 0, 'A': 0}),
            "reason": llm_result.get('reason', "No reason provided"),
            "evidence_strength": max_evi_str_for_prompt if llm_result.get('is_passed', False) else 0.0,
            "max_relevant_score": highest_rerank_score,
            "summary_thai": thai_summary_data.get("summary"),
            "temp_map_for_level": temp_map_for_level, # ‡∏™‡πà‡∏á‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡πÉ‡∏´‡πâ worker ‡∏ï‡∏•‡∏≠‡∏î‡πÄ‡∏ß‡∏•‡∏≤
            "duration": time.time() - start_time
        }