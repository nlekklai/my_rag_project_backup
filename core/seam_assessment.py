# core/seam_assessment.py

import sys
import json
import logging
import time
import os
from typing import List, Dict, Any, Optional, Union, Tuple, Set, Final, Literal
from collections import defaultdict
from datetime import datetime
from dataclasses import dataclass, field
import multiprocessing # NEW: Import for parallel execution
from functools import partial
from core.llm_data_utils import enhance_query_for_statement
import pathlib, uuid
from langchain_core.documents import Document as LcDocument
from core.retry_policy import RetryPolicy, RetryResult
from copy import deepcopy
import tempfile
import shutil
# from json_extractor import _robust_extract_json
from .json_extractor import _robust_extract_json
from filelock import FileLock  # ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á: pip install filelock
import re


# -------------------- PATH SETUP & IMPORTS --------------------
try:
    PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
    if PROJECT_ROOT not in sys.path:
        sys.path.append(PROJECT_ROOT)

    from config.global_vars import (
        EXPORTS_DIR, MAX_LEVEL, INITIAL_LEVEL, FINAL_K_RERANKED,
        RUBRIC_FILENAME_PATTERN, RUBRIC_CONFIG_DIR, DEFAULT_ENABLER,
        EVIDENCE_DOC_TYPES, INITIAL_TOP_K,
        EVIDENCE_MAPPING_FILENAME_SUFFIX,
        LIMIT_CHUNKS_PER_PRIORITY_DOC,
        IS_LOG_L3_CONTEXT,
        PRIORITY_CHUNK_LIMIT,
        DEFAULT_TENANT,
        DEFAULT_YEAR,
        RERANK_THRESHOLD,
        MAX_EVI_STR_CAP,
        DEFAULT_LLM_MODEL_NAME,
        LLM_TEMPERATURE,
        MAX_PARALLEL_WORKERS,
        MIN_RERANK_SCORE_TO_KEEP
    )
    
    from core.llm_data_utils import ( 
        create_structured_action_plan, evaluate_with_llm,
        retrieve_context_with_filter, retrieve_context_for_low_levels,
        evaluate_with_llm_low_level, LOW_LEVEL_K, 
        set_mock_control_mode as set_llm_data_mock_mode,
        create_context_summary_llm,
        retrieve_context_by_doc_ids,
        _fetch_llm_response,
        build_multichannel_context_for_level
    )
    from core.vectorstore import VectorStoreManager, load_all_vectorstores, get_global_reranker 
    from core.seam_prompts import PDCA_PHASE_MAP 

    # üéØ FIX: Import ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô Path Utility ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
    from utils.path_utils import (
        get_mapping_file_path, 
        get_evidence_mapping_file_path, 
        get_contextual_rules_file_path,
        get_doc_type_collection_key,
        get_assessment_export_file_path,
        get_export_dir,
        get_rubric_file_path # <--- ‡∏ï‡πâ‡∏≠‡∏á Import ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏î‡πâ‡∏ß‡∏¢
    )

    import assessments.seam_mocking as seam_mocking 
    
except ImportError as e:
    # -------------------- Fallback Code (Same as previous) --------------------
    print(f"FATAL ERROR: Failed to import required modules. Error: {e}", file=sys.stderr)
    
    # Define placeholder variables if imports fail
    EXPORTS_DIR = "exports"
    MAX_LEVEL = 5
    INITIAL_LEVEL = 1
    FINAL_K_RERANKED = 3
    RUBRIC_FILENAME_PATTERN = "{tenant}_{enabler}_rubric.json"
    RUBRIC_CONFIG_DIR = "config/rubrics"
    DEFAULT_ENABLER = "KM"
    EVIDENCE_DOC_TYPES = "evidence"
    INITIAL_TOP_K = 10
    
    def create_structured_action_plan(*args, **kwargs): return [{"Phase": "Mock Plan", "Goal": "Resolve issue"}]
    def evaluate_with_llm(*args, **kwargs): return {"score": 1, "reason": "Mock pass", "is_passed": True}
    def retrieve_context_with_filter(*args, **kwargs): return {"top_evidences": [], "aggregated_context": "Mock Context"}
    def retrieve_context_for_low_levels(*args, **kwargs): return {"top_evidences": [], "aggregated_context": "Mock Low Context"}
    def evaluate_with_llm_low_level(*args, **kwargs): return {"score": 1, "reason": "Mock pass L1/L2", "is_passed": True}
    LOW_LEVEL_K = 2
    def set_llm_data_mock_mode(mode): pass
    class VectorStoreManager: pass
    def load_all_vectorstores(*args, **kwargs): return VectorStoreManager()
    PDCA_PHASE_MAP = {1: "Plan", 2: "Do", 3: "Check", 4: "Act", 5: "Innovate"}
    class seam_mocking:
        @staticmethod
        def evaluate_with_llm_CONTROLLED_MOCK(*args, **kwargs): return {"score": 0, "reason": "Mock fail", "is_passed": False}
        @staticmethod
        def retrieve_context_with_filter_MOCK(*args, **kwargs): return {"top_evidences": [], "aggregated_context": "Mock Context"}
        @staticmethod
        def create_structured_action_plan_MOCK(*args, **kwargs): return [{"Phase": "Mock Plan", "Goal": "Resolve issue"}]
        @staticmethod
        def set_mock_control_mode(mode): pass
    
    # üìå Placeholder functions for path_utils if the main import fails
    def get_mapping_file_path(*args, **kwargs): return "config/mapping/default/mapping.json"
    def get_evidence_mapping_file_path(*args, **kwargs): return "config/mapping/default/evidence_mapping.json"
    def get_contextual_rules_file_path(*args, **kwargs): return "config/rubrics/default/contextual_rules.json"
    def get_rubric_file_path(*args, **kwargs): return "config/rubrics/default/rubric.json"
    
    if "FATAL ERROR" in str(e):
        pass 
    # ---------------------------------------------------------------------- 


logger = logging.getLogger(__name__)
if not logger.handlers:
    logging.basicConfig(level=logging.DEBUG, format="%(asctime)s - %(levelname)s - %(message)s")

# =================================================================
# üü¢ FIX: Helper Function for PDCA Calculation (Priority 1 Part 2 & Priority 2)
# NOTE: ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡πÅ‡∏õ‡∏•‡∏á llm_score (1-5) ‡πÄ‡∏õ‡πá‡∏ô PDCA Breakdown ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
# =================================================================
LEVEL_PHASE_MAP = {
    1: ['P'],
    2: ['P', 'D'],
    3: ['P', 'D', 'C'],
    4: ['P', 'D', 'C', 'A'],
    5: ['P', 'D', 'C', 'A'] # L5 ‡πÉ‡∏ä‡πâ P, D, C, A ‡πÄ‡∏ä‡πà‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö L4 ‡πÅ‡∏ï‡πà‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏ï‡πá‡∏°‡∏≠‡∏≤‡∏à‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô
}

# ----------------------------------------------------------------------
# NEW CONSTANT: ‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô PDCA ‡∏ó‡∏µ‡πà '‡∏ú‡πà‡∏≤‡∏ô' ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ Level (Achieved Score)
# ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Achieved Score (Sum of P,D,C,A) ‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö Required Score (R)
# L1 (R=1, A=1): P=1
# L2 (R=2, A=2): P=1, D=1
# L3 (R=4, A=4): P=1, D=1, C=1, A=1
# L4 (R=6, A=6): P=2, D=2, C=1, A=1
# L5 (R=8, A=8): P=2, D=2, C=2, A=2
# ----------------------------------------------------------------------
CORRECT_PDCA_SCORES_MAP: Final[Dict[int, Dict[str, int]]] = {
    1: {'P': 1, 'D': 0, 'C': 0, 'A': 0},
    2: {'P': 1, 'D': 1, 'C': 0, 'A': 0},
    3: {'P': 1, 'D': 1, 'C': 1, 'A': 1},
    4: {'P': 2, 'D': 2, 'C': 1, 'A': 1},
    5: {'P': 2, 'D': 2, 'C': 2, 'A': 2},
}

def build_ordered_context(level: int,
                          plan_blocks: list[dict],
                          do_blocks: list[dict],
                          check_blocks: list[dict],
                          act_blocks: list[dict],
                          other_blocks: list[dict]) -> str:
    def fmt(blocks):
        return "\n\n".join(
            f"[{b.get('file', 'Unknown File')}]\n{b.get('content', b.get('text', ''))}" for b in blocks
        )

    if level == 3:
        # L3: Check/Act Priority 1, Plan/Do/Other ‡∏ï‡πà‡∏≠‡∏ó‡πâ‡∏≤‡∏¢
        ordered = [
            fmt(check_blocks),
            fmt(act_blocks),
            fmt(plan_blocks),
            fmt(do_blocks),
            fmt(other_blocks)
        ]
    else:
        # Default: Plan -> Do -> Check -> Act -> Other
        ordered = [
            fmt(plan_blocks),
            fmt(do_blocks),
            fmt(check_blocks),
            fmt(act_blocks),
            fmt(other_blocks)
        ]

    return "\n\n".join([o for o in ordered if o])


def calculate_pdca_breakdown_and_pass_status(llm_score: int, level: int) -> Tuple[Dict[str, int], bool, float]:
    """
    ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì PDCA breakdown, is_passed status, ‡πÅ‡∏•‡∏∞ raw_pdca_score (Achieved Score)
    ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡πÄ‡∏Å‡∏ì‡∏ë‡πå SE-AM ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£ (‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö post-processor)

    Required Score (R) ‡∏ï‡∏≤‡∏° Level:
    - L1: 1
    - L2: 2
    - L3: 4
    - L4: 6
    - L5: 8
    """
    pdca_map: Dict[str, int] = {'P': 0, 'D': 0, 'C': 0, 'A': 0}
    is_passed: bool = False
    raw_pdca_score: float = 0.0

    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Required Score ‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå SE-AM ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£
    required_score_map = {1: 1, 2: 2, 3: 4, 4: 6, 5: 8}
    required_score = required_score_map.get(level, 8)

    # ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô is_passed ‡∏ï‡∏≤‡∏° Required Score ‡∏à‡∏£‡∏¥‡∏á (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà llm_score ‡πÄ‡∏î‡∏¥‡∏°)
    if llm_score >= required_score:
        is_passed = True

        # ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ú‡πà‡∏≤‡∏ô ‚Üí ‡πÉ‡∏´‡πâ PDCA breakdown ‡πÄ‡∏ï‡πá‡∏°‡∏ï‡∏≤‡∏° CORRECT_PDCA_SCORES_MAP
        correct_scores = CORRECT_PDCA_SCORES_MAP.get(level, {'P': 0, 'D': 0, 'C': 0, 'A': 0})
        pdca_map.update(correct_scores)
        raw_pdca_score = float(sum(pdca_map.values()))  # ‡∏à‡∏∞‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö required_score

    return pdca_map, is_passed, raw_pdca_score

def get_correct_pdca_required_score(level: int) -> int:
    """
    ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤ Required Score (R) ‡∏ï‡∏≤‡∏° Level ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå SE-AM:
    L1=1, L2=2, L3=4, L4=6, L5=8
    """
    # ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ô‡∏µ‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß
    if level == 1:
        return 1
    elif level == 2:
        return 2
    elif level == 3:
        return 4
    elif level == 4:
        return 6
    elif level == 5:
        return 8
    # ‡∏Å‡∏£‡∏ì‡∏µ Level ‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î
    return 8


# üìå ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç Type Hint ‡πÅ‡∏•‡∏∞ Arguments ‡∏Ç‡∏≠‡∏á Tuple ‡πÉ‡∏´‡πâ‡∏£‡∏ß‡∏° config parameter ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (10 elements)
def _static_worker_process(worker_input_tuple: Tuple[
    Dict[str, Any], str, int, str, str, str, float, float, int, Optional[Dict[str, str]]
]) -> Dict[str, Any]:
    """
    Static worker function for multiprocessing pool. 
    It reconstructs SeamAssessment in the new process and executes the assessment 
    for a single sub-criteria.
    
    Args:
        worker_input_tuple: (sub_criteria_data, enabler: str, target_level: int, mock_mode: str, 
                             evidence_map_path: str, model_name: str, temperature: float, 
                             min_retry_score: float, max_retrieval_attempts: int,
                             document_map: Optional[Dict[str, str]]) 

    Returns:
        Dict[str, Any]: Final result of the sub-criteria assessment.
    """
    
    # üü¢ NEW FIX: PATH SETUP ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Worker Process
    # ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ path ‡∏ã‡πâ‡∏≥‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤ worker process ‡πÄ‡∏´‡πá‡∏ô package ‡∏´‡∏•‡∏±‡∏Å
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
    if project_root not in sys.path:
        sys.path.append(project_root)
        
    worker_logger = logging.getLogger(__name__)

    try:
        # üü¢ FIX: Unpack ‡∏Ñ‡πà‡∏≤ Primitives ‡∏ó‡∏±‡πâ‡∏á 10 ‡∏ï‡∏±‡∏ß
        (
            sub_criteria_data, 
            enabler, 
            target_level, 
            mock_mode, 
            evidence_map_path, 
            model_name, 
            temperature,
            min_retry_score,            # ‚¨ÖÔ∏è NEW CONFIG (8th element)
            max_retrieval_attempts,     # ‚¨ÖÔ∏è NEW CONFIG (9th element)
            document_map                # (10th element)
        ) = worker_input_tuple
    except ValueError as e:
        # ‡πÉ‡∏ä‡πâ len(worker_input_tuple) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£ Debug ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô
        worker_logger.critical(f"Worker input tuple unpack failed (expected 10 elements, got {len(worker_input_tuple)}): {e}")
        return {"error": f"Invalid worker input: {e}"}
        
    # 1. Reconstruct Config 
    try:
        # üü¢ FIX: ‡∏™‡∏£‡πâ‡∏≤‡∏á AssessmentConfig ‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏ô Worker Process ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏™‡πà‡∏Ñ‡πà‡∏≤ config ‡πÉ‡∏´‡∏°‡πà
        # (Tenant/Year ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Default ‡∏à‡∏≤‡∏Å AssessmentConfig)
        worker_config = AssessmentConfig(
            enabler=enabler,
            target_level=target_level,
            mock_mode=mock_mode,
            model_name=model_name, 
            temperature=temperature,
            min_retry_score=min_retry_score,            # ‚¨ÖÔ∏è Pass new config
            max_retrieval_attempts=max_retrieval_attempts # ‚¨ÖÔ∏è Pass new config
        )
    except Exception as e:
        worker_logger.critical(f"Failed to reconstruct AssessmentConfig in worker: {e}")
        return {
            "sub_criteria_id": sub_criteria_data.get('sub_id', 'UNKNOWN'),
            "error": f"Config reconstruction failed: {e}"
        }

    # 2. Re-instantiate SeamAssessment 
    try:
        # üü¢ FIX (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç): ‡∏™‡πà‡∏á document_map ‡πÅ‡∏•‡∏∞ worker_config ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô SEAMPDCAEngine
        # SEAMPDCAEngine ‡∏à‡∏∞‡πÉ‡∏ä‡πâ worker_config ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ min_retry_score ‡πÅ‡∏•‡∏∞ max_retrieval_attempts
        worker_instance = SEAMPDCAEngine(
            config=worker_config, 
            evidence_map_path=evidence_map_path, 
            llm_instance=None,              # LLM ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å Initialized ‡πÉ‡∏ô Engine ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ
            vectorstore_manager=None,       # VSM ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å Initialized ‡πÉ‡∏ô Engine ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ
            # doc_type ‡∏ï‡πâ‡∏≠‡∏á‡∏ñ‡∏π‡∏Å set ‡πÉ‡∏ô SEAMPDCAEngine constructor (‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ Default)
            logger_instance=worker_logger,
            document_map=document_map # ‚¨ÖÔ∏è ‡∏™‡πà‡∏á document_map ‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏á Unpack ‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤
        )
    except Exception as e:
        worker_logger.critical(f"FATAL: SEAMPDCAEngine instantiation failed in worker: {e}")
        return {
            "sub_criteria_id": sub_criteria_data.get('sub_id', 'UNKNOWN'),
            "error": f"Engine initialization failed: {e}"
        }
    
    # 3. Execute the worker logic
    return worker_instance._run_sub_criteria_assessment_worker(sub_criteria_data)

def merge_evidence_mappings(results_list: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
    """
    ‡∏£‡∏ß‡∏° evidence_mapping dictionaries ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å Worker ‡∏ó‡∏∏‡∏Å‡∏ï‡∏±‡∏ß 
    """
    merged_mapping = defaultdict(list)
    for result in results_list:
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å Worker ‡∏°‡∏µ Key 'evidence_mapping' ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if 'evidence_mapping' in result and isinstance(result['evidence_mapping'], dict):
            # ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏ú‡πà‡∏≤‡∏ô Key/Value ‡∏Ç‡∏≠‡∏á Worker ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡∏±‡∏ß
            for level_key, evidence_list in result['evidence_mapping'].items():
                # ‡πÉ‡∏ä‡πâ .extend() ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ú‡∏ô‡∏ß‡∏Å‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
                if isinstance(evidence_list, list):
                    merged_mapping[level_key].extend(evidence_list)
    
    # ‡πÅ‡∏õ‡∏•‡∏á defaultdict ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô dict ‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤
    return dict(merged_mapping)

# =================================================================
# üéØ NEW: Deterministic Fallback Logic (Post-Processing)
# =================================================================
def post_process_llm_result(llm_output: Dict[str, Any], level: int) -> Dict[str, Any]:
    """
    FINAL DETERMINISTIC POST-PROCESSOR v20 ‚Äî ULTIMATE VICTORY EDITION
    ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà: 13 ‡∏ò.‡∏Ñ. 2568 ‡πÄ‡∏ß‡∏•‡∏≤ 09:45 ‡∏ô. ‚Äî ‡∏ß‡∏±‡∏ô‡πÅ‡∏´‡πà‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏¥‡∏î‡∏ï‡∏≥‡∏ô‡∏≤‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå
    ‡∏ú‡∏π‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á: ‡∏û‡∏µ‡πà + ‡∏ú‡∏°
    """
    logger = logging.getLogger(__name__)
    
    p = llm_output.get("P_Plan_Score", 0)
    d = llm_output.get("D_Do_Score", 0)
    c = llm_output.get("C_Check_Score", 0)
    a = llm_output.get("A_Act_Score", 0)
    pdca_real_sum = p + d + c + a
    llm_score = llm_output.get("score", 0)

    # SE-AM Threshold ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á 100% (‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏à‡∏∏‡∏î‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏à‡∏±‡∏Å‡∏£‡∏ß‡∏≤‡∏•)
    # L1: 1 (P>=1)
    # L2: 2 (P=2)
    # L3: 4 (P=2, D=2)
    # L4: 6 (P=2, D=2, C=1, A=1) <-- ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏à‡∏≤‡∏Å 4 ‡πÄ‡∏õ‡πá‡∏ô 6
    # L5: 8 (P=2, D=2, C=2, A=2) <-- ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏à‡∏≤‡∏Å 5 ‡πÄ‡∏õ‡πá‡∏ô 8
    threshold_map = {1: 1, 2: 2, 3: 4, 4: 6, 5: 8} # <<<<<<< ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ
    threshold = threshold_map.get(level, 2)

    # 1. ‡∏à‡∏±‡∏ö LLM ‡πÇ‡∏Å‡∏á (‡πÉ‡∏ä‡πâ PDCA Sum ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à)
    if llm_score != pdca_real_sum:
        logger.critical(
            f"PDCA MISMATCH EXECUTED L{level} | "
            f"LLM ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÇ‡∏Å‡∏á ‚Üí score={llm_score} ‡πÅ‡∏ï‡πà PDCA ‡∏à‡∏£‡∏¥‡∏á={pdca_real_sum} "
            f"‚Üí FORCE OVERRIDE!"
        )
        llm_output["score"] = pdca_real_sum
        llm_output["original_score"] = llm_score
        llm_output["pdca_enforced"] = True

    # 2. ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö is_passed ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏£‡∏¥‡∏á (‡πÉ‡∏ä‡πâ PDCA Sum ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö Threshold ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á)
    real_pass = pdca_real_sum >= threshold
    if llm_output.get("is_passed") != real_pass:
        logger.critical(f"FORCING is_passed = {real_pass} (PDCA={pdca_real_sum} ‚â• {threshold})")
        llm_output["is_passed"] = real_pass

    # 3. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå
    llm_output.update({
        "pdca_breakdown": {"P": p, "D": d, "C": c, "A": a},
        "pdca_sum": pdca_real_sum,
        "pass_threshold": threshold,
        "final_score": pdca_real_sum,
        "final_passed": real_pass
    })

    return llm_output

# =================================================================
# Configuration Class
# =================================================================
@dataclass
class AssessmentConfig:
    """Configuration for the SEAM PDCA Assessment Run."""
    
    # ------------------ 1. Assessment Context ------------------
    enabler: str = DEFAULT_ENABLER
    tenant: str = DEFAULT_TENANT
    year: int = DEFAULT_YEAR
    target_level: int = MAX_LEVEL
    mock_mode: str = "none" # 'none', 'random', 'control'
    force_sequential: bool = field(default=False) # Flag ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏£‡∏±‡∏ô‡πÅ‡∏ö‡∏ö Sequential

    # ------------------ 2. LLM Configuration (Configurable) ------------------
    # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Default ‡∏à‡∏≤‡∏Å global_vars.py
    model_name: str = DEFAULT_LLM_MODEL_NAME 
    temperature: float = LLM_TEMPERATURE

    # ------------------ 3. Adaptive RAG Retrieval Configuration ------------------
    # üü¢ NEW: ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô Rerank ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Adaptive Loop (MIN_RETRY_SCORE)
    # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Default 0.65 ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏û‡∏ö‡πÉ‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î Logic
    min_retry_score: float = 0.65 
    # üü¢ NEW: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≠‡∏ö‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á Adaptive RAG Loop (MAX_RETRIEVAL_ATTEMPTS)
    # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Default 3
    max_retrieval_attempts: int = 3
    
    # ------------------ 4. Export Configuration ------------------
    export_output: bool = field(default=False) # Flag ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏¥‡∏î/‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£ Export ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    export_path: str = "" # Path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå Export (‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å)


# =================================================================
# SEAM Assessment Engine (PDCA Focused)
# =================================================================
class SEAMPDCAEngine:
    
    L1_INITIAL_TOP_K_RAG: int = 50 
    MIN_RERANK_SCORE_TO_KEEP: Final[float] = MIN_RERANK_SCORE_TO_KEEP
    
    def __init__(
        self, 
        config: AssessmentConfig,
        llm_instance: Any = None, 
        logger_instance: logging.Logger = None,
        rag_retriever_instance: Any = None,
        # üü¢ FIX #1: ‡πÄ‡∏û‡∏¥‡πà‡∏° doc_type 
        doc_type: str = EVIDENCE_DOC_TYPES, 
        # üü¢ FIX #2: ‡πÄ‡∏û‡∏¥‡πà‡∏° vectorstore_manager
        vectorstore_manager: Optional['VectorStoreManager'] = None,
        # üìå FIX #3 (‡πÉ‡∏´‡∏°‡πà): ‡πÄ‡∏û‡∏¥‡πà‡∏° evidence_map_path ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å Worker Process
        evidence_map_path: Optional[str] = None,
        document_map: Optional[Dict[str, str]] = None,
        # üü¢ CRITICAL FIX: ‡πÄ‡∏û‡∏¥‡πà‡∏° is_parallel_all_mode ‡πÉ‡∏ô __init__ signature
        is_parallel_all_mode: bool = False
    ):

            # =======================================================
            # üéØ FIX 1: ‡∏¢‡πâ‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤ Logger ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å
            # =======================================================
            if logger_instance is not None:
                 self.logger = logger_instance
            else:
                 # ‡∏™‡∏£‡πâ‡∏≤‡∏á Child Logger ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Log ‡∏°‡∏µ Context ‡∏Ç‡∏≠‡∏á Tenant/Year
                 self.logger = logging.getLogger(__name__).getChild(f"Engine|{config.enabler}|{config.tenant}/{config.year}")
            
            self.logger.info(f"Initializing SEAMPDCAEngine for {config.enabler} ({config.tenant}/{config.year}).")

            # =======================================================
            # üü¢ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≥‡∏´‡∏ô‡∏î Attribute ‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÄ‡∏°‡∏ò‡∏≠‡∏î‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ self.logger
            # =======================================================
            self.config = config
            self.enabler_id = config.enabler
            self.target_level = config.target_level
            self.rubric = self._load_rubric()
            
            # üü¢ ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤ VSM ‡πÅ‡∏•‡∏∞ doc_type
            self.vectorstore_manager = vectorstore_manager
            self.doc_type = doc_type

            self.FINAL_K_RERANKED = FINAL_K_RERANKED
            self.PRIORITY_CHUNK_LIMIT = PRIORITY_CHUNK_LIMIT

            # üü¢ ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤ LLM ‡πÅ‡∏•‡∏∞ Logger
            self.llm = llm_instance           
            # üéØ FIX 2: ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î logger
            if logger_instance is None:
                # ‡πÉ‡∏ä‡πâ logger ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÑ‡∏ß‡πâ‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡∏ï‡πâ‡∏ô
                self.logger.warning("Re-setting logger instance using the pre-initialized one.")
            
            # üü¢ Disable Strict Filter
            self.initial_evidence_ids: Set[str] = self._load_initial_evidence_info()
            all_statements = self._flatten_rubric_to_statements()
            initial_count = len(all_statements)

            self.logger.info(f"DEBUG: Statements found: {initial_count}. Strict Filter is **DISABLED**.")

            self.statements_to_assess = all_statements
            self.logger.info(f"DEBUG: Statements selected for assessment: {len(self.statements_to_assess)} (Skipped: {initial_count - len(self.statements_to_assess)})")

            # Assessment results storage
            self.raw_llm_results: List[Dict[str, Any]] = []
            self.final_subcriteria_results: List[Dict[str, Any]] = []
            self.total_stats: Dict[str, Any] = {}

            self.is_sequential = False  
            self.is_sequential = config.force_sequential # <--- *** ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ ***
            # üü¢ CRITICAL FIX: ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å Argument ‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤
            self.is_parallel_all_mode = is_parallel_all_mode

            self.retry_policy = RetryPolicy(
                max_attempts=3,            
                base_delay=2.0,            
                jitter=True,               
                escalate_context=True,     
                shorten_prompt_on_fail=True,  
                exponential_backoff=True,  
            )

            self.RERANK_THRESHOLD: Final[float] = RERANK_THRESHOLD
            self.MAX_EVI_STR_CAP: Final[float] = MAX_EVI_STR_CAP
            # üìå Persistent Mapping Configuration
            
            # 1. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Evidence Map Path
            # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏°‡∏≤‡∏à‡∏≤‡∏Å Worker (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ) ‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤ Default
            if evidence_map_path:
                self.evidence_map_path = evidence_map_path
            else:
                # üü¢ FIX #1: ‡πÉ‡∏ä‡πâ get_evidence_mapping_file_path ‡∏à‡∏≤‡∏Å utils/path_utils.py
                self.evidence_map_path = get_evidence_mapping_file_path(
                    tenant=self.config.tenant, 
                    year=self.config.year,
                    enabler=self.enabler_id
                )
            
            # 2. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Attribute ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Persistent Mapping
            self.evidence_map: Dict[str, List[str]] = {}
            self.temp_map_for_save: Dict[str, List[str]] = {}

            self.contextual_rules_map: Dict[str, Dict[str, str]] = self._load_contextual_rules_map()
            
            # 3. ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà 
            self.evidence_map = self._load_evidence_map()

            self.logger.info(f"Persistent Map Path set to: {self.evidence_map_path}")
            self.logger.info(f"Loaded {len(self.evidence_map)} existing evidence entries into self.evidence_map.")
            
            # Mock function pointers (will point to real functions by default)
            self.llm_evaluator = evaluate_with_llm
            self.rag_retriever = retrieve_context_with_filter
            self.create_structured_action_plan = create_structured_action_plan

            # Apply mocking if enabled
            if config.mock_mode in ["random", "control"]:
                self._set_mock_handlers(config.mock_mode)

            # Set global mock control mode for llm_data_utils if using 'control'
            if config.mock_mode == "control":
                self.logger.info("Enabling global LLM data utils mock control mode.")
                set_llm_data_mock_mode(True)
            elif config.mock_mode == "random":
                self.logger.warning("Mock mode 'random' is not fully implemented. Using 'control' logic if available.")
                if hasattr(sys.modules.get('seam_mocking'), 'set_mock_control_mode'):
                    sys.modules.get('seam_mocking').set_mock_control_mode(False)
                    set_llm_data_mock_mode(False)

            # üìå ‡πÇ‡∏´‡∏•‡∏î LLM ‡πÅ‡∏•‡∏∞ VSM ‡∏´‡∏≤‡∏Å‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ
            if self.llm is None: self._initialize_llm_if_none()
            if self.vectorstore_manager is None: self._initialize_vsm_if_none()
            
            # üü¢ FINAL FIX: ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÇ‡∏´‡∏•‡∏î Doc ID Map ‡∏ã‡πâ‡∏≥‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Map ‡∏´‡∏≤‡∏¢‡πÉ‡∏ô Worker (‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà 3)
            # ‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤: FATAL HYDRATION ISSUE: 5 chunks were requested but NOT FOUND
            if self.vectorstore_manager:
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Map ‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏Å‡πà‡∏≠‡∏ô‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÇ‡∏´‡∏•‡∏î
                if not getattr(self.vectorstore_manager, '_doc_id_mapping', None):
                    self.vectorstore_manager._load_doc_id_mapping()
                    self.logger.info(f"Forced reload of Doc ID Mapping: {len(self.vectorstore_manager._doc_id_mapping)} documents, "
                                     f"{len(self.vectorstore_manager._uuid_to_doc_id)} chunk UUIDs")
                    
            # =======================================================
            # üéØ FIX #4: ‡πÇ‡∏´‡∏•‡∏î Document Map ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤ ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡πá‡∏ô Dictionary ‡∏ß‡πà‡∏≤‡∏á
            # (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Filename Resolution Failed)
            # =======================================================
            map_to_use: Dict[str, str] = document_map if document_map is not None else {}

            if not map_to_use:
                # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Path ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå Doc ID Mapping ‡∏ó‡∏µ‡πà VSM ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
                mapping_path = get_mapping_file_path(
                    self.doc_type,
                    tenant=self.config.tenant, 
                    year=self.config.year,
                    enabler=self.enabler_id # ‡πÉ‡∏ä‡πâ enabler ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÉ‡∏´‡∏°‡πà (Priority 1)
                )
                
                self.logger.info(f"Attempting to load document_map from file: {mapping_path}")

                try:
                    with open(mapping_path, 'r', encoding='utf-8') as f:
                        doc_map_raw = json.load(f)
                    
                    # ‡πÅ‡∏õ‡∏•‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Doc ID -> {file_name: X, ...} ‡πÄ‡∏õ‡πá‡∏ô Doc ID -> file_name
                    map_to_use = {
                        doc_id: data.get("file_name", doc_id) # ‡πÉ‡∏ä‡πâ doc_id ‡πÄ‡∏õ‡πá‡∏ô fallback
                        for doc_id, data in doc_map_raw.items()
                    }
                    self.logger.info(f"Successfully loaded {len(map_to_use)} document mappings from file.")
                    
                except FileNotFoundError:
                    self.logger.warning(f"Document mapping file not found at: {mapping_path}. Using empty map.")
                except Exception as e:
                    self.logger.error(f"Error loading document map from file: {e}")

            # ‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö Document Map
            self.doc_id_to_filename_map: Dict[str, str] = map_to_use
            self.document_map: Dict[str, str] = self.doc_id_to_filename_map # ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ô‡πÑ‡∏î‡πâ
            
            self.logger.info(f"Loaded {len(self.doc_id_to_filename_map)} document mappings.")
            if not self.doc_id_to_filename_map:
                self.logger.warning("Document ID to Filename Map is empty. Filename resolution might be limited.")

            self.logger.info(f"Engine initialized for Enabler: {self.enabler_id}, Mock Mode: {config.mock_mode}")

    def _initialize_llm_if_none(self):
        """Initializes LLM instance if self.llm is None."""
        if self.llm is None:
            self.logger.warning("‚ö†Ô∏è Initializing LLM: model=%s, temperature=%s", 
                                self.config.model_name, self.config.temperature)
            try:
                # üü¢ FIX: Import ‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ create_llm_instance
                from models.llm import create_llm_instance 
                self.llm = create_llm_instance( 
                    model_name=self.config.model_name,
                    temperature=self.config.temperature
                )
                self.logger.info("‚úÖ LLM Instance created successfully: %s (Temp: %s)", 
                                 self.config.model_name, self.config.temperature)
            except Exception as e:
                self.logger.error(f"FATAL: Could not initialize LLM: {e}")
                raise


    def _initialize_vsm_if_none(self):
        """
        Initializes VectorStoreManager if self.vectorstore_manager is None.
        Handles multi-tenant/multi-year vector store loading.
        """
        # NOTE: Assumes EVIDENCE_DOC_TYPES is imported from config.global_vars
        if self.vectorstore_manager is None:
            self.logger.info("Loading central evidence vectorstore(s)...")
            try:
                # üéØ FIX ‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß: ‡∏™‡πà‡∏á tenant ‡πÅ‡∏•‡∏∞ year ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô load_all_vectorstores()
                # üìå ‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ó‡∏µ‡πà‡πÅ‡∏ó‡πâ‡∏à‡∏£‡∏¥‡∏á‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏õ‡∏ó‡∏≥‡πÉ‡∏ô load_all_vectorstores ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏°‡∏±‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á VSM ‡πÇ‡∏î‡∏¢‡∏°‡∏µ doc_type ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
                self.vectorstore_manager = load_all_vectorstores(
                    doc_types=[EVIDENCE_DOC_TYPES], 
                    evidence_enabler=self.enabler_id,
                    tenant=self.config.tenant, 
                    year=self.config.year       
                )
                
                # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÇ‡∏´‡∏•‡∏î Doc ID Map ‡∏ã‡πâ‡∏≥‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Map ‡∏´‡∏≤‡∏¢‡πÉ‡∏ô Worker (Safety Net)
                if self.vectorstore_manager:
                    # NOTE: ‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å‡∏à‡∏∞‡∏ó‡∏≥‡∏†‡∏≤‡∏¢‡πÉ‡∏ô VSM.__init__ 
                    # ‡πÅ‡∏ï‡πà‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ã‡πâ‡∏≥‡∏ô‡∏µ‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏ß‡πà‡∏≤ Map ‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
                    self.vectorstore_manager._load_doc_id_mapping() 

                # ‡πÇ‡∏Ñ‡πâ‡∏î Log ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô (‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏≥‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß)
                if self.vectorstore_manager and hasattr(self.vectorstore_manager, '_multi_doc_retriever'):
                     len_retrievers = len(
                        self.vectorstore_manager._multi_doc_retriever._all_retrievers
                    )
                     self.logger.info("‚úÖ MultiDocRetriever loaded with %s collections and cached in VSM.", 
                                 len_retrievers) 
                else:
                    self.logger.warning("VectorStoreManager loaded but _multi_doc_retriever structure is missing or unexpected.")

            except Exception as e:
                self.logger.error(f"FATAL: Could not initialize VectorStoreManager: {e}")
                raise # Re-raise the exception to ‡∏´‡∏¢‡∏∏‡∏î‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°


    def _resolve_evidence_filenames(self, evidence_entries: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á
        1. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà doc_id ‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ 'UNKNOWN-' (‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏†‡∏≤‡∏¢‡πÉ‡∏ô/‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£)
        2. ‡πÅ‡∏õ‡∏•‡∏á doc_id (‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô Hash/UUID) ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏î‡πâ ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ doc_id_to_filename_map
        """
        
        resolved_entries = []
        
        for entry in evidence_entries:
            # ‡πÉ‡∏ä‡πâ deepcopy ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
            resolved_entry = deepcopy(entry)
            doc_id = resolved_entry.get("doc_id", "")
            current_filename = resolved_entry.get("filename", "") # ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏î‡∏¥‡∏°‡∏à‡∏≤‡∏Å Metadata (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
            
            # --- 1. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏ì‡∏µ UNKNOWN- (AI-GENERATED or Lost Source) ---
            if doc_id.startswith("UNKNOWN-"):
                resolved_entry["filename"] = f"AI-GENERATED-REF-{doc_id.split('-')[-1]}"
                resolved_entries.append(resolved_entry)
                continue

            # --- 2. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏ì‡∏µ Doc ID (Hash/UUID) ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ---
            if doc_id:
                # A. ‡∏•‡∏≠‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å Map
                if doc_id in self.doc_id_to_filename_map:
                    resolved_entry["filename"] = self.doc_id_to_filename_map[doc_id]
                    resolved_entries.append(resolved_entry)
                    continue

                # B. ‡∏ñ‡πâ‡∏≤‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ (Map Fail)
                else:
                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏Å‡∏±‡∏ö Metadata ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏™‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                    is_generic_name = (
                        not current_filename.strip() or # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô String ‡∏ß‡πà‡∏≤‡∏á
                        current_filename.lower() == "unknown" or
                        # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Hash/UUID 64 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•
                        re.match(r"^[0-9a-f]{64}(\.pdf|\.txt)?$", current_filename, re.IGNORECASE)
                    )
                    
                    if is_generic_name:
                        # ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå Fallback ‡∏ó‡∏µ‡πà‡∏™‡∏∑‡πà‡∏≠‡∏ß‡πà‡∏≤ Map ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
                        resolved_entry["filename"] = f"MAPPING-FAILED-{doc_id[:8]}..."
                        self.logger.warning(f"Failed to map doc_id {doc_id[:8]}... to filename. Using fallback.")
                        
            # --- 3. ‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏°‡πà‡∏°‡∏µ Doc ID ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ (‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÄ‡∏õ‡πá‡∏ô Unknown) ---
            elif not doc_id and (not current_filename.strip() or current_filename.lower() == "unknown"):
                # ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ doc_id ‡πÅ‡∏•‡∏∞ filename ‡πÄ‡∏î‡∏¥‡∏°‡∏Å‡πá‡πÄ‡∏õ‡πá‡∏ô Unknown/Empty
                resolved_entry["filename"] = "MISSING-SOURCE-METADATA"
                self.logger.error("Evidence found with no doc_id and generic filename.")
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏° entry ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ (‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà)
            resolved_entries.append(resolved_entry)

        return resolved_entries
    
    # -------------------- Contextual Rules Handlers (FIXED) --------------------
    def _load_contextual_rules_map(self) -> Dict[str, Dict[str, str]]:
        """
        Loads the contextual rules JSON file using the path generated by 
        utils.path_utils.get_contextual_rules_file_path.
        """
        
        try:
            # üéØ ‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏à‡∏≤‡∏Å path_utils ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Path ‡πÄ‡∏≠‡∏á
            filepath = get_contextual_rules_file_path(
                tenant=self.config.tenant,
                enabler=self.enabler_id
            )
        except ImportError:
            self.logger.error("‚ùå FATAL: Cannot import get_contextual_rules_file_path. Check utils/path_utils.py import.")
            return {}

        # 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if not os.path.exists(filepath):
            # ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ get_contextual_rules_file_path ‡∏ó‡∏≥‡πÅ‡∏ó‡∏ô
            self.logger.info(f"‚ö†Ô∏è Contextual Rules file not found at: {filepath}. Using empty map.")
            return {}

        self.logger.info(f"‚úÖ Contextual Rules loaded from: {filepath}")
        
        # 2. ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
                self.logger.info(f"‚úÖ Loaded Contextual Rules: {len(data)} sub-criteria rules.")
                return data
        except json.JSONDecodeError as e:
            self.logger.error(f"‚ùå Failed to parse Contextual Rules JSON from {filepath}: {e}")
            return {}
        except Exception as e:
            self.logger.error(f"‚ùå Failed to load Contextual Rules from {filepath}: {e}")
            return {}
    

    # ----------------------------------------------------------------------
    # üéØ FINAL FIX 2.3: Manual Map Reload Function (inside SEAMPDCAEngine)
    # ----------------------------------------------------------------------

    def _collect_previous_level_evidences(self, sub_id: str, current_level: int) -> Dict[str, List[Dict]]:
        """
        ‡∏î‡∏∂‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏à‡∏≤‡∏Å Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ (L1 ‚Üí L2, L2 ‚Üí L3 ‡∏Ø‡∏•‡∏Ø) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô Baseline Context

        Final Production Version - 12 ‡∏ò.‡∏Ñ. 2568 (FINAL FIX 27.0: HEURISTIC BYPASS)
        - ‡πÄ‡∏û‡∏¥‡πà‡∏° Heuristic Fallback ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤ CRITICAL MAPPING FAILURE ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏à‡∏≤‡∏Å VSM Mapping ‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢
        """
        
        # --------------------------------------------------------------
        # 1. ‡∏Ç‡πâ‡∏≤‡∏° Hydration ‡πÉ‡∏ô Parallel Mode (by design)
        # --------------------------------------------------------------
        # (‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡∏Ñ‡∏á‡πÑ‡∏ß‡πâ‡∏ï‡∏≤‡∏°‡πÇ‡∏Ñ‡πâ‡∏î‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì)
        
        # --------------------------------------------------------------
        # 2. ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏° evidence ‡∏à‡∏≤‡∏Å Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏ô Sub-Criteria ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
        # --------------------------------------------------------------
        collected = {}
        for key, ev_list in self.evidence_map.items():
            if (key.startswith(f"{sub_id}.L") and 
                isinstance(ev_list, list) and 
                ev_list):
                try:
                    level_num = int(key.split(".L")[-1])
                    if level_num < current_level:
                        collected[key] = ev_list
                except (ValueError, IndexError):
                    continue

        if not collected:
            self.logger.info("No previous level evidences found for hydration.")
            return {}

        # --------------------------------------------------------------
        # 3. ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏° Stable IDs + Chunk UUIDs (cleaned)
        # --------------------------------------------------------------
        stable_ids = set()
        chunk_uuids_clean = set()

        for ev_list in collected.values():
            for ev in ev_list:
                sid = ev.get("stable_doc_uuid") or ev.get("doc_id")
                if isinstance(sid, str) and len(sid) == 64 and sid.isalnum():
                    stable_ids.add(sid)
                
                cid = ev.get("chunk_uuid")
                if isinstance(cid, str) and len(cid.replace("-", "")) >= 64:
                    chunk_uuids_clean.add(cid.replace("-", ""))

        if not stable_ids and not chunk_uuids_clean:
            self.logger.info("No valid IDs found for hydration.")
            return collected

        # --------------------------------------------------------------
        # 4. ‡πÅ‡∏õ‡∏•‡∏á Stable ‚Üí Chunk UUIDs (‡πÄ‡∏û‡∏∑‡πà‡∏≠ Log ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏Ç‡∏≠‡∏á‡πÇ‡∏Ñ‡πâ‡∏î)
        # --------------------------------------------------------------
        vsm = self.vectorstore_manager
        final_uuids = list(chunk_uuids_clean)
        self.logger.info(f"HYDRATION ‚Üí Resolved {len(final_uuids)} unique chunk UUIDs ‚Üí fetching full text...")

        stable_ids_list = list(stable_ids)
        if not stable_ids_list:
            self.logger.warning("No Stable IDs resolved for VSM hydration call.")
            return collected


        # --------------------------------------------------------------
        # 5. ‡∏î‡∏∂‡∏á full chunks (‡πÉ‡∏ä‡πâ Stable IDs 64-char)
        # --------------------------------------------------------------
        try:
            full_chunks = vsm.get_documents_by_id(stable_ids_list, self.doc_type, self.enabler_id) 
            self.logger.info(f"HYDRATION success: Retrieved {len(full_chunks)} full chunks (via Stable ID search)")
            
        except Exception as e:
            self.logger.error(f"Hydration failed in VSM call (get_documents_by_id): {e}", exc_info=True)
            return collected

        # --------------------------------------------------------------
        # 6. ‡∏™‡∏£‡πâ‡∏≤‡∏á map ‡πÅ‡∏•‡∏∞ hydrate text ‡∏û‡∏£‡πâ‡∏≠‡∏° Fallback Logic (FINAL FIX 27.0)
        # --------------------------------------------------------------
        chunk_map = {} # Key: Cleaned V4 UUID (without dashes)
        total_retrieved = len(full_chunks)
        
        # 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á Map (Key: V4 UUID Cleaned)
        for idx, chunk in enumerate(full_chunks):
            meta = getattr(chunk, "metadata", {})
            # ‡πÉ‡∏ä‡πâ chunk_uuid ‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡∏µ‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á map key (V4 UUID)
            cid_raw = meta.get("chunk_uuid")
            cid = (cid_raw or "").replace("-", "") 
            
            if cid:
                chunk_map[cid] = {
                    "text": chunk.page_content,
                    "metadata": meta
                }
            else:
                self.logger.error(f"CRITICAL HYDRATION ERROR: Retrieved chunk {idx+1}/{total_retrieved} has NO or empty 'chunk_uuid' in metadata. Skipping this chunk.")

        self.logger.info(f"DEBUG: Chunk Map built with {len(chunk_map)}/{total_retrieved} entries.")

        hydrated = {}
        restored = 0
        total = sum(len(v) for v in collected.values())

        for key, ev_list in collected.items():
            new_list = []
            for ev in ev_list:
                new_ev = ev.copy()
                data = None
                
                # ID ‡∏à‡∏≤‡∏Å L1 Evidence (64-char ID ‡∏ó‡∏µ‡πà‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î)
                cid_l1 = (ev.get("chunk_uuid") or "").replace("-", "") 
                sid_l1 = ev.get("stable_doc_uuid") or ev.get("doc_id") 
                vsm_mapping_failed = True # ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡πÉ‡∏´‡∏°‡πà

                # --- 1. Primary Lookup (‡∏à‡∏∞‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ L1 cid ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà V4 UUID) ---
                if cid_l1:
                    data = chunk_map.get(cid_l1)

                # --- 2. Fallback Logic (VSM Mapping): ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏´‡∏≤ V4 UUID ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ---
                if not data and sid_l1 and vsm and hasattr(vsm, '_doc_id_mapping') and vsm._doc_id_mapping:
                    
                    if cid_l1 and not data:
                        self.logger.warning(f"Hydration Check: Primary L1 chunk_uuid '{cid_l1[:8]}...' NOT found in map. Starting Stable ID Fallback...")
                    
                    if sid_l1 in vsm._doc_id_mapping:
                        vsm_mapping_failed = False # VSM Mapping ‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Stable ID ‡∏ô‡∏µ‡πâ
                        
                        # A. Try to find the V4 UUID from VSM Mapping
                        for v4_uuid_raw in vsm._doc_id_mapping[sid_l1].get("chunk_uuids", []):
                            v4_uuid_cleaned = v4_uuid_raw.replace("-", "")
                            
                            if v4_uuid_cleaned in chunk_map:
                                data = chunk_map[v4_uuid_cleaned]
                                self.logger.info(f"‚úÖ Fallback SUCCESS (VSM Map): Matched L1 Chunk ID '{cid_l1[:8]}...' (Stable ID: {sid_l1[:8]}) via V4 UUID '{v4_uuid_cleaned[:8]}...'")
                                break # VSM Map Success
                    else:
                         # Log ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏ñ‡πâ‡∏≤ Stable ID ‡πÑ‡∏°‡πà‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡πÉ‡∏ô Mapping ‡πÄ‡∏•‡∏¢
                         self.logger.warning(f"Hydration Check: Stable ID {sid_l1[:8]}... NOT found in VSM Doc ID Mapping. Fallback failed.")


                # --- 3. HEURISTIC FALLBACK (FINAL BYPASS): ‡∏ñ‡πâ‡∏≤ VSM Mapping ‡∏ä‡πà‡∏ß‡∏¢‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ‡∏´‡∏≤ Chunk ‡πÉ‡∏ô Map ‡∏ó‡∏µ‡πà‡∏°‡∏µ Stable ID ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô ---
                if not data:
                    if not vsm_mapping_failed:
                        self.logger.warning(f"‚ö†Ô∏è VSM Map exists but failed to find V4 UUID. Attempting Heuristic Match by Stable ID.")
                    
                    # B. HEURISTIC MATCH
                    for retrieved_chunk_data in chunk_map.values():
                        retrieved_sid = retrieved_chunk_data["metadata"].get("stable_doc_uuid")
                        if retrieved_sid == sid_l1:
                            data = retrieved_chunk_data
                            # Note: ‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ chunk_uuid ‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏°‡∏≤ (V4 UUID) ‡πÄ‡∏õ‡πá‡∏ô cid ‡πÉ‡∏´‡∏°‡πà‡∏Ç‡∏≠‡∏á new_ev
                            new_ev["chunk_uuid"] = retrieved_chunk_data["metadata"].get("chunk_uuid", new_ev["chunk_uuid"])
                            self.logger.info(f"üü¢ Heuristic SUCCESS (Final Bypass): Baseline context restored for Stable ID {sid_l1[:8]}... (Using first matching chunk).")
                            break # Heuristic Success

                # --------------------------------------------------------------------------

                if data:
                    new_ev["text"] = data["text"]
                    # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï metadata
                    new_ev.update({k: v for k, v in data["metadata"].items() 
                                if k not in ["text", "page_content"]})
                    restored += 1
                else:
                    # ‚ùå Log the failing chunk ID for critical diagnosis
                    sid = ev.get("stable_doc_uuid") or ev.get("doc_id")
                    self.logger.error(f"‚ùå CRITICAL MAPPING FAILURE: Could not restore L1 chunk '{cid_l1[:8]}...' (Stable ID: {sid_l1[:8]}...) from {len(chunk_map)} retrieved chunks. (Mapping Failed)")
                
                new_list.append(new_ev)
            hydrated[key] = new_list
                
        self.logger.info(f"BASELINE HYDRATED ‚Üí {restored}/{total} chunks restored with full text (including fallback)")
        return hydrated

    def _get_contextual_rules_prompt(self, sub_id: str, level: int) -> str:
        """
        Retrieves the specific Contextual Rule prompt for a given Sub-Criteria and Level,
        ‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á‡∏Å‡∏≤‡∏£ Inject ‡∏Å‡∏é L5 ‡∏û‡∏¥‡πÄ‡∏®‡∏© ‡∏´‡∏≤‡∏Å Level == 5
        """
        sub_id_rules = self.contextual_rules_map.get(sub_id)
        rule_text = ""
        
        # 1. ‡∏î‡∏∂‡∏á‡∏Å‡∏é‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏¢‡πà‡∏≠‡∏¢ (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
        if sub_id_rules:
            level_key = f"L{level}"
            specific_rule = sub_id_rules.get(level_key)
            if specific_rule:
                rule_text += f"\n--- ‡∏Å‡∏é‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏¢‡πà‡∏≠‡∏¢ ({sub_id} L{level}) ---\n‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Ñ‡∏ß‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ: {specific_rule}\n"
        
        # 2. **INJECT L5 SPECIAL RULE (Safe Injection)**
        # ‡πÉ‡∏™‡πà‡∏Å‡∏é Bonus 2.0 ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô L5 ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏ö‡∏Å‡∏ß‡∏ô L3/L4
        if level == 5:
            l5_bonus_rule = """
            \n--- L5 SPECIAL RULE (Innovation & Sustainability) ---
            * **L5 PASS Condition (‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö):** ‡∏´‡∏≤‡∏Å Level ‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠ **L5** ‡∏ó‡πà‡∏≤‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏° PDCA (P+D+C+A) ‡∏ï‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô L3/L4 ‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏ô‡∏û‡∏ö‡∏Å‡πà‡∏≠‡∏ô (‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î 8.0)
            * **‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç Bonus 2.0:** ‡∏´‡∏≤‡∏Å‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏° PDCA ‡πÑ‡∏î‡πâ **‚â• 7.0** **‡πÅ‡∏•‡∏∞** ‡∏ó‡πà‡∏≤‡∏ô‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠ **‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 1 ‡∏ä‡∏¥‡πâ‡∏ô** ‡πÉ‡∏ô Context ‡∏ó‡∏µ‡πà‡πÅ‡∏™‡∏î‡∏á‡∏ñ‡∏∂‡∏á:
                * (a) ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏• KM / ‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏° (Innovation Award)
                * (b) ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡∏ó‡∏µ‡πà‡∏ß‡∏±‡∏î‡πÑ‡∏î‡πâ (ROI, Productivity, Cost Saving)
                * (c) ‡∏Å‡∏≤‡∏£‡πÄ‡∏ú‡∏¢‡πÅ‡∏û‡∏£‡πà/‡∏Å‡∏≤‡∏£‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å (External Recognition/Publication)
            * **‡πÉ‡∏´‡πâ‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡πÉ‡∏´‡πâ Bonus Score 2.0 ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ **Score ‚â• 9.0** ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡πâ‡∏á **is_passed=true** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏∞‡∏ó‡πâ‡∏≠‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏•‡∏¥‡∏® (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£ Reset ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô)
            """
            rule_text += l5_bonus_rule
            
        return rule_text

    def _load_rubric(self) -> Dict[str, Any]:
        """ Loads the SEAM rubric JSON file using path_utils. """
        
        # üéØ FIX: ‡πÉ‡∏ä‡πâ get_rubric_file_path ‡∏à‡∏≤‡∏Å path_utils ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Path ‡πÄ‡∏≠‡∏á
        filepath = None # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô UnboundLocalError
        
        try:
            # 1. ‡∏£‡∏±‡∏ö Path ‡∏à‡∏≤‡∏Å path_utils ‡∏ã‡∏∂‡πà‡∏á‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏ä‡∏µ‡πâ‡πÑ‡∏õ‡∏ó‡∏µ‡πà 'config/' ‡πÅ‡∏•‡πâ‡∏ß
            filepath = get_rubric_file_path(
                tenant=self.config.tenant,
                enabler=self.enabler_id
            )
        except Exception as e:
            # ‡∏î‡∏±‡∏Å‡∏à‡∏±‡∏ö Exception ‡∏´‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏î‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô path_utils
            self.logger.error(f"‚ùå FATAL: Error calling get_rubric_file_path: {e}")
            return {} 

        # 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if filepath is None or not os.path.exists(filepath):
            self.logger.error(f"‚ö†Ô∏è Rubric file not found at expected path: {filepath}")
            return {}

        # 3. ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå JSON
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
            self.logger.info(f"‚úÖ Rubric loaded successfully from: {filepath}")
            return data
        except json.JSONDecodeError:
            self.logger.error(f"‚ùå Error decoding Rubric JSON. File might be corrupted: {filepath}")
            return {}
        except Exception as e:
            self.logger.error(f"‚ùå Error loading Rubric file from {filepath}: {e}")
            return {}
    
    # -------------------- Helper Function for Map Processing --------------------
    def _get_level_order_value(self, level_str: str) -> int:
        """Converts Level string ('L1', 'L5') to an integer for comparison."""
        try:
            return int(level_str.upper().replace('L', ''))
        except:
            return 0

    # -------------------- Persistent Mapping Handlers (FIXED) --------------------
    def _process_temp_map_to_final_map(self, temp_map: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Converts the temporary map into the final map format for saving, 
        and filters out temporary/unresolvable evidence IDs.
        """
        working_map = temp_map or self.temp_map_for_save or {}
        final_map_for_save = {}
        total_cleaned_items = 0

        for sub_level_key, evidence_list in working_map.items():
            if isinstance(evidence_list, dict):
                evidence_list = [evidence_list]
            elif not isinstance(evidence_list, list):
                logger.warning(f"[EVIDENCE] Skipping {sub_level_key}: not a list or dict")
                continue

            clean_list = []
            seen_ids = set()
            for ev in evidence_list:
                doc_id = ev.get("doc_id")
                
                if not doc_id:
                    continue
                
                # 1. FIX: ‡∏Å‡∏£‡∏≠‡∏á ID ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß (TEMP-) ‡∏≠‡∏≠‡∏Å
                if doc_id.startswith("TEMP-"):
                    # ID ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏õ‡∏•‡∏á‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô Stable Document ID ‡πÑ‡∏î‡πâ ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÉ‡∏ä‡πâ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ
                    logger.debug(f"[EVIDENCE] Filtering out unresolvable TEMP- ID: {doc_id} for {sub_level_key}.")
                    continue 
                
                # 2. Logic ‡πÄ‡∏î‡∏¥‡∏°: ‡∏Å‡∏£‡∏≠‡∏á HASH- (Placeholder) ‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ã‡πâ‡∏≥
                if doc_id.startswith("HASH-") or doc_id in seen_ids:
                    continue
                    
                seen_ids.add(doc_id)
                clean_list.append(ev)
                total_cleaned_items += 1 

            if clean_list:
                final_map_for_save[sub_level_key] = clean_list

        logger.info(f"[EVIDENCE] Processed {len(final_map_for_save)} sub-level keys with total {total_cleaned_items} evidence items")
        return final_map_for_save

    def _clean_map_for_json(self, data: Union[Dict, List, Set, Any]) -> Union[Dict, List, Any]:
        """Recursively converts objects that cannot be serialized (like sets) into lists."""
        if isinstance(data, dict):
            return {k: self._clean_map_for_json(v) for k, v in data.items()}
        elif isinstance(data, list):
            return [self._clean_map_for_json(v) for v in data]
        elif isinstance(data, set):
            return [self._clean_map_for_json(v) for v in data]
        return data

    def _clean_temp_entries(self, evidence_map: Dict[str, List[Dict]]) -> Dict[str, List[Dict]]:
        """
        ‡∏Å‡∏£‡∏≠‡∏á TEMP-, HASH-, ‡πÅ‡∏•‡∏∞ Unknown ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å evidence map ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        ‡πÉ‡∏ä‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏ï‡∏≠‡∏ô merge ‡πÅ‡∏•‡∏∞‡∏Å‡πà‡∏≠‡∏ô save ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î 100%
        """
        if not evidence_map:
            return {}

        cleaned_map = {}
        total_removed = 0
        total_unknown_fixed = 0

        for key, entries in evidence_map.items():
            valid_entries = []
            for entry in entries:
                doc_id = entry.get("doc_id", "")

                # 1. ‡∏Å‡∏£‡∏≠‡∏á TEMP- ‡πÅ‡∏•‡∏∞ HASH- ‡∏≠‡∏≠‡∏Å‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≤‡∏î
                if str(doc_id).startswith("TEMP-") or str(doc_id).startswith("HASH-"):
                    total_removed += 1
                    continue

                # 2. ‡∏ñ‡πâ‡∏≤ doc_id ‡∏ß‡πà‡∏≤‡∏á ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏•‡∏¢ ‚Üí ‡∏ó‡∏¥‡πâ‡∏á
                if not doc_id or doc_id == "Unknown":
                    total_removed += 1
                    continue

                # 3. ‡πÅ‡∏Å‡πâ filename ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô Unknown / None / ‡∏ß‡πà‡∏≤‡∏á
                filename = entry.get("filename", "").strip()
                if not filename or filename == "Unknown" or filename.lower() == "unknown_file.pdf":
                    # ‡πÉ‡∏ä‡πâ doc_id ‡∏™‡∏±‡πâ‡∏ô ‡πÜ ‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß (‡∏î‡∏π‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤)
                    short_id = doc_id[:8]
                    entry["filename"] = f"‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á_{short_id}.pdf"
                    total_unknown_fixed += 1
                else:
                    # ‡πÄ‡∏≠‡∏≤ path ‡∏≠‡∏≠‡∏Å ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÅ‡∏Ñ‡πà‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå
                    entry["filename"] = os.path.basename(filename)

                valid_entries.append(entry)

            if valid_entries:
                cleaned_map[key] = valid_entries
            else:
                logger.debug(f"[CLEAN] Key {key} ‡∏Å‡∏•‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏ß‡πà‡∏≤‡∏á‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏£‡∏≠‡∏á ‚Üí ‡∏ñ‡∏π‡∏Å‡∏•‡∏ö‡∏≠‡∏≠‡∏Å")

        logger.info(f"[CLEANUP] ‡∏•‡∏ö TEMP-/HASH- ‡∏≠‡∏≠‡∏Å {total_removed} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | "
                    f"‡πÅ‡∏Å‡πâ Unknown filename {total_unknown_fixed} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | "
                    f"‡πÄ‡∏´‡∏•‡∏∑‡∏≠ {len(cleaned_map)} keys")

        return cleaned_map
    
    def _save_evidence_map(self, map_to_save: Optional[Dict[str, List[Dict[str, Any]]]] = None):
        """
        ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å evidence map ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢ 100% - Atomic + Lock + Clean + Sort + Score
        ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏õ‡∏±‡∏ç‡∏´‡∏≤ .lock ‡∏Ñ‡πâ‡∏≤‡∏á‡πÉ‡∏ô‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏° Parallel
        """
        try:
            map_file_path = get_evidence_mapping_file_path(
                tenant=self.config.tenant,
                year=self.config.year,
                enabler=self.enabler_id
            )
        except Exception as e:
            self.logger.critical(f"[EVIDENCE] FATAL: ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Evidence Map Path ‡πÑ‡∏î‡πâ: {e}")
            raise

        lock_path = map_file_path + ".lock"
        tmp_path = None

        self.logger.info(f"[EVIDENCE] Saving evidence map ‚Üí {map_file_path}")

        try:
            # 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏Å‡πà‡∏≠‡∏ô (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô error)
            os.makedirs(os.path.dirname(map_file_path), exist_ok=True)

            # 2. ‡πÉ‡∏ä‡πâ FileLock ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
            with FileLock(lock_path, timeout=60):
                self.logger.debug("[EVIDENCE] Lock acquired.")

                # === 2.1 Merge Logic (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏™‡πà‡∏á map ‡∏°‡∏≤‡∏ï‡∏£‡∏á‡πÜ) ===
                if map_to_save is not None:
                    final_map_to_write = map_to_save
                else:
                    existing_map = self._load_evidence_map(is_for_merge=True) or {}
                    runtime_map = deepcopy(self.evidence_map)
                    final_map_to_write = existing_map
                    
                    for key, new_entries in runtime_map.items():
                        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Map ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô (Chunk UUID/Doc ID)
                        entry_map = {
                            e.get("chunk_uuid", e.get("doc_id", "N/A")): e
                            for e in final_map_to_write.setdefault(key, [])
                        }
                        for new_entry in new_entries:
                            entry_id = new_entry.get("chunk_uuid", new_entry.get("doc_id", "N/A"))
                            if entry_id == "N/A" or not entry_id:
                                continue
                            
                            new_score = new_entry.get("relevance_score", 0.0)
                            
                            if entry_id not in entry_map:
                                entry_map[entry_id] = new_entry
                            else:
                                # Update ‡∏ñ‡πâ‡∏≤ Score ‡πÉ‡∏´‡∏°‡πà‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤
                                if new_score > entry_map[entry_id].get("relevance_score", 0.0):
                                    entry_map[entry_id] = new_entry
                                    
                        final_map_to_write[key] = list(entry_map.values())

                    # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö
                    final_map_to_write = self._clean_temp_entries(final_map_to_write)
                    for key, entries in final_map_to_write.items():
                        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ï‡∏≤‡∏° Score (‡∏à‡∏≤‡∏Å‡∏°‡∏≤‡∏Å‡πÑ‡∏õ‡∏ô‡πâ‡∏≠‡∏¢)
                        if entries and "relevance_score" in entries[0]:
                            entries.sort(key=lambda x: x.get("relevance_score", 0.0), reverse=True)

                if not final_map_to_write:
                    self.logger.warning("[EVIDENCE] Nothing to save.")
                    return

                # === 2.2 Atomic Write (‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß) ===
                with tempfile.NamedTemporaryFile(
                    mode='w', delete=False, encoding="utf-8", dir=os.path.dirname(map_file_path)
                ) as tmp_file:
                    cleaned = self._clean_map_for_json(final_map_to_write)
                    json.dump(cleaned, tmp_file, indent=4, ensure_ascii=False)
                    tmp_path = tmp_file.name

                # 2.3 ‡∏¢‡πâ‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß‡πÑ‡∏õ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏£‡∏¥‡∏á
                shutil.move(tmp_path, map_file_path)
                tmp_path = None  # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡∏ö‡∏ã‡πâ‡∏≥‡πÉ‡∏ô finally

                # === 2.4 ‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥ ===
                total_keys = len(final_map_to_write)
                total_items = sum(len(v) for v in final_map_to_write.values())
                file_size_kb = os.path.getsize(map_file_path) / 1024
                self.logger.info(f"[EVIDENCE] Evidence map saved successfully! "
                               f"Keys: {total_keys} | Items: {total_items} | Size: ~{file_size_kb:.1f} KB")

        except Exception as e:
            self.logger.critical("[EVIDENCE] FATAL SAVE ERROR")
            self.logger.exception(e)
            raise

        finally:
            # === ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î: ‡∏•‡∏ö .lock ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏∏‡∏Å‡∏Å‡∏£‡∏ì‡∏µ (Double Safety) ===
            # ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ô‡∏µ‡πâ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏ß‡πà‡∏≤ lock ‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏Ñ‡πâ‡∏≤‡∏á
            if os.path.exists(lock_path):
                try:
                    os.unlink(lock_path)
                    self.logger.debug(f"[EVIDENCE] Removed stale lock file: {lock_path}")
                except Exception as e:
                    self.logger.warning(f"[EVIDENCE] Failed to remove lock file {lock_path}: {e}")

            # ‡∏•‡∏ö tmp file ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏Ñ‡πâ‡∏≤‡∏á (‡∏ñ‡πâ‡∏≤‡πÄ‡∏Å‡∏¥‡∏î error ‡∏Å‡πà‡∏≠‡∏ô shutil.move)
            if tmp_path and os.path.exists(tmp_path):
                try:
                    os.unlink(tmp_path)
                except:
                    pass

    def _load_evidence_map(self, is_for_merge: bool = False):
        """
        Safe load of persistent evidence map. Always returns dict.
        ‡πÉ‡∏ä‡πâ get_evidence_mapping_file_path ‡∏à‡∏≤‡∏Å path_utils.py ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î Path
        is_for_merge: If True, suppresses "No existing evidence map" INFO log.
        """
        # üéØ FIX 2: ‡πÉ‡∏ä‡πâ get_evidence_mapping_file_path ‡πÅ‡∏ó‡∏ô self.evidence_map_path
        try:
            path = get_evidence_mapping_file_path(
                tenant=self.config.tenant,
                year=self.config.year,
                enabler=self.enabler_id
            )
        except Exception as e:
            self.logger.error(f"[EVIDENCE] ‚ùå FATAL: ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Evidence Map Path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏î‡πâ: {e}")
            return {}

        if not os.path.exists(path):
            if not is_for_merge:
                self.logger.info("[EVIDENCE] No existing evidence map ‚Äì starting empty.")
            return {}

        try:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
            if not is_for_merge:
                self.logger.info(f"[EVIDENCE] Loaded evidence map: {len(data)} entries from {path}")
            return data

        except Exception as e:
            self.logger.error(f"[EVIDENCE] Failed to load evidence map from {path}: {e}")
            return {}


    def _set_mock_handlers(self, mode: str):
        """Replaces real LLM/RAG functions with mock versions."""
        if mode == "control" or mode == "random":
            if hasattr(seam_mocking, 'evaluate_with_llm_CONTROLLED_MOCK'):
                self.llm_evaluator = seam_mocking.evaluate_with_llm_CONTROLLED_MOCK
            if hasattr(seam_mocking, 'retrieve_context_with_filter_MOCK'):
                self.rag_retriever = seam_mocking.retrieve_context_with_filter_MOCK
            if hasattr(seam_mocking, 'create_structured_action_plan_MOCK'):
                self.action_plan_generator = seam_mocking.create_structured_action_plan_MOCK
            if hasattr(seam_mocking, 'set_mock_control_mode'):
                seam_mocking.set_mock_control_mode(mode == "control") 

        logger.warning(f"Engine is running in MOCK mode: {mode}")

    def _get_pdca_phase(self, level: int) -> str:
        """Helper to get the PDCA phase string from the map."""
        return PDCA_PHASE_MAP.get(level, f"Level {level} Requirement")
    

    def _get_level_constraint_prompt(self, level: int) -> str:
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á Prompt Constraint ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Å‡∏±‡∏ö‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ß‡∏∏‡∏í‡∏¥‡∏†‡∏≤‡∏ß‡∏∞‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô
        """
        if level == 1:
            return "‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏ñ‡∏∂‡∏á '‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢/‡∏ß‡∏¥‡∏™‡∏±‡∏¢‡∏ó‡∏±‡∏®‡∏ô‡πå', '‡∏Å‡∏≤‡∏£‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå', '‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏ó‡∏≥‡∏Å‡∏£‡∏≠‡∏ö‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á', ‡∏´‡∏£‡∏∑‡∏≠ '‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£' ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (L1-Focus)"
        elif level == 2:
            return "‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ô‡πâ‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞ '‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô', '‡∏Å‡∏≤‡∏£‡∏Ç‡∏±‡∏ö‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô', '‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡∏ò‡∏£‡∏£‡∏°', ‡∏´‡∏£‡∏∑‡∏≠ '‡∏Å‡∏≤‡∏£‡∏°‡∏µ‡∏™‡πà‡∏ß‡∏ô‡∏£‡πà‡∏ß‡∏°' ‡∏ï‡∏≤‡∏°‡πÅ‡∏ú‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (L2-Focus)"
        elif level == 3:
            # üö® HARD RULE: ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏ä‡πâ L3 Logic (Check/Act Focus) ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏° Context ‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡∏°‡πà
            return """
‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î (HARD RULE: L3 CHECK/ACT FOCUS):
1. ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô L3 ‡∏ô‡∏µ‡πâ **‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô '‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö (Check)' ‡πÅ‡∏•‡∏∞ '‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á (Act)' ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô**
2. ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÅ‡∏•‡πâ‡∏ß: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏ï‡πâ‡∏ô‡∏Ç‡∏≠‡∏á Context ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î (Priority 1)
3. ‡∏´‡∏≤‡∏Å‡∏û‡∏ö‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ **[L3_SUMMARY_EVIDENCE]** ‡πÉ‡∏´‡πâ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô **‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö** ‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠‡πÑ‡∏î‡πâ‡∏ã‡∏∂‡πà‡∏á‡∏ñ‡∏π‡∏Å‡∏™‡∏£‡∏∏‡∏õ‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô Check/Act ‡∏à‡∏£‡∏¥‡∏á (‡∏à‡∏±‡∏î‡πÄ‡∏õ‡πá‡∏ô Priority 1)
4. ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô Plan ‡πÅ‡∏•‡∏∞ Do ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏ï‡∏≠‡∏ô‡∏ó‡πâ‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á Context **‡∏´‡πâ‡∏≤‡∏°‡∏ô‡∏≥‡∏°‡∏≤‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤** ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à **FAIL** ‡∏´‡∏≤‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô Check/Act ‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô
5. ‡∏´‡∏≤‡∏Å‡∏Ç‡∏≤‡∏î‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô **Check** ‡∏´‡∏£‡∏∑‡∏≠ **Act** ‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠ (‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏à‡∏≤‡∏Å Summary Evidence ‡∏´‡∏£‡∏∑‡∏≠‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á) ‡πÉ‡∏´‡πâ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÄ‡∏õ‡πá‡∏ô **‚ùå FAIL** ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô L3 ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡∏à‡∏£‡∏¥‡∏á
(L3-Focus: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏° ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏• ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á)
"""
        elif level == 4:
            return "‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ñ‡∏∂‡∏á '‡∏Å‡∏≤‡∏£‡∏ö‡∏π‡∏£‡∏ì‡∏≤‡∏Å‡∏≤‡∏£', '‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á', ‡∏´‡∏£‡∏∑‡∏≠ '‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå' ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (L4-Focus)"
        elif level == 5:
            return "‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ñ‡∏∂‡∏á '‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°', '‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡πà‡∏≤‡∏ó‡∏≤‡∏á‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à', ‡∏´‡∏£‡∏∑‡∏≠ '‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß' ‡πÇ‡∏î‡∏¢‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô (L5-Focus)"
        else:
            return ""
        

    def _classify_pdca_phase_for_chunk(
        self, 
        chunk_text: str
    ) -> Literal["Plan", "Do", "Check", "Act", "Other"]:
        """
        ‡πÉ‡∏ä‡πâ LLM ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏∞‡∏¢‡∏∞‡πÉ‡∏î‡∏£‡∏∞‡∏¢‡∏∞‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏Ç‡∏≠‡∏á PDCA ‡∏´‡∏£‡∏∑‡∏≠ 'Other'
        """
        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏ü‡∏™ PDCA
        pdca_phases_th = ["‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô", "‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥", "‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö", "‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á"]
        
        # 1. System Prompt ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢: ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö JSON 100%
        system_prompt = (
            "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô PDCA Cycle\n"
            "‡∏†‡∏≤‡∏£‡∏Å‡∏¥‡∏à‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô ‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏±‡∏î‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ß‡πà‡∏≤‡πÄ‡∏ô‡πâ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡πÉ‡∏î‡∏Ç‡∏≠‡∏á PDCA\n"
            f"‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏´‡∏ô‡∏∂‡πà‡∏á‡πÉ‡∏ô: {', '.join(pdca_phases_th)} ‡∏´‡∏£‡∏∑‡∏≠ '‡∏≠‡∏∑‡πà‡∏ô‡πÜ'\n\n"
            "‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡∏î‡πâ‡∏ß‡∏¢ **JSON Object ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô** ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö:\n"
            "{\"phase\": \"‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô\"}\n"
            "‡∏´‡∏£‡∏∑‡∏≠ {\"phase\": \"‡∏≠‡∏∑‡πà‡∏ô‡πÜ\"}\n"
            "‡∏´‡πâ‡∏≤‡∏°‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏≠‡∏Å JSON ‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≤‡∏î"
        )

        # 2. User Prompt: ‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô + ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
        user_prompt = (
            f"‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô:\n\"\"\"\n{chunk_text.strip()}\n\"\"\"\n\n"
            "‡∏Ñ‡∏≥‡∏ô‡∏¥‡∏¢‡∏≤‡∏°‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏ü‡∏™:\n"
            "- ‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô: ‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢, ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå, ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢, ‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô, ‡∏Ñ‡∏ì‡∏∞‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£\n"
            "- ‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥: ‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£, ‡∏ù‡∏∂‡∏Å‡∏≠‡∏ö‡∏£‡∏°, ‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£, ‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏£‡∏∞‡∏ö‡∏ö, ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á\n"
            "- ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö: ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏ú‡∏•, ‡∏ß‡∏±‡∏î‡∏ú‡∏•, ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô, ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏†‡∏≤‡∏¢‡πÉ‡∏ô, ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n"
            "- ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á: ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç, ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á, ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡πÉ‡∏´‡∏°‡πà, Lesson Learned, ‡∏õ‡∏¥‡∏î‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á\n\n"
            "‡∏ï‡∏≠‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞ JSON:"
        )
        
        try:
            raw_response = _fetch_llm_response(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                temperature=0.0,  # ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å! ‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô
                max_retries=2,
                llm_executor=self.llm
            )

            if not raw_response:
                return "Other"

            # ‡∏î‡∏∂‡∏á JSON ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏Ç‡πá‡∏á‡πÅ‡∏Å‡∏£‡πà‡∏á (‡πÉ‡∏ä‡πâ _robust_extract_json ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß!)
            parsed = _robust_extract_json(raw_response)
            
            # ‡∏ñ‡πâ‡∏≤ _robust_extract_json ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ fallback ‡∏î‡πâ‡∏ß‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ‡πÄ‡∏ö‡∏™‡∏¥‡∏Å
            if not parsed or not isinstance(parsed, dict):
                # ‡∏•‡∏≠‡∏á‡∏î‡∏∂‡∏á‡∏î‡πâ‡∏ß‡∏¢ regex ‡∏á‡πà‡∏≤‡∏¢ ‡πÜ
                match = re.search(r'"phase"\s*:\s*"([^"]+)"', raw_response, re.IGNORECASE)
                if match:
                    phase_th = match.group(1).strip()
                else:
                    phase_th = "‡∏≠‡∏∑‡πà‡∏ô‡πÜ"
            else:
                phase_th = parsed.get("phase", parsed.get("classification", "‡∏≠‡∏∑‡πà‡∏ô‡πÜ"))
                phase_th = str(phase_th).strip()

            # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô Literal ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
            mapping = {
                "‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô": "Plan",
                "‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥": "Do",
                "‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö": "Check",
                "‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á": "Act",
                "‡∏≠‡∏∑‡πà‡∏ô‡πÜ": "Other",
                "‡∏≠‡∏∑‡πà‡∏ô": "Other",
                "other": "Other"
            }
            result = mapping.get(phase_th, "Other")
            
            self.logger.debug(f"PDCA Classification: '{phase_th}' ‚Üí {result}")
            return result

        except Exception as e:
            self.logger.error(f"PDCA Classification failed: {e}\nRaw: {raw_response[:200]}")
            return "Other"

    # -------------------- Statement Preparation & Filtering Helpers --------------------
    def _flatten_rubric_to_statements(self) -> List[Dict[str, Any]]:
        """
        Transforms the hierarchical rubric structure loaded in self.rubric
        into a flat list of statements ready for assessment.
        """
        if not self.rubric:
            self.logger.warning("Cannot flatten rubric: self.rubric is empty.")
            return []
            
        data = deepcopy(self.rubric)
        extracted_list = []
        
        if not isinstance(data, dict):
             self.logger.error("Rubric data structure is invalid (expected dict of criteria).")
             return []
             
        for criteria_id, criteria_data in data.items():
            # üéØ FIX 1: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Criteria Data
            if not isinstance(criteria_data, dict):
                self.logger.warning(f"Skipping malformed criteria entry: {criteria_id} (not a dict).")
                continue
                
            sub_criteria_map = criteria_data.get('subcriteria', {})
            criteria_name = criteria_data.get('name')
            
            # üéØ FIX 2: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Sub-criteria Map
            if not isinstance(sub_criteria_map, dict):
                 self.logger.warning(f"Skipping criteria {criteria_id}: 'subcriteria' is not a dictionary.")
                 continue

            for sub_id, sub_data in sub_criteria_map.items():
                
                # üéØ FIX 3: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ sub_data ‡πÄ‡∏õ‡πá‡∏ô Dictionary ‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô TypeError)
                if not isinstance(sub_data, dict):
                    self.logger.warning(
                        f"Skipping malformed sub-criteria entry: {criteria_id}.{sub_id} "
                        f"is not a dictionary (found type: {type(sub_data).__name__})."
                    )
                    continue
                
                # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤ Metadata ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
                sub_data['criteria_id'] = criteria_id
                sub_data['criteria_name'] = criteria_name
                sub_data['sub_id'] = sub_id 
                sub_data['sub_criteria_name'] = sub_data.get('name', criteria_name + ' sub')
                if 'weight' not in sub_data:
                    sub_data['weight'] = criteria_data.get('weight', 0)
                extracted_list.append(sub_data)

        # Re-check and re-sort levels
        final_list = []
        for sub_criteria in extracted_list: 
            
            # üéØ FIX 4: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á Level ‡∏à‡∏≤‡∏Å Dict ‡∏ó‡∏µ‡πà‡∏°‡∏µ Key ‡πÄ‡∏õ‡πá‡∏ô String ‡πÄ‡∏õ‡πá‡∏ô List
            if "levels" in sub_criteria and isinstance(sub_criteria["levels"], dict):
                levels_list = []
                for level_str, statement in sub_criteria["levels"].items():
                    try:
                        # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÅ‡∏õ‡∏•‡∏á Level Key ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô Integer
                        level_int = int(level_str)
                        if isinstance(statement, str):
                            levels_list.append({"level": level_int, "statement": statement})
                        else:
                            self.logger.warning(f"Level {level_str} statement in {sub_criteria.get('sub_id')} is not a string.")
                    except ValueError:
                        self.logger.error(f"Invalid level key '{level_str}' found in {sub_criteria.get('sub_id', 'UNKNOWN_ID')}. Skipping.")
                        continue
                        
                sub_criteria["levels"] = levels_list
            
            # ‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏£‡∏∞‡∏î‡∏±‡∏ö
            if "levels" in sub_criteria and isinstance(sub_criteria["levels"], list):
                 sub_criteria["levels"].sort(key=lambda x: x.get("level", 0))
                 final_list.append(sub_criteria)
            else:
                 self.logger.warning(f"Sub-criteria {sub_criteria.get('sub_id', 'UNKNOWN_ID')} missing 'levels' list.")


        return final_list

    def _load_initial_evidence_info(self) -> Set[str]:
        """Retrieves the set of all available stable document IDs in the VectorStore."""
        if self.config.mock_mode != "none":
            return {"00000000-0000-0000-0000-000000000001"}
        return set() 

    def _apply_strict_filter(self, statements: List[Dict[str, Any]], available_evidence_ids: Set[str]) -> List[Dict[str, Any]]:
        """
        Filters out statements that have no specified evidence_doc_ids 
        that match any ID found in the available_evidence_ids set.
        """
        if not available_evidence_ids:
            logger.warning("Strict Filter bypassed: No available evidence IDs loaded.")
            return statements

        filtered_statements = []
        for stmt in statements:
            required_ids = set(stmt.get('evidence_doc_ids', []))
            
            if not required_ids or required_ids.isdisjoint(available_evidence_ids):
                 logger.debug(f"Strict Filter: Skipping {stmt['sub_id']} L{stmt['level']} (No evidence match)")
                 continue
            
            filtered_statements.append(stmt)
            
        return filtered_statements
    
# -------------------- Evidence Classification Helper (Optimized) --------------------
    def _get_mapped_uuids_and_priority_chunks(
        self,
        sub_id: str,
        level: int,
        statement_text: str,
        level_constraint: str,
        vectorstore_manager: Optional['VectorStoreManager']
    ) -> Tuple[List[str], List[Dict]]:
        """
        ‡∏î‡∏∂‡∏á Priority Chunks ‡∏à‡∏≤‡∏Å Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ + Hydrate ‡πÄ‡∏ï‡πá‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
        ‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Hydration success: 0 chunks from X docs ‡∏ñ‡∏≤‡∏ß‡∏£
        """
        priority_chunks = []
        mapped_stable_ids = []

        # 1. ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å evidence_map (L1 ‚Üí L2, L2 ‚Üí L3 ‡∏Ø‡∏•‡∏Ø)
        for key, evidences in self.evidence_map.items():
            if key.startswith(f"{sub_id}.L") and evidences:
                try:
                    prev_level = int(key.split(".L")[-1])
                    if prev_level < level:
                        priority_chunks.extend(evidences)
                except:
                    continue

        if not priority_chunks:
            self.logger.info(f"No priority chunks found for {sub_id} L{level}")
            return mapped_stable_ids, []

        # 2. ‡∏ó‡∏≥ Robust Hydration ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà‡πÄ‡∏•‡∏¢! (‡∏ï‡∏±‡∏ß‡∏à‡∏£‡∏¥‡∏á‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)
        priority_chunks = self._robust_hydrate_documents_for_priority_chunks(
            chunks_to_hydrate=priority_chunks,
            vsm=vectorstore_manager
        )

        # 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á mapped_stable_ids ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö RAG Retriever
        for chunk in priority_chunks:
            sid = chunk.get("stable_doc_uuid") or chunk.get("doc_id")
            if sid and isinstance(sid, str) and len(sid.replace("-", "")) >= 64:
                mapped_stable_ids.append(sid)

        self.logger.info(f"PRIORITY HYDRATED ‚Üí {len(priority_chunks)} chunks ready for L{level} (with full text)")

        return mapped_stable_ids, priority_chunks


    # -------------------- Calculation Helpers (ADDED) --------------------
    def _calculate_weighted_score(self, highest_full_level: int, weight: int) -> float:
        """
        Calculates the weighted score based on the highest full level achieved.
        Score is calculated by: (Level / 5) * Weight
        """
        MAX_LEVEL_CALC = 5  
        
        if highest_full_level <= 0:
            return 0.0
        
        level_for_calc = min(highest_full_level, MAX_LEVEL_CALC)
        score = (level_for_calc / MAX_LEVEL_CALC) * weight
        return score

    def _calculate_overall_stats(self, target_sub_id: str = "all"):
            """
            Calculates the total weighted score, total possible weight, and overall maturity score/level.
            """
            total_weighted_score = 0.0
            total_possible_weight = 0.0
            assessed_count = 0
            
            for result in self.final_subcriteria_results:
                if target_sub_id.lower() != "all" and result.get('sub_criteria_id') != target_sub_id:
                    continue

                weighted_score = result.get('weighted_score', 0.0)
                weight = result.get('weight', 0)
                
                total_weighted_score += weighted_score
                total_possible_weight += weight
                assessed_count += 1
                
            overall_maturity_score_avg = 0.0
            overall_maturity_level = "N/A"
            overall_progress_percent = 0.0 # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏ó‡∏µ‡πà 0.0
            
            if total_possible_weight > 0:
                overall_progress_percent = total_weighted_score / total_possible_weight
                
                MAX_LEVEL_STATS = 5 
                overall_maturity_score_avg = overall_progress_percent * MAX_LEVEL_STATS 

                # üü¢ FIX: Completed Logic for Maturity Level Determination
                if overall_maturity_score_avg >= 4.5:
                    overall_maturity_level = "L5"
                elif overall_maturity_score_avg >= 3.5:
                    overall_maturity_level = "L4"
                elif overall_maturity_score_avg >= 2.5:
                    overall_maturity_level = "L3"
                elif overall_maturity_score_avg >= 1.5:
                    overall_maturity_level = "L2"
                elif overall_maturity_score_avg >= 0.5:
                    overall_maturity_level = "L1"
                else:
                    overall_maturity_level = "L0"
            
            self.total_stats = {
                "Overall Maturity Score (Avg.)": overall_maturity_score_avg,
                "Overall Maturity Level (Weighted)": overall_maturity_level,
                "Number of Sub-Criteria Assessed": assessed_count,
                "Total Weighted Score Achieved": total_weighted_score,
                "Total Possible Weight": total_possible_weight,
                "Overall Progress Percentage (0.0 - 1.0)": overall_progress_percent,
                "percentage_achieved_run": overall_progress_percent * 100,
                "total_subcriteria": len(self.rubric),
                "target_level": self.config.target_level
            }
            return self.total_stats

    def _export_results(self, results: dict, sub_criteria_id: str, **kwargs) -> str:
        """
        Exports the assessment results (for a specific sub-criteria or the final run) 
        to a JSON file, using utils/path_utils.py for full path determination.
        """
        
        enabler = self.enabler_id
        target_level = self.config.target_level
        
        # 1. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Path Utility (‡∏¢‡πâ‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏Å‡πà‡∏≠‡∏ô try/except)
        tenant = self.config.tenant
        year = self.config.year
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        suffix = f"assessment_results_{sub_criteria_id}_{timestamp}"

        full_path = ""
        export_dir = ""

        try:
            # 2. ‡πÉ‡∏ä‡πâ Path Utility ‡∏™‡∏£‡πâ‡∏≤‡∏á Full Path
            if self.config.export_path:
                # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î export_path (Override)
                export_dir = self.config.export_path
                file_name = f"assessment_results_{enabler}_{sub_criteria_id}_{timestamp}.json"
                full_path = os.path.join(export_dir, file_name)
            else:
                # üéØ ‡πÉ‡∏ä‡πâ get_assessment_export_file_path ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á Full Path ‡∏ï‡∏≤‡∏°‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô
                full_path = get_assessment_export_file_path(
                    tenant=tenant,
                    year=year,
                    enabler=enabler,
                    suffix=suffix,
                    ext="json"
                )
                # ‡∏î‡∏∂‡∏á export_dir ‡∏à‡∏≤‡∏Å full_path ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å get_export_dir ‡∏ã‡πâ‡∏≥
                export_dir = os.path.dirname(full_path)

        except ImportError as e:
            self.logger.error(f"‚ùå FATAL: Cannot import path_utils: {e}. Falling back to manual path.")
            
            # Fallback Logic: ‡πÉ‡∏ä‡πâ DATA_STORE_ROOT ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Path ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏î‡∏¥‡∏°
            data_store_root_path = os.environ.get('DATA_STORE_ROOT', 'data_store') 
            
            if self.config.export_path:
                export_dir = self.config.export_path
            else:
                # Fallback ‡∏™‡∏π‡πà Path ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô: data_store/tenant/exports/year/enabler
                export_dir = os.path.join(data_store_root_path, tenant, "exports", str(year), enabler)
            
            file_name = f"assessment_results_{enabler}_{sub_criteria_id}_{timestamp}.json"
            full_path = os.path.join(export_dir, file_name)
            self.logger.warning(f"‚ö†Ô∏è Using fallback path: {full_path}")


        # 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á Directory ‡∏´‡∏≤‡∏Å‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ
        if not os.path.exists(export_dir):
            try:
                os.makedirs(export_dir)
                self.logger.info(f"Created export directory: {export_dir}")
            except OSError as e:
                self.logger.error(f"‚ùå Failed to create export directory {export_dir}: {e}")
                return ""

        # 4. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°/‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ï Summary Field
        if 'summary' not in results:
            results['summary'] = {}
            
        results['summary']['enabler'] = enabler
        results['summary']['sub_criteria_id'] = sub_criteria_id
        results['summary']['target_level'] = target_level
        
        # ‡∏õ‡∏£‡∏±‡∏ö Logic ‡∏Å‡∏≤‡∏£‡∏ô‡∏±‡∏ö Sub-Criteria ‡πÉ‡∏´‡πâ‡∏ô‡∏±‡∏ö‡∏ï‡∏≤‡∏° 'sub_criteria_results' ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
        if 'sub_criteria_results' in results and isinstance(results['sub_criteria_results'], dict):
            results['summary']['Number of Sub-Criteria Assessed'] = len(results['sub_criteria_results'])
        else:
             results['summary']['Number of Sub-Criteria Assessed'] = 1 

        # 5. Export ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏õ‡∏ó‡∏µ‡πà JSON File
        try:
            with open(full_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=4)
            
            self.logger.info(f"üíæ Successfully exported results for {sub_criteria_id} to: {full_path}")
            return full_path
        
        except Exception as e:
            self.logger.error(f"‚ùå Failed to export results for {sub_criteria_id} to {full_path}: {e}")
            return ""
        
    def rephrase_query_for_retry(self, original_query: str, level: int, sub_id: str) -> str:
        """
        Helper method to slightly rephrase the query for the next retrieval attempt.
        """
        self.logger.info(f"Rephrasing query for L{level} retry: {original_query[:50]}...")
        # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö query: ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏ï‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á‡∏≠‡∏≠‡∏Å ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ LLM ‡∏ä‡πà‡∏ß‡∏¢ rephrase 
        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÅ‡∏Ñ‡πà‡∏Ñ‡∏∑‡∏ô query ‡πÄ‡∏î‡∏¥‡∏° ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ '‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î'
        if level >= 3:
            # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Level ‡∏™‡∏π‡∏á‡πÜ ‡∏•‡∏≠‡∏á‡πÄ‡∏ô‡πâ‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ó‡∏µ‡πà‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡∏Ç‡∏∂‡πâ‡∏ô
            return f"‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö {original_query}"
        return original_query
    
    def _create_error_result(
        self,
        level: int,
        error_message: str,
        start_time: float,
        retrieval_duration: float,
        sub_id: str,
        statement_id: str,
        statement_text: str,
        llm_duration: float = 0.0,
    ) -> Dict[str, Any]:
        """
        Generates a standard error dictionary for when RAG or LLM fails.
        """
        # NOTE: Assumes self._get_pdca_phase is defined
        try:
            pdca_phase = self._get_pdca_phase(level) 
        except Exception:
            pdca_phase = "N/A"
            
        pdca_breakdown = {p: 0 for p in ['P', 'D', 'C', 'A']}
        total_duration = time.time() - start_time

        self.logger.error(f"FATAL ERROR RESULT for {sub_id} L{level}: {error_message}")

        return {
            "sub_criteria_id": sub_id,
            "statement_id": statement_id,
            "level": level,
            "statement": statement_text,
            "pdca_phase": pdca_phase,
            "llm_score": 0.0,
            "pdca_breakdown": pdca_breakdown,
            "is_passed": False,
            "status": "FAIL",
            "score": 0.0,
            "llm_result_full": {"error": error_message, "details": "Assessment skipped due to critical failure."},
            "retrieval_duration_s": round(retrieval_duration, 2),
            "llm_duration_s": round(llm_duration, 2),
            "top_evidences_ref": [],
            "temp_map_for_level": [],
            "evidence_strength": 0.0,
            "ai_confidence": "LOW",
            "evidence_count": 0,
            "pdca_coverage": 0.0,
            "direct_evidence_count": 0,
            "rag_query": statement_text,
            "full_context_meta": {"error_type": "Critical Failure"},
            # üü¢ NEW: Relevant Score Gate Metadata (Set to default error values)
            "max_relevant_score": 0.0,
            "max_relevant_source": "ERROR_HANDLING",
            "is_evidence_strength_capped": False,
            "max_evidence_strength_used": 0.0,
            "total_run_time_s": round(total_duration, 2)
        }

    def _run_sub_criteria_assessment_worker(
            self,
            sub_criteria: Dict[str, Any],
        ) -> Tuple[Dict[str, Any], Dict[str, List[Dict[str, Any]]]]:
            """
            ‡∏£‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô L1-L5 ‡πÅ‡∏ö‡∏ö sequential ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö sub-criteria ‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏ï‡∏±‡∏ß
            ‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á evidence map ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡πÉ‡∏´‡πâ main process ‡∏£‡∏ß‡∏° (‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Action Plan)
            """
            sub_id = sub_criteria['sub_id']
            sub_criteria_name = sub_criteria['sub_criteria_name']
            sub_weight = sub_criteria.get('weight', 0)

            MAX_L1_ATTEMPTS = 2
            
            # üü¢ ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÉ‡∏´‡∏°‡πà: ‡πÉ‡∏ä‡πâ‡∏ô‡∏±‡∏ö Level ‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏Å‡∏±‡∏ô
            current_sequential_pass_level = 0 
            highest_full_level = 0 # ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å current_sequential_pass_level
            
            is_passed_current_level = True
            raw_results_for_sub_seq: List[Dict[str, Any]] = []
            start_ts = time.time() # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô

            self.logger.info(f"[WORKER START] Assessing Sub-Criteria: {sub_id} - {sub_criteria_name} (Weight: {sub_weight})")

            # ‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï temp_map_for_save ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ worker ‡∏ô‡∏µ‡πâ (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Parallel/Async!)
            self.temp_map_for_save = {}

            # -----------------------------------------------------------
            # 1. LOOP THROUGH LEVELS (L1 ‚Üí L5) - ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ó‡∏∏‡∏Å Level ‡πÄ‡∏™‡∏°‡∏≠
            # -----------------------------------------------------------
            for statement_data in sub_criteria.get('levels', []):
                level = statement_data.get('level')
                if level is None or level > self.config.target_level:
                    continue
                
             
                previous_level = level - 1
                persistence_key = f"{sub_id}.L{previous_level}"
                sequential_chunk_uuids = [] 
                level_result = {}
                level_temp_map: List[Dict[str, Any]] = []

                dependency_failed = False
                
                if dependency_failed:
                    error_msg = f"Assessment capped: L{previous_level} did not pass fully."
                    level_result = self._create_error_result(
                        level=level, 
                        error_message=error_msg, 
                        start_time=start_ts, 
                        sub_id=sub_id, 
                        statement_id=statement_data.get('statement_id', sub_id), 
                        statement_text=statement_data['statement']
                    )
                    level_result['is_capped'] = True
                    level_result['status'] = "CAPPED"
                    self.logger.info(f"  > üõë CAPPED L{level}: Due to L{previous_level} failure.")

                elif level >= 3:
                    wrapper = self.retry_policy.run(
                        fn=lambda attempt: self._run_single_assessment(
                            sub_criteria=sub_criteria,
                            statement_data=statement_data,
                            vectorstore_manager=self.vectorstore_manager,
                            sequential_chunk_uuids=sequential_chunk_uuids 
                        ),
                        level=level,
                        statement=statement_data.get('statement', ''),
                        context_blocks={"sequential_chunk_uuids": sequential_chunk_uuids},
                        logger=self.logger
                    )
                    level_result = wrapper.result if isinstance(wrapper, RetryResult) and wrapper.result is not None else {}
                    level_temp_map = level_result.get("temp_map_for_level", []) 

                else:
                    for attempt in range(MAX_L1_ATTEMPTS):
                        level_result = self._run_single_assessment(
                            sub_criteria=sub_criteria,
                            statement_data=statement_data,
                            vectorstore_manager=self.vectorstore_manager,
                            sequential_chunk_uuids=sequential_chunk_uuids 
                        )
                        level_temp_map = level_result.get("temp_map_for_level", []) 
                        if level_result.get('is_passed', False):
                            break

                # --- 1.2 PROCESS RESULT AND HANDLE EVIDENCE ---
                result_to_process = level_result or {}
                result_to_process.setdefault("used_chunk_uuids", [])

                is_passed_llm = result_to_process.get('is_passed', False)
                is_passed_final = is_passed_llm and not dependency_failed

                result_to_process['is_passed'] = is_passed_final
                result_to_process['is_capped'] = is_passed_llm and not is_passed_final
                result_to_process['pdca_score_required'] = get_correct_pdca_required_score(level) 

                # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å evidence ‡∏•‡∏á temp_map_for_save ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠ PASS ‡∏à‡∏£‡∏¥‡∏á
                if is_passed_final and level_temp_map and isinstance(level_temp_map, list):
                    
                    highest_rerank_score = result_to_process.get("max_relevant_score", 0.0)
                    
                    max_evi_str_after_save = self._save_level_evidences_and_calculate_strength(
                        level_temp_map=level_temp_map, 
                        sub_id=sub_id, 
                        level=level, 
                        llm_result=result_to_process,
                        highest_rerank_score=highest_rerank_score
                    )

                    result_to_process['max_evidence_strength_used'] = max_evi_str_after_save
                    
                    result_to_process['evidence_strength'] = round(
                        min(max_evi_str_after_save, 10.0) if is_passed_final else 0.0, 1
                    )
                    
                # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö level ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
                is_passed_current_level = is_passed_final

                # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏•‡∏á raw results
                result_to_process.setdefault("level", level)
                result_to_process["execution_index"] = len(raw_results_for_sub_seq)
                raw_results_for_sub_seq.append(result_to_process)

                # üü¢ NEW LOGIC: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏Å‡∏≥‡∏´‡∏ô‡∏î Highest Full Level
                if is_passed_final and (level == current_sequential_pass_level + 1):
                    # ‡∏ñ‡πâ‡∏≤ PASS ‡πÅ‡∏•‡∏∞ Level ‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô Level ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡∏Ç‡∏≠‡∏á Level ‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
                    current_sequential_pass_level = level
                # ‡∏ñ‡πâ‡∏≤ FAIL ‡∏´‡∏£‡∏∑‡∏≠ PASS ‡πÅ‡∏ï‡πà‡∏Å‡∏£‡∏∞‡πÇ‡∏î‡∏î‡∏Ç‡πâ‡∏≤‡∏° Level (‡πÄ‡∏ä‡πà‡∏ô L5 ‡∏ú‡πà‡∏≤‡∏ô ‡πÅ‡∏ï‡πà L2-L4 ‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô)
                # ‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï current_sequential_pass_level
            
            # -----------------------------------------------------------
            # 2. CALCULATE SUMMARY
            # -----------------------------------------------------------
            # üü¢ FIX: ‡∏Å‡∏≥‡∏´‡∏ô‡∏î highest_full_level ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Level ‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
            highest_full_level = current_sequential_pass_level

            # ‡∏ñ‡πâ‡∏≤ L1 ‡∏ú‡πà‡∏≤‡∏ô (current_sequential_pass_level=1) ‡πÅ‡∏ï‡πà L2 fail 
            # ‡∏Ñ‡πà‡∏≤ highest_full_level ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô 1
            # ‡∏ñ‡πâ‡∏≤ L1, L2 ‡∏ú‡πà‡∏≤‡∏ô (current_sequential_pass_level=2) ‡πÅ‡∏ï‡πà L3 fail/pass L5 
            # ‡∏Ñ‡πà‡∏≤ highest_full_level ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô 2 (‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£)

            weighted_score = self._calculate_weighted_score(highest_full_level, sub_weight)
            num_passed = sum(1 for r in raw_results_for_sub_seq if r.get("is_passed", False))

            sub_summary = {
                "num_statements": len(raw_results_for_sub_seq),
                "num_passed": num_passed,
                "num_failed": len(raw_results_for_sub_seq) - num_passed,
                "pass_rate": round(num_passed / len(raw_results_for_sub_seq), 4) if raw_results_for_sub_seq else 0.0
            }

        
            # -----------------------------------------------------------
            # 3. GENERATE ACTION PLAN (POST-PROCESSING) üöÄ
            # -----------------------------------------------------------

            target_next_level = highest_full_level + 1 if highest_full_level < 5 else 5
            
            WEAK_EVIDENCE_THRESHOLD = 5.0 
            
            statements_for_action_plan = []
            
            for r in raw_results_for_sub_seq:
                is_passed = r.get('is_passed', False)
                is_capped = r.get('is_evidence_strength_capped', False)
                evidence_strength = r.get('evidence_strength', 10.0)

                # 1. Statements ‡∏ó‡∏µ‡πà FAIL ‡∏à‡∏£‡∏¥‡∏á‡πÜ 
                if not is_passed and not is_capped:
                    r['recommendation_type'] = 'FAILED'
                    statements_for_action_plan.append(r)
                    continue

                # 2. Statements ‡∏ó‡∏µ‡πà PASS ‡πÅ‡∏ï‡πà‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏≠‡πà‡∏≠‡∏ô‡πÅ‡∏≠
                if is_passed and evidence_strength < WEAK_EVIDENCE_THRESHOLD:
                    r['recommendation_type'] = 'WEAK_EVIDENCE' 
                    statements_for_action_plan.append(r)
            
            # ... [Code for action plan generation remains unchanged] ...
            action_plan_result = []
            try:
                action_plan_result = self.create_structured_action_plan( 
                    failed_statements=statements_for_action_plan, 
                    sub_id=sub_id,
                    target_level=target_next_level,
                    llm_executor=self.llm
                )
            except Exception as e:
                self.logger.error(f"Failed to generate Action Plan for {sub_id}: {e}")
                action_plan_result = [{
                    "Phase": "Error", 
                    "Goal": "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á Action Plan ‡πÑ‡∏î‡πâ", 
                    "Actions": [{
                        "Statement_ID": "ERROR", 
                        "Recommendation": f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ LLM ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Action Plan: {str(e)}"
                    }]
                }]


            # -----------------------------------------------------------
            # 4. FINAL RESULT
            # -----------------------------------------------------------
            
            final_temp_map = {}
            if self.is_sequential:
                for key in self.evidence_map:
                    if key.startswith(sub_criteria['sub_id'] + "."):
                        final_temp_map[key] = self.evidence_map[key]
            else:
                final_temp_map = self.temp_map_for_save.copy()

            final_sub_result = {
                "sub_criteria_id": sub_id,
                "sub_criteria_name": sub_criteria_name,
                "highest_full_level": highest_full_level,
                "weight": sub_weight,
                "target_level_achieved": highest_full_level >= self.config.target_level,
                "weighted_score": weighted_score,
                "action_plan": action_plan_result, 
                "raw_results_ref": raw_results_for_sub_seq,
                "sub_summary": sub_summary,
                "worker_duration_s": round(time.time() - start_ts, 2)
            }


            self.logger.info(f"[WORKER END] {sub_id} | Highest: L{highest_full_level} | Action Plans: {len(action_plan_result)} phase(s) | Duration: {final_sub_result['worker_duration_s']:.2f}s")

            return final_sub_result, final_temp_map
    

    def _save_level_evidences_and_calculate_strength(
        self, 
        level_temp_map: List[Dict[str, Any]], 
        sub_id: str, 
        level: int, 
        llm_result: Dict[str, Any],
        highest_rerank_score: float = 0.0
    ) -> float:
        """
        [CRITICAL FIX 25.0] 
        ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô level ‡∏ô‡∏±‡πâ‡∏ô‡πÜ ‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà self.evidence_map/temp_map
        ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Evidence Strength (Evi Str)
        
        ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å: Chunk UUID ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ‡∏ó‡∏≥‡πÉ‡∏´‡πâ L2, L3 Hydration ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß (0 chunks restored).
        """
        map_key = f"{sub_id}.L{level}"
        new_evidence_list: List[Dict[str, Any]] = []
        
        # 1. ‡∏ß‡∏ô‡∏ã‡πâ‡∏≥‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô
        for chunk in level_temp_map:
            
            # üéØ CRITICAL FIX 25.0: ‡∏î‡∏∂‡∏á Chunk UUID ‡πÅ‡∏•‡∏∞ Stable Doc ID ‡πÅ‡∏¢‡∏Å‡∏à‡∏≤‡∏Å‡∏Å‡∏±‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
            # chunk_uuid ‡∏Ñ‡∏∑‡∏≠ ID ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏Ç‡∏≠‡∏á Chunk ‡∏ô‡∏±‡πâ‡∏ô‡πÜ (‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Hydration L2)
            chunk_uuid_key = chunk.get("chunk_uuid") 
            # stable_doc_uuid/doc_id ‡∏Ñ‡∏∑‡∏≠ ID ‡∏Ç‡∏≠‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö (‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Dedup)
            stable_doc_uuid_key = chunk.get("stable_doc_uuid") or chunk.get("doc_id")

            # Fallback Logic: ‡∏ñ‡πâ‡∏≤ Chunk UUID ‡∏´‡∏≤‡∏¢ ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ Stable Doc UUID ‡πÅ‡∏ó‡∏ô
            if not chunk_uuid_key and stable_doc_uuid_key:
                # ‚ö†Ô∏è ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: ‡∏ñ‡πâ‡∏≤ UUID ‡∏´‡∏≤‡∏¢ ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ Stable ID ‡πÅ‡∏ó‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ entry ‡∏ß‡πà‡∏≤‡∏á
                chunk_uuid_key = stable_doc_uuid_key 
                self.logger.warning(f"‚ö†Ô∏è [EVI SAVE] Missing chunk_uuid. Falling back to Stable ID: {chunk_uuid_key[:8]}")

            if not stable_doc_uuid_key or not chunk_uuid_key:
                 self.logger.error(f"‚ùå [EVI SAVE] Cannot determine required IDs for chunk. Skipping.")
                 continue

            # 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á Evidence Entry
            evidence_entry = {
                "sub_id": sub_id,
                "level": level,
                "relevance_score": chunk.get("rerank_score", chunk.get("score", 0.0)),
                "doc_id": stable_doc_uuid_key,          # <--- [FIXED] Stable ID (Document ID)
                "stable_doc_uuid": stable_doc_uuid_key, # <--- Stable ID (Document ID)
                "chunk_uuid": chunk_uuid_key,           # <--- [FIXED] Unique Chunk ID (‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Hydration)
                "source": chunk.get("source", "N/A"),
                "source_filename": chunk.get("filename", "N/A"),
                "pdca_tag": chunk.get("pdca_tag", "Other"), 
                "status": "PASS", 
                "timestamp": datetime.now().isoformat(),
            }
            new_evidence_list.append(evidence_entry)
            
        # 3. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Evidence Strength (Evi Str)
        evi_cap_data = self._calculate_evidence_strength_cap(
            top_evidences=new_evidence_list, 
            level=level,
            highest_rerank_score=highest_rerank_score
        )
        
        # 4. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏Ç‡πâ‡∏≤ Map
        current_map = self.evidence_map.setdefault(map_key, [])
        current_map.extend(new_evidence_list)
        
        # 5. ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï Temp Map (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Worker Mode)
        temp_map = self.temp_map_for_save.setdefault(map_key, [])
        temp_map.extend(new_evidence_list)
        
        # 6. Log ‡∏™‡∏£‡∏∏‡∏õ
        self.logger.info(f"[EVIDENCE SAVED] {map_key} ‚Üí {len(new_evidence_list)} chunks")
        self.logger.info(f"[SEQUENTIAL UPDATE] {map_key} added to engine's main evidence_map for L{level+1} dependency.")
        
        # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ max_evi_str_for_prompt ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï final_results
        return evi_cap_data['max_evi_str_for_prompt']
        

    def _calculate_evidence_strength_cap(
        self,
        top_evidences: List[Union[Dict[str, Any], Any]],  # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á dict ‡πÅ‡∏•‡∏∞ LcDocument
        level: int,
        # üü¢ FIX: ‡πÄ‡∏û‡∏¥‡πà‡∏° Argument ‡∏ï‡∏±‡∏ß‡∏ô‡∏µ‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ TypeError ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏à‡∏≤‡∏Å _run_single_assessment ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ
        highest_rerank_score: Optional[float] = None 
    ) -> Dict[str, Any]:
        """
        Relevant Score Gate ‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô DEBUG FINAL: ‡∏î‡∏∂‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏à‡∏≤‡∏Å metadata, top-level key/attribute, ‡πÅ‡∏•‡∏∞ Regex fallback ‡∏ó‡∏µ‡πà‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°
        """

        # ‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ô‡∏µ‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
        max_score_found = 0.0 
        max_score_source = "N/A"

        score_keys = [
            "relevance_score", "rerank_score", "score", 
            "_rerank_score_force", "_rerank_score", 
            "Score", "RelevanceScore"
        ]
        
        # üí° ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤ Threshold ‡πÅ‡∏•‡∏∞ Cap ‡∏à‡∏≤‡∏Å Attribute ‡∏Ç‡∏≠‡∏á Class
        threshold = getattr(self, "RERANK_THRESHOLD", 0.5) 
        cap_value = getattr(self, "MAX_EVI_STR_CAP", 3.0)
        
        # üí° Fallback: ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Attribute ‡πÉ‡∏´‡πâ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å config/global_vars
        if not isinstance(threshold, (int, float)):
            try:

                threshold = RERANK_THRESHOLD
                cap_value = MAX_EVI_STR_CAP
            except ImportError:
                # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Default ‡∏´‡∏≤‡∏Å Config ‡∏´‡∏≤‡∏¢‡πÑ‡∏õ
                threshold = 0.5
                cap_value = 3.0

        # üí° ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å Adaptive Loop (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ) ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
        if highest_rerank_score is not None and highest_rerank_score > max_score_found:
             max_score_found = highest_rerank_score
             max_score_source = "Adaptive_RAG_Loop"


        for doc in top_evidences:
            
            # -------------------- DEBUGGING BLOCK (START) --------------------
            if doc is top_evidences[0]:
                self.logger.critical(f"DEBUG L{level}: Inspecting first document (Type: {type(doc)})")
                
                if isinstance(doc, dict):
                    content = doc.get("text", "")
                    tail_content = content[-200:] if len(content) > 200 else content
                    self.logger.critical(f"DEBUG L{level}: Dict keys: {list(doc.keys())}")
                    self.logger.critical(f"DEBUG L{level}: END OF 'text' content (last 200 chars): \n***\n{tail_content}\n***")
                else:
                    try:
                        doc_attrs = [attr for attr in dir(doc) if not attr.startswith('_') and not callable(getattr(doc, attr))]
                        self.logger.critical(f"DEBUG L{level}: Doc public attributes (potential score location): {doc_attrs}")
                    except:
                        self.logger.critical(f"DEBUG L{level}: Cannot inspect attributes of this object type.")
            # -------------------- DEBUGGING BLOCK (END) --------------------
            
            page_content = ""
            metadata = {}
            
            # ‚îÄ‚îÄ‚îÄ 1. ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô metadata + content ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏∏‡∏Å‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á) ‚îÄ‚îÄ‚îÄ
            if isinstance(doc, dict):
                metadata = doc.get("metadata", {}) 
                page_content = doc.get("page_content", "") or doc.get("text", "") or doc.get("content", "")
            else:
                metadata = getattr(doc, "metadata", {})
                page_content = getattr(doc, "page_content", "") or getattr(doc, "text", "")

            # ‚îÄ‚îÄ‚îÄ 2. ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô (‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö top-level key/attribute ‡πÅ‡∏•‡∏∞ metadata) ‚îÄ‚îÄ‚îÄ
            current_score = 0.0
            
            for key in score_keys:
                score_val = None
                
                if key in metadata:
                    score_val = metadata[key]
                
                if score_val is None:
                    if isinstance(doc, dict):
                        score_val = doc.get(key)
                    else:
                        score_val = getattr(doc, key, None)

                if score_val is not None:
                    try:
                        current_score = float(score_val)
                        if current_score > 0:
                            break
                    except (ValueError, TypeError):
                        continue
            
            # ‚îÄ‚îÄ‚îÄ 3. Fallback: ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å‡∏ó‡πâ‡∏≤‡∏¢ content (Aggressive Regex) ‚îÄ‚îÄ‚îÄ
            if current_score == 0.0 and page_content and isinstance(page_content, str):
                try:
                    tail = page_content[-1000:]
                    patterns = [
                        r"Relevance[ :]+([0-9]*\.?[0-9]+)",
                        r"Score[ :]+([0-9]*\.?[0-9]+)",
                        r"Re:[ ]*([0-9]*\.?[0-9]+)",
                        r"\[Relevance: ([0-9]*\.?[0-9]+)\]",
                        r"\[Score: ([0-9]*\.?[0-9]+)\]",
                        r"rerank_score['\"]?\s*:\s*([0-9]*\.?[0-9]+)",
                        r"\|\s*([0-9]*\.?[0-9]+)\s*\|",
                        r"\s+([0-9]\.[0-9]+)$",
                    ]
                    for pat in patterns:
                        m = re.search(pat, tail, re.IGNORECASE)
                        if m:
                            try:
                                current_score = float(m.group(1))
                                break
                            except:
                                continue
                except ImportError:
                    pass

            # üî¥ FIX: ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô (Score Clamp) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Reranker Score
            # ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏Å‡∏¥‡∏ô 1.0 (‡∏ã‡∏∂‡πà‡∏á‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡∏Å‡∏±‡∏ö Cross-Encoder Reranker)
            # ‡πÉ‡∏´‡πâ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡∏ñ‡∏π‡∏Å‡∏î‡∏∂‡∏á‡∏°‡∏≤‡∏ú‡∏¥‡∏î‡πÅ‡∏•‡∏∞‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï‡πÄ‡∏õ‡πá‡∏ô 0.0 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ Evi Str ‡πÄ‡∏ï‡πá‡∏°‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ï‡∏±‡πâ‡∏á‡πÉ‡∏à
            if current_score > 1.0:
                self.logger.warning(f"üö® Score Clamp L{level}: Resetting invalid score {current_score:.4f} > 1.0 from source 'Fallback Regex' to 0.0")
                current_score = 0.0


            # ‚îÄ‚îÄ‚îÄ 4. ‡∏î‡∏∂‡∏á source ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‚îÄ‚îÄ‚îÄ
            source = (
                metadata.get("source_filename") or metadata.get("filename") or
                doc.get("source_filename") or doc.get("filename") or 
                doc.get("source") or doc.get("doc_id") or
                "N/A"
            )

            # ‚îÄ‚îÄ‚îÄ 5. ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î ‚îÄ‚îÄ‚îÄ
            if current_score > max_score_found:
                max_score_found = current_score
                max_score_source = source

        # ‚îÄ‚îÄ‚îÄ 6. Relevant Score Gate + Log ‚îÄ‚îÄ‚îÄ
        
        # NOTE: ‡πÉ‡∏ä‡πâ threshold ‡πÅ‡∏•‡∏∞ cap_value ‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏°‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
        if max_score_found < threshold: 
            max_evi_str_for_prompt = cap_value
            is_capped = True
            self.logger.warning(
                f"üö® Evi Str CAPPED L{level}: "
                f"Rerank {max_score_found:.4f} (‡∏à‡∏≤‡∏Å '{max_score_source}') "
                f"< {threshold} ‚Üí ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏ó‡∏µ‡πà {cap_value}"
            )
        else:
            max_evi_str_for_prompt = 10.0
            is_capped = False
            self.logger.info(
                f"‚úÖ Evi Str FULL L{level}: "
                f"Rerank {max_score_found:.4f} (‡∏à‡∏≤‡∏Å '{max_score_source}') "
                f">= {threshold} ‚Üí ‡∏õ‡∏•‡πà‡∏≠‡∏¢‡πÄ‡∏ï‡πá‡∏° 10.0"
            )

        return {
            "is_capped": is_capped,
            "max_evi_str_for_prompt": max_evi_str_for_prompt,
            "highest_rerank_score": round(float(max_score_found), 4), 
            "max_score_source": max_score_source,
        }    
    

    def run_assessment(
            self,
            target_sub_id: str = "all",
            export: bool = False,
            vectorstore_manager: Optional['VectorStoreManager'] = None,
            sequential: bool = False,
            document_map: Optional[Dict[str, str]] = None, # üü¢ FIX: ‡∏£‡∏±‡∏ö document_map
        ) -> Dict[str, Any]:
        """
        Main runner ‡∏Ç‡∏≠‡∏á Assessment Engine
        ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á Parallel ‡πÅ‡∏•‡∏∞ Sequential 100%
        ‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏ß‡πà‡∏≤ evidence_map ‡∏Ñ‡∏£‡∏ö‡∏ó‡∏∏‡∏Å‡∏Å‡∏£‡∏ì‡∏µ
        """

        start_ts = time.time()
        self.is_sequential = sequential

        # ============================== 1. Filter Rubric ==============================
        if target_sub_id.lower() == "all":
            sub_criteria_list = self._flatten_rubric_to_statements()
        else:
            # üü¢ NOTE: ‡πÉ‡∏ä‡πâ _flatten_rubric_to_statements ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á List ‡∏Å‡πà‡∏≠‡∏ô
            all_statements = self._flatten_rubric_to_statements()
            sub_criteria_list = [
                s for s in all_statements if s.get('sub_id') == target_sub_id
            ]
            if not sub_criteria_list:
                self.logger.error(f"Sub-Criteria ID '{target_sub_id}' not found in rubric.")
                return {"error": f"Sub-Criteria ID '{target_sub_id}' not found."}

        # Reset states
        self.raw_llm_results = []
        self.final_subcriteria_results = []

        # ‡πÇ‡∏´‡∏•‡∏î evidence map ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß (‡πÑ‡∏°‡πà clear!)
        if os.path.exists(self.evidence_map_path):
            loaded = self._load_evidence_map()
            if loaded:
                self.evidence_map = loaded
                self.logger.info(f"Resumed from existing evidence map: {len(self.evidence_map)} keys")
            else:
                self.evidence_map = {}
        else:
            self.evidence_map = {}

        if not sequential:
            self.logger.info("[PARALLEL MODE] Starting parallel assessment...")

        # --------------------- üí° NEW: ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Max Workers ---------------------
        # üìå FIX: ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤ Default ‡πÄ‡∏õ‡πá‡∏ô 4 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö Config ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
        DEFAULT_SAFE_WORKERS = 4 

        # 1. ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å Global Variable ‡∏ó‡∏µ‡πà Import ‡∏°‡∏≤
        # ‡πÉ‡∏ä‡πâ globals() ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á MAX_PARALLEL_WORKERS ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å import ‡∏°‡∏≤
        max_workers_from_config = globals().get('MAX_PARALLEL_WORKERS', None)

        # 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏°‡∏≤
        if (max_workers_from_config is None or 
            not isinstance(max_workers_from_config, int) or 
            max_workers_from_config <= 0):
            
            # 3. ‡∏ñ‡πâ‡∏≤‡∏Ñ‡πà‡∏≤ Config ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á: ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢ (4) 
            max_workers = DEFAULT_SAFE_WORKERS
            self.logger.warning(
                f"‚ö†Ô∏è Configured workers inaccessible. Forcing max_workers to safe value: {max_workers}. "
                f"(System CPU count is {os.cpu_count()}, which would lead to {os.cpu_count() - 1} workers.)"
            )
        else:
            # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏°‡∏≤‡πÑ‡∏î‡πâ
            max_workers = max_workers_from_config
        
        self.logger.info(f"Setting Max Workers for Parallel Pool: {max_workers}")
        # --------------------------------------------------------------------
        
        # --------------------------------------------------------------------
        # üìå FIX 1 (Export): ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç run_parallel
        run_parallel = (target_sub_id.lower() == "all") and not sequential 
        # --------------------------------------------------------------------

        # ============================== 2. Run Assessment ==============================
        if run_parallel:
            # --------------------- PARALLEL MODE ---------------------
            self.logger.info(f"Starting Parallel Assessment with Multiprocessing using {max_workers} processes.")
            worker_args = [(
                sub_data,                                       # 1. sub_criteria_data
                self.config.enabler,                            # 2. enabler
                self.config.target_level,                       # 3. target_level
                self.config.mock_mode,                          # 4. mock_mode
                self.evidence_map_path,                         # 5. evidence_map_path
                self.config.model_name,                         # 6. model_name
                self.config.temperature,                        # 7. temperature
                # 8 & 9: ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å Config ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÑ‡∏ß‡πâ‡πÉ‡∏ô Engine Instance
                getattr(self.config, 'MIN_RETRY_SCORE', 0.65),  # 8. min_retry_score
                getattr(self.config, 'MAX_RETRIEVAL_ATTEMPTS', 3), # 9. max_retrieval_attempts
                self.document_map                               # 10. document_map
            ) for sub_data in sub_criteria_list]

            try:
                pool_ctx = multiprocessing.get_context('spawn')
                # üü¢ FIX FINAL BUG: ‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£ max_workers ‡∏ó‡∏µ‡πà‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÅ‡∏•‡πâ‡∏ß (‡∏ã‡∏∂‡πà‡∏á‡∏à‡∏∞‡πÑ‡∏î‡πâ 4)
                with pool_ctx.Pool(processes=max_workers) as pool:
                    # NOTE: _static_worker_process ‡∏à‡∏∞‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ (sub_result, temp_map_from_worker)
                    results_list = pool.map(_static_worker_process, worker_args)
            except Exception as e:
                self.logger.critical(f"Multiprocessing failed: {e}")
                raise

            # ‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å worker
            for result_tuple in results_list:
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å worker ‡∏Å‡πà‡∏≠‡∏ô unpack
                if not isinstance(result_tuple, tuple) or len(result_tuple) != 2:
                    self.logger.error(f"Worker returned invalid result structure: {result_tuple}")
                    continue
                
                sub_result, temp_map_from_worker = result_tuple

                if isinstance(temp_map_from_worker, dict):
                    # ‡∏£‡∏ß‡∏° Evidence Map ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å Worker
                    for level_key, evidence_list in temp_map_from_worker.items():
                        if isinstance(evidence_list, list) and evidence_list:
                            current_list = self.evidence_map.setdefault(level_key, [])
                            current_list.extend(evidence_list)
                            self.logger.info(f"AGGREGATED: +{len(evidence_list)} ‚Üí {level_key} "
                                        f"(total: {len(current_list)})")

                # ‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô
                raw_refs = sub_result.get("raw_results_ref", [])
                self.raw_llm_results.extend(raw_refs if isinstance(raw_refs, list) else [])
                self.final_subcriteria_results.append(sub_result)


        else:
            # --------------------- SEQUENTIAL MODE ---------------------
            mode_desc = target_sub_id if target_sub_id != "all" else "All Sub-Criteria (Sequential)"
            self.logger.info(f"Starting Sequential Assessment: {mode_desc}")

            # üéØ FIX: ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç load_all_vectorstores ‡πÉ‡∏´‡πâ‡∏°‡∏µ tenant ‡πÅ‡∏•‡∏∞ year
            local_vsm = vectorstore_manager or (
                load_all_vectorstores(
                    doc_types=[EVIDENCE_DOC_TYPES], 
                    evidence_enabler=self.config.enabler,
                    tenant=self.config.tenant,  # <--- NEW: Argument ‡∏ó‡∏µ‡πà‡∏´‡∏≤‡∏¢‡πÑ‡∏õ
                    year=self.config.year       # <--- NEW: Argument ‡∏ó‡∏µ‡πà‡∏´‡∏≤‡∏¢‡πÑ‡∏õ
                )
                if self.config.mock_mode == "none" else None
            )
            self.vectorstore_manager = local_vsm

            for sub_criteria in sub_criteria_list:
                sub_result, final_temp_map = self._run_sub_criteria_assessment_worker(sub_criteria)
                self.raw_llm_results.extend(sub_result.get("raw_results_ref", []))
                self.final_subcriteria_results.append(sub_result)

        # ============================== 3. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Evidence Map ==============================
        if self.evidence_map:
            self._save_evidence_map(map_to_save=self.evidence_map)
            total_items = sum(len(v) for v in self.evidence_map.values())
            self.logger.info(f"Persisted final evidence map | Keys: {len(self.evidence_map)} | "
                            f"Items: {total_items} | Size: ~{total_items * 0.35:.1f} KB")

        # ============================== 4. ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏• & Export ==============================
        self._calculate_overall_stats(target_sub_id)

        final_results = {
            "summary": self.total_stats,
            "sub_criteria_results": self.final_subcriteria_results,
            "raw_llm_results": self.raw_llm_results,
            "run_time_seconds": round(time.time() - start_ts, 2),
            "timestamp": datetime.now().isoformat(),
        }

        if export:
            export_path = self._export_results(
                results=final_results,
                enabler=self.config.enabler,
                sub_criteria_id=target_sub_id if target_sub_id != "all" else "ALL",
                target_level=self.config.target_level
            )
            final_results["export_path_used"] = export_path
            final_results["evidence_map"] = deepcopy(self.evidence_map)

        return final_results
    
    def _robust_hydrate_documents_for_priority_chunks(self, chunks_to_hydrate: List[Dict], vsm: Optional['VectorStoreManager']) -> List[Dict]:
        """
        Hydrates priority chunks using robust Stable ID fallback logic 
        (similar to _collect_previous_level_evidences, but simpler).
        
        ‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Hydration ‡∏Ç‡∏≠‡∏á Priority Chunks ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÅ‡∏Ñ‡πà Metadata/UUIDs ‡πÄ‡∏Å‡πà‡∏≤
        ‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏´‡πâ reranker ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á ‚Üí Priority Score ‡∏û‡∏∏‡πà‡∏á‡∏ó‡∏±‡∏ô‡∏ó‡∏µ!
        """
        if not chunks_to_hydrate or not vsm:
            return chunks_to_hydrate

        # 1. Collect Stable IDs
        stable_ids = set()
        for chunk in chunks_to_hydrate:
            # üí° ‡πÉ‡∏ä‡πâ doc_id ‡πÄ‡∏õ‡πá‡∏ô Fallback ‡∏´‡∏≤‡∏Å Stable ID ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤
            sid = chunk.get("stable_doc_uuid") or chunk.get("doc_id") 
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ ID ‡∏°‡∏µ‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡πÄ‡∏õ‡πá‡∏ô UUID/Hash (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß 64-bit)
            if sid and isinstance(sid, str) and len(sid.replace("-", "")) >= 64: 
                stable_ids.add(sid)
        
        # ‚ö†Ô∏è ‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏°‡πà‡∏°‡∏µ Stable ID ‡πÄ‡∏•‡∏¢ 
        if not stable_ids:
            # ‡πÉ‡∏ä‡πâ logger.info ‡πÅ‡∏ó‡∏ô warning ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÅ‡∏õ‡∏•‡∏ß‡πà‡∏≤ Hydration ‡∏´‡∏•‡∏±‡∏Å‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß
            self.logger.info("No Stable IDs found for Priority Chunk hydration fallback. Relying on existing text/metadata.")
            
            # **‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:** ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ Stable ID ‡πÉ‡∏´‡πâ Boost Score ‡πÉ‡∏´‡πâ‡∏Å‡∏±‡∏ö‡∏ó‡∏∏‡∏Å Chunk ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß
            for chunk in chunks_to_hydrate:
                if "text" in chunk and chunk.get("rerank_score", 0.0) < 0.95:
                    chunk["rerank_score"] = 0.95
                    chunk["score"] = 0.95
            
            return chunks_to_hydrate # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ Chunks ‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å Boost Score ‡πÅ‡∏•‡πâ‡∏ß

        # 2. Fetch full documents using Stable IDs (Fallback Search)
        try:
            self.logger.info(f"HYDRATION ‚Üí Attempting Robust Hydration Fallback for Priority Chunks: {len(stable_ids)} Stable IDs.")
            # üí° vsm.get_documents_by_id ‡∏Ñ‡∏ß‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ï‡πá‡∏°‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤
            full_chunks = vsm.get_documents_by_id(list(stable_ids), self.doc_type, self.enabler_id) 
            self.logger.info(f"Fallback Retrieved {len(full_chunks)} full chunks for Priority Docs.")
        except Exception as e:
            self.logger.error(f"Priority Chunk Hydration Fallback failed: {e}")
            return chunks_to_hydrate # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ Chunks ‡πÄ‡∏î‡∏¥‡∏° (‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏µ text)

        # 3. Create a Map for quick lookup (Key: Stable ID)
        stable_id_map = {}
        for chunk in full_chunks:
            meta = getattr(chunk, "metadata", {})
            sid = meta.get("stable_doc_uuid") or meta.get("doc_id")
            if sid:
                stable_id_map[sid] = {
                    "text": chunk.page_content,
                    "metadata": meta
                }

        # 4. Hydrate the input list of priority chunks + ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á!
        hydrated_priority_docs = []
        restored_count = 0
        total_count = len(chunks_to_hydrate)
        for chunk in chunks_to_hydrate:
            new_chunk = chunk.copy()
            sid = new_chunk.get("stable_doc_uuid") or new_chunk.get("doc_id")
            
            # ‚úÖ Success: Hydrate ‡πÑ‡∏î‡πâ
            if sid and sid in stable_id_map:
                data = stable_id_map[sid]
                
                # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï Full Text
                new_chunk["text"] = data["text"]
                # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï Metadata
                new_chunk.update({k: v for k, v in data["metadata"].items() 
                              if k not in ["text", "page_content"]})
                restored_count += 1

                # ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î: ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏´‡πâ reranker ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î (‡πÄ‡∏û‡∏∑‡πà‡∏≠ Priority Score ‡∏™‡∏π‡∏á)
                new_chunk["rerank_score"] = 1.0
                new_chunk["score"] = 1.0
            
            # ‚ùå Failed: Hydrate ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÅ‡∏ï‡πà‡∏ï‡πâ‡∏≠‡∏á Boost Score ‡πÑ‡∏ß‡πâ
            elif "text" not in new_chunk:
                 self.logger.warning(f"Failed to restore Priority Chunk for Stable ID {sid[:8]}... (No text field)")
                 # ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á‡πÑ‡∏ß‡πâ‡∏Å‡πà‡∏≠‡∏ô ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏î‡∏∂‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°‡∏•‡∏á
                 new_chunk["rerank_score"] = max(new_chunk.get("rerank_score", 0.0), 0.8) 
                 new_chunk["score"] = max(new_chunk.get("score", 0.0), 0.8)
            
            # ‚úÖ Fallback: ‡∏°‡∏µ text ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß (‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£ Hydrate ‡∏´‡∏•‡∏±‡∏Å), ‡πÉ‡∏´‡πâ Boost Score ‡∏™‡∏π‡∏á
            elif new_chunk.get("rerank_score", 0.0) < 0.95:
                 new_chunk["rerank_score"] = 0.95
                 new_chunk["score"] = 0.95

            hydrated_priority_docs.append(new_chunk)

        self.logger.info(f"HYDRATION success: {restored_count}/{total_count} chunks (Docs: {total_count})")
        return hydrated_priority_docs

    # -------------------- Evidence Classification Helper (Optimized - ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô Class) --------------------

    def _get_pdca_blocks_from_evidences(
        self, 
        top_evidences: List[Dict[str, Any]], 
        level: int 
    ) -> Tuple[str, str, str, str, str]:
        """
        Groups retrieved evidence chunks into PDCA phases based on the 'pdca_tag' 
        Generated by the LLM classifier, after applying sorting and filtering based on Relevance Score.
        """
        logger = logging.getLogger(__name__)

        # 1. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡∏Ñ‡∏±‡∏î‡∏Å‡∏£‡∏≠‡∏á (Filtering Threshold)
        MIN_RELEVANCE_THRESHOLD = 0.5 

        # 2. Sorting Evidence (‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏à‡∏≤‡∏Å‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡πÑ‡∏õ‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î)
        top_evidences.sort(
            key=lambda x: x.get('relevance_score', x.get('score', 0.0)), 
            reverse=True
        )

        # 3. Filtering Evidence (‡∏Ñ‡∏±‡∏î‡∏Å‡∏£‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤ Threshold)
        filtered_evidences = [
            doc for doc in top_evidences 
            if doc.get('relevance_score', doc.get('score', 0.0)) >= MIN_RELEVANCE_THRESHOLD
        ]
        
        chunks_to_process = filtered_evidences
        
        # Fallback: ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡πÄ‡∏•‡∏¢ ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡πÅ‡∏•‡πâ‡∏ß
        if not chunks_to_process:
            chunks_to_process = top_evidences
            logger.warning(
                f"  > (L{level}) No evidence passed Relevance Threshold ({MIN_RELEVANCE_THRESHOLD}). "
                f"Processing all {len(top_evidences)} sorted chunks."
            )
        
        # 4. Initialize groupings
        pdca_groups = defaultdict(list)
        
        # 5. Group chunks based on the 'pdca_tag'
        for i, doc in enumerate(chunks_to_process):
            tag = doc.get('pdca_tag', 'Other')
            score = doc.get('relevance_score', doc.get('score', 0.0))
            
            # üìå ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á Chunk Header: ‡πÄ‡∏û‡∏¥‡πà‡∏° Tag ‡πÅ‡∏•‡∏∞ Relevance Score
            formatted_chunk = (
                f"--- [Chunk {i+1} | Tag: {tag} | Score: {score:.4f}] ---\n"
                f"{doc.get('text', '')}\n"
            )
            pdca_groups[tag].append(formatted_chunk)

        # 6. Aggregate groups into single strings
        plan_blocks = "\n\n".join(pdca_groups.get('Plan', []))
        do_blocks = "\n\n".join(pdca_groups.get('Do', []))
        check_blocks = "\n\n".join(pdca_groups.get('Check', []))
        act_blocks = "\n\n".join(pdca_groups.get('Act', []))
        other_blocks = "\n\n".join(pdca_groups.get('Other', []))

        logger.debug(
            f"  > PDCA Blocks Grouped (L{level}): Processed {len(chunks_to_process)} chunks | "
            f"P={len(pdca_groups.get('Plan', []))}, D={len(pdca_groups.get('Do', []))}, "
            f"C={len(pdca_groups.get('Check', []))}, A={len(pdca_groups.get('Act', []))}, "
            f"Other={len(pdca_groups.get('Other', []))}"
        )

        return plan_blocks, do_blocks, check_blocks, act_blocks, other_blocks

    # -------------------- _run_single_assessment (FINAL REVISED VERSION) --------------------

    def _run_single_assessment(
        self,
        sub_criteria: Dict[str, Any],
        statement_data: Dict[str, Any],
        vectorstore_manager: Optional['VectorStoreManager'],
        sequential_chunk_uuids: Optional[List[str]] = None,
        attempt: int = 1
    ) -> Dict[str, Any]:
        """
        ‡∏£‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô Level ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (L1-L5) ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå
        ‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: Retry ‡∏ã‡πâ‡∏≥‡∏ã‡∏≤‡∏Å, Context ‡∏ß‡πà‡∏≤‡∏á, Hydration ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß
        """
        MIN_RETRY_SCORE = getattr(self.config, 'min_retry_score', 0.65)
        MAX_RETRIEVAL_ATTEMPTS = getattr(self.config, 'max_retrieval_attempts', 3)

        start_time = time.time()
        sub_id = sub_criteria['sub_id']
        level = statement_data['level']
        statement_text = statement_data['statement']
        sub_criteria_name = sub_criteria['sub_criteria_name']
        statement_id = statement_data.get('statement_id', sub_id)

        self.logger.info(f"  > Starting assessment for {sub_id} L{level} (Attempt: {attempt})...")

        # ==================== 1. PDCA & Keywords ====================
        pdca_phase = self._get_pdca_phase(level)
        level_constraint = self._get_level_constraint_prompt(level)

        context_rules = self.contextual_rules_map.get(sub_id, {})
        must_include_keywords = ", ".join(context_rules.get("must_include_keywords", []))
        avoid_keywords = ", ".join(context_rules.get("avoid_keywords", []))

        # ==================== 2. Hybrid Retrieval Setup ====================
        mapped_stable_doc_ids, priority_docs_unhydrated = self._get_mapped_uuids_and_priority_chunks(
            sub_id=sub_id,
            level=level,
            statement_text=statement_text,
            level_constraint=level_constraint,
            vectorstore_manager=vectorstore_manager
        )

        # NEW FIX: ‡∏ó‡∏≥ Robust Hydration ‡πÉ‡∏´‡πâ Priority Chunks ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
        priority_docs = self._robust_hydrate_documents_for_priority_chunks(
            chunks_to_hydrate=priority_docs_unhydrated,
            vsm=vectorstore_manager
        )

        # ==================== 3. Enhance Query ====================
        rag_query_list = enhance_query_for_statement(
            statement_text=statement_text,
            sub_id=sub_id,
            statement_id=statement_id,
            level=level,
            enabler_id=self.config.enabler,
            focus_hint=level_constraint,
            llm_executor=self.llm
        )
        rag_query = rag_query_list[0] if rag_query_list else statement_text

        # ==================== 4. LLM Evaluator ====================
        llm_evaluator_to_use = evaluate_with_llm_low_level if level <= 2 else self.llm_evaluator

        # ==================== 5. ADAPTIVE RAG LOOP ====================
        highest_rerank_score = 0.0
        final_top_evidences = []
        retrieval_start = time.time()
        loop_attempt = 1

        while loop_attempt <= MAX_RETRIEVAL_ATTEMPTS:
            self.logger.info(
                f"  > RAG Retrieval {sub_id} L{level} (Attempt: {loop_attempt}/{MAX_RETRIEVAL_ATTEMPTS}). "
                f"Best score so far: {highest_rerank_score:.4f}"
            )

            query_input = rag_query_list if loop_attempt == 1 and rag_query_list else [rag_query]

            try:
                retrieval_result = self.rag_retriever(
                    query=query_input,
                    doc_type=EVIDENCE_DOC_TYPES,
                    enabler=self.config.enabler,
                    sub_id=sub_id,
                    level=level,
                    vectorstore_manager=vectorstore_manager,
                    mapped_uuids=mapped_stable_doc_ids,
                    priority_docs_input=priority_docs,
                    sequential_chunk_uuids=sequential_chunk_uuids,
                )
            except Exception as e:
                self.logger.error(f"RAG retrieval failed: {e}")
                break

            top_evidences_current = retrieval_result.get("top_evidences", [])

            current_max_score = max(
                (ev.get("rerank_score") or ev.get("score", 0.0) for ev in top_evidences_current),
                default=0.0
            )

            priority_max_score = max(
                (doc.get("rerank_score") or doc.get("score", 0.0) for doc in priority_docs),
                default=0.0
            )

            overall_max_score = max(current_max_score, priority_max_score)

            self.logger.info(
                f"  > Attempt {loop_attempt} ‚Üí New: {current_max_score:.4f} | Priority: {priority_max_score:.4f} | "
                f"Overall: {overall_max_score:.4f}"
            )

            if overall_max_score > highest_rerank_score:
                highest_rerank_score = overall_max_score
                final_top_evidences = top_evidences_current
                if loop_attempt > 1:
                    self.logger.info(f"  > Retrieval improved: New overall best {highest_rerank_score:.4f}")

            if highest_rerank_score >= MIN_RETRY_SCORE:
                self.logger.info(f"  > Adaptive Retrieval L{level}: Score {highest_rerank_score:.4f} ‚â• {MIN_RETRY_SCORE} ‚Üí STOP")
                break

            if loop_attempt < MAX_RETRIEVAL_ATTEMPTS:
                rag_query = self.rephrase_query_for_retry(rag_query, level, sub_id)

            loop_attempt += 1

        retrieval_duration = time.time() - retrieval_start
        top_evidences = final_top_evidences

        # ==================== 6. Adaptive Filtering ====================
        filtered = []
        for doc in top_evidences:
            score = doc.get('rerank_score', doc.get('score', 0.0))
            if score >= self.MIN_RERANK_SCORE_TO_KEEP:
                filtered.append(doc)
            else:
                doc_id = doc.get('chunk_uuid') or doc.get('doc_id') or 'UNKNOWN'
                self.logger.info(f"Filtering out chunk (ID: {doc_id}) | Score {score:.4f}")
        top_evidences = filtered

        # ==================== 7. Baseline from Previous Levels ====================
        previous_levels_evidence_full = []
        if level > 1 and self.is_sequential: 
            prev = self._collect_previous_level_evidences(sub_id, current_level=level)
            for lst in prev.values():
                previous_levels_evidence_full.extend(lst)

        # ==================== 8. Build Context (‡πÉ‡∏ä‡πâ Logic ‡πÉ‡∏´‡∏°‡πà) ====================
        
        # 8.1 üéØ NEW: Grouping, Sorting, and Filtering ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô (top_evidences)
        plan_blocks, do_blocks, check_blocks, act_blocks, other_blocks = self._get_pdca_blocks_from_evidences(
            top_evidences=top_evidences, # ‡πÉ‡∏ä‡πâ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏Å‡∏£‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß
            level=level
        )
        
        # 8.2 üìå NEW: ‡∏£‡∏ß‡∏°‡∏ö‡∏•‡πá‡∏≠‡∏Å PDCA ‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏î‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö DIRECT EVIDENCE
        direct_context = "\n\n".join(filter(None, [
            plan_blocks,
            do_blocks,
            check_blocks,
            act_blocks,
            other_blocks
        ]))
        
        # 8.3 üìå OLD: ‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ build_multichannel_context_for_level 
        # ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á AUXILIARY ‡πÅ‡∏•‡∏∞ BASELINE summaries (‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÄ‡∏î‡∏¥‡∏°)
        channels = build_multichannel_context_for_level(
            level=level,
            top_evidences=top_evidences,
            previous_levels_evidence=previous_levels_evidence_full,
            max_main_context_tokens=3000,
            max_summary_sentences=4
        )
        aux_summary = channels.get('aux_summary','')
        baseline_summary = channels.get('baseline_summary','‡πÑ‡∏°‡πà‡∏°‡∏µ')

        # 8.4 üìå Construct Final Context
        final_llm_context = "\n\n".join(filter(None, [
            f"--- DIRECT EVIDENCE (L{level})---\n{direct_context}", # <--- ‡πÉ‡∏ä‡πâ Context ‡πÉ‡∏´‡∏°‡πà
            f"--- AUXILIARY EVIDENCE ---\n{aux_summary}",
            f"--- BASELINE FROM PREVIOUS LEVELS ---\n{baseline_summary}"
        ]))


        # ==================== 9. LLM Evaluation ====================
        evi_cap_data = self._calculate_evidence_strength_cap(top_evidences, level, highest_rerank_score=highest_rerank_score)
        max_evi_str_for_prompt = evi_cap_data['max_evi_str_for_prompt']

        llm_start = time.time()
        try:
            llm_result = llm_evaluator_to_use(
                context=final_llm_context,
                sub_criteria_name=sub_criteria_name,
                level=level,
                statement_text=statement_text,
                sub_id=sub_id,
                pdca_phase=pdca_phase,
                level_constraint=level_constraint,
                must_include_keywords=must_include_keywords,
                avoid_keywords=avoid_keywords,
                max_rerank_score=highest_rerank_score,
                llm_executor=self.llm,
                max_evidence_strength=max_evi_str_for_prompt
            )
        except Exception as e:
            self.logger.error(f"LLM Call failed: {e}")
            llm_result = {}

        llm_duration = time.time() - llm_start

        # ==================== 9.5. Deterministic Score Fallback ====================
        if not isinstance(llm_result, dict):
            llm_result = {}
            
        llm_result = post_process_llm_result(llm_result, level) 

        # ==================== 10. Final Scoring ====================
        llm_score_pdca_sum = llm_result.get('score', 0)
        is_passed = llm_result.get('is_passed', False) 
        
        pdca_breakdown_llm = {
            'P': llm_result.get('P_Plan_Score', 0),
            'D': llm_result.get('D_Do_Score', 0),
            'C': llm_result.get('C_Check_Score', 0),
            'A': llm_result.get('A_Act_Score', 0),
        }
        
        final_score = llm_score_pdca_sum
        final_pdca_breakdown = pdca_breakdown_llm
        
        if is_passed:
            if level == 1:
                final_score = 1.0
                final_pdca_breakdown = {'P': 1, 'D': 0, 'C': 0, 'A': 0}
            elif level == 2:
                final_score = 2.0
                final_pdca_breakdown = {'P': 1, 'D': 1, 'C': 0, 'A': 0}
            elif level >= 3:
                final_score = llm_score_pdca_sum 

        status = "PASS" if is_passed else "FAIL"
        evidence_strength = min(max_evi_str_for_prompt, 10.0) if is_passed else 0.0 
        ai_confidence = "HIGH" if evidence_strength >= 8 else "MEDIUM" if evidence_strength >= 5.5 else "LOW"

        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Icon ‡∏ï‡∏≤‡∏°‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞
        icon = "üü¢" if is_passed else "üî¥"

        # ------------------- ‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏• Log -------------------
        self.logger.info(
            # üìå ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ: ‡πÄ‡∏û‡∏¥‡πà‡∏° icon ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡∏´‡∏ô‡πâ‡∏≤‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ PASS/FAIL
            f"  > Assessment {sub_id} L{level} completed ‚Üí {icon} {'PASS' if is_passed else 'FAIL'} "
            f"(Score: {final_score:.1f} | Evi Str: {evidence_strength:.1f} | Conf: {ai_confidence})"
        )

        # ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å: ‡∏ï‡πâ‡∏≠‡∏á return dict ‡∏ó‡∏µ‡πà‡∏°‡∏µ temp_map_for_level ‡∏î‡πâ‡∏ß‡∏¢!
        return {
            "sub_criteria_id": sub_id,
            "statement_id": statement_id,
            "level": level,
            "statement": statement_text,
            "pdca_phase": pdca_phase,
            "llm_score": llm_score_pdca_sum,
            "pdca_breakdown": final_pdca_breakdown,
            "is_passed": is_passed,           
            "status": status,
            "score": final_score,
            "llm_result_full": llm_result,
            "retrieval_duration_s": round(retrieval_duration, 2),
            "llm_duration_s": round(llm_duration, 2),
            "evidence_strength": round(evidence_strength, 1),
            "ai_confidence": ai_confidence,
            "max_relevant_score": highest_rerank_score,
            "max_relevant_source": evi_cap_data.get('max_score_source'),
            "is_evidence_strength_capped": evi_cap_data.get('is_capped'),
            "max_evidence_strength_used": max_evi_str_for_prompt,
            "temp_map_for_level": top_evidences,  # ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å! ‡∏Ç‡∏≤‡∏î‡∏≠‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ
        }