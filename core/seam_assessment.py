# -*- coding: utf-8 -*-
# core/seam_assessment.py

import sys
import json
import logging
import time
from datetime import datetime, date
import os
from typing import List, Dict, Any, Optional, Union, Tuple, Set, Final, Literal
from collections import defaultdict, OrderedDict
from dataclasses import dataclass, field
import multiprocessing 
from functools import partial
import pathlib, uuid
from copy import deepcopy
import tempfile
import shutil
import re
import hashlib
import unicodedata 
import random

from core.json_extractor import _robust_extract_json, _robust_extract_json_list

# -------------------- 1. PROTECTIVE IMPORTS --------------------
# ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á FileLock ‡πÅ‡∏•‡∏∞ Database ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏™‡πà‡∏ß‡∏ô‡∏≠‡∏∑‡πà‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏ä‡∏±‡∏ß‡∏£‡πå‡πÜ
try:
    from filelock import FileLock
except ImportError:
    class FileLock:
        def __init__(self, *args, **kwargs): pass
        def __enter__(self): return self
        def __exit__(self, *args): pass
    print("‚ö†Ô∏è WARNING: 'filelock' not installed.")

try:
    from database import init_db, db_update_task_status
    update_db_core = db_update_task_status
except ImportError:
    def init_db(): pass
    def update_db_core(*args, **kwargs): pass

# -------------------- 2. PATH SETUP --------------------
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

# -------------------- 3. CORE LOGIC & CONFIG IMPORTS --------------------
try:
    # --- Configs ---
    from config.global_vars import (
        MAX_LEVEL, EVIDENCE_DOC_TYPES, RERANK_THRESHOLD, MAX_EVI_STR_CAP,
        DEFAULT_LLM_MODEL_NAME, LLM_TEMPERATURE, MIN_RETRY_SCORE,
        MAX_PARALLEL_WORKERS, PDCA_PRIORITY_ORDER, TARGET_DEVICE,
        PDCA_PHASE_MAP, INITIAL_TOP_K, FINAL_K_RERANKED,
        MAX_CHUNKS_PER_FILE, MAX_CHUNKS_PER_BLOCK, MATURITY_LEVEL_GOALS,
        SEAM_ENABLER_FULL_NAME_TH, SEAM_ENABLER_FULL_NAME_EN,
        SCORING_MODE, MAX_CHUNKS_LOW, MAX_CHUNKS_HIGH,
        CRITICAL_CA_THRESHOLD, EVIDENCE_SELECTION_STRATEGY, EVIDENCE_CUMULATIVE_CAP,
        GLOBAL_EVIDENCE_INSTRUCTION, DEFAULT_ENABLER, PDCA_CONFIG_MAP, PDCA_PHASE_DESCRIPTIONS,
        SEAM_ENABLER_FULL_NAME_TH, ANALYSIS_FINAL_K, RETRIEVAL_RERANK_FLOOR, 
        RETRIEVAL_EARLY_EXIT_COUNT, RETRIEVAL_HIGH_RERANK_THRESHOLD,
        RETRIEVAL_EARLY_EXIT_SCORE_THRESHOLD, RETRIEVAL_RELEVANCE_THRESHOLD
    )
    

    # --- Utilities ---
    from core.llm_data_utils import ( 
        evaluate_with_llm, retrieve_context_with_filter, 
        action_plan_normalize_keys, evaluate_with_llm_low_level, 
        LOW_LEVEL_K, create_context_summary_llm, _fetch_llm_response,
        _check_and_handle_empty_context, set_mock_control_mode as set_llm_data_mock_mode
    )
    from core.vectorstore import VectorStoreManager, load_all_vectorstores
    from core.retry_policy import RetryPolicy, RetryResult  # <-- ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å
    from core.json_extractor import _robust_extract_json

    # --- Path Utils ---
    from utils.path_utils import (
        get_mapping_file_path, get_evidence_mapping_file_path, 
        get_contextual_rules_file_path, get_assessment_export_file_path,
        get_export_dir, get_rubric_file_path, _n, get_doc_type_collection_key,
        get_tenant_info_file_path
    )

    # --- Prompts ---
    try:
        from core.seam_prompts import (
            ATOMIC_ACTION_PROMPT, MASTER_ROADMAP_PROMPT,
            SYSTEM_ATOMIC_ACTION_PROMPT, SYSTEM_MASTER_ROADMAP_PROMPT
        )
    except ImportError:
        ATOMIC_ACTION_PROMPT = "Recommendation: {coaching_insight} Level: {level}"
        MASTER_ROADMAP_PROMPT = "Roadmap for {sub_criteria_name}: {aggregated_insights}"
        SYSTEM_ATOMIC_ACTION_PROMPT = "Assistant mode."
        SYSTEM_MASTER_ROADMAP_PROMPT = "Strategy mode."

except ImportError as e:
    # üö® EMERGENCY FALLBACK: ‡∏î‡πà‡∏≤‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ üö®
    print(f"‚ö†Ô∏è EMERGENCY: Core Module missing, initializing safety fallbacks: {e}")
    
    # Constants
    MAX_LEVEL = 5
    EVIDENCE_DOC_TYPES = "evidence"
    LOW_LEVEL_K = 5  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ï‡∏±‡∏ß‡∏ô‡∏µ‡πâ
    
    # [FIX] Class/Policy
    class RetryResult:
        def __init__(self, data=None): self.data = data or {}
        def get(self, k, d=None): return self.data.get(k, d)
    class RetryPolicy:
        def __init__(self, *args, **kwargs): pass
        def execute(self, func, *args, **kwargs): return RetryResult(func(*args, **kwargs))

    # [FIX] Path Functions
    def _n(s): return str(s).lower().strip()
    def get_rubric_file_path(tenant, enabler, **kwargs): 
        return f"data_store/{_n(tenant)}/config/{_n(tenant)}_{_n(enabler)}_rubric.json"
    def get_contextual_rules_file_path(tenant, enabler, **kwargs):
        return f"data_store/{_n(tenant)}/config/{_n(tenant)}_{_n(enabler)}_contextual_rules.json"
    def get_evidence_mapping_file_path(tenant, year, enabler, **kwargs):
        return f"data_store/{_n(tenant)}/mapping/{year}/{_n(tenant)}_{year}_{_n(enabler)}_evidence_mapping.json"
    def get_mapping_file_path(doc_type, tenant, year=None, enabler=None, **kwargs):
        return f"data_store/{_n(tenant)}/mapping/{year}/{_n(tenant)}_{year}_{_n(enabler)}_doc_id_mapping.json"
    def get_assessment_export_file_path(*args, **kwargs): return "exports/temp_report.json"
    def get_export_dir(*args, **kwargs): return "exports"

    # [FIX] Logic Functions (‡∏î‡πà‡∏≤‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏ö‡∏ñ‡∏≤‡∏°‡∏´‡∏≤)
    def evaluate_with_llm_low_level(*args, **kwargs): 
        return {"score": 0.0, "reason": "Fallback mode active", "is_passed": False}
    
    def evaluate_with_llm(*args, **kwargs): 
        return {"score": 0.0, "reason": "Fallback mode active", "is_passed": False}

    def retrieve_context_with_filter(*args, **kwargs): 
        return {"top_evidences": [], "aggregated_context": ""}


    # Placeholders
    def _fetch_llm_response(*args, **kwargs): return "{}"
    def _robust_extract_json(t): return {}
    def set_llm_data_mock_mode(m): pass
    def action_plan_normalize_keys(d): return d
    def create_context_summary_llm(*args, **kwargs): return {"summary": "N/A", "coaching": "N/A"}
    def _check_and_handle_empty_context(*args, **kwargs): return None, False

    ATOMIC_ACTION_PROMPT = "Level {level}: {coaching_insight}"
    MASTER_ROADMAP_PROMPT = "Roadmap: {aggregated_insights}"
    SYSTEM_ATOMIC_ACTION_PROMPT = "Assistant"
    SYSTEM_MASTER_ROADMAP_PROMPT = "Strategist"

# -------------------- 5. LOGGER SETUP --------------------
logger = logging.getLogger(__name__)
if not logger.handlers:
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

def get_enabler_full_name(enabler: str, lang: str = "th") -> str:
    """
    ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡πá‡∏°‡∏Ç‡∏≠‡∏á Enabler ‡∏ï‡∏≤‡∏°‡∏£‡∏´‡∏±‡∏™ (‡πÄ‡∏ä‡πà‡∏ô "KM" ‚Üí "‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ")
    
    Args:
        enabler_code (str): ‡∏£‡∏´‡∏±‡∏™ enabler ‡πÄ‡∏ä‡πà‡∏ô "KM", "CG", "SP"
        lang (str): "th" ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢, "en" ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏© (default: "th")
    
    Returns:
        str: ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡πá‡∏° ‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏∑‡∏ô‡∏£‡∏´‡∏±‡∏™‡πÄ‡∏î‡∏¥‡∏°‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏ö
    
    Example:
        get_enabler_full_name("KM") ‚Üí "‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ"
        get_enabler_full_name("CG", "en") ‚Üí "Corporate Governance"
    """
    code = str(enabler).upper().strip()
    if lang.lower() == "th":
        return SEAM_ENABLER_FULL_NAME_TH.get(code, code)
    return SEAM_ENABLER_FULL_NAME_EN.get(code, code)


def get_pdca_goal_for_level(level: int) -> str:
    """
    ‡∏î‡∏∂‡∏á‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á Maturity Level ‡∏ô‡∏±‡πâ‡∏ô ‡πÜ
    
    Args:
        level (int): ‡∏£‡∏∞‡∏î‡∏±‡∏ö 1-5
    
    Returns:
        str: ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ ‡∏´‡∏£‡∏∑‡∏≠ "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢" ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏ö
    
    Example:
        get_pdca_goal_for_level(5) ‚Üí "‡πÄ‡∏ô‡πâ‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏±‡πà‡∏á‡∏¢‡∏∑‡∏ô ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÄ‡∏ä‡∏¥‡∏á‡∏£‡∏∏‡∏Å...‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏ô‡πÅ‡∏ö‡∏ö (Role Model)"
    """
    return MATURITY_LEVEL_GOALS.get(int(level), "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢")
    
def _static_worker_process(worker_input_tuple: Tuple) -> Any:
    """
    [ULTIMATE WORKER v2026.1.23] Isolated Execution with Evidence Streaming
    ---------------------------------------------------------------------
    - ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°‡πÉ‡∏´‡∏°‡πà‡πÅ‡∏¢‡∏Å‡∏Ç‡∏≤‡∏î‡∏à‡∏≤‡∏Å‡∏Å‡∏±‡∏ô (Zero Memory Leak)
    - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô Evidence Map ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏¢‡∏±‡∏á Main Process ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏≤‡∏¢
    - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏∞‡∏ö‡∏ö Error Handling ‡πÅ‡∏ö‡∏ö‡∏Å‡πâ‡∏≠‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Object) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏û‡∏±‡∏á
    """
    # 1. üìÇ PATH & ENVIRONMENT SETUP
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
    if project_root not in sys.path:
        sys.path.append(project_root)
        
    worker_logger = logging.getLogger(f"Worker_{os.getpid()}")

    # 2. üì¶ UNPACKING ARGS
    try:
        (
            sub_criteria_data, enabler, target_level, mock_mode, 
            evidence_map_path, model_name, temperature,
            min_retry_score, max_retrieval_attempts, document_map, 
            action_plan_model, year, tenant
        ) = worker_input_tuple
        
        sub_id = sub_criteria_data.get('sub_id', 'UNKNOWN')
    except Exception as e:
        return {"error": f"Worker unpacking failed: {str(e)}", "status": "failure"}, {}

    # 3. üèóÔ∏è RECONSTRUCT ISOLATED ENGINE
    try:
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Config ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Å‡∏¥‡∏à
        worker_config = AssessmentConfig(
            enabler=enabler,
            tenant=tenant,
            year=int(year) if year else None,     
            target_level=target_level,
            mock_mode=mock_mode,
            model_name=model_name, 
            temperature=temperature,
            min_retry_score=min_retry_score,            
            max_retrieval_attempts=max_retrieval_attempts 
        )

        # ‡∏Ñ‡∏∑‡∏ô‡∏ä‡∏µ‡∏û Engine (‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤ Class SEAMPDCAEngine ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ö document_map)
        worker_instance = SEAMPDCAEngine(
            config=worker_config, 
            evidence_map_path=evidence_map_path, 
            llm_instance=None,              
            vectorstore_manager=None,       
            logger_instance=worker_logger,
            document_map=document_map,      
            ActionPlanActions=action_plan_model
        )
    except Exception as e:
        worker_logger.error(f"‚ùå Worker initialization failed: {e}")
        return {"sub_id": sub_id, "error": f"Init Error: {str(e)}"}, {}

    # 4. ‚ö° EXECUTE & STREAM BACK RESULTS
    try:
        # üéØ ‡∏à‡∏∏‡∏î‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: _run_sub_criteria_assessment_worker ‡∏ï‡πâ‡∏≠‡∏á‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ (result, evidence_mem)
        # evidence_mem ‡∏Ñ‡∏∑‡∏≠ dict ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö chunks/docs ‡∏ó‡∏µ‡πà AI ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ä‡πâ‡∏à‡∏£‡∏¥‡∏á
        result, worker_evidence_mem = worker_instance._run_sub_criteria_assessment_worker(sub_criteria_data)
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ result ‡∏°‡∏µ sub_id ‡∏ï‡∏¥‡∏î‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Merge ‡∏ñ‡∏π‡∏Å‡∏ï‡∏±‡∏ß
        if isinstance(result, dict) and 'sub_id' not in result:
            result['sub_id'] = sub_id

        return result, worker_evidence_mem
        
    except Exception as e:
        worker_logger.error(f"‚ùå Execution error for {sub_id}: {str(e)}")
        # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÅ‡∏ö‡∏ö Fallback ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Main Process ‡πÑ‡∏°‡πà‡∏Ñ‡πâ‡∏≤‡∏á
        return {
            "sub_id": sub_id,
            "error": str(e),
            "status": "failed",
            "is_passed": False,
            "score": 0.0,
            "reason": f"Worker Exception: {str(e)}"
        }, {}
        
# =================================================================
# Configuration Class
# =================================================================
@dataclass
class AssessmentConfig:
    """Configuration for the SEAM PDCA Assessment Run."""
    
    # ------------------ 1. Assessment Context ------------------
    enabler: str = None
    tenant: str = None
    year: int = None  # üëà ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å DEFAULT_YEAR ‡πÄ‡∏õ‡πá‡∏ô None
    target_level: int = MAX_LEVEL
    mock_mode: str = "none" # 'none', 'random', 'control'
    force_sequential: bool = field(default=False) # Flag ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏£‡∏±‡∏ô‡πÅ‡∏ö‡∏ö Sequential

    # ------------------ 2. LLM Configuration (Configurable) ------------------
    # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Default ‡∏à‡∏≤‡∏Å global_vars.py
    model_name: str = DEFAULT_LLM_MODEL_NAME 
    temperature: float = LLM_TEMPERATURE

    # ------------------ 3. Adaptive RAG Retrieval Configuration ------------------
    # üü¢ NEW: ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô Rerank ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Adaptive Loop (MIN_RETRY_SCORE)
    # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Default 0.65 ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏û‡∏ö‡πÉ‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î Logic
    min_retry_score: float = MIN_RETRY_SCORE
    # üü¢ NEW: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≠‡∏ö‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á Adaptive RAG Loop (MAX_RETRIEVAL_ATTEMPTS)
    # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Default 3
    max_retrieval_attempts: int = 3
    
    # ------------------ 4. Export Configuration ------------------
    export_output: bool = field(default=False) # Flag ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏¥‡∏î/‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£ Export ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    export_path: str = "" # Path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå Export (‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å)


# =================================================================
# SEAM Assessment Engine (PDCA Focused) - Full Revise v2026
# =================================================================
class SEAMPDCAEngine:
    
    def __init__(
        self, 
        config: AssessmentConfig,
        llm_instance: Any = None, 
        logger_instance: logging.Logger = None,
        rag_retriever_instance: Any = None,
        doc_type: str = None, 
        vectorstore_manager: Optional['VectorStoreManager'] = None,
        evidence_map_path: Optional[str] = None,
        document_map: Optional[Dict[str, str]] = None,
        is_parallel_all_mode: bool = False,
        sub_id: str = 'all',
        record_id: Optional[str] = None,
        **kwargs  
    ):
        """
        [ULTIMATE REVISE v2026.3.1] 
        - FIXED: flattened_rubric logic corruption (No more overwrite)
        - FIXED: Robust doc_type comparison (is_evidence_mode)
        - FIXED: Safe VSM/LLM initialization order
        """
        # -------------------------------------------------------
        # 1. Basic Config & Logger Setup
        # -------------------------------------------------------
        self.config = config
        self.doc_type = doc_type or getattr(config, 'doc_type', EVIDENCE_DOC_TYPES)
        
        # [REVISED] Robust Comparison Pattern ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥
        self.is_evidence_mode = str(self.doc_type).strip().lower() in (
            t.lower() for t in (EVIDENCE_DOC_TYPES if isinstance(EVIDENCE_DOC_TYPES, (list, tuple)) else [EVIDENCE_DOC_TYPES])
        )
        
        log_year = config.year if self.is_evidence_mode else "general"

        if logger_instance is not None:
            self.logger = logger_instance
        else:
            self.logger = logging.getLogger(__name__).getChild(
                f"Engine|{config.enabler}|{config.tenant}/{log_year}"
            )

        self.logger.info(f"üöÄ Initializing SEAMPDCAEngine: {config.enabler} ({config.tenant}/{log_year})")

        # -------------------------------------------------------
        # 2. Core Configuration & Sanity Check
        # -------------------------------------------------------
        if not self.config.enabler or not self.config.tenant:
            self.logger.critical("‚ùå Mandatory Config Missing: enabler and tenant must be provided!")
            raise ValueError("Enabler and Tenant are required for SEAMPDCAEngine.")

        self.enabler = config.enabler
        self.tenant_id = config.tenant
        self.year = config.year
        self.target_level = config.target_level
        self.sub_id = sub_id
        self.record_id = record_id
        
        # State Management
        self.is_parallel_all_mode = is_parallel_all_mode
        self.is_sequential = getattr(config, 'force_sequential', True)
        
        # [REVISED] results vs assessment_results_map
        # results ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠ legacy compatibility, assessment_results_map ‡∏Ñ‡∏∑‡∏≠ source of truth
        self.results = {} 
        self.assessment_results_map = {} 

        # -------------------------------------------------------
        # 3. System Warm-up
        # -------------------------------------------------------
        try:
            init_db()
        except Exception as e:
            self.logger.error(f"‚ö†Ô∏è DB Init Warning: {e}")

        # -------------------------------------------------------
        # 4. Data Loading (Rubric & Rules)
        # -------------------------------------------------------
        self.rubric = self._load_rubric()

        # [REVISED] Flatten Rubric - ‡∏ó‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà Overwrite ‡∏ã‡πâ‡∏≥‡∏ï‡∏≠‡∏ô‡∏ó‡πâ‡∏≤‡∏¢
        try:
            self.flattened_rubric = self._flatten_rubric_to_statements(self.rubric)
            self.logger.info(f"‚úÖ Rubric Meta-Data Flattened: {len(self.flattened_rubric)} levels ready.")
        except Exception as e:
            self.logger.error(f"‚ö†Ô∏è Rubric Flattening Failed: {e}")
            self.flattened_rubric = []
        
        self.contextual_rules_map = self._load_contextual_rules_map()
        self.retry_policy = RetryPolicy(max_attempts=3, base_delay=2.0)

        # -------------------------------------------------------
        # 5. Mapping & Evidence Setup
        # -------------------------------------------------------
        self.evidence_map = {}
        if self.is_evidence_mode:
            self.evidence_map_path = evidence_map_path or get_evidence_mapping_file_path(
                tenant=self.config.tenant, year=self.config.year, enabler=self.enabler
            )
            self.evidence_map = self._load_evidence_map()

        # Document Mapping
        loaded_map = document_map or {}
        if not loaded_map:
            mapping_path = get_mapping_file_path(
                self.doc_type, 
                tenant=self.config.tenant, 
                year=self.config.year if self.is_evidence_mode else None,
                enabler=self.enabler if self.is_evidence_mode else None
            )
            if os.path.exists(mapping_path):
                try:
                    with open(mapping_path, 'r', encoding='utf-8') as f:
                        raw_data = json.load(f)
                    loaded_map = {k: v.get("file_name", k) for k, v in raw_data.items()}
                except Exception as e:
                    self.logger.error(f"‚ùå Error parsing mapping file: {e}")

        self.doc_id_to_filename_map = loaded_map
        self.document_map = loaded_map

        # -------------------------------------------------------
        # 6. Lazy Engine Initialization (VSM & LLM)
        # -------------------------------------------------------
        # [REVISED] Safe initialization order
        if llm_instance is None: 
            self._initialize_llm_if_none()
        else:
            self.llm = llm_instance

        if vectorstore_manager is None: 
            # ‡∏™‡πà‡∏á LLM ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡∏ñ‡πâ‡∏≤ VSM ‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ embedding ‡∏à‡∏≤‡∏Å LLM
            self._initialize_vsm_if_none() 
        else:
            self.vectorstore_manager = vectorstore_manager

        if self.vectorstore_manager:
            try:
                self.vectorstore_manager._load_doc_id_mapping()
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è VSM mapping not loaded: {e}")

       
        # -------------------------------------------------------
        # 7. Function Registry & Final States (CLEAN)
        # -------------------------------------------------------
        # self.standard_audit_agent = evaluate_with_llm
        # Register agents (‡∏ï‡∏±‡∏ß‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á)
        self.standard_audit_agent = evaluate_with_llm              # L3‚ÄìL5
        self.foundation_coaching_agent = evaluate_with_llm_low_level  # L1‚ÄìL2

        # Entry
        self.assessment_router = self.evaluate_pdca

        self.rag_retriever = retrieve_context_with_filter

        # State Initialization
        self.final_subcriteria_results = []
        self.total_stats = {}
        self.raw_llm_results = []
        self.level_details_map = {} 
        self.previous_levels_evidence = [] 
        self.level_evidence_cache = {}
        self._cumulative_rules_cache = {}

        # [CRITICAL] ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏™‡πà self.flattened_rubric = [] ‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≤‡∏î!

        self.logger.info(f"‚úÖ Engine Initialized: Ready for Assessment (Sub-ID: {self.sub_id})")

    # =================================================================
    # DB Proxy Methods (Enhanced v2026)
    # =================================================================
    def db_update_task_status(self, message: str, progress: Optional[int] = None, status: str = "RUNNING"):
        """
        Enhanced Wrapper ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞
        - ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏™‡πà‡∏á record_id ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á (‡πÉ‡∏ä‡πâ self.current_record_id ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥)
        - ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏™‡πà‡∏á progress ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÄ‡∏â‡∏û‡∏≤‡∏∞ message (‡∏Ñ‡∏á‡∏Ñ‡πà‡∏≤ % ‡πÄ‡∏î‡∏¥‡∏°‡πÑ‡∏ß‡πâ)
        """
        # 1. ‡∏î‡∏∂‡∏á record_id ‡∏à‡∏≤‡∏Å instance ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≤‡∏°
        rid = getattr(self, 'current_record_id', None) or getattr(self, 'record_id', None)
        if not rid: 
            return

        try:
            # 2. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï
            # ‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà progress ‡πÄ‡∏õ‡πá‡∏ô None ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å memory ‡∏´‡∏£‡∏∑‡∏≠ database 
            # ‡πÅ‡∏ï‡πà‡πÇ‡∏î‡∏¢‡∏õ‡∏Å‡∏ï‡∏¥ database.db_update_task_status ‡∏Ñ‡∏ß‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≤‡∏°‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô None ‡πÉ‡∏´‡πâ‡πÄ‡∏≠
            
            # 3. ‡∏™‡πà‡∏á‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï
            update_db_core(
                record_id=rid, 
                progress=progress, 
                message=message, 
                status=status
            )
            
            self.logger.debug(f"[DB-PROGRESS] {rid}: {progress if progress is not None else 'KEEP'}% - {message}")
            
        except Exception as e:
            self.logger.error(f"‚ùå DB Update Error for {rid}: {e}")

    def _initialize_llm_if_none(self):
        """Initializes LLM instance if self.llm is None."""
        if self.llm is None:
            self.logger.warning("‚ö†Ô∏è Initializing LLM: model=%s, temperature=%s", 
                                self.config.model_name, self.config.temperature)
            try:
                # üü¢ FIX: Import ‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ create_llm_instance
                from models.llm import create_llm_instance 
                self.llm = create_llm_instance( 
                    model_name=self.config.model_name,
                    temperature=self.config.temperature
                )
                self.logger.info("‚úÖ LLM Instance created successfully: %s (Temp: %s)", 
                                 self.config.model_name, self.config.temperature)
            except Exception as e:
                self.logger.error(f"FATAL: Could not initialize LLM: {e}")
                raise

    def _initialize_vsm_if_none(self):
        """
        Initializes VectorStoreManager with Smart Year Selection.
        - If Evidence: Priority 1 = Specific Year, Priority 2 = Root Fallback.
        - If Document: Priority 1 = Root (General), Priority 2 = Specific Year Fallback.
        """
        if self.vectorstore_manager is not None:
            return

        # 1. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ç‡∏≠‡∏á DocType
        clean_dt = str(self.doc_type or getattr(self.config, 'doc_type', EVIDENCE_DOC_TYPES)).strip().lower()
        is_evidence = (clean_dt == EVIDENCE_DOC_TYPES.lower())
        
        self.logger.info(f"üöÄ Loading vectorstore(s) for DocType: '{clean_dt}' (Mode: {'Evidence' if is_evidence else 'General'})")

        try:
            target_enabler = str(self.enabler).lower() if self.enabler else None
            
            # üéØ [SMART SELECTION] ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å
            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô evidence ‡πÉ‡∏´‡πâ‡∏´‡∏≤‡∏ï‡∏≤‡∏°‡∏õ‡∏µ (2568) ‡∏Å‡πà‡∏≠‡∏ô ‡πÅ‡∏ï‡πà‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô document ‡πÉ‡∏´‡πâ‡∏´‡∏≤‡∏ó‡∏µ‡πà Root (None) ‡∏Å‡πà‡∏≠‡∏ô
            primary_year = self.config.year if is_evidence else None
            secondary_year = None if is_evidence else self.config.year

            # 2. First Attempt: ‡πÇ‡∏´‡∏•‡∏î‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
            self.vectorstore_manager = load_all_vectorstores(
                doc_types=[clean_dt], 
                enabler_filter=target_enabler, 
                tenant=self.config.tenant, 
                year=primary_year       
            )
            
            def count_retrievers(vsm):
                if vsm and hasattr(vsm, '_multi_doc_retriever') and vsm._multi_doc_retriever:
                    return len(vsm._multi_doc_retriever._all_retrievers)
                return 0

            len_retrievers = count_retrievers(self.vectorstore_manager)
            
            # 3. Second Attempt (Fallback): ‡∏ñ‡πâ‡∏≤‡∏´‡∏≤‡πÅ‡∏ö‡∏ö‡πÅ‡∏£‡∏Å‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ ‡πÉ‡∏´‡πâ‡∏™‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏´‡∏≤‡∏≠‡∏µ‡∏Å‡πÅ‡∏ö‡∏ö
            if len_retrievers == 0:
                lookup_label = f"year {secondary_year}" if secondary_year else "tenant root"
                self.logger.info(f"‚ö†Ô∏è No collections found in primary path, searching in {lookup_label}...")
                
                self.vectorstore_manager = load_all_vectorstores(
                    doc_types=[clean_dt], 
                    enabler_filter=target_enabler, 
                    tenant=self.config.tenant, 
                    year=secondary_year       
                )
                len_retrievers = count_retrievers(self.vectorstore_manager)

            # 4. Post-Load Process
            if self.vectorstore_manager and len_retrievers > 0:
                self.vectorstore_manager._load_doc_id_mapping() 
                self.logger.info(f"‚úÖ MultiDocRetriever loaded with {len_retrievers} collections.") 
            else:
                # 5. Final Error Handling
                expected_p = f"data_store/{self.config.tenant}/vectorstore/{primary_year or 'root'}"
                self.logger.error(f"‚ùå FATAL: 0 vector store collections loaded. Please check folder: {expected_p}")
                raise ValueError(f"No vector collections found for '{target_enabler}' in {self.config.tenant}")

        except Exception as e:
            self.logger.error(f"‚ùå FATAL: Could not initialize VectorStoreManager: {str(e)}")
            raise

    # -------------------- Contextual Rules Handlers (FIXED) --------------------
    def _load_contextual_rules_map(self) -> Dict[str, Any]:
        """
        [FINAL REVISED v2026.5] ‡πÇ‡∏´‡∏•‡∏î Contextual Rules ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
        - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö multi-tenant ‡πÅ‡∏•‡∏∞ multi-enabler ‡πÄ‡∏ï‡πá‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö
        - ‡∏°‡∏µ fallback ‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
        - ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Maturity (L1-L5) ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏Ç‡πâ‡∏°‡∏á‡∏ß‡∏î
        - Logging ‡∏ó‡∏µ‡πà‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏ä‡πà‡∏ß‡∏¢ debug
        - ‡πÑ‡∏°‡πà raise exception (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ engine ‡∏•‡πâ‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏£‡∏∞‡∏ö‡∏ö)
        """
        # 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á path ‡∏ï‡∏≤‡∏° tenant + enabler
        try:
            filepath = get_contextual_rules_file_path(
                tenant=self.config.tenant,
                enabler=self.enabler
            )
            self.logger.debug(f"üîç Attempting to load contextual rules from: {filepath}")
        except Exception as e:
            self.logger.error(f"‚ùå FATAL: Failed to generate rules file path: {e}")
            return {"_enabler_defaults": {}}

        # 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if not os.path.exists(filepath):
            self.logger.warning(
                f"‚ö†Ô∏è Contextual Rules file not found: {filepath}\n"
                f"   ‚Üí Using only global defaults (if available from fallback)."
            )
            return {"_enabler_defaults": {}}

        # 3. ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞ parse JSON
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
        except json.JSONDecodeError as e:
            self.logger.error(f"‚ùå JSON Decode Error in {filepath}: {e} (line {e.lineno}, col {e.colno})")
            return {"_enabler_defaults": {}}
        except Exception as e:
            self.logger.error(f"‚ùå Unexpected error reading {filepath}: {e}")
            return {"_enabler_defaults": {}}

        # 4. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        if not isinstance(data, dict):
            self.logger.error(f"‚ùå Invalid rules format in {filepath}: Expected dict, got {type(data)}")
            return {"_enabler_defaults": {}}

        # 5. ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô criteria ‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Maturity Structure
        sub_criteria_keys = [k for k in data.keys() if not k.startswith("_")]
        num_criteria = len(sub_criteria_keys)

        if num_criteria == 0:
            self.logger.warning(f"‚ö†Ô∏è No sub-criteria found in {filepath} (only defaults).")
        else:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ Maturity Levels (L1, L2, ...) ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            sample_sub = sub_criteria_keys[0]
            sub_data = data[sample_sub]
            level_keys = [k for k in sub_data.keys() if k.startswith("L") and len(k) >= 2]
            
            if level_keys:
                detected_levels = sorted(level_keys)
                self.logger.info(
                    f"‚úÖ Maturity-Based Rules loaded successfully!\n"
                    f"   File: {filepath}\n"
                    f"   Criteria: {num_criteria} (e.g., {sample_sub})\n"
                    f"   Levels detected: {', '.join(detected_levels)}"
                )
            else:
                self.logger.warning(
                    f"‚ö†Ô∏è Rules for '{sample_sub}' in {filepath} do not contain Maturity Levels (L1-L5).\n"
                    f"   Falling back to flat structure or defaults."
                )

        # 6. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö _enabler_defaults
        if "_enabler_defaults" in data:
            defaults = data["_enabler_defaults"]
            if isinstance(defaults, dict) and any(k.endswith("_keywords") for k in defaults.keys()):
                self.logger.info("‚úÖ Global PDCA Keywords (_enabler_defaults) loaded.")
            else:
                self.logger.warning("‚ö†Ô∏è _enabler_defaults exists but has invalid structure.")
        else:
            self.logger.info("‚ÑπÔ∏è No _enabler_defaults section found. Using empty defaults.")

        # 7. ‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô data ‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
        self.logger.info(f"‚úÖ Contextual Rules fully loaded from {filepath} ({num_criteria} criteria).")
        return data
    
    def get_rule_content(self, sub_id: str, level: int, key_type: str):
        """
        ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏à‡∏≤‡∏Å Contextual Rules ‡πÅ‡∏ö‡∏ö‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ä‡∏±‡πâ‡∏ô
        Priority: Specific Level > Sub-ID Root > Global Defaults > Fallback
        """
        rule = self.contextual_rules_map.get(sub_id, {})
        level_key = f"L{level}"

        # 1. Specific Level (e.g., 1.2 -> L1 -> query_synonyms)
        if key_type in rule.get(level_key, {}):
            return rule[level_key][key_type]

        # 2. Sub-ID Root (e.g., 1.2 -> query_synonyms)
        if key_type in rule:
            return rule[key_type]

        # 3. Global Defaults (e.g., _enabler_defaults -> plan_keywords)
        defaults = self.contextual_rules_map.get("_enabler_defaults", {})
        if key_type in defaults:
            return defaults[key_type]

        # 4. Fallback (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Error ‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ Key)
        fallbacks = {
            "require_phase": ["P", "D"], # Default ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
            "must_include_keywords": [],
            "plan_keywords": [],
            "do_keywords": [],
            "check_keywords": [],
            "act_keywords": [],
            "query_synonyms": ""
        }
        return fallbacks.get(key_type, "")

    def get_cumulative_rules_cached(self, sub_id: str, level: int) -> Dict[str, Any]:
        """
        Return cumulative rules with per-engine caching.
        Cache key: (sub_id, level)
        """
        key = (sub_id, level)

        if key not in self._cumulative_rules_cache:
            self._cumulative_rules_cache[key] = self.get_cumulative_rules(
                sub_id=sub_id,
                current_level=level
            )

        return self._cumulative_rules_cache[key]


    def get_cumulative_rules(self, sub_id: str, current_level: int) -> Dict[str, Any]:
        """
        [FINAL REVISED v2026.1.20] - SMART ACCUMULATION & ROBUST MAPPING
        ------------------------------------------------------------------
        - ‡∏™‡∏∞‡∏™‡∏° Keywords (PDCA) ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏à‡∏≤‡∏Å L1 ‡∏à‡∏ô‡∏ñ‡∏∂‡∏á‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
        - **Robust Synonym Split**: ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥‡∏î‡πâ‡∏ß‡∏¢ ‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á, ‡∏Ñ‡∏≠‡∏°‡∏°‡πà‡∏≤, ‡πÅ‡∏•‡∏∞‡πÄ‡∏ã‡∏°‡∏¥‡πÇ‡∏Ñ‡∏•‡∏≠‡∏ô
        - **Auto-Fallback Phases**: ‡πÄ‡∏ï‡∏¥‡∏° Required Phases ‡πÉ‡∏´‡πâ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡∏´‡∏≤‡∏Å Config ‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏
        - **Case-Insensitive**: ‡∏õ‡∏£‡∏±‡∏ö Keywords ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏û‡∏¥‡∏°‡∏û‡πå‡πÄ‡∏•‡πá‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Match
        """
        defaults = self.contextual_rules_map.get('_enabler_defaults', {})
        sub_rules = self.contextual_rules_map.get(sub_id, {})
        
        # 1. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° OrderedDict ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÅ‡∏•‡∏∞‡∏Å‡∏≥‡∏à‡∏±‡∏î‡∏Ñ‡∏≥‡∏ã‡πâ‡∏≥
        cum_keywords = {
            "plan": OrderedDict((k.lower(), None) for k in defaults.get('plan_keywords', [])),
            "do":   OrderedDict((k.lower(), None) for k in defaults.get('do_keywords', [])),
            "check": OrderedDict((k.lower(), None) for k in defaults.get('check_keywords', [])),
            "act":  OrderedDict((k.lower(), None) for k in defaults.get('act_keywords', []))
        }
        
        cum_must_include = OrderedDict()
        required_phases = set()
        level_specific_instructions = {}
        source_levels = []

        # 2. ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏™‡∏∞‡∏™‡∏°‡∏Å‡∏é‡∏à‡∏≤‡∏Å L1 ‡∏à‡∏ô‡∏ñ‡∏∂‡∏á‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
        for lv in range(1, current_level + 1):
            lv_key = f"L{lv}"
            level_rule = sub_rules.get(lv_key, {})
            
            if not level_rule:
                continue
            
            source_levels.append(lv)

            # A) ‡∏î‡∏∂‡∏á query_synonyms ‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô must_include (Robust Split)
            synonyms_str = level_rule.get('query_synonyms', "")
            if synonyms_str:
                # ‡πÉ‡∏ä‡πâ Regex ‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á "‡∏Ñ‡∏≥1 ‡∏Ñ‡∏≥2" ‡∏´‡∏£‡∏∑‡∏≠ "‡∏Ñ‡∏≥1,‡∏Ñ‡∏≥2" ‡∏´‡∏£‡∏∑‡∏≠ "‡∏Ñ‡∏≥1;‡∏Ñ‡∏≥2"
                words = re.split(r'[,\s;|]+', synonyms_str)
                for word in words:
                    clean_word = word.strip().lower()
                    if clean_word:
                        cum_must_include[clean_word] = None

            # B) ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï PDCA Keywords (‡∏™‡∏∞‡∏™‡∏°‡∏ï‡πà‡∏≠‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤)
            for phase, key_name in [("plan", "plan_keywords"), ("do", "do_keywords"),
                                ("check", "check_keywords"), ("act", "act_keywords")]:
                new_kws = level_rule.get(key_name, [])
                for kw in new_kws:
                    cum_keywords[phase][kw.lower()] = None
            
            # C) ‡∏™‡∏∞‡∏™‡∏° Required Phases
            if 'require_phase' in level_rule:
                # ‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ‡∏ó‡∏±‡πâ‡∏á List ["P", "D"] ‡∏´‡∏£‡∏∑‡∏≠ String "P,D"
                phases = level_rule['require_phase']
                if isinstance(phases, str):
                    phases = re.split(r'[,\s]+', phases)
                required_phases.update([p.upper() for p in phases if p])
            
            # D) ‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ (Specific Instructions)
            specific = level_rule.get('specific_contextual_rule')
            if specific:
                level_specific_instructions[lv] = specific.strip()

        # 3. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å (Finalize & Fallback)
        result_keywords = {phase: list(cum_keywords[phase].keys()) for phase in cum_keywords}
        
        # Smart Fallback ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Required Phases (‡∏ñ‡πâ‡∏≤‡πÉ‡∏ô Config ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏∞‡∏ö‡∏∏‡πÄ‡∏•‡∏¢)
        final_phases = sorted(list(required_phases))
        if not final_phases:
            if current_level <= 3: final_phases = ["P", "D"]
            elif current_level == 4: final_phases = ["P", "D", "C"]
            else: final_phases = ["P", "D", "C", "A"]

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Instructions String ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Prompt
        instructions_lines = [f"‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {sub_id} ‡∏£‡∏∞‡∏î‡∏±‡∏ö Maturity L{current_level}:"]
        for lv in sorted(level_specific_instructions.keys()):
            icon = "üéØ" if lv == current_level else "‚úÖ"
            instructions_lines.append(f"{icon} [Level {lv}]: {level_specific_instructions[lv]}")

        # 4. Logging & Return
        self.logger.info(
            f"üöÄ [RULE_CUMULATIVE] {sub_id} L{current_level} | "
            f"Must-Include: {len(cum_must_include)} words | "
            f"Phases: {final_phases}"
        )

        return {
            "plan_keywords": result_keywords["plan"],
            "do_keywords": result_keywords["do"],
            "check_keywords": result_keywords["check"],
            "act_keywords": result_keywords["act"],
            "required_phases": final_phases,
            "must_include_keywords": list(cum_must_include.keys()),
            "level_specific_instructions": level_specific_instructions,
            "all_instructions": "\n".join(instructions_lines),
            "source_summary": f"Accumulated from levels: {source_levels}"
        }

    def _check_contextual_rule_condition(
        self, 
        condition: Dict[str, Any], 
        sub_id: str, 
        level: int, 
        top_evidences: List[Dict[str, Any]]
    ) -> bool:
        """
        [SIMPLIFIED v2026] ‡πÅ‡∏Ñ‡πà log ‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô continuity + min evidence
        ‡πÑ‡∏°‡πà block ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô (return True ‡πÄ‡∏™‡∏°‡∏≠)
        """
        if level > 1:
            prev_level = level - 1
            is_prev_passed = False
            if hasattr(self, 'level_details_map') and str(prev_level) in self.level_details_map:
                is_prev_passed = self.level_details_map[str(prev_level)].get('is_passed', False)
            
            if not is_prev_passed:
                self.logger.warning(f"‚ö†Ô∏è [GAP DETECTED] L{prev_level} not passed for {sub_id} L{level} - may affect validity")

        min_docs = condition.get('min_evidences', 1)
        if len(top_evidences) < min_docs:
            self.logger.warning(f"‚ö†Ô∏è [LOW EVIDENCE] {sub_id} L{level}: {len(top_evidences)} docs (required: {min_docs})")

        return True  # ‡πÑ‡∏°‡πà block

    def post_process_llm_result(
        self,
        llm_output: Any,
        level: int,
        sub_id: str = None,
        contextual_config: Optional[Dict] = None,
        top_evidences: Optional[List[Dict[str, Any]]] = None
    ) -> Dict[str, Any]:
        """
        [ULTIMATE REVISED v2026.01.31] Post-process LLM output for SE-AM Assessment
        ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢: ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÑ‡∏°‡πà‡∏ï‡∏Å‡∏ï‡πà‡∏≥‡πÄ‡∏Å‡∏¥‡∏ô‡∏à‡∏£‡∏¥‡∏á, ‡∏Å‡∏≥‡∏à‡∏±‡∏î IT Ghost, Dashboard ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°
        """
        log_prefix = f"Sub:{sub_id or '??'} L{level}"

        # 1. Robust JSON Repair
        if isinstance(llm_output, str):
            cleaned = re.sub(r'```json\s*|\s*```|\n+', ' ', llm_output.strip())
            cleaned = re.sub(r',\s*([}\]])', r'\1', cleaned)  # ‡∏•‡∏ö trailing comma
            try:
                llm_output = json.loads(cleaned)
                self.logger.debug(f"[JSON-REPAIR-SUCCESS] {log_prefix}")
            except json.JSONDecodeError as e:
                self.logger.warning(f"[JSON-REPAIR-FAIL] {log_prefix}: {e}")
                llm_output = {}
        if not isinstance(llm_output, dict) or not llm_output:
            return self._get_fallback_result(log_prefix)

        # 2. Required Phases
        required_phases = contextual_config.get("required_phases", []) or (
            ["P", "D"] if level <= 3 else ["P", "D", "C", "A"]
        )
        self.logger.debug(f"[REQUIRED-PHASES] {log_prefix}: {required_phases}")

        # 3. PDCA Extraction + Keyword Rescue
        pdca_results = {"P": 0.0, "D": 0.0, "C": 0.0, "A": 0.0}
        reason_text = str(llm_output.get('reason', '')).lower()
        ext_texts = {p: str(llm_output.get(f"Extraction_{p}", "")).lower() for p in "PDCA"}

        for phase in "PDCA":
            score = 0.0
            for k in [f"{phase}_Score", f"score_{phase.lower()}", f"Extraction_{phase}_Score"]:
                if k in llm_output:
                    try:
                        score = float(llm_output[k])
                        break
                    except:
                        continue

            # Keyword Rescue
            phase_kws = contextual_config.get(f"{phase.lower()}_keywords", [])
            combined_text = reason_text + " " + ext_texts.get(phase, "")
            if score < 1.0 and any(kw.lower() in combined_text for kw in phase_kws):
                score = max(score, 1.5)
                self.logger.info(f"[PHASE-RESCUE] {log_prefix} {phase} boosted to 1.5 by keyword")

            pdca_results[phase] = min(max(score, 0.0), 2.0)

        # 4. Mandatory Floor (L1-L3 leniency)
        floor_value = 1.0 if level == 1 else 0.8 if level <= 3 else 0.5
        for phase in required_phases:
            if pdca_results[phase] < floor_value:
                pdca_results[phase] = floor_value
                self.logger.info(f"[PHASE-FLOOR] {log_prefix} {phase} forced to {floor_value} (L{level})")

        # 5. Normalized Score
        sum_req = sum(pdca_results[p] for p in required_phases)
        max_req = len(required_phases) * 2.0
        normalized_score = round((sum_req / max_req) * 2.0 if max_req > 0 else 0.0, 2)

        # 6. Safety Net (Rerank-based override)
        max_rr = max([ev.get('rerank_score', ev.get('score', 0.0)) for ev in top_evidences] or [0.0])
        explicit_pass = llm_output.get("is_passed") is True
        is_force_pass = (normalized_score < 1.2 and max_rr >= 0.75)

        is_passed = explicit_pass or is_force_pass or (normalized_score >= 1.0)

        if is_passed and normalized_score < 1.2:
            normalized_score = max(normalized_score, 1.2)
            self.logger.info(f"[PASS-BOOST] {log_prefix} Score forced to {normalized_score} (safety net)")

        # 7. Anti-IT Ghost Cleanup
        coaching = str(llm_output.get("coaching_insight", "")).strip()
        self.logger.debug(f"[IT-CHECK-BEFORE] {log_prefix} Original: {coaching[:150]}...")
        it_patterns = r"(‡∏£‡∏∞‡∏ö‡∏ö‡∏™‡∏≤‡∏£‡∏™‡∏ô‡πÄ‡∏ó‡∏®‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥|‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏£‡∏∞‡∏ö‡∏ö|KMS|Software|IT System|‡πÅ‡∏û‡∏•‡∏ï‡∏ü‡∏≠‡∏£‡πå‡∏°‡∏î‡∏¥‡∏à‡∏¥‡∏ó‡∏±‡∏•|Automation|‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏£‡∏∞‡∏ö‡∏ö‡∏™‡∏≤‡∏£‡∏™‡∏ô‡πÄ‡∏ó‡∏®|‡∏£‡∏∞‡∏ö‡∏ö‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥)"
        cleaned_coaching = re.sub(it_patterns, "‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏°‡∏µ‡∏™‡πà‡∏ß‡∏ô‡∏£‡πà‡∏ß‡∏°", coaching, flags=re.IGNORECASE)
        cleaned_coaching = re.sub(r"‡∏Ñ‡∏ß‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤", "‡∏Ñ‡∏ß‡∏£‡∏à‡∏±‡∏î‡∏ó‡∏≥/‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£", cleaned_coaching)
        if cleaned_coaching != coaching:
            self.logger.info(f"[ANTI-IT-CLEAN] {log_prefix} Cleaned: {cleaned_coaching[:150]}...")

        # 8. Dashboard Phase Sync (‡πÉ‡∏´‡πâ ‚úÖ ‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°)
        if is_passed:
            for p in required_phases:
                if pdca_results[p] < 1.0:
                    pdca_results[p] = 1.0
                    self.logger.info(f"[DASHBOARD-SYNC] {log_prefix} {p} synced to 1.0")

        # 9. Final Summary Log
        self.logger.info(
            f"[POST-PROCESS-SUMMARY] {log_prefix} | "
            f"Raw score: {llm_output.get('score', 'N/A')} | "
            f"Normalized: {normalized_score:.2f} | "
            f"Passed: {is_passed} | "
            f"Force pass: {is_force_pass} (max_rr={max_rr:.3f}) | "
            f"PDCA: {pdca_results} | "
            f"Insight (cleaned): {cleaned_coaching[:120]}..."
        )

        return {
            "score": normalized_score,
            "is_passed": is_passed,
            "pdca_breakdown": pdca_results,
            "reason": llm_output.get("reason", "N/A"),
            "coaching_insight": cleaned_coaching,
            "required_phases": required_phases,
            "is_force_pass": is_force_pass,
            "max_rerank": max_rr
        }


    def _get_fallback_result(self, prefix: str) -> Dict[str, Any]:
        """Fallback ‡πÄ‡∏°‡∏∑‡πà‡∏≠ LLM output ‡∏û‡∏±‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡∏á"""
        self.logger.error(f"[FALLBACK] {prefix} using zero-score fallback")
        return {
            "score": 0.0,
            "is_passed": False,
            "pdca_breakdown": {"P": 0.0, "D": 0.0, "C": 0.0, "A": 0.0},
            "reason": "AI Output Error - Fallback triggered",
            "coaching_insight": "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö log ‡πÅ‡∏•‡∏∞‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô",
            "required_phases": []
        }
        

    def _expand_context_with_neighbor_pages(self, top_evidences: List[Any], collection_name: str) -> List[Dict[str, Any]]:
        """
        [ULTIMATE REVISE v2026.1.25]
        - Log Transparency: ‡πÅ‡∏™‡∏î‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏£‡∏¥‡∏á‡πÅ‡∏ó‡∏ô UUID ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ Neighbor Fetch
        - Smart Offsets: ‡∏õ‡∏£‡∏±‡∏ö‡∏ä‡πà‡∏ß‡∏á‡∏Å‡∏≤‡∏£‡∏Å‡∏ß‡∏≤‡∏î‡∏´‡∏ô‡πâ‡∏≤‡∏Å‡∏£‡∏∞‡∏î‡∏≤‡∏©‡∏ï‡∏≤‡∏°‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
        - Metadata Enrichment: ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡πÇ‡∏¢‡∏á ID ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏£‡∏¥‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô
        """
        if not self.vectorstore_manager or not top_evidences:
            return top_evidences

        standardized_evidences = []
        for d in top_evidences:
            # 1. Normalize ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á Dict ‡πÅ‡∏•‡∏∞ Langchain Document)
            orig_score = d.get('score', d.get('rerank_score', 0.5)) if isinstance(d, dict) else getattr(d, 'metadata', {}).get('score', 0.5)
            
            if hasattr(d, 'page_content'):
                standardized_evidences.append({
                    "text": d.page_content, 
                    "metadata": getattr(d, 'metadata', {}), 
                    "score": orig_score
                })
            elif isinstance(d, dict):
                d['score'] = orig_score
                standardized_evidences.append(deepcopy(d))

        expanded_evidences = list(standardized_evidences)
        # ‡πÉ‡∏ä‡πâ stable_doc_uuid ‡∏´‡∏£‡∏∑‡∏≠ source_id ‡πÄ‡∏õ‡πá‡∏ô key ‡∏´‡∏•‡∏±‡∏Å‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Å‡∏±‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ã‡πâ‡∏≥
        seen_keys = {
            f"{ev.get('metadata', {}).get('stable_doc_uuid', ev.get('metadata', {}).get('doc_id'))}_{ev.get('metadata', {}).get('page_label')}" 
            for ev in standardized_evidences
        }
        
        added_count = 0
        max_neighbors = 15  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏µ‡∏î‡∏à‡∏≥‡∏Å‡∏±‡∏î‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°
        
        for doc in standardized_evidences:
            if added_count >= max_neighbors: break
            
            meta = doc.get('metadata', {})
            doc_uuid = meta.get("stable_doc_uuid") or meta.get("doc_id") or meta.get("source_id")
            if not doc_uuid: continue

            try:
                curr_page = int(str(meta.get("page_label", "1")).strip())
            except (ValueError, TypeError): 
                continue

            # üéØ ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å Map (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏û‡πà‡∏ô Log ‡πÅ‡∏•‡∏∞‡πÉ‡∏™‡πà‡πÉ‡∏ô Metadata)
            display_filename = self.doc_id_to_filename_map.get(doc_uuid, f"DOC-{str(doc_uuid)[:8]}")

            # üß† Smart Offsets Logic: ‡∏õ‡∏£‡∏±‡∏ö‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ï‡∏≤‡∏°‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
            text_lower = doc.get('text', '').lower()
            offsets = [1] # Default ‡∏Ñ‡∏∑‡∏≠‡∏î‡∏π‡∏´‡∏ô‡πâ‡∏≤‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
            
            # ‡∏Å‡∏£‡∏ì‡∏µ‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏ß‡∏Å‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢/‡πÅ‡∏ú‡∏ô (‡∏°‡∏±‡∏Å‡∏à‡∏∞‡∏¢‡∏≤‡∏ß‡πÑ‡∏õ‡∏Ç‡πâ‡∏≤‡∏á‡∏´‡∏ô‡πâ‡∏≤)
            if any(k in text_lower for k in ["‡∏ß‡∏¥‡∏™‡∏±‡∏¢‡∏ó‡∏±‡∏®‡∏ô‡πå", "‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢", "‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå", "‡πÅ‡∏ú‡∏ô‡πÅ‡∏°‡πà‡∏ö‡∏ó"]):
                offsets = [-1, 1, 2]
            # ‡∏Å‡∏£‡∏ì‡∏µ‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏ß‡∏Å‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô/‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏• (‡∏°‡∏±‡∏Å‡∏à‡∏∞‡∏°‡∏µ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏ô‡πâ‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤)
            elif any(k in text_lower for k in ["‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•", "‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô", "‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô", "‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô", "lesson learned"]):
                offsets = [-2, -1, 1]

            for off in sorted(list(set(offsets))):
                if off == 0: continue
                target_page = curr_page + off
                
                # ‡∏Ç‡πâ‡∏≤‡∏°‡∏ñ‡πâ‡∏≤‡∏´‡∏ô‡πâ‡∏≤ < 1 ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏Ñ‡∏¢‡∏î‡∏∂‡∏á‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß
                if target_page < 1 or f"{doc_uuid}_{target_page}" in seen_keys: 
                    continue
                
                # ‡∏î‡∏∂‡∏á Chunks ‡∏à‡∏≤‡∏Å VectorStoreManager
                neighbor_chunks = self.vectorstore_manager.get_chunks_by_page(
                    collection_name, 
                    doc_uuid, 
                    str(target_page)
                )
                
                if neighbor_chunks:
                    # ‚ûï ‡∏û‡πà‡∏ô Log ‡∏ó‡∏µ‡πà‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå‡∏≠‡πà‡∏≤‡∏ô‡∏≠‡∏≠‡∏Å (‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏£‡∏¥‡∏á)
                    self.logger.info(
                        f"‚ûï Neighbor Fetch: ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏ô‡πâ‡∏≤ {target_page} "
                        f"‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå {os.path.basename(display_filename)} ({len(neighbor_chunks)} chunks)"
                    )

                    for nc in neighbor_chunks:
                        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Metadata ‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏Ç‡∏∂‡πâ‡∏ô
                        new_meta = {**nc.metadata}
                        new_meta["is_supplemental"] = True
                        new_meta["pdca_tag"] = "Support"
                        new_meta["filename"] = display_filename  # ‡πÉ‡∏™‡πà‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏£‡∏¥‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÄ‡∏•‡∏¢
                        
                        new_ev = {
                            "text": nc.page_content,
                            "metadata": new_meta,
                            "score": doc.get('score', 0.5) * 0.85, # ‡∏•‡∏î‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏•‡∏á‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢
                            "is_supplemental": True
                        }
                        expanded_evidences.append(new_ev)
                        seen_keys.add(f"{doc_uuid}_{target_page}")
                    
                    added_count += 1

        return expanded_evidences
    
    def _resolve_evidence_filenames(self, evidence_entries: List[Any]) -> List[Dict[str, Any]]:
        """
        [ULTIMATE SAFE v2026.3] - ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Error 'str' has no attribute 'get'
        - ‡∏Å‡∏≤‡∏£‡∏±‡∏ô‡∏ï‡∏µ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ List of Dictionary ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
        - ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô String ‡∏´‡∏£‡∏∑‡∏≠ None
        """
        resolved_entries = []
        if not evidence_entries:
            return []

        for entry in evidence_entries:
            # üõ°Ô∏è GUARD 1: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô Dictionary ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            if not isinstance(entry, dict):
                # ‡∏ñ‡πâ‡∏≤‡∏´‡∏•‡∏∏‡∏î‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô String ‡∏´‡∏£‡∏∑‡∏≠ Langchain Document ‡πÉ‡∏´‡πâ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÅ‡∏õ‡∏•‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πâ‡∏≤‡∏°
                if hasattr(entry, 'page_content'): # ‡∏Å‡∏£‡∏ì‡∏µ‡πÄ‡∏õ‡πá‡∏ô Document Object
                    entry = {
                        "content": entry.page_content,
                        "metadata": getattr(entry, 'metadata', {}),
                        "doc_id": getattr(entry, 'metadata', {}).get('doc_id', str(hash(entry.page_content)))
                    }
                else:
                    self.logger.warning(f"‚ö†Ô∏è Skipping non-dict evidence: {type(entry)}")
                    continue

            resolved_entry = deepcopy(entry)
            doc_id = resolved_entry.get("doc_id", "")
            
            # üõ°Ô∏è GUARD 2: Metadata Check
            meta = resolved_entry.get("metadata", {})
            if not isinstance(meta, dict): meta = {}
            
            # ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏´‡∏•‡πà‡∏á
            meta_filename = meta.get("source") or meta.get("source_filename") or meta.get("filename")
            content_raw = resolved_entry.get('content') or resolved_entry.get('text', '')
            page_label = str(meta.get("page_label") or meta.get("page") or meta.get("page_number") or "N/A")

            # 1. AI Generated / Missing Content Case
            if not content_raw:
                # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ ‡πÅ‡∏ï‡πà‡∏°‡∏µ ID ‡πÉ‡∏´‡πâ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô Reference ‡∏ß‡πà‡∏≤‡∏á
                resolved_entry["filename"] = "MISSING-CONTENT"
                resolved_entry["display_source"] = f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ (ID: {doc_id})"
            
            # 2. Match ‡πÉ‡∏ô Map
            elif doc_id in self.doc_id_to_filename_map:
                mapped_name = self.doc_id_to_filename_map[doc_id]
                resolved_entry["filename"] = mapped_name
                resolved_entry["display_source"] = f"{os.path.basename(mapped_name)} (‡∏´‡∏ô‡πâ‡∏≤ {page_label})"
            
            # 3. Match ‡πÉ‡∏ô Metadata
            elif meta_filename:
                clean_name = os.path.basename(str(meta_filename))
                resolved_entry["filename"] = clean_name
                resolved_entry["display_source"] = f"{clean_name} (‡∏´‡∏ô‡πâ‡∏≤ {page_label})"

            # 4. Fallback ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
            else:
                short_id = str(doc_id)[:8] if doc_id else "UNKNOWN"
                resolved_entry["filename"] = f"DOC-{short_id}"
                resolved_entry["display_source"] = f"‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á {short_id} (‡∏´‡∏ô‡πâ‡∏≤ {page_label})"

            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ Key ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ñ‡∏£‡∏ö‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏û‡∏¥‡πà‡∏°
            if 'content' not in resolved_entry and content_raw:
                resolved_entry['content'] = content_raw

            resolved_entries.append(resolved_entry)
            
        return resolved_entries

    # ----------------------------------------------------------------------
    # üéØ FINAL FIX 2.3: Manual Map Reload Function (inside SEAMPDCAEngine)
    # ----------------------------------------------------------------------
    def _collect_previous_level_evidences(self, sub_id: str, current_level: int) -> Dict[str, List[Dict]]:
        """
        [REVISED v2026.1.25] - Nested Key & Context Hydration
        - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Format ‡∏Ñ‡∏µ‡∏¢‡πå‡πÉ‡∏´‡∏°‡πà "1.1_L1"
        - ‡∏î‡∏∂‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏ï‡πá‡∏°‡∏à‡∏≤‡∏Å VectorStore ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡πá‡∏ô Baseline ‡πÉ‡∏´‡πâ‡πÄ‡∏•‡πÄ‡∏ß‡∏•‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
        """
        if getattr(self, 'is_parallel_all_mode', False):
            return {}

        collected = {}
        # üîÑ ‡∏õ‡∏£‡∏±‡∏ö Logic ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á Key ‡πÉ‡∏´‡πâ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á 1.1.L1 ‡πÅ‡∏•‡∏∞ 1.1_L1
        for key, bucket in self.evidence_map.items():
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏µ‡∏¢‡πå (‡πÄ‡∏ä‡πà‡∏ô 1.1_L1 ‡∏´‡∏£‡∏∑‡∏≠ 1.1.L1)
            if key.startswith(f"{sub_id}_L") or key.startswith(f"{sub_id}.L"):
                try:
                    # ‡πÅ‡∏¢‡∏Å‡πÄ‡∏≠‡∏≤‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç Level ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤ (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á "_" ‡πÅ‡∏•‡∏∞ ".")
                    level_part = key.replace(f"{sub_id}_L", "").replace(f"{sub_id}.L", "")
                    level_num = int(level_part)
                    
                    if level_num < current_level:
                        # ‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ List ‡∏Ç‡∏≠‡∏á evidences ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤
                        ev_list = bucket.get("evidences", []) if isinstance(bucket, dict) else bucket
                        collected[key] = ev_list
                except: continue

        if not collected: return {}

        # 1. ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏° Unique IDs (Stable ID logic)
        stable_ids = set()
        for ev_list in collected.values():
            for ev in ev_list:
                sid = ev.get("stable_doc_uuid") or ev.get("doc_id")
                if sid and str(sid).lower() not in ["n/a", "none", ""]:
                    stable_ids.add(str(sid))

        if not stable_ids: return collected

        # 2. Bulk Hydration (Query ‡∏à‡∏≤‡∏Å VectorStore ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏≠‡∏≤ Text ‡πÄ‡∏ï‡πá‡∏°)
        vsm = self.vectorstore_manager
        chunk_map = {}
        try:
            # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å VectorStore ‡∏°‡∏≤‡∏Ñ‡∏∑‡∏ô‡∏ä‡∏µ‡∏û (Restore) ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
            full_chunks = vsm.get_documents_by_id(list(stable_ids), self.doc_type, self.enabler)
            for chunk in full_chunks:
                m = chunk.metadata
                keys = [str(m.get(k)) for k in ["stable_doc_uuid", "doc_id", "chunk_uuid"] if m.get(k)]
                for k in keys:
                    chunk_map[k] = {"text": chunk.page_content, "metadata": m}
                    chunk_map[k.replace("-", "")] = {"text": chunk.page_content, "metadata": m}
        except Exception as e:
            self.logger.error(f"‚ùå Hydration VSM Error: {e}")
            return collected

        # 3. Restoration Loop (‡∏¢‡∏±‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏ï‡πá‡∏°‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤ Evidence List)
        restored_count = 0
        for key, ev_list in collected.items():
            for ev in ev_list:
                sid = str(ev.get("stable_doc_uuid") or ev.get("doc_id") or "")
                data = chunk_map.get(sid) or chunk_map.get(sid.replace("-", ""))

                if data:
                    ev.update({
                        "text": data["text"],
                        "metadata": data.get("metadata", {}),
                        "is_baseline": True  # üö© Mark ‡πÑ‡∏ß‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ AI ‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏á‡πÄ‡∏Å‡πà‡∏≤
                    })
                    restored_count += 1
                
        self.logger.info(f"‚úÖ Hydrated {restored_count} baseline chunks for {sub_id} L{current_level}")
        return collected

    def _get_contextual_rules_prompt(self, sub_id: str, level: int) -> str:
        """
        [REVISED v2026.1.25]
        - ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö (Specific Rules)
        - ‡∏â‡∏µ‡∏î L5 Special Rule ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£ "Reset ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô" ‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
        """
        sub_id_rules = self.contextual_rules_map.get(sub_id, {})
        rule_text = ""
        
        # 1. ‡∏Å‡∏é‡∏£‡∏≤‡∏¢ Level (‡∏î‡∏∂‡∏á‡∏°‡∏≤‡∏à‡∏≤‡∏Å config)
        level_key = f"L{level}"
        specific_rule = sub_id_rules.get(level_key)
        if specific_rule:
            rule_text += f"\n[CRITICAL RULE L{level}]\n{specific_rule}\n"
        
        # 2. üéñÔ∏è L5 SPECIAL RULE: ‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏•‡∏¥‡∏®
        if level == 5:
            rule_text += """
            \n--- [JUDICIAL GUIDELINE FOR LEVEL 5] ---
            * ‡∏ó‡πà‡∏≤‡∏ô‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö 'Excellence' (L5)
            * **Score Continuity:** ‡∏´‡∏≤‡∏Å‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô P-D-C-A ‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÄ‡∏î‡∏¥‡∏° (Baseline) ‡∏°‡∏±‡πà‡∏ô‡∏Ñ‡∏á‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß ‡∏´‡πâ‡∏≤‡∏°‡∏•‡∏î‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤ 7.0
            * **Excellence Bonus (+2.0):** ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô '‡∏Ñ‡∏ß‡∏≤‡∏°‡πÇ‡∏î‡∏î‡πÄ‡∏î‡πà‡∏ô' ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 1 ‡∏≠‡∏¢‡πà‡∏≤‡∏á ‡πÄ‡∏ä‡πà‡∏ô:
                - ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏ô‡πÅ‡∏ö‡∏ö (Best Practice/Role Model)
                - ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ä‡∏≤‡∏ï‡∏¥/‡∏ô‡∏≤‡∏ô‡∏≤‡∏ä‡∏≤‡∏ï‡∏¥
                - ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç ROI ‡∏´‡∏£‡∏∑‡∏≠ Business Impact ‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
            * **Final Decision:** ‡∏´‡∏≤‡∏Å‡∏û‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÇ‡∏î‡∏î‡πÄ‡∏î‡πà‡∏ô ‡πÉ‡∏´‡πâ‡∏õ‡∏£‡∏±‡∏ö Score ‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏õ‡πá‡∏ô 9.0+ ‡πÅ‡∏•‡∏∞‡πÄ‡∏ã‡∏ï is_passed = true ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
            """
            
        return rule_text

    def _load_rubric(self) -> Dict[str, Any]:
        """ Loads the SEAM rubric JSON file using path_utils. """
        
        # üéØ FIX: ‡πÉ‡∏ä‡πâ get_rubric_file_path ‡∏à‡∏≤‡∏Å path_utils ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Path ‡πÄ‡∏≠‡∏á
        filepath = None # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô UnboundLocalError
        
        try:
            # 1. ‡∏£‡∏±‡∏ö Path ‡∏à‡∏≤‡∏Å path_utils ‡∏ã‡∏∂‡πà‡∏á‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏ä‡∏µ‡πâ‡πÑ‡∏õ‡∏ó‡∏µ‡πà 'config/' ‡πÅ‡∏•‡πâ‡∏ß
            filepath = get_rubric_file_path(
                tenant=self.config.tenant,
                enabler=self.enabler
            )
        except Exception as e:
            # ‡∏î‡∏±‡∏Å‡∏à‡∏±‡∏ö Exception ‡∏´‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏î‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô path_utils
            self.logger.error(f"‚ùå FATAL: Error calling get_rubric_file_path: {e}")
            return {} 

        # 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if filepath is None or not os.path.exists(filepath):
            self.logger.error(f"‚ö†Ô∏è Rubric file not found at expected path: {filepath}")
            return {}

        # 3. ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå JSON
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
            self.logger.info(f"‚úÖ Rubric loaded successfully from: {filepath}")
            return data
        except json.JSONDecodeError:
            self.logger.error(f"‚ùå Error decoding Rubric JSON. File might be corrupted: {filepath}")
            return {}
        except Exception as e:
            self.logger.error(f"‚ùå Error loading Rubric file from {filepath}: {e}")
            return {}
    
    # -------------------- Helper Function for Map Processing --------------------
    def _get_level_order_value(self, level_str: str) -> int:
        """Converts Level string ('L1', 'L5') to an integer for comparison."""
        try:
            return int(level_str.upper().replace('L', ''))
        except:
            return 0

    
    def _clean_temp_entries(self, evidence_map: Dict[str, List[Any]]) -> Dict[str, List[Dict]]:
        """
        [ULTIMATE SANITIZER v2026.1.23]
        ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î Evidence Map ‡πÅ‡∏ö‡∏ö‡πÄ‡∏ö‡πá‡∏î‡πÄ‡∏™‡∏£‡πá‡∏à:
        1. ‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡∏¢‡∏∞ (TEMP-, HASH-, Unknown)
        2. ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ã‡πâ‡∏≥ (Deduplication) ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô Level ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
        3. ‡∏ã‡πà‡∏≠‡∏°‡πÅ‡∏ã‡∏° Metadata (Filename, Page)
        4. ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Type Error (Type-Safe Processing)
        """
        if not evidence_map or not isinstance(evidence_map, dict):
            return {}

        cleaned_map = {}
        stats = {"removed": 0, "fixed": 0, "dupes": 0}

        for key, entries in evidence_map.items():
            if not isinstance(entries, list):
                continue
                
            valid_entries = []
            seen_doc_ids = set() # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Deduplication ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô Key (Level) ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô

            for entry in entries:
                # --- üõ°Ô∏è ‡∏ä‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà 1: Type Validation ---
                if not isinstance(entry, dict):
                    if isinstance(entry, str) and len(entry.strip()) > 5:
                        # ‡∏Å‡∏£‡∏ì‡∏µ‡∏´‡∏•‡∏∏‡∏î‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô String (‡πÄ‡∏ä‡πà‡∏ô ID ‡∏´‡∏£‡∏∑‡∏≠ Content) ‡πÉ‡∏´‡πâ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á Dict ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
                        entry = {
                            "doc_id": entry.strip(), 
                            "filename": "Unknown_Reference.pdf", 
                            "relevance_score": 0.1,
                            "page": "N/A"
                        }
                    else:
                        stats["removed"] += 1
                        continue

                # --- üõ°Ô∏è ‡∏ä‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà 2: Garbage Filtering ---
                doc_id = entry.get("doc_id") or entry.get("chunk_uuid")
                if not doc_id:
                    stats["removed"] += 1
                    continue
                
                doc_id_str = str(doc_id).strip()

                # ‡∏Å‡∏£‡∏≠‡∏á‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á ‡∏´‡∏£‡∏∑‡∏≠ Keyword ‡∏Ç‡∏¢‡∏∞
                if doc_id_str.lower() in ["none", "unknown", "n/a", "", "null"]:
                    stats["removed"] += 1
                    continue

                # ‡∏Å‡∏£‡∏≠‡∏á Prefix ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß
                if doc_id_str.startswith(("TEMP-", "HASH-", "REF-")):
                    stats["removed"] += 1
                    continue

                # --- üõ°Ô∏è ‡∏ä‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà 3: Deduplication (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡∏ã‡πâ‡∏≥) ---
                if doc_id_str in seen_doc_ids:
                    stats["dupes"] += 1
                    continue
                seen_doc_ids.add(doc_id_str)

                # --- üõ°Ô∏è ‡∏ä‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà 4: Metadata Repair & Normalization ---
                # 1. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå (Filename)
                filename = str(entry.get("filename") or entry.get("source", "")).strip()
                if not filename or filename.lower() in ["unknown", "none", "n/a", "unknown_file.pdf"]:
                    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ ID ‡∏¢‡πà‡∏≠‡∏°‡∏≤‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏ó‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ User ‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÑ‡∏î‡πâ
                    short_id = doc_id_str[:8]
                    entry["filename"] = f"Reference_{short_id}.pdf"
                    stats["fixed"] += 1
                else:
                    # Clean path ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÅ‡∏Ñ‡πà‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå
                    try:
                        entry["filename"] = os.path.basename(filename)
                    except:
                        entry["filename"] = filename

                # 2. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô (Relevance Score)
                try:
                    score = float(entry.get("relevance_score", 0.0))
                    entry["relevance_score"] = max(0.0, min(1.0, score)) # ‡∏ö‡∏µ‡∏ö‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á 0-1
                except (ValueError, TypeError):
                    entry["relevance_score"] = 0.0

                # 3. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤ (Page)
                if "page" not in entry or entry["page"] is None:
                    entry["page"] = entry.get("page_label") or "N/A"

                valid_entries.append(entry)

            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Key ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û
            if valid_entries:
                # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á (‡∏à‡∏≤‡∏Å‡∏°‡∏≤‡∏Å‡πÑ‡∏õ‡∏ô‡πâ‡∏≠‡∏¢)
                valid_entries.sort(key=lambda x: x.get("relevance_score", 0.0), reverse=True)
                cleaned_map[key] = valid_entries

        self.logger.info(
            f"üßπ [CLEAN-MAP] Stats: Removed={stats['removed']}, Fixed={stats['fixed']}, Dupes={stats['dupes']}"
        )
        return cleaned_map

    def _clean_map_for_json(self, data_map: Dict[str, Any]) -> Dict[str, Any]:
        """
        [v2026.1.23] ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô JSON-Compatible
        - ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô TypeError ‡∏à‡∏≤‡∏Å Object ‡∏ó‡∏µ‡πà JSON ‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å
        - ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô AttributeError ‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ï‡πà‡∏≠
        """
        if not isinstance(data_map, dict):
            return {}
        
        clean_data = {}
        for k, v in data_map.items():
            str_key = str(k) # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö Key ‡πÄ‡∏õ‡πá‡∏ô String
            
            if isinstance(v, dict):
                clean_data[str_key] = self._clean_map_for_json(v)
            elif isinstance(v, list):
                clean_data[str_key] = [
                    (self._clean_map_for_json(item) if isinstance(item, dict) 
                     else (str(item) if not isinstance(item, (str, int, float, bool)) and item is not None else item))
                    for item in v
                ]
            elif isinstance(v, (str, int, float, bool)) or v is None:
                clean_data[str_key] = v
            else:
                # ‡πÅ‡∏õ‡∏•‡∏á Object ‡∏≠‡∏∑‡πà‡∏ô‡πÜ (‡πÄ‡∏ä‡πà‡∏ô Datetime, UUID) ‡πÄ‡∏õ‡πá‡∏ô String ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
                clean_data[str_key] = str(v)
                
        return clean_data
    

    def _group_statements_by_sub_criteria(
        self,
        flat_statements: List[Dict[str, Any]]
    ) -> Dict[str, Dict[str, Any]]:
        """
        Group flattened rubric statements into Sub-Criteria bundles.

        Expected input item structure (PEA compatible):
        {
            "sub_id": "1.1",
            "sub_criteria_name": "...",
            "weight": 4,
            "levels": [
                { "level": 1, "statement": "..." },
                ...
            ]
        }
        """
        grouped: Dict[str, Dict[str, Any]] = {}

        for item in flat_statements:
            sub_id = item.get("sub_id")

            # üîí Minimal validation only (‡∏≠‡∏¢‡πà‡∏≤ strict ‡πÄ‡∏Å‡∏¥‡∏ô)
            if not sub_id:
                self.logger.warning(f"‚ö†Ô∏è Skip item without sub_id: {item}")
                continue

            levels = item.get("levels")

            if not isinstance(levels, list) or not levels:
                self.logger.warning(
                    f"‚ö†Ô∏è Skip sub_id {sub_id}: invalid or empty levels"
                )
                continue

            sub_id = str(sub_id)

            # ‚úÖ init group
            if sub_id not in grouped:
                grouped[sub_id] = {
                    "sub_id": sub_id,
                    "sub_criteria_name": item.get("sub_criteria_name", ""),
                    "weight": float(item.get("weight", 0.0)),
                    "levels": []
                }

            # ‚úÖ normalize each level
            for lv in levels:
                level_no = lv.get("level")
                statement = lv.get("statement")

                if level_no is None or not statement:
                    self.logger.warning(
                        f"‚ö†Ô∏è Skip invalid level in {sub_id}: {lv}"
                    )
                    continue

                grouped[sub_id]["levels"].append({
                    "level": int(level_no),
                    "statement": statement,
                    "keywords": lv.get("keywords", []),
                    "score_threshold": lv.get("score_threshold"),
                    "raw": lv
                })

        # üîí sort L1 ‚Üí L5 and drop empty subs
        cleaned_grouped = {}
        for sub_id, sub in grouped.items():
            if not sub["levels"]:
                self.logger.warning(
                    f"‚ö†Ô∏è Drop sub_id {sub_id}: no valid levels after normalization"
                )
                continue

            sub["levels"] = sorted(
                sub["levels"], key=lambda x: x["level"]
            )
            cleaned_grouped[sub_id] = sub

        self.logger.info(
            f"üì¶ Grouped rubric into {len(cleaned_grouped)} sub-criteria bundles"
        )

        return cleaned_grouped

    # -------------------- Statement Preparation & Filtering Helpers --------------------
    def _flatten_rubric_to_statements(self, rubric_data: Optional[Dict] = None) -> List[Dict[str, Any]]:
        """
        [ULTIMATE REVISED v2026.1.24 - PROMPT-READY VERSION]
        ‡πÅ‡∏õ‡∏•‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Rubric ‡∏ó‡∏µ‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô Flat List ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡∏Å‡∏±‡∏î Focus Points 
        ‡πÅ‡∏•‡∏∞ Evidence Guidelines ‡∏£‡∏≤‡∏¢ Level ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ LLM Agent ‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥
        """
        # 1. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Source ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: Argument > self.rubric)
        source_rubric = rubric_data if rubric_data is not None else getattr(self, 'rubric', None)
        
        if not source_rubric:
            self.logger.warning("‚ö†Ô∏è [FLATTEN] Cannot proceed: Source rubric is empty or None.")
            return []
            
        try:
            # ‡πÉ‡∏ä‡πâ deepcopy ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö (Thread-safety)
            data = deepcopy(source_rubric)
            criteria_map = data.get('criteria', {}) if isinstance(data, dict) else {}
            
            if not criteria_map:
                 self.logger.error("‚ùå [FLATTEN] Invalid structure: 'criteria' key not found.")
                 return []
                 
            extracted_list = []
            
            # 2. Loop ‡πÄ‡∏à‡∏≤‡∏∞‡∏•‡∏∂‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö Criteria ‡πÅ‡∏•‡∏∞ Sub-Criteria
            for criteria_id, criteria_data in criteria_map.items():
                if not isinstance(criteria_data, dict): continue
                    
                sub_criteria_map = criteria_data.get('subcriteria', {})
                criteria_name = criteria_data.get('name', 'Unknown Criteria')
                
                for sub_id, sub_data in sub_criteria_map.items():
                    if not isinstance(sub_data, dict): continue
                    
                    # --- [CORE EXTRACTION] ---
                    # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Focus Points: ‡πÅ‡∏õ‡∏•‡∏á List ‡πÄ‡∏õ‡πá‡∏ô String ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏™‡πà‡πÉ‡∏ô Prompt ‡∏á‡πà‡∏≤‡∏¢‡πÜ
                    fps = sub_data.get('focus_points', [])
                    focus_points_str = " | ".join(fps) if isinstance(fps, list) else str(fps or "-")
                    
                    # ‡πÄ‡∏Å‡πá‡∏ö Evidence Guidelines ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÑ‡∏ß‡πâ‡∏Å‡πà‡∏≠‡∏ô (Dictionary)
                    all_guidelines = sub_data.get('evidence_guidelines', {})

                    # ‡∏™‡∏£‡πâ‡∏≤‡∏á Base Object ‡∏Ç‡∏≠‡∏á Sub-Criteria ‡∏ô‡∏µ‡πâ
                    item = {
                        'criteria_id': criteria_id,
                        'criteria_name': criteria_name,
                        'sub_id': sub_id,
                        'sub_criteria_name': sub_data.get('name', f"{criteria_name} - {sub_id}"),
                        'weight': sub_data.get('weight', criteria_data.get('weight', 0)),
                        'focus_points': focus_points_str,            # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Prompt {focus_points}
                        'evidence_guidelines_all': all_guidelines,    # ‡πÅ‡∏´‡∏•‡πà‡∏á‡πÄ‡∏Å‡πá‡∏ö‡∏ï‡πâ‡∏ô‡∏ó‡∏≤‡∏á
                        'raw_levels': sub_data.get('levels', {})      # ‡∏£‡∏≠‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ï‡πà‡∏≠
                    }
                    extracted_list.append(item)

            # 3. [LEVEL PROCESSING] ‡πÅ‡∏ï‡∏Å‡∏¢‡πà‡∏≠‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏≤‡∏¢ Level ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ú‡∏π‡∏Å Guideline ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ï‡∏±‡∏ß
            final_list = []
            for sub_item in extracted_list:
                raw_levels = sub_item.pop('raw_levels') 
                processed_levels = []
                
                if isinstance(raw_levels, dict):
                    for level_str, statement in raw_levels.items():
                        try:
                            level_int = int(level_str)
                            # üéØ ‡∏î‡∏∂‡∏á Guideline ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡∏≠‡∏á Level ‡∏ô‡∏±‡πâ‡∏ô‡πÜ (e.g., 'level_1', 'level_2')
                            # ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Default ‡πÄ‡∏õ‡πá‡∏ô "-"
                            current_guideline = sub_item['evidence_guidelines_all'].get(f"level_{level_int}", "-")
                            
                            processed_levels.append({
                                "level": level_int, 
                                "statement": statement,
                                "level_specific_guideline": current_guideline # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Prompt {evidence_guidelines}
                            })
                        except (ValueError, TypeError):
                            self.logger.error(f"‚ùå [FLATTEN] Invalid level key '{level_str}' in Sub-ID: {sub_item['sub_id']}")
                            continue
                
                if processed_levels:
                    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö Level 1 -> 5 ‡πÄ‡∏™‡∏°‡∏≠
                    processed_levels.sort(key=lambda x: x.get("level", 0))
                    sub_item["levels"] = processed_levels
                    final_list.append(sub_item)
                else:
                    self.logger.warning(f"‚ö†Ô∏è [FLATTEN] Sub-criteria {sub_item['sub_id']} has no valid levels.")

            self.logger.info(f"‚úÖ [FLATTEN] Rubric transformation complete. Processed {len(final_list)} sub-criteria.")
            return final_list

        except Exception as e:
            self.logger.error(f"üõë [FLATTEN-ERROR] Failure during flattening: {str(e)}", exc_info=True)
            return []
    
    # -------------------- Evidence Classification Helper (Robust 2026) --------------------
    def _get_mapped_uuids_and_priority_chunks(
        self,
        sub_id: str,
        level: int,
        statement_text: str = "",
        level_constraint: Optional[Any] = None, # üü¢ ‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô Optional ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Error Missing Argument
        vectorstore_manager: Optional['VectorStoreManager'] = None,
        evidence_map: Optional[Dict] = None 
    ) -> Tuple[List[str], List[Dict]]:
        """
        [DYNAMIC CONTINUITY v2026.6.2] - ROBUST SIGNATURE
        ----------------------------------------------
        - Fix: ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÅ‡∏ö‡∏ö‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Error 'missing 1 required positional argument'
        - Logic: ‡∏î‡∏∂‡∏á Baseline ‡∏à‡∏≤‡∏Å Memory/Map ‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á (Inheritance)
        """
        from copy import deepcopy
        priority_chunks = []
        mapped_stable_ids = []

        # 1. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ä‡πâ Evidence Map (‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: 1. Argument ‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏°‡∏≤ -> 2. Class Attribute -> 3. Empty Dict)
        target_map = evidence_map if evidence_map is not None else getattr(self, 'evidence_map', {})

        # 2. üß† [AUTO-HISTORY & INHERITANCE]
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡∏ú‡πà‡∏≤‡∏ô‡πÉ‡∏ô Level ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏≥‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡πÉ‡∏ô Level ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
        for key, evidences in target_map.items():
            if key.startswith(f"{sub_id}.L") and isinstance(evidences, list):
                try:
                    lvl_in_key = int(key.split(".L")[-1])
                    # ‡∏Å‡∏é Inheritance: ‡πÉ‡∏ä‡πâ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏à‡∏≤‡∏Å Level ‡∏ó‡∏µ‡πà <= ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
                    if lvl_in_key <= level:
                        history_items = deepcopy(evidences)
                        for item in history_items:
                            item["is_baseline"] = True
                            # ‡∏ö‡∏π‡∏™‡∏ï‡πå‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡∏≠‡∏á RAG
                            item["rerank_score"] = max(item.get("rerank_score", 0.0), 0.85)
                        priority_chunks.extend(history_items)
                except (ValueError, IndexError):
                    continue

        # 3. üîç [SEMANTIC HINTING] ‡∏Å‡∏£‡∏ì‡∏µ L1 ‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡πÉ‡∏ô Map
        if not priority_chunks and level == 1:
            rule_config = getattr(self, 'contextual_rules_map', {}).get(sub_id, {}).get(str(level), {})
            hints = rule_config.get("plan_keywords", [])[:2]
            if hints and vectorstore_manager:
                self.logger.info(f"üîé L1 Discovery: Searching using hints: {hints}")
                try:
                    # ‡πÅ‡∏Å‡πâ‡∏à‡∏∏‡∏î 1: ‡πÉ‡∏ä‡πâ helper ‡∏à‡∏£‡∏¥‡∏á + ‡∏î‡∏∂‡∏á enabler ‡∏à‡∏≤‡∏Å self
                    enabler = getattr(self, 'enabler', 'KM').upper()  # fallback KM ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ
                    collection_name = get_doc_type_collection_key("evidence", enabler)
                    
                    discovery_docs = vectorstore_manager.retrieve(
                        query=f"{sub_id} {' '.join(hints)}",
                        collection_name=collection_name,
                        top_k=5
                    )
                    
                    for doc in discovery_docs:
                        # ‡πÅ‡∏Å‡πâ‡∏à‡∏∏‡∏î 2: fallback chunk_uuid ‡πÅ‡∏•‡∏∞ source ‡πÉ‡∏´‡πâ‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
                        chunk_uuid = (
                            doc.metadata.get("chunk_uuid") or
                            doc.metadata.get("id") or
                            doc.metadata.get("chunk_id") or
                            hashlib.sha256(doc.page_content.encode()).hexdigest()[:32]  # last resort
                        )
                        
                        source = (
                            doc.metadata.get("source") or
                            doc.metadata.get("source_filename") or
                            doc.metadata.get("file_path") or
                            "unknown_source"
                        )
                        
                        chunk = {
                            "page_content": doc.page_content,
                            "metadata": doc.metadata or {},
                            "rerank_score": 0.85,
                            "chunk_uuid": chunk_uuid,
                            "source": source
                        }
                        priority_chunks.append(chunk)
                        
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Discovery retrieve failed for {collection_name}: {e}")
                    

        if not priority_chunks:
            return [], []

        # 4. üíß [ROBUST HYDRATION] ‡πÄ‡∏ï‡∏¥‡∏°‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ Full Text ‡∏à‡∏≤‡∏Å Vector DB
        try:
            priority_chunks = self._robust_hydrate_documents_for_priority_chunks(
                chunks_to_hydrate=priority_chunks,
                vsm=vectorstore_manager
            )
        except Exception as e:
            self.logger.error(f"‚ùå Hydration failed in priority module: {e}")

        # 5. üéØ [ID SYNC] ‡∏™‡∏Å‡∏±‡∏î UUID ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ Filter ‡∏Ç‡∏≠‡∏á Main Retriever
        seen_ids = set()
        for chunk in priority_chunks:
            sid = chunk.get("stable_doc_uuid") or chunk.get("doc_id")
            if sid and isinstance(sid, str):
                if sid not in seen_ids and len(sid) >= 32:
                    mapped_stable_ids.append(sid)
                    seen_ids.add(sid)

        self.logger.info(f"‚úÖ Continuity Ready: {len(priority_chunks)} priority chunks | Sub:{sub_id} L{level}")
        return mapped_stable_ids, priority_chunks

    
    def get_actual_score(self, ev: Any) -> float:
        """
        [v2026.1 - ROBUST SCORING EXTRACTOR]
        ‡∏î‡∏∂‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô ‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö Dict ‡∏´‡∏£‡∏∑‡∏≠ Object
        ‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô 0.0 ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ã‡πà‡∏≠‡∏ô‡∏Ñ‡∏µ‡∏¢‡πå‡πÉ‡∏ô Metadata
        """
        if not ev:
            return 0.0

        # 1. ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ñ‡∏µ‡∏¢‡πå‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏ö‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö (‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç)
        score_keys = ["rerank_score", "score", "relevance_score"]
        
        # 2. ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö Top-level ‡∏Å‡πà‡∏≠‡∏ô
        for key in score_keys:
            # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á dict.get() ‡πÅ‡∏•‡∏∞ getattr() ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Document Object
            val = ev.get(key) if isinstance(ev, dict) else getattr(ev, key, None)
            if val is not None:
                try:
                    return float(val)
                except (ValueError, TypeError):
                    continue

        # 3. Fallback: ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏†‡∏≤‡∏¢‡πÉ‡∏ô Metadata
        meta = ev.get("metadata", {}) if isinstance(ev, dict) else getattr(ev, "metadata", {})
        if isinstance(meta, dict):
            for key in score_keys:
                val = meta.get(key)
                if val is not None:
                    try:
                        return float(val)
                    except (ValueError, TypeError):
                        continue

        return 0.0
    
    def _calculate_evidence_strength_cap(
        self,
        top_evidences: List[Any],
        level: int,
        highest_rerank_score: Optional[float] = None
    ) -> Dict[str, Any]:
        """
        [PROTECTED v2026.1.17 - REVISED]
        - ‡πÉ‡∏ä‡πâ get_actual_score ‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô
        - ‡πÅ‡∏¢‡∏Å Logic ‡∏Å‡∏≤‡∏£‡∏´‡∏≤ '‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•' ‡πÉ‡∏´‡πâ‡∏â‡∏•‡∏≤‡∏î‡∏Ç‡∏∂‡πâ‡∏ô
        - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏∞‡∏ö‡∏ö Logging ‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ Audit
        """
        try:
            # ‚öôÔ∏è Load Configuration
            threshold = getattr(self, "RERANK_THRESHOLD", 0.35)
            cap_value = getattr(self, "MAX_EVI_STR_CAP", 5.0)
            
            # 1. ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏°‡∏≤ (Baseline)
            max_score_found = 0.0
            try:
                if highest_rerank_score is not None:
                    max_score_found = float(highest_rerank_score)
            except (ValueError, TypeError):
                max_score_found = 0.0

            max_score_source = "Adaptive_RAG_Loop"
            
            if not isinstance(top_evidences, list):
                top_evidences = []

            # 2. Iterate ‡∏´‡∏≤‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ
            for idx, doc in enumerate(top_evidences, 1):
                # ‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡∏î‡∏∂‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏ö
                current_score = self.get_actual_score(doc)

                # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏à‡∏≠‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤ ‡πÉ‡∏´‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ß‡πâ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏≤
                if current_score > max_score_found:
                    max_score_found = current_score
                    
                    # --- ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö Robust ---
                    meta = doc.get("metadata", {}) if isinstance(doc, dict) else getattr(doc, "metadata", {})
                    if not isinstance(meta, dict): meta = {}
                    
                    max_score_source = (
                        meta.get("source_filename") or 
                        meta.get("file_name") or 
                        meta.get("source") or 
                        f"Doc_{idx}"
                    )

            # 3. Decision Logic (Gated Check)
            # ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏¢‡∏±‡∏á‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤ Threshold (0.35) -> ‡∏™‡∏±‡πà‡∏á Capped
            is_capped = max_score_found < threshold
            # ‡∏Ñ‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤ Prompt: 5.0 (‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô) ‡∏´‡∏£‡∏∑‡∏≠ 10.0 (‡∏°‡∏µ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏ú‡πà‡∏≤‡∏ô)
            max_evi_str_for_prompt = float(cap_value) if is_capped else 10.0

            # üìä Internal Audit Log
            status_icon = "üö® [CAPPED]" if is_capped else "‚úÖ [FULL-STRENGTH]"
            self.logger.info(
                f"{status_icon} L{level} | Best Score: {max_score_found:.4f} "
                f"from: '{os.path.basename(str(max_score_source))}' | Threshold: {threshold}"
            )

            return {
                "is_capped": bool(is_capped),
                "max_evi_str_for_prompt": float(max_evi_str_for_prompt),
                "top_score": round(float(max_score_found), 4),
                "max_score_source": str(max_score_source),
                "threshold_used": threshold
            }

        except Exception as e:
            self.logger.error(f"‚ùå [CRITICAL-CAP-ERROR] {e}", exc_info=True)
            # ‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡πÑ‡∏ß‡πâ‡∏Å‡πà‡∏≠‡∏ô: ‡∏ñ‡πâ‡∏≤ Error ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á Cap ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ LLM ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏î‡πâ
            return {
                "is_capped": False, 
                "max_evi_str_for_prompt": 10.0, 
                "top_score": 0.0, 
                "max_score_source": "Fallback-Error"
            }
       
    
    def _extract_strategic_gaps(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        [HELPER] ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏´‡∏≤‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á (Gaps) ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á‡∏Å‡∏±‡∏ö‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢
        ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡πÄ‡∏™‡∏ô‡∏≠ Coaching Insight ‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°
        """
        gaps = []
        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á‡∏Ç‡∏≠‡∏á Gap (‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ô‡πâ‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡∏Å‡πà‡∏≠‡∏ô)
        sorted_results = sorted(results, key=lambda x: x.get('score', 0.0))
        
        for res in sorted_results:
            sub_id = res.get('sub_id', 'Unknown')
            current_score = res.get('score', 0.0)
            passed = res.get('is_passed', False)
            
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô ‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤ 0.8 ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô Gap
            if not passed or current_score < 0.8:
                gap_info = {
                    "sub_id": sub_id,
                    "level": res.get('level'),
                    "current_score": current_score,
                    "impact": "High" if current_score < 0.5 else "Medium",
                    "reason": res.get('reason', '‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡∏µ‡πâ'),
                    "suggestion": res.get('coaching_insight', '‡∏Ñ‡∏ß‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏Ç‡∏≤‡∏î‡∏´‡∏≤‡∏¢‡∏ï‡∏≤‡∏° PDCA')
                }
                gaps.append(gap_info)
        
        # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ Gap ‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î 5 ‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å
        return gaps[:5]

    def calculate_audit_confidence(
        self,
        matched_chunks: List[Any],
        sub_id: str = "unknown",
        level: int = 1
    ) -> Dict[str, Any]:
        """
        [ULTIMATE AUDIT CONFIDENCE v2026.3.10 ‚Äì Real-Count Enabled]
        - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏•‡∏Ç (1) ‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á Raw Tags ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏ô‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà (Frequency Counting)
        - ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö PDCA ‡∏ú‡πà‡∏≤‡∏ô 3 ‡∏ä‡∏±‡πâ‡∏ô: Metadata -> Tagging -> Keyword Fallback
        - ‡∏õ‡∏£‡∏±‡∏ö Decision Matrix ‡πÉ‡∏´‡πâ‡∏™‡∏°‡∏î‡∏∏‡∏•‡∏Å‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á
        """
        if not matched_chunks:
            return {
                "level": "NONE", "reason": "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö", "source_count": 0,
                "coverage_ratio": 0.0, "pdca_found": [], "valid_chunks_count": 0,
                "traceability_score": 0.0, "recency_bonus": 0.0
            }

        # 1. Quality Gate: ‡∏Ñ‡∏±‡∏î‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Chunk ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏ô‡πâ‡∏ô‡πÜ (Relevance >= 0.40)
        valid_chunks = [doc for doc in matched_chunks if self.get_actual_score(doc) >= 0.40]
        valid_count = len(valid_chunks)

        if valid_count == 0:
            return {
                "level": "LOW", "reason": "‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û",
                "source_count": 0, "coverage_ratio": 0.0, "pdca_found": [], "valid_chunks_count": 0
            }

        # 2. Independence Check: ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        unique_sources = set()
        for doc in valid_chunks:
            meta = getattr(doc, 'metadata', {}) if hasattr(doc, 'metadata') else doc.get('metadata', {})
            src = next((meta.get(k) for k in ['source_filename', 'filename', 'source'] if meta.get(k)), None)
            if src:
                unique_sources.add(os.path.basename(str(src).strip()))
        
        independence_score = len(unique_sources)

        # 3. PDCA Detection (Revised: Multi-Tag Frequency Collection)
        all_detected_tags = [] # ‡πÄ‡∏Å‡πá‡∏ö List ‡∏Ç‡∏≠‡∏á Tag ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠‡πÉ‡∏ô‡∏ó‡∏∏‡∏Å Chunks (‡∏´‡πâ‡∏≤‡∏°‡∏ó‡∏≥ Unique)
        
        # ‡∏î‡∏∂‡∏á Keyword rules ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Fallback
        cum_rules = {}
        if hasattr(self, "get_cumulative_rules_cached"):
            try:
                loaded = self.get_cumulative_rules_cached(sub_id, level)
                if isinstance(loaded, dict):
                    cum_rules = loaded
                else:
                    self.logger.warning(
                        f"[PDCA] cumulative rules invalid type for {sub_id}-{level}"
                    )
            except Exception as e:
                self.logger.error(
                    f"[PDCA] failed to load cumulative rules for {sub_id}-{level}: {e}"
                )
                
        kw_map = {
            "P": [k.lower() for k in cum_rules.get('plan_keywords', [])],
            "D": [k.lower() for k in cum_rules.get('do_keywords', [])],
            "C": [k.lower() for k in cum_rules.get('check_keywords', [])],
            "A": [k.lower() for k in cum_rules.get('act_keywords', [])]
        }

        for doc in valid_chunks:
            meta = getattr(doc, 'metadata', {}) if hasattr(doc, 'metadata') else doc.get('metadata', {})
            tag = (getattr(doc, 'pdca_tag', None) or meta.get('pdca_tag') or meta.get('tag') or "").strip().upper()
            
            chunk_detected_phases = []
            
            # ‡∏ä‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà 1: ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏≤‡∏Å Tag ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
            if tag in ["P", "D", "C", "A"]:
                chunk_detected_phases.append(tag)
            
            # ‡∏ä‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà 2: Fallback Keyword Detection (‡∏ñ‡πâ‡∏≤ Tag ‡∏ß‡πà‡∏≤‡∏á ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á)
            text = (doc.get('text') or doc.get('page_content') or '').lower()
            for phase, kws in kw_map.items():
                if any(k in text for k in kws):
                    if phase not in chunk_detected_phases: # 1 chunk ‡∏ô‡∏±‡∏ö 1 ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå‡∏ï‡πà‡∏≠ 1 ‡πÄ‡∏ü‡∏™
                        chunk_detected_phases.append(phase)
            
            # ‡∏ô‡∏≥ Tags ‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠‡πÉ‡∏ô Chunk ‡∏ô‡∏µ‡πâ‡πÑ‡∏õ‡∏™‡∏∞‡∏™‡∏°‡∏£‡∏ß‡∏° (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÅ‡∏™‡∏î‡∏á‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô Log)
            all_detected_tags.extend(chunk_detected_phases)

        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Coverage Ratio (‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏°‡∏ß‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠ / 4)
        unique_found_phases = set(all_detected_tags)
        coverage_ratio = len(unique_found_phases) / 4.0

        # 4. Traceability & Recency Check
        traceable_count = 0
        recent_count = 0
        # ‚úÖ ‡πÉ‡∏ä‡πâ self.year ‡∏à‡∏≤‡∏Å argument ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏´‡πâ‡∏ñ‡∏≠‡∏¢‡πÑ‡∏õ‡πÉ‡∏ä‡πâ DEFAULT_YEAR ‡∏à‡∏≤‡∏Å global_vars
        from config.global_vars import DEFAULT_YEAR
        try:
            current_year = int(self.year) if self.year else DEFAULT_YEAR
        except (ValueError, TypeError):
            current_year = DEFAULT_YEAR
            
        self.logger.debug(f"üìä [CONFIDENCE] Evaluating recency using year baseline: {current_year}")

        
        for doc in valid_chunks:
            meta = getattr(doc, 'metadata', {}) if hasattr(doc, 'metadata') else doc.get('metadata', {})
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏Ç‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏≤ (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏ó‡∏±‡πâ‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡∏∞‡∏´‡∏ô‡πâ‡∏≤)
            if any(meta.get(k) for k in ['page', 'page_label']) and any(meta.get(k) for k in ['source', 'filename']):
                traceable_count += 1
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏´‡∏°‡πà (Bonus 2-3 ‡∏õ‡∏µ‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á)
            year_val = str(meta.get('year') or meta.get('doc_year') or "")
            if not year_val: # Fallback search in filename
                year_match = re.search(r'(25[67]\d)', str(meta.get('source', '')))
                if year_match: year_val = year_match.group(1)
            
            if year_val.isdigit() and int(year_val) >= current_year - 2:
                recent_count += 1

        trace_score = traceable_count / valid_count if valid_count > 0 else 0
        recency_score = recent_count / valid_count if valid_count > 0 else 0

        # 5. Decision Matrix: ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏±‡πà‡∏ô (Confidence Level)
        if independence_score >= 8 and coverage_ratio >= 0.75:
            conf_level = "HIGH"
            reason = "‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏£‡∏ö‡∏ß‡∏á‡∏à‡∏£ PDCA ‡∏à‡∏≤‡∏Å‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏£‡∏∞‡∏ö‡∏∏‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô"
        elif independence_score >= 4 and coverage_ratio >= 0.50:
            conf_level = "MEDIUM"
            reason = "‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏°‡∏¥‡∏ï‡∏¥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô"
        else:
            conf_level = "LOW"
            reason = "‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏Ñ‡∏£‡∏ö‡∏ß‡∏á‡∏à‡∏£ PDCA ‡∏´‡∏£‡∏∑‡∏≠‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏û‡∏≠"

        # Penalty: ‡∏•‡∏î‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏´‡∏≤‡∏Å‡∏£‡∏∞‡∏ö‡∏∏‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö
        if trace_score < 0.6 and conf_level != "LOW":
            conf_level = "MEDIUM" if conf_level == "HIGH" else "LOW"
            reason += " (‡∏•‡∏î‡∏£‡∏∞‡∏î‡∏±‡∏ö‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå)"

        return {
            "level": conf_level,
            "reason": reason,
            "source_count": independence_score,
            "coverage_ratio": round(coverage_ratio, 3),
            "traceability_score": round(trace_score, 3),
            "recency_bonus": round(recency_score, 3),
            "valid_chunks_count": valid_count,
            "pdca_found": all_detected_tags  # <--- ‡∏™‡πà‡∏á LIST ‡∏î‡∏¥‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏à‡∏£‡∏¥‡∏á‡πÑ‡∏î‡πâ
        }

    def _calculate_weighted_score(
        self, 
        highest_full_level: int, 
        weight: float, 
        level_details: Dict[str, Any] = None
    ) -> float:
        """
        [ULTIMATE REVISED v2026.01.24 - MATURITY-DRIVEN SCORING]
        - üß© Logic: Continuous Base Level + Partial PDCA from the FIRST GAP level.
        - üõ°Ô∏è Governance: ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô Partial ‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏Ç‡πâ‡∏≤‡∏°‡πÄ‡∏•‡πÄ‡∏ß‡∏• (‡∏ô‡∏±‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏•‡πÄ‡∏ß‡∏•‡∏ñ‡∏±‡∏î‡∏à‡∏≤‡∏Å‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏ï‡∏Å‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á)
        - üéØ Precision: ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Scaling ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏∞‡∏ó‡πâ‡∏≠‡∏ô Maturity ‡∏à‡∏£‡∏¥‡∏á‡∏ï‡∏≤‡∏°‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ SE-AM
        """
        # 1. Configuration Setup
        max_lv = getattr(self.config, 'max_level', 5) or 5
        safe_weight = float(weight) if weight else 4.0
        # ‡πÉ‡∏ä‡πâ‡πÇ‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏à‡∏≤‡∏Å Global Vars (PARTIAL_PDCA)
        mode = getattr(self, 'scoring_mode', SCORING_MODE)
        
        # 2. Base Maturity Calculation
        # highest_full_level ‡∏Ñ‡∏∑‡∏≠‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á (‡πÄ‡∏ä‡πà‡∏ô ‡∏ñ‡πâ‡∏≤‡∏ú‡πà‡∏≤‡∏ô L1, L2 ‡πÅ‡∏•‡πâ‡∏ß L3 ‡∏ï‡∏Å ‡∏Ñ‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô 2)
        base_level = float(max(0, min(highest_full_level, max_lv)))
        partial_contribution = 0.0

        # 3. Partial Score Logic (‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏Å‡πá‡∏ö‡∏ï‡∏Å‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà "‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô")
        if mode == 'PARTIAL_PDCA' and level_details:
            # ‡πÄ‡∏•‡πÄ‡∏ß‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏≠‡∏≤‡∏°‡∏≤‡∏Ñ‡∏¥‡∏î‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏®‡∏©‡∏™‡πà‡∏ß‡∏ô ‡∏Ñ‡∏∑‡∏≠‡πÄ‡∏•‡πÄ‡∏ß‡∏•‡∏ñ‡∏±‡∏î‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á
            # ‡πÄ‡∏ä‡πà‡∏ô ‡∏ñ‡πâ‡∏≤‡∏ú‡πà‡∏≤‡∏ô L2 ‡πÄ‡∏ï‡πá‡∏°‡∏ï‡∏±‡∏ß ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÑ‡∏õ‡∏î‡∏π PDCA ‡∏Ç‡∏≠‡∏á L3 ‡∏ß‡πà‡∏≤‡∏ó‡∏≥‡πÑ‡∏î‡πâ‡∏Å‡∏µ‡πà %
            next_lv_idx = int(base_level + 1)
            
            if next_lv_idx <= max_lv:
                lv_data = level_details.get(str(next_lv_idx), {})
                pdca = lv_data.get('pdca_breakdown', {})
                
                if isinstance(pdca, dict) and pdca:
                    # ‡∏î‡∏∂‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏≤‡∏¢ Phase (P, D, C, A) ‡∏°‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢
                    scores = [float(v) for v in pdca.values() if v is not None]
                    if scores:
                        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ç‡∏≠‡∏á PDCA (0.0 - 1.0)
                        raw_partial = sum(scores) / len(scores)
                        # ‡∏ñ‡πà‡∏ß‡∏á‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å: ‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ó‡∏®‡∏ô‡∏¥‡∏¢‡∏° (Max 0.99)
                        partial_contribution = round(raw_partial, 4)
                        
                        self.logger.info(f"‚ûï [PARTIAL-BOOST] Found Gap at L{next_lv_idx}: +{partial_contribution:.2f} PDCA progress")

        # 4. Final Maturity Level Assembly
        # Formula: ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á + ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏∑‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
        # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: ‡∏ú‡πà‡∏≤‡∏ô L2 (Base 2.0) + ‡∏ó‡∏≥ L3 ‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏∂‡πà‡∏á‡∏´‡∏ô‡∏∂‡πà‡∏á (Partial 0.5) = 2.50
        effective_level = min(base_level + partial_contribution, float(max_lv))
        
        # 5. Scaling to Weighted Score
        # ‡πÅ‡∏õ‡∏•‡∏á‡∏£‡∏∞‡∏î‡∏±‡∏ö Maturity ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ñ‡πà‡∏ß‡∏á‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏ï‡∏≤‡∏°‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô
        # Formula: (Effective Level / Max Level) * Weight
        # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: (2.5 / 5.0) * 4.0 = 2.0 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô
        base_ratio = effective_level / max_lv
        final_score = base_ratio * safe_weight

        # 6. Step-Ladder Final Touch
        # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏´‡∏°‡∏î STEP_LADDER ‡πÅ‡∏ó‡πâ‡πÜ ‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏ô‡∏±‡∏ö‡πÄ‡∏®‡∏©‡∏™‡πà‡∏ß‡∏ô (‡πÉ‡∏ä‡πâ base_level ‡πÄ‡∏û‡∏µ‡∏¢‡∏ß‡πÜ)
        if mode == 'STEP_LADDER':
            final_score = (base_level / max_lv) * safe_weight

        final_score = round(final_score, 4)
        
        # 7. Detailed Audit Logging (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÉ‡∏ô Dashboard)
        self.logger.info(
            f"üìä [SCORING-SUMMARY] Sub-ID Weighting:\n"
            f"   > Scoring Mode: {mode}\n"
            f"   > Maturity Level: {base_level} (Continuous Full)\n"
            f"   > Partial Progress: +{partial_contribution} (from Level {int(base_level+1)})\n"
            f"   > Weighted Score: {final_score} / {safe_weight} ({base_ratio:.2%} of target)"
        )
        
        return final_score
    
    def _calculate_overall_stats(self, target_sub_id: str):
        """
        [ULTIMATE REVISED v2026.1.25 - SMART AGGREGATOR]
        - üõ°Ô∏è Data Resilience: ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô 0
        - ‚öñÔ∏è Step-Ladder Logic: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏î‡∏±‡∏ö Maturity (1->5)
        - üß¨ Analytics: ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ï‡∏≤‡∏° Rubric Weight (Normalized Scoring)
        """
        from datetime import datetime
        
        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Memory ‡∏´‡∏•‡∏±‡∏Å
        results = getattr(self, 'final_subcriteria_results', [])
        
        if not results:
            self.logger.critical(f"‚ùå [STATS-FAIL] No results found for {target_sub_id}. Assessment might have crashed.")
            self.total_stats = self._get_empty_stats_template()
            return

        passed_levels_pool = []
        sub_details = []
        total_weighted_sum = 0.0
        total_weight_sum = 0.0
        
        for r in results:
            if not isinstance(r, dict): continue
            
            sub_id = r.get('sub_id', 'Unknown')
            # ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤ Weight ‡∏à‡∏≤‡∏Å Rubric (‡πÄ‡∏ä‡πà‡∏ô 4.0)
            weight = float(r.get('weight', 4.0))
            
            # 1. SMART DETECTION: ‡∏î‡∏∂‡∏á level_details
            details_map = r.get('level_details', {})
            if not details_map:
                possible_wrapper = r.get(sub_id) or r.get('results')
                if isinstance(possible_wrapper, dict):
                    details_map = possible_wrapper.get('level_details', {})

            # 2. STEP-LADDER MATURITY CALCULATION (1 -> 5)
            # ‡∏ï‡πâ‡∏≠‡∏á‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏•‡πÄ‡∏ß‡∏•‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
            current_maturity_lvl = 0
            for l_idx in range(1, 6):
                lv_data = details_map.get(str(l_idx))
                if not lv_data: break
                
                # ‡πÄ‡∏ä‡πá‡∏Ñ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡∏ú‡πà‡∏≤‡∏ô (Score >= 0.7 ‡∏´‡∏£‡∏∑‡∏≠ is_passed ‡πÄ‡∏õ‡πá‡∏ô True)
                is_passed = lv_data.get('is_passed') is True or float(lv_data.get('score', 0)) >= 0.7
                
                if is_passed:
                    current_maturity_lvl = l_idx
                else:
                    break # ‡∏ï‡∏Å‡πÄ‡∏•‡πÄ‡∏ß‡∏•‡πÑ‡∏´‡∏ô ‡∏´‡∏¢‡∏∏‡∏î‡∏ô‡∏±‡∏ö‡∏ó‡∏±‡∏ô‡∏ó‡∏µ

            # 3. NORMALIZED SCORE CALCULATION (‡πÅ‡∏Å‡πâ‡∏à‡∏≤‡∏Å‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô 20 ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ï‡∏≤‡∏° Rubric)
            # üéØ ‡∏™‡∏π‡∏ï‡∏£: (Maturity Level / 5) * Weight
            # ‡πÄ‡∏ä‡πà‡∏ô (L5 / 5) * 4.0 = 4.00 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô (‡πÄ‡∏ï‡πá‡∏° Rubric)
            # ‡πÄ‡∏ä‡πà‡∏ô (L4 / 5) * 4.0 = 3.20 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô
            sub_weighted_score = (float(current_maturity_lvl) / 5.0) * weight
            
            total_weighted_sum += sub_weighted_score
            total_weight_sum += weight

            # 4. PREPARE ANALYTICS DATA
            passed_levels_pool.append(current_maturity_lvl)
            sub_details.append({
                "sub_id": sub_id,
                "sub_name": r.get('sub_criteria_name', 'N/A'),
                "maturity": current_maturity_lvl,
                "score": round(sub_weighted_score, 2), # ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ï‡∏≤‡∏°‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô weight
                "weight": weight,
                "evidence_count": len(details_map)
            })

        # 5. FINAL CALCULATION (OVERALL)
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏° (Overall Level) ‡∏°‡∏±‡∏Å‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏ê‡∏≤‡∏ô (Min) ‡∏Ç‡∏≠‡∏á‡∏ó‡∏∏‡∏Å‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠
        overall_min = min(passed_levels_pool) if passed_levels_pool else 0
        overall_max = max(passed_levels_pool) if passed_levels_pool else 0
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Average Score (0.0 - 5.0) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ Radar Chart
        # ‡∏™‡∏π‡∏ï‡∏£: (‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ / ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏£‡∏ß‡∏°) * 5
        avg_maturity_score = (total_weighted_sum / total_weight_sum * 5.0) if total_weight_sum > 0 else 0.0

        self.total_stats = {
            "overall_max_level": int(overall_max),
            "overall_min_level": int(overall_min),
            "overall_level_label": f"L{int(overall_min)}", 
            "overall_avg_score": round(avg_maturity_score, 2), # ‡∏™‡πÄ‡∏Å‡∏• 0-5
            "total_weighted_score": round(total_weighted_sum, 2), # ‡∏™‡πÄ‡∏Å‡∏•‡∏ï‡∏≤‡∏°‡∏ú‡∏•‡∏£‡∏ß‡∏° weight
            "total_weight": round(total_weight_sum, 2),
            "evaluated_at": datetime.now().isoformat(),
            "status": "SUCCESS",
            "analytics": {
                "sub_details": sub_details,
                "total_sub_items": len(results)
            }
        }

        self.logger.info(
            f"‚úÖ [AGGREGATION SUCCESS] Maturity: {self.total_stats['overall_level_label']} | "
            f"Final Score: {self.total_stats['total_weighted_score']}/{total_weight_sum}"
        )

    def _get_empty_stats_template(self):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á Template ‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Error ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Export"""
        return {
            "overall_max_level": 0,
            "overall_min_level": 0,
            "overall_level_label": "L0",
            "overall_avg_score": 0.0,
            "total_weighted_score": 0.0,
            "total_weight": 0.0,
            "status": "NO_DATA"
        }

    def _export_results(self, results_data: Any, sub_criteria_id: str, **kwargs) -> str:
        """
        [ULTIMATE EXPORTER v2026.1.25 - DATA INTEGRITY]
        - üõ°Ô∏è Score Sync: ‡∏î‡∏∂‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡∏à‡∏≤‡∏Å total_stats ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô 0
        - üß¨ Evidence Recovery: ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Map ‡∏ó‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö List ‡πÅ‡∏•‡∏∞ Dict (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Map ‡∏´‡∏≤‡∏¢)
        - üìä Deep Audit Trail: ‡πÄ‡∏Å‡πá‡∏ö Snippet ‡πÅ‡∏•‡∏∞ Confidence ‡∏£‡∏≤‡∏¢‡πÄ‡∏•‡πÄ‡∏ß‡∏•‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏¢‡πâ‡∏≠‡∏ô‡∏Å‡∏•‡∏±‡∏ö
        """
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            record_id = kwargs.get("record_id", getattr(self, "current_record_id", f"auto_{timestamp}"))
            tenant = getattr(self.config, 'tenant', 'unknown')
            year = getattr(self.config, 'year', 'unknown')
            enabler = getattr(self, 'enabler', 'unknown').upper()

            # 1. üîç Data Source Selection (‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•)
            # ‡∏•‡∏≥‡∏î‡∏±‡∏ö: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏°‡∏≤ > ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô Memory > ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ß‡πà‡∏≤‡∏á
            if results_data is None:
                results_data = getattr(self, 'final_subcriteria_results', [])
            
            if isinstance(results_data, dict):
                results_data = [results_data]
            
            if not results_data:
                self.logger.warning(f"‚ö†Ô∏è [EXPORT] No result data found for {sub_criteria_id}")
                return ""

            # 2. üìä Summary Retrieval (‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏£‡∏ß‡∏°)
            # ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å total_stats ‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Smart Mapping ‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß
            stats = getattr(self, 'total_stats', {})
            if not stats or stats.get('total_weighted_score') == 0:
                # Fallback: ‡∏ñ‡πâ‡∏≤ stats ‡∏ß‡πà‡∏≤‡∏á ‡πÉ‡∏´‡πâ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏™‡∏î‡∏à‡∏≤‡∏Å results_data
                highest_lvl = max([int(r.get('highest_full_level', 0)) for r in results_data])
                total_weighted = sum([float(r.get('weighted_score', 0.0)) for r in results_data])
                is_passed = highest_lvl >= 1
            else:
                highest_lvl = stats.get('overall_max_level', 0)
                total_weighted = stats.get('total_weighted_score', 0.0)
                is_passed = stats.get('overall_level_label') != "L0"

            # 3. üõ°Ô∏è Robust Evidence Mapping (The Fix for Empty Maps)
            master_map = getattr(self, 'evidence_map', {})
            processed_evidence = {}
            
            for lv_key, val in master_map.items():
                if not val: continue
                
                # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà {"evidences": [...]} ‡πÅ‡∏•‡∏∞‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏Å‡πà‡∏≤ [...]
                v_list = val.get("evidences", []) if isinstance(val, dict) else val
                
                if not isinstance(v_list, list) or not v_list:
                    continue
                
                try:
                    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ Rerank Score ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡πÄ‡∏•‡πÄ‡∏ß‡∏•‡∏ô‡∏±‡πâ‡∏ô
                    sorted_ev = sorted(
                        [ev for ev in v_list if isinstance(ev, dict)], 
                        key=lambda x: x.get('rerank_score', x.get('relevance_score', 0)), 
                        reverse=True
                    )
                    
                    if sorted_ev:
                        top_ev = sorted_ev[0]
                        doc_id = top_ev.get("doc_id") or top_ev.get("stable_doc_uuid")
                        
                        # ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏≤‡∏Å Map ‡∏Å‡∏•‡∏≤‡∏á
                        filename = self.document_map.get(doc_id) if hasattr(self, 'document_map') else None
                        filename = filename or top_ev.get("filename") or top_ev.get("source") or "Unknown_Source"

                        processed_evidence[str(lv_key)] = {
                            "file": filename,
                            "page": top_ev.get("page", top_ev.get("page_label", "N/A")),
                            "pdca": str(top_ev.get("pdca_tag", "N/A")).upper(),
                            "confidence": round(float(top_ev.get("rerank_score", 0)), 4),
                            "snippet": str(top_ev.get("content", ""))[:150] + "..."
                        }
                except Exception as ev_err:
                    self.logger.debug(f"‚ö†Ô∏è Skip evidence key {lv_key}: {ev_err}")

            # 4. üìù Build Final Payload (Standard Schema v2026)
            payload = {
                "metadata": {
                    "record_id": record_id,
                    "tenant": tenant,
                    "year": year,
                    "enabler": enabler,
                    "engine_version": "SEAM-ENGINE-v2026.1.25",
                    "exported_at": datetime.now().isoformat()
                },
                "result_summary": {
                    "maturity_level": stats.get('overall_level_label', f"L{highest_lvl}"),
                    "is_passed": is_passed,
                    "total_weighted_score": round(total_weighted, 4),
                    "evidence_used_count": len(processed_evidence),
                    "evaluated_sub_count": len(results_data),
                    "status": "COMPLETED"
                },
                "sub_criteria_details": results_data,
                "evidence_audit_trail": processed_evidence,
                "strategic_roadmap": getattr(self, 'master_roadmap_data', {
                    "status": "GENERATED",
                    "overall_strategy": "‡πÇ‡∏õ‡∏£‡∏î‡∏î‡∏π‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô sub_criteria_details"
                })
            }

            # 5. üíæ Save to JSON
            # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÉ‡∏ä‡πâ path ‡∏à‡∏≤‡∏Å config ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ local exports
            try:
                from utils.path_utils import get_assessment_export_file_path
                export_path = get_assessment_export_file_path(
                    tenant=tenant, year=year, enabler=enabler.lower(),
                    suffix=f"{sub_criteria_id}_{timestamp}", ext="json"
                )
            except ImportError:
                out_dir = f"exports/{tenant}/{year}/{enabler.lower()}"
                os.makedirs(out_dir, exist_ok=True)
                export_path = f"{out_dir}/REPORT_{sub_criteria_id}_{timestamp}.json"

            with open(export_path, 'w', encoding='utf-8') as f:
                json.dump(payload, f, indent=2, ensure_ascii=False)

            self.logger.info(f"‚úÖ [EXPORT SUCCESS] Report generated: {export_path}")
            return export_path

        except Exception as e:
            self.logger.error(f"üõë [EXPORT CRITICAL ERROR] {str(e)}", exc_info=True)
            return ""
    

    def _guarantee_text_key(
        self,
        chunks: List[Dict],
        total_count: int = 0,
        restored_count: int = 0
    ) -> List[Dict]:
        """
        Guarantee 'text' key exists in every chunk
        """
        final_chunks = []

        for chunk in chunks:
            if "text" not in chunk or not isinstance(chunk["text"], str):
                chunk["text"] = ""
                cid = str(chunk.get("chunk_uuid", "N/A"))
                self.logger.debug(f"Guaranteed 'text' key for chunk (ID: {cid[:8]})")
            final_chunks.append(chunk)

        if total_count > 0:
            baseline_count = sum(1 for c in final_chunks if c.get("is_baseline"))
            self.logger.info(
                f"HYDRATION FINAL: Restored {restored_count}/{total_count} "
                f"(Baseline={baseline_count}, Total final={len(final_chunks)})"
            )

        return final_chunks

    
    def _normalize_meta(self, c: Dict) -> Tuple[str, str]:
        """
        [REVISED] ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡∏∞‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏£‡∏∞‡∏ö‡∏ö Priority Fallback
        - ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤ 0 ‡∏´‡∏≤‡∏¢ (Index-based)
        - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏à‡∏≤‡∏Å Ingest ‡πÉ‡∏´‡∏°‡πà ‡πÅ‡∏•‡∏∞ Reranker Output
        - ‡πÄ‡∏ô‡πâ‡∏ô page_label ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏ö‡∏ô UI
        """
        if not isinstance(c, dict):
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡πá‡∏ô LangChain Document ‡πÉ‡∏´‡πâ‡∏î‡∏∂‡∏á metadata ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤
            if hasattr(c, "metadata"):
                meta = c.metadata
                # ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á dict ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÇ‡∏Ñ‡πâ‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏•‡πà‡∏≤‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏î‡πâ
                c = {"metadata": meta}
            else:
                return "Unknown Source", "N/A"

        # ‡∏î‡∏∂‡∏á metadata ‡∏°‡∏≤‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÑ‡∏ß‡πâ (‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏Å‡∏£‡∏ì‡∏µ None ‡∏´‡∏£‡∏∑‡∏≠ Dict)
        meta = c.get("metadata") or {}

        # 1. üîç ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå (Source Name Priority)
        source_priority = [
            c.get("source_filename"),
            meta.get("source_filename"),
            c.get("filename"),
            meta.get("source"),
            meta.get("file_name"),
            c.get("id") 
        ]
        
        source = "Unknown"
        for s in source_priority:
            if s and str(s).strip():
                source = str(s).strip()
                break

        # 2. üîç ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤ (Page Label Priority)
        page_keys = ["page_label", "page", "page_number"]
        found_page = None
        for key in page_keys:
            val_root = c.get(key)
            if val_root is not None and str(val_root).strip().lower() != "n/a":
                found_page = val_root
                break
            val_meta = meta.get(key)
            if val_meta is not None and str(val_meta).strip().lower() != "n/a":
                found_page = val_meta
                break

        # 3. ‚ú® Final Cleaning & UI Formatting
        clean_source = os.path.basename(source) if "/" in source or "\\" in source else source
        if found_page is not None:
            page_str = str(found_page).strip()
            clean_page = page_str if page_str.lower() != "n/a" else "N/A"
        else:
            clean_page = "N/A"

        return clean_source, clean_page
    
    def audit_agent_router(
        self,
        *,
        context: str,
        sub_criteria_name: str,
        level: int,
        statement_text: str,
        sub_id: str,
        llm_executor,
        confidence_reason: str = "",
        **kwargs
    ):
        """
        [AUDIT AGENT ROUTER ‚Äì FINAL]
        - L1‚ÄìL2 ‚Üí foundation_coaching_agent
        - L3‚ÄìL5 ‚Üí standard_audit_agent
        - ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏ä‡πâ llm_executor ‡πÅ‡∏ö‡∏ö keyword
        """

        if llm_executor is None:
            raise RuntimeError("LLM executor missing in audit_agent_router")

        # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å agent ‡∏ï‡∏≤‡∏° level
        if level <= 2:
            agent = self.foundation_coaching_agent
        else:
            agent = self.standard_audit_agent

        return agent(
            context=context,
            sub_criteria_name=sub_criteria_name,
            level=level,
            statement_text=statement_text,
            sub_id=sub_id,
            llm_executor=llm_executor,        # üî• keyword ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
            confidence_reason=confidence_reason,
            **kwargs
        )


    def evaluate_pdca(
        self,
        pdca_blocks: Union[Dict[str, Any], str],
        sub_id: str,
        level: int,
        audit_confidence: Any,
        audit_instruction: str = ""
    ) -> Dict[str, Any]:
        """
        [FINAL CANONICAL VERSION]
        - ‡πÉ‡∏ä‡πâ audit_agent_router ‡πÄ‡∏õ‡πá‡∏ô entry point ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
        - L1‚ÄìL2 ‚Üí evaluate_with_llm_low_level
        - L3‚ÄìL5 ‚Üí evaluate_with_llm
        - ‡πÅ‡∏Å‡πâ‡∏Ñ‡∏£‡∏ö:
            ‚Ä¢ LLM instance not initialized
            ‚Ä¢ agent routing ‡∏ú‡∏¥‡∏î logic
            ‚Ä¢ duplicate argument
            ‚Ä¢ positional vs keyword mismatch
        """

        log_prefix = f"üß† [{sub_id}-L{level}]"

        # --------------------------------------------------
        # [1] Build PDCA Context
        # --------------------------------------------------
        pdca_summary = []

        if isinstance(pdca_blocks, dict):
            for tag in ["P", "D", "C", "A"]:
                val = pdca_blocks.get(tag)
                if val:
                    clean_val = str(val).replace('"', "'")
                    pdca_summary.append(
                        f"### {tag} PHASE EVIDENCE ###\n{clean_val}"
                    )
        else:
            pdca_summary.append(str(pdca_blocks))

        final_context_str = "\n\n".join(pdca_summary)

        # --------------------------------------------------
        # [2] Rubric Lookup
        # --------------------------------------------------
        sub_item = next(
            (i for i in self.flattened_rubric if i.get("sub_id") == sub_id),
            {}
        )

        sub_name = sub_item.get("sub_criteria_name", sub_id)

        level_info = next(
            (lv for lv in sub_item.get("levels", []) if lv.get("level") == level),
            {}
        )

        statement = level_info.get("statement", "")

        # --------------------------------------------------
        # [3] Confidence Normalize
        # --------------------------------------------------
        try:
            if isinstance(audit_confidence, dict):
                conf_val = float(audit_confidence.get("coverage_ratio", 0.0))
            else:
                conf_val = float(audit_confidence or 0.0)
        except Exception:
            conf_val = 0.0

        # --------------------------------------------------
        # [4] Ensure LLM Ready (CRITICAL)
        # --------------------------------------------------
        if self.llm is None:
            self._initialize_llm_if_none()

        if self.llm is None:
            raise RuntimeError("LLM instance not initialized (post-init).")

        # --------------------------------------------------
        # [5] Build Agent Payload (KEYWORD-ONLY)
        # --------------------------------------------------
        agent_payload = {
            # core
            "context": final_context_str,
            "pdca_context": final_context_str,

            # rubric
            "sub_id": sub_id,
            "sub_criteria_name": sub_name,
            "level": level,
            "statement_text": statement,

            # llm
            "llm_executor": self.llm,

            # confidence
            "confidence_reason": f"Coverage: {conf_val:.2f}",
            "ai_confidence": "HIGH" if conf_val >= 0.7 else "MEDIUM",

            # enrichment
            "enabler": self.enabler,
            "enabler_full_name": getattr(
                self, "enabler_full_name", f"‡∏î‡πâ‡∏≤‡∏ô {self.enabler}"
            ),
            "focus_points": sub_item.get("focus_points", "-"),
            "evidence_guidelines": level_info.get(
                "level_specific_guideline", "-"
            ),
            "specific_contextual_rule": audit_instruction,
        }

        # --------------------------------------------------
        # [6] Execute via audit_agent_router (ONLY ENTRY POINT)
        # --------------------------------------------------
        try:
            return self.audit_agent_router(**agent_payload)

        except Exception as e:
            self.logger.error(
                f"üõë [EVAL-ERROR] {log_prefix}: {str(e)}",
                exc_info=True
            )
            return {
                "sub_id": sub_id,
                "level": level,
                "score": 0.0,
                "is_passed": False,
                "reason": f"Evaluation Failure: {str(e)}"
            }

        
    def _prepare_worker_tuple(self, sub_criteria_data: Dict, document_map: Optional[Dict]) -> Tuple:
        """‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤ Process ‡πÉ‡∏´‡∏°‡πà (Pickle-friendly)"""
        return (
            sub_criteria_data,                 # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Rubric ‡∏£‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠
            self.config.enabler,               # Enabler Code
            self.config.target_level,          # ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢
            self.config.mock_mode,             # ‡πÇ‡∏´‡∏°‡∏î Mock
            getattr(self, 'evidence_map_path', None), 
            self.config.model_name,            # ‡∏ä‡∏∑‡πà‡∏≠‡∏£‡∏∏‡πà‡∏ô LLM
            self.config.temperature,           # ‡∏Ñ‡πà‡∏≤ Temp
            self.config.min_retry_score,       # ‡πÄ‡∏Å‡∏ì‡∏ë‡πå Rerank
            self.config.max_retrieval_attempts,
            document_map or self.document_map, # ID Mapping
            None,                              # Action Plan Model (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
            self.config.year,
            self.config.tenant
        )
    
    def _create_failed_result(self, record_id: str, message: str, start_ts: float) -> Dict[str, Any]:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô Response ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏Å‡∏¥‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô Orchestrator"""
        self.logger.error(f"‚ùå Assessment Failed: {message}")
        return {
            "record_id": record_id,
            "status": "FAILED",
            "error": message,
            "run_time_seconds": round(time.time() - start_ts, 2),
            "summary": {},
            "sub_criteria_results": {}
        }
    
    def _normalize_evidence_metadata(self, evidence_list: List[Dict[str, Any]]):
        """
        [ULTIMATE REVISED v2026.01.25]
        - üõ°Ô∏è Type Safety & Resolve: ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Metadata ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
        """
        for ev in evidence_list:
            if not isinstance(ev, dict): continue
            
            meta = ev.get("metadata", {})
            if not isinstance(meta, dict): meta = {}
            
            # 1. Resolve ID & UUID
            doc_id = (
                ev.get("doc_id") or 
                ev.get("stable_doc_uuid") or 
                meta.get("stable_doc_uuid") or 
                meta.get("doc_id") or
                f"gen_{uuid.uuid4().hex[:8]}"
            )
            ev["doc_id"] = doc_id
            ev["stable_doc_uuid"] = doc_id

            # 2. Resolve Filename (‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏´‡∏•‡πà‡∏á‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á document_map)
            raw_source = (
                meta.get("source_filename") or 
                meta.get("file_name") or 
                ev.get("filename") or 
                ev.get("source") or 
                meta.get("source")
            )
            filename = os.path.basename(str(raw_source)) if raw_source else "Unknown_File"
            
            # Cross-check ‡∏Å‡∏±‡∏ö‡∏Ñ‡∏•‡∏±‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏Å‡∏•‡∏≤‡∏á
            if (filename == "Unknown_File" or not filename) and hasattr(self, 'document_map'):
                filename = self.document_map.get(doc_id, "Unknown_File")
                
            ev["filename"] = filename
            ev["source_filename"] = filename
            ev["source"] = filename

            # 3. Resolve Page Label
            raw_page = meta.get("page_label") or meta.get("page") or meta.get("page_number") or ev.get("page") or "0"
            ev["page"] = str(raw_page)

            # 4. Resolve Scoring (‡πÉ‡∏ä‡πâ Get Actual Score ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏£‡∏∞‡∏ö‡∏∏)
            actual_score = 0.0
            if hasattr(self, 'get_actual_score'):
                actual_score = self.get_actual_score(ev)
            else:
                actual_score = float(ev.get("relevance_score") or ev.get("rerank_score") or 0.0)
            ev["relevance_score"] = actual_score

            # 5. UI Fields Consistency
            ev["source_type"] = ev.get("source_type") or meta.get("source_type") or "system_gen"
            ev["is_selected"] = ev.get("is_selected") if ev.get("is_selected") is not None else True
            ev["pdca_tag"] = ev.get("pdca_tag") or meta.get("pdca_tag") or "Other"
            ev["note"] = ev.get("note") or ""

        return evidence_list
    
    # ------------------------------------------------------------------------------------------
    # [ULTIMATE REVISE v2026.01.30] üß† LAYER 1: Decision Engine (The Brain ‚Äì Final Hardened)
    # ------------------------------------------------------------------------------------------
    def _get_semantic_tag(self, text: str, sub_id: str, level: int, filename: str = "") -> str:
        """
        [ULTIMATE REVISED v2026.01.31]
        ‡∏®‡∏π‡∏ô‡∏¢‡πå‡∏Å‡∏•‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à Tag: Heuristic ‚Üí AI Semantic ‚Üí Contextual Fallback
        ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Multi-Tenant ‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î Config ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ ‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Error ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡∏ö‡∏ö
        - ‡πÄ‡∏û‡∏¥‡πà‡∏° fallback ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö enabler_name_th ‡πÅ‡∏•‡∏∞ keywords
        - ‡πÄ‡∏û‡∏¥‡πà‡∏° log ‡∏ó‡∏∏‡∏Å‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠ debug ‡∏á‡πà‡∏≤‡∏¢
        - LLM call ‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢ + retry JSON parse
        - ‡∏•‡∏î "Other" ‡πÇ‡∏î‡∏¢‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö fallback phase ‡∏ñ‡πâ‡∏≤ LLM ‡πÉ‡∏´‡πâ Other
        """
        # --- [PREPARATION] ‡∏î‡∏∂‡∏á Metadata ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô ---
        enabler_key = getattr(self.config, 'enabler', 'DEFAULT').upper()
        # fallback ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡πá‡∏°‡∏ñ‡πâ‡∏≤ dict ‡πÑ‡∏°‡πà‡∏°‡∏µ key ‡∏´‡∏£‡∏∑‡∏≠ error
        try:
            enabler_name_th = SEAM_ENABLER_FULL_NAME_TH.get(enabler_key, f"‡∏î‡πâ‡∏≤‡∏ô {enabler_key}")
        except NameError:
            enabler_name_th = f"‡∏î‡πâ‡∏≤‡∏ô {enabler_key}"
            self.logger.error("[CRITICAL] SEAM_ENABLER_FULL_NAME_TH not defined ‚Üí fallback")

        if enabler_name_th == f"‡∏î‡πâ‡∏≤‡∏ô {enabler_key}":
            self.logger.warning(f"[FALLBACK-NAME] No full name for {enabler_key} ‚Üí using '{enabler_name_th}'")

        # fallback keywords ‡∏ñ‡πâ‡∏≤ dict ‡πÑ‡∏°‡πà‡∏°‡∏µ key ‡∏´‡∏£‡∏∑‡∏≠ error
        try:
            enabler_keywords = PDCA_CONFIG_MAP.get(enabler_key, PDCA_CONFIG_MAP["DEFAULT"])
        except NameError:
            enabler_keywords = PDCA_CONFIG_MAP["DEFAULT"]
            self.logger.error("[CRITICAL] PDCA_CONFIG_MAP not defined ‚Üí using DEFAULT")

        # ‡∏î‡∏∂‡∏á require_phase (reuse)
        require_phases = self.get_rule_content(sub_id, level, "require_phase") or []

        # Tenant info + fallback
        tenant_id = getattr(self.config, 'tenant', 'default').lower()
        tenant_name_th = "‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô"
        tenant_code = tenant_id.upper()
        tenant_info_path = get_tenant_info_file_path(tenant_id)
        if os.path.exists(tenant_info_path):
            try:
                with open(tenant_info_path, 'r', encoding='utf-8') as f:
                    t_data = json.load(f)
                    tenant_name_th = t_data.get("tenant_name_th", tenant_name_th)
                    tenant_code = t_data.get("tenant_abbreviation", tenant_code)
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è Load tenant_info failed: {e}")

        # Log preparation summary (‡∏ä‡πà‡∏ß‡∏¢ debug)
        self.logger.debug(f"[TAG-PREPARE] Enabler: {enabler_key} ({enabler_name_th}) | Tenant: {tenant_code} ({tenant_name_th}) | Req Phases: {require_phases}")

        text_clean = (text or "").strip()
        if len(text_clean) < 20:
            fallback = require_phases[0] if require_phases else ("P" if level == 1 else "D")
            self.logger.debug(f"[TAG-SHORT] Text too short ‚Üí fallback {fallback} | {filename[:30]}")
            return fallback

        text_lower = text_clean.lower()

        # --- [LAYER 1] Heuristic: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢ Keyword ---
        for tag, keywords in enabler_keywords.items():
            if any(k.lower() in text_lower for k in keywords):
                self.logger.debug(f"‚ö° [HEURISTIC-HIT] {enabler_key}:{tag} | {filename[:30]}")
                return tag

        # --- [LAYER 2] AI Semantic: ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏î‡πâ‡∏ß‡∏¢ LLM ---
        require_str = ", ".join(require_phases) if require_phases else "P, D, C, A"
        desc_bullets = "\n".join(f"- {v}" for v in PDCA_PHASE_DESCRIPTIONS.values())

        system_prompt = (
            f"‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£ **{tenant_name_th}** ({tenant_code}) "
            f"‡πÉ‡∏ô‡∏î‡πâ‡∏≤‡∏ô '{enabler_name_th}' ({enabler_key}) ‡∏ï‡∏≤‡∏°‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô SE-AM\n"
            f"‡∏†‡∏≤‡∏£‡∏Å‡∏¥‡∏à: ‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡∏´‡∏°‡∏ß‡∏î PDCA ‡∏ï‡∏≤‡∏°‡∏ô‡∏¥‡∏¢‡∏≤‡∏°:\n{desc_bullets}\n\n"
            f"‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏£‡∏∞‡∏î‡∏±‡∏ö Level {level}: **‡πÄ‡∏ô‡πâ‡∏ô‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤ {require_str} ‡πÄ‡∏õ‡πá‡∏ô‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å**\n"
            f"‡∏ñ‡πâ‡∏≤‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á phase ‡πÉ‡∏ô {require_str} ‡πÉ‡∏´‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å tag ‡∏ô‡∏±‡πâ‡∏ô‡∏Å‡πà‡∏≠‡∏ô 'Other'\n"
            f"‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏î‡∏≤ ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏à‡∏£‡∏¥‡∏á ‡πÜ ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ 'Other'\n\n"
            f"‡∏ï‡∏≠‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞ JSON Object ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡∏´‡πâ‡∏≤‡∏°‡∏°‡∏µ Markdown ‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:\n"
            f"{{'tag': 'P' | 'D' | 'C' | 'A' | 'Other', 'reason': '‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏™‡∏±‡πâ‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢'}}\n\n"
            f"‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á (‡∏ö‡∏£‡∏¥‡∏ö‡∏ó {tenant_name_th}):\n"
            f"{{'tag': 'P', 'reason': '‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡πÅ‡∏ú‡∏ô‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå {enabler_key} ‡∏Ç‡∏≠‡∏á {tenant_code}'}}\n"
            f"{{'tag': 'Other', 'reason': '‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö PDCA ‡∏´‡∏£‡∏∑‡∏≠ {enabler_name_th}'}}\n\n"
            f"‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏ä‡πâ ```json ‡∏´‡∏£‡∏∑‡∏≠‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÅ‡∏ö‡∏ö Unicode escape"
        )

        user_prompt = f"‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå: {filename}\n‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤: {text_clean[:600]}\n‡∏£‡∏∞‡∏ö‡∏∏ Tag ‡πÅ‡∏•‡∏∞‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•"

        try:
            json_str = _fetch_llm_response(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                llm_executor=self.llm,
                max_retries=3
            )
            
            self.logger.debug(f"[LLM-RAW-TAG] {filename[:30]} | {json_str[:300]}...")
            
            # ‡∏ã‡πà‡∏≠‡∏° JSON ‡∏ñ‡πâ‡∏≤‡∏û‡∏±‡∏á (‡∏•‡∏ö code block)
            json_str = re.sub(r'```json\s*|\s*```', '', json_str).strip()
            
            data = json.loads(json_str)
            if isinstance(data, list) and data: data = data[0]
            
            tag = str(data.get("tag", "Other")).upper().strip()
            reason = data.get("reason", "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•")
            
            if tag in {"P", "D", "C", "A"}:
                self.logger.info(f"üéØ [AI-TAG SUCCESS] {tenant_code} | {enabler_key}:{tag} | reason: {reason[:60]}")
                return tag
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è [AI-TAG ERR] {tenant_code}:{enabler_key}: {str(e)}")

        # --- [LAYER 3] Contextual Fallback (‡πÑ‡∏°‡πà‡∏¢‡∏≠‡∏° Other ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô level ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç) ---
        if require_phases:
            primary_phase = require_phases[0]
            self.logger.debug(f"[FALLBACK-REQ] {primary_phase} (from {require_str})")
            return primary_phase

        # Ultimate fallback ‡∏ï‡∏≤‡∏° Maturity Level
        fallback = "P" if level == 1 else "D" if level <= 3 else "C" if level == 4 else "A"
        self.logger.debug(f"[ULTIMATE-FALLBACK] {fallback} for L{level}")
        return fallback

    def _get_heuristic_pdca_tag(self, text: str, level: int) -> Optional[str]:
        t = text.lower()
        
        # Do-specific ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö L1 (‡πÄ‡∏ô‡πâ‡∏ô‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏ó‡∏≥‡∏à‡∏£‡∏¥‡∏á)
        do_keywords = [
            "‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£", "‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥", "‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°", "‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°", "‡∏≠‡∏ö‡∏£‡∏°", "‡∏à‡∏±‡∏î‡∏ó‡∏≥", "‡∏•‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà", 
            "‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏ú‡∏•", "‡∏†‡∏≤‡∏û‡∏ñ‡πà‡∏≤‡∏¢", "‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£", "‡∏°‡∏∏‡πà‡∏á‡∏°‡∏±‡πà‡∏ô", "‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á", "‡∏™‡∏ô‡∏±‡∏ö‡∏™‡∏ô‡∏∏‡∏ô", 
            "‡∏Ç‡∏±‡∏ö‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô", "‡∏ô‡∏≥‡∏£‡πà‡∏≠‡∏á", "‡∏•‡∏á‡∏°‡∏∑‡∏≠‡∏ó‡∏≥", "‡∏ô‡∏≥‡πÑ‡∏õ‡πÉ‡∏ä‡πâ"
        ]
        if level <= 2 and any(k in t for k in do_keywords):
            return "D"

        # Check keywords (‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å log)
        check_keywords = [
            "‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏•", "‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô", "‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°", "‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î", "kpi", "‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå", "‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•", 
            "‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥", "‡∏™‡∏≥‡∏£‡∏ß‡∏à", "‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö", "‡∏ß‡∏±‡∏î‡∏ú‡∏•"
        ]
        if any(k in t for k in check_keywords):
            return "C"

        # Plan & Act (‡∏Ñ‡∏á‡πÄ‡∏î‡∏¥‡∏° ‡πÅ‡∏ï‡πà‡∏•‡∏î priority)
        if any(k in t for k in ["‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢", "‡πÅ‡∏ú‡∏ô", "‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå", "‡∏°‡∏ï‡∏¥", "‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á", "‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢", "‡πÄ‡∏à‡∏ï‡∏ô‡∏≤‡∏£‡∏°‡∏ì‡πå"]):
            return "P"
        if any(k in t for k in ["‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á", "‡∏û‡∏±‡∏í‡∏ô‡∏≤", "‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç", "‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô", "lesson learned", "‡∏ï‡πà‡∏≠‡∏¢‡∏≠‡∏î", "‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°"]):
            return "A"

        return None
    
    # ------------------------------------------------------------------------------------------
    # [ULTIMATE REVISE v2026.01.28] üìä LAYER 2: Contextual Blocker (The Focus)
    # ------------------------------------------------------------------------------------------
    def _get_pdca_blocks_from_evidences(
        self,
        evidences: List[Dict[str, Any]],
        baseline_evidences: List[Dict[str, Any]],
        level: int,
        sub_id: str,
        contextual_rules_map: Dict[str, Any],
        record_id: str = None
    ) -> Dict[str, Any]:
        """
        ‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÅ‡∏ö‡∏ö Multi-Layer Tagging:
        1. Semantic (AI/Engine) -> 2. Heuristic (Keyword-based) -> 3. Forced (Fallback)
        """

        pdca_groups = defaultdict(list)
        seen_texts = set()
        all_candidate = (evidences or []) + (baseline_evidences or [])

        # ‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠ Phase ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡πÄ‡∏•‡πÄ‡∏ß‡∏•‡∏ô‡∏µ‡πâ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ (‡πÄ‡∏ä‡πà‡∏ô L1 ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ P ‡πÅ‡∏•‡∏∞ D)
        require_phases = self.get_rule_content(sub_id, level, "require_phase") or ["P", "D"]

        for idx, chunk in enumerate(all_candidate, start=1):
            # --- 1. Data Cleaning & Deduplication ---
            txt = (chunk.get("text") or chunk.get("page_content") or "").strip()
            if not txt or len(txt) < 10:
                continue

            txt_hash = hashlib.sha256(txt.encode()).hexdigest()
            if txt_hash in seen_texts:
                continue
            seen_texts.add(txt_hash)

            # --- 2. Metadata Preparation ---
            meta = chunk.get("metadata", {}) or {}
            fname = chunk.get("source_filename") or meta.get("source_filename") or "Unknown"
            page = meta.get("page_label") or meta.get("page") or "N/A"
            is_baseline = chunk.get("source") == "BASELINE" or chunk.get("is_baseline", False)
            
            prefix = "[BASELINE] " if is_baseline else ""
            source_display = f"{prefix}{fname} (P.{page})"

            # --- 3. MULTI-LAYER TAGGING LOGIC (‡∏´‡∏±‡∏ß‡πÉ‡∏à‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç) ---
            is_forced = False
            
            # Layer 1: Semantic Tag (‡∏•‡∏≠‡∏á‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å AI/Engine ‡πÄ‡∏î‡∏¥‡∏°)
            final_tag = self._get_semantic_tag(txt, sub_id, level, fname)
            tag_source = "Semantic-Engine"

            # Layer 2: Heuristic Fallback (‡∏ñ‡πâ‡∏≤ Layer 1 ‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏î‡πâ‡∏Ñ‡πà‡∏≤‡∏Å‡∏•‡∏≤‡∏á‡πÜ)
            if final_tag in [None, "Other", "OTHER", "N/A"]:
                heuristic_tag = self._get_heuristic_pdca_tag(text=txt, level=level)
                if heuristic_tag:
                    final_tag = heuristic_tag
                    tag_source = "Heuristic-Rule-Base"

            # Layer 3: Forced Contextual Fallback (‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢)
            if final_tag in [None, "Other", "OTHER", "N/A"]:
                if level >= 4:
                    # ‡πÄ‡∏•‡πÄ‡∏ß‡∏•‡∏™‡∏π‡∏á‡πÄ‡∏£‡∏≤‡πÄ‡∏ô‡πâ‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û ‡πÑ‡∏°‡πà‡πÅ‡∏°‡πà‡∏ô‡∏à‡∏£‡∏¥‡∏á‡πÄ‡∏£‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏≠‡∏≤‡∏°‡∏≤‡∏Ñ‡∏¥‡∏î (Strict Mode)
                    self.logger.debug(f"üö´ Excluded Other (L{level} strict): {source_display}")
                    continue

                is_forced = True
                # ‡πÅ‡∏à‡∏Å Tag ‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö Require Phase (Round-robin)
                final_tag = require_phases[(idx - 1) % len(require_phases)]
                tag_source = f"Forced-Contextual-L{level} ({final_tag})"
                self.logger.debug(f"‚ö†Ô∏è Forced {final_tag} ‚Üí {source_display}")

            # --- 4. Append to Group ---
            pdca_groups[final_tag].append({
                "text": txt,
                "source_display": source_display,
                "filename": fname,
                "page": page,
                "is_forced": is_forced,
                "is_baseline": is_baseline,
                "relevance": float(chunk.get("rerank_score") or chunk.get("score") or 0.5),
                "tag_source": tag_source,
                "pdca_tag": final_tag  # üëà ‡∏™‡πà‡∏á‡∏Ñ‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Router/UI ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á
            })

        # --- 5. Block Construction for LLM ---
        max_ch = getattr(self.config, 'MAX_CHUNKS_PER_BLOCK', 5)
        blocks = {
            "sources": {}, 
            "actual_counts": {},
            "all_evidences_with_tags": [] # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡πà‡∏á‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏ó‡∏≥ Report metadata
        }

        for tag in ["P", "D", "C", "A"]:
            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠: ‡∏Ç‡∏≠‡∏á‡∏à‡∏£‡∏¥‡∏á(‡πÑ‡∏°‡πà forced) > ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á > ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà baseline
            ranked = sorted(
                pdca_groups.get(tag, []),
                key=lambda x: (x["is_forced"], -x["relevance"], x["is_baseline"])
            )[:max_ch]

            if ranked:
                blocks[tag] = "\n\n".join([
                    f"[{c['source_display']} | {c['tag_source']}{' ‚ö†Ô∏èFORCED' if c['is_forced'] else ''}]\n"
                    f"{c['text'][:1000]}"
                    for c in ranked
                ])
                # ‡πÄ‡∏Å‡πá‡∏ö‡∏Å‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÑ‡∏ß‡πâ‡∏ó‡∏≥ UI JSON
                blocks["all_evidences_with_tags"].extend(ranked)
            else:
                blocks[tag] = f"[‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÉ‡∏ô‡∏´‡∏°‡∏ß‡∏î {tag}]"

            blocks["sources"][tag] = [c["source_display"] for c in ranked]
            blocks["actual_counts"][tag] = len([c for c in ranked if not c["is_forced"]])

        return blocks

    # ------------------------------------------------------------------------------------------
    # [ULTIMATE REVISE v2026.01.28] üíæ LAYER 3: Persistence & Retroactive Sync
    # ------------------------------------------------------------------------------------------
    def _save_level_evidences_and_calculate_strength(
        self,
        level_temp_map: List[Dict[str, Any]],
        sub_id: str,
        level: int,
        llm_result: Dict[str, Any],
        highest_rerank_score: float = 0.0
    ) -> float:
        """
        ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô + Retroactive Sync ‡∏à‡∏≤‡∏Å AI Extraction + ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Strength
        """
        map_key = f"{sub_id}.L{level}"
        ai_contexts = {t: str(llm_result.get(f"Extraction_{t}", "")).lower() for t in "PDCA"}

        new_evidence_list = []
        seen_keys = set()
        PASS_STATUS = "PASS" if llm_result.get("is_passed", False) else "FAIL"

        # ‡∏î‡∏∂‡∏á require_phase ‡πÑ‡∏ß‡πâ‡πÉ‡∏ä‡πâ fallback ‡∏•‡πà‡∏ß‡∏á‡∏´‡∏ô‡πâ‡∏≤ (‡∏•‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ã‡πâ‡∏≥)
        require_phases = self.get_rule_content(sub_id, level, "require_phase") or []
        default_fallback = require_phases[0] if require_phases else ("P" if level == 1 else "D")

        for chunk in level_temp_map:
            text = chunk.get("text") or ""
            if not text.strip():
                continue

            meta = chunk.get("metadata", {})
            fname = os.path.basename(str(meta.get("source") or "Unknown")).lower()

            doc_id = chunk.get("stable_doc_uuid") or meta.get("stable_doc_uuid") or "unknown"
            chunk_uuid = chunk.get("chunk_uuid") or hashlib.sha256(text.encode()).hexdigest()[:16]
            unique_key = f"{doc_id}:{chunk_uuid}"
            if unique_key in seen_keys:
                continue
            seen_keys.add(unique_key)

            pdca_tag = chunk.get("pdca_tag") or "Other"
            self.logger.debug(f"[EVI-TAG-INPUT] {fname} | raw_pdca_tag: {pdca_tag}")

            # 1. Retroactive Sync ‡∏à‡∏≤‡∏Å AI Extraction (‡∏Å‡πà‡∏≠‡∏ô retry)
            for tag, summary in ai_contexts.items():
                if fname in summary and len(summary.strip()) > 5:
                    pdca_tag = tag
                    self.logger.info(f"[EVI-TAG-RETRO] {fname} ‚Üí {pdca_tag} (from AI extraction)")
                    break

            # 2. ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÄ‡∏õ‡πá‡∏ô "Other" ‚Üí ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏° tag ‡πÉ‡∏´‡∏°‡πà‡∏î‡πâ‡∏ß‡∏¢ _get_semantic_tag
            if pdca_tag == "Other":
                try:
                    pdca_tag = self._get_semantic_tag(text, sub_id, level, fname)
                    self.logger.info(f"[EVI-TAG-RETRY] {fname} ‚Üí {pdca_tag} (retry from Other)")
                except Exception as e:
                    self.logger.warning(f"[EVI-TAG-RETRY-ERR] {fname}: {e}")

            # 3. ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÄ‡∏õ‡πá‡∏ô "Other" ‡∏≠‡∏µ‡∏Å ‚Üí ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö fallback ‡πÑ‡∏õ require_phase[0] ‡∏´‡∏£‡∏∑‡∏≠ default
            if pdca_tag == "Other":
                pdca_tag = default_fallback
                self.logger.info(f"[EVI-TAG-FORCE] {fname} ‚Üí {pdca_tag} (force from require_phase/default)")

            entry = {
                "sub_id": sub_id,
                "level": level,
                "pdca_tag": pdca_tag,
                "doc_id": doc_id,
                "chunk_uuid": chunk_uuid,
                "source_filename": fname,
                "page": str(meta.get("page_label") or meta.get("page") or "N/A"),
                "relevance_score": float(chunk.get("rerank_score") or chunk.get("score") or 0.5),
                "text_preview": text[:300].replace("\n", " ") + "..." if len(text) > 300 else text,
                "status": PASS_STATUS,
                "timestamp": datetime.now().isoformat(),
            }
            new_evidence_list.append(entry)

        if not new_evidence_list:
            return 0.0

        self.evidence_map.setdefault(map_key, []).extend(deepcopy(new_evidence_list))

        tags_set = {"P", "D", "C", "A"}
        found_tags = {ev["pdca_tag"] for ev in new_evidence_list if ev["pdca_tag"] in tags_set}
        coverage = len(found_tags) / 4.0
        strength = round((highest_rerank_score * 0.6) + (coverage * 0.4), 2)

        self.assessment_results_map[map_key] = {
            "is_passed": llm_result.get("is_passed", False),
            "score": llm_result.get("score", 0.0),
            "strength": strength
        }

        counts = {t: sum(1 for e in new_evidence_list if e["pdca_tag"] == t) for t in list(tags_set) + ["Other"]}
        self.logger.info(
            f"[EVI-SAVED] {map_key} | items:{len(new_evidence_list)} "
            f"P:{counts['P']} D:{counts['D']} C:{counts['C']} A:{counts['A']} Other:{counts['Other']} "
            f"strength:{strength:.2f}"
        )

        return strength

    def _robust_hydrate_documents_for_priority_chunks(
        self,
        chunks_to_hydrate: List[Dict],
        vsm: Optional['VectorStoreManager'],
        current_sub_id: Optional[str] = None,
        level: Optional[int] = None
    ) -> List[Dict]:
        """
        [ULTIMATE HYDRATION v2026.01.28]
        - ‡∏î‡∏∂‡∏á Full Text + Pre-tag ‡∏î‡πâ‡∏ß‡∏¢ Decision Engine ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
        - Dedup ‡∏î‡πâ‡∏ß‡∏¢ full text hash ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏™‡∏π‡∏á
        - Boost score + Log ‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏û‡∏∑‡πà‡∏≠ audit/debug
        """

        active_sub_id = current_sub_id or getattr(self, 'sub_id', 'unknown')
        if not chunks_to_hydrate:
            self.logger.debug(f"‚ÑπÔ∏è [HYDRATION] No chunks for {active_sub_id} L{level}")
            return []

        def _safe_classify(text: str, filename: str = "") -> str:
            try:
                tag = self._get_semantic_tag(text, active_sub_id, level or 1, filename)
                self.logger.debug(f"[SAFE-CLASSIFY] Raw from engine: {tag} | file: {filename[:30]}")
                
                # ‡∏ñ‡πâ‡∏≤ engine return PDCA ‚Üí ‡πÉ‡∏ä‡πâ‡πÄ‡∏•‡∏¢ (‡πÑ‡∏°‡πà‡∏ó‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô Other)
                if tag in {"P", "D", "C", "A"}:
                    return tag
                
                # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô Other ‡∏à‡∏£‡∏¥‡∏á ‚Üí fallback ‡∏ï‡∏≤‡∏° require_phase (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà "Other" ‡∏ó‡∏±‡∏ö)
                reqs = self.get_rule_content(active_sub_id, level or 1, "require_phase") or []
                fallback_tag = reqs[0] if reqs else ("P" if (level or 1) == 1 else "D")
                self.logger.debug(f"[SAFE-CLASSIFY-FALLBACK] {tag} ‚Üí {fallback_tag} | file: {filename[:30]}")
                return fallback_tag
                
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è [CLASSIFY-ERR] Hybrid Fallback: {e} | file: {filename[:30]}")
                reqs = self.get_rule_content(active_sub_id, level or 1, "require_phase") or []
                return reqs[0] if reqs else ("P" if (level or 1) <= 1 else "D")

        def _standardize_chunk(chunk: Dict, boost_score: float) -> Dict:
            chunk = chunk.copy()
            # is_baseline ‡∏Ñ‡∏ß‡∏£‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà set True ‡πÄ‡∏™‡∏°‡∏≠
            chunk.setdefault("is_baseline", chunk.get("is_baseline", False))
            text = chunk.get("text", "").strip()
            if not text:
                return chunk
            meta = chunk.get("metadata", {})
            fname = os.path.basename(str(meta.get("source") or meta.get("file_name") or "unknown"))
            chunk["pdca_tag"] = _safe_classify(text, fname)
            chunk["rerank_score"] = max(float(chunk.get("rerank_score", 0.0)), boost_score)
            chunk["score"] = max(float(chunk.get("score", 0.0)), boost_score)
            return chunk

        # 3. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° stable IDs (‡πÄ‡∏û‡∏¥‡πà‡∏° chunk_uuid ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥)
        stable_ids = set()
        for c in chunks_to_hydrate:
            sid = c.get("stable_doc_uuid") or c.get("doc_id")
            if sid:
                stable_ids.add(sid)

        if not stable_ids or not vsm:
            self.logger.warning(f"[HYDRATION] No stable IDs or VSM ‚Üí fallback boost")
            return [_standardize_chunk(c.copy(), 0.9) for c in chunks_to_hydrate]

        # 4. Fetch full documents
        stable_id_map = defaultdict(list)
        try:
            retrieved_docs = vsm.get_documents_by_id(
                list(stable_ids), doc_type=self.doc_type, enabler=self.config.enabler
            )
            for doc in retrieved_docs:
                sid = doc.metadata.get("stable_doc_uuid") or doc.metadata.get("doc_id")
                if sid:
                    stable_id_map[sid].append({"text": doc.page_content, "metadata": doc.metadata})
        except Exception as e:
            self.logger.error(f"‚ùå [HYDRATION] VSM Fetch failed: {e}")
            return [_standardize_chunk(c.copy(), 0.9) for c in chunks_to_hydrate]

        # 5. Hydrate + Dedup + Tag
        hydrated_docs = []
        seen_hashes = set()  # ‡πÉ‡∏ä‡πâ hash ‡πÄ‡∏ï‡πá‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠ dedup ‡∏à‡∏£‡∏¥‡∏á
        hydrated_count = 0
        total = len(chunks_to_hydrate)

        SAFE_META_KEYS = {"source", "file_name", "page", "page_label", "page_number"}

        for chunk in chunks_to_hydrate:
            new_chunk = chunk.copy()
            sid = new_chunk.get("stable_doc_uuid") or new_chunk.get("doc_id")

            hydrated = False
            if sid and sid in stable_id_map:
                full_doc = stable_id_map[sid][0]
                new_chunk["text"] = full_doc["text"]
                new_chunk.update({k: v for k, v in full_doc["metadata"].items() if k in SAFE_META_KEYS})
                hydrated = True
                hydrated_count += 1

            # Standardize + Tag + Boost
            boost = 1.0 if hydrated else 0.85
            new_chunk = _standardize_chunk(new_chunk, boost)

            # Dedup ‡∏î‡πâ‡∏ß‡∏¢ full text hash (‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Å‡∏ß‡πà‡∏≤ [:100])
            text_hash = hashlib.sha256(new_chunk.get("text", "").encode()).hexdigest()
            if text_hash not in seen_hashes:
                seen_hashes.add(text_hash)
                hydrated_docs.append(new_chunk)

        self.logger.info(
            f"‚úÖ [HYDRATION] Complete: {len(hydrated_docs)} chunks ready "
            f"(hydrated: {hydrated_count}/{total}, dedup removed: {total - len(hydrated_docs)})"
        )

        # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á ‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ó‡πâ‡∏≤‡∏¢‡∏™‡∏∏‡∏î
        return self._guarantee_text_key(hydrated_docs) if hasattr(self, '_guarantee_text_key') else hydrated_docs


    def _is_previous_level_passed(self, sub_id: str, level: int) -> bool:
        """
        [STRICT REVISE v2026.01.18.1] - ‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏ö‡∏ö‡πÄ‡∏Ç‡πâ‡∏°‡∏á‡∏ß‡∏î
        """
        if level <= 1: 
            return True
            
        prev_level = level - 1
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Key ‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏ö‡∏≠‡∏≤‡∏à‡πÉ‡∏ä‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å
        possible_keys = [f"{sub_id}.L{prev_level}", f"{sub_id}_L{prev_level}"]
        
        for key in possible_keys:
            # ‡πÉ‡∏ä‡πâ get() ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô KeyError ‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
            result = getattr(self, 'assessment_results_map', {}).get(key)
            if result:
                if result.get('is_passed') is True:
                    self.logger.info(f"‚úÖ [LEVEL-GATE] Level {prev_level} passed for {sub_id}")
                    return True
                else:
                    self.logger.warning(f"‚ö†Ô∏è [LEVEL-GATE] Level {prev_level} found but status is FAIL")
                    return False

        # Safe Guard: ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≤‡∏° Level ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å
        self.logger.warning(f"üö´ [LEVEL-GATE] No assessment record for L{prev_level}. Blocking L{level}.")
        return False

    def _normalize_thai_text(self, text: str) -> str:
        """
        [ULTIMATE THAI NORMALIZE v2026.1.31 ‚Äì FULL REVISED]
        - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤ '‡∏™‡∏£‡∏∞‡∏´‡∏≤‡∏¢' (‡πÄ‡∏ä‡πà‡∏ô ‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£ -> ‡∏ú‡∏ö‡∏£‡∏´‡∏≤‡∏£) ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ñ‡∏≤‡∏ß‡∏£
        - High Performance: ‡∏ß‡∏ô‡∏•‡∏π‡∏õ Filter + Lowercase ‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏£‡∏≠‡∏ö‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏à‡∏ö
        - Unicode NFC: ‡∏£‡∏ß‡∏° Combining Characters (‡∏™‡∏£‡∏∞/‡∏ß‡∏£‡∏£‡∏ì‡∏¢‡∏∏‡∏Å‡∏ï‡πå) ‡∏Å‡πà‡∏≠‡∏ô‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
        - Strict Thai Range: ‡∏£‡∏±‡∏Å‡∏©‡∏≤ ‡∏Å-‡∏Æ, ‡∏™‡∏£‡∏∞, ‡∏ß‡∏£‡∏£‡∏ì‡∏¢‡∏∏‡∏Å‡∏ï‡πå ‡πÅ‡∏•‡∏∞‡πÄ‡∏•‡∏Ç‡πÑ‡∏ó‡∏¢ (\u0E00-\u0E7F) ‡∏Ñ‡∏£‡∏ö 100%
        """
        if not text or not isinstance(text, str):
            return ""

        # 1. Unicode NFC normalization 
        # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡πÑ‡∏ó‡∏¢ (‡πÄ‡∏ä‡πà‡∏ô ‡∏™‡∏£‡∏∞‡∏≠‡∏≥ ‡∏´‡∏£‡∏∑‡∏≠ ‡∏™‡∏£‡∏∞‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≠‡∏ô‡∏Å‡∏±‡∏ô)
        text = unicodedata.normalize('NFC', text)

        # 2. Compile Regex Pattern ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏ó‡∏µ‡πà‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï
        # ‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏© (a-zA-Z), ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (\u0E00-\u0E7F), ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç (0-9), ‡πÅ‡∏•‡∏∞‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á (\s)
        allowed_pattern = re.compile(r"[a-zA-Z0-9\u0E00-\u0E7F\s]")

        # 3. Single-pass Filter & Lowercase
        # ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£
        res = []
        for char in text:
            if allowed_pattern.match(char):
                # ‡∏ó‡∏≥ Lowercase ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏© (isascii ‡∏ä‡πà‡∏ß‡∏¢‡πÅ‡∏¢‡∏Å‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏≠‡∏≠‡∏Å‡∏ó‡∏±‡∏ô‡∏ó‡∏µ)
                if char.isascii() and char.isalpha():
                    res.append(char.lower())
                else:
                    res.append(char)
        
        # ‡∏£‡∏ß‡∏° List ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô String
        text = "".join(res)

        # 4. Collapse whitespace (‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ã‡πâ‡∏≥‡πÄ‡∏õ‡πá‡∏ô 1 ‡∏ä‡πà‡∏≠‡∏á) ‡πÅ‡∏•‡∏∞ Trim
        text = re.sub(r"\s+", " ", text).strip()

        return text
    
    
    def enhance_query_for_statement(
        self,
        statement_text: str,
        sub_id: str,
        statement_id: str,
        level: int,
        focus_hint: str = "",
    ) -> List[str]:
        """
        [STRATEGIC QUERY GEN v2026.1.25 ‚Äì HYBRID OPTIMIZED]
        - Robust Safety: ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ input ‡∏ß‡πà‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á fallback ‡πÄ‡∏™‡∏°‡∏≠
        - Level-Aware Negatives: ‡∏õ‡∏•‡∏î‡∏•‡πá‡∏≠‡∏Å '-MasterPlan' ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö L1-L2 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏à‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢
        - Phase-Targeted: ‡∏î‡∏∂‡∏á keywords ‡∏ï‡∏≤‡∏° PDCA phases ‡∏à‡∏≤‡∏Å JSON Config
        - Precision Shuffle: ‡∏à‡∏≥‡∏Å‡∏±‡∏î 8 queries ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
        """
        logger = logging.getLogger(__name__)
        log_prefix = f"[QUERY-GEN] {sub_id} L{level}"

        # 0. Safety guard
        if not statement_text or not isinstance(statement_text, str):
            logger.warning(f"{log_prefix} Empty/invalid statement_text ‚Üí fallback basic")
            fallback_q = f"{sub_id} {focus_hint or '‡∏ß‡∏¥‡∏™‡∏±‡∏¢‡∏ó‡∏±‡∏®‡∏ô‡πå ‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢'} ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ"
            return [fallback_q, f"{sub_id} ‡πÅ‡∏ú‡∏ô‡πÅ‡∏°‡πà‡∏ö‡∏ó KM"]

        # Anchors
        enabler_id = getattr(self.config, 'enabler', 'KM').upper()
        tenant_name = getattr(self.config, 'tenant', 'PEA').upper()
        id_anchor = f"{enabler_id} {sub_id}"

        # --- üõ°Ô∏è 1. Dynamic Negative Keywords (The Core Fix) ---
        if level <= 2:
            # ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢/‡∏ß‡∏≤‡∏á‡∏£‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô ‡∏´‡πâ‡∏≤‡∏°‡∏ö‡∏•‡πá‡∏≠‡∏Å‡πÅ‡∏ú‡∏ô‡πÅ‡∏°‡πà‡∏ö‡∏ó‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≤‡∏î
            neg_strict = "-‡∏†‡∏≤‡∏Ñ‡∏ú‡∏ô‡∏ß‡∏Å -‡∏†‡∏≤‡∏û‡∏ñ‡πà‡∏≤‡∏¢‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°"
        else:
            # ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥/‡∏¢‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö ‡∏ö‡∏•‡πá‡∏≠‡∏Å‡πÅ‡∏ú‡∏ô‡πÅ‡∏°‡πà‡∏ö‡∏ó‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏à‡∏£‡∏¥‡∏á
            neg_strict = "-‡πÅ‡∏ú‡∏ô‡πÅ‡∏°‡πà‡∏ö‡∏ó -‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏ä‡∏≤‡∏ï‡∏¥ -MasterPlan -‡∏†‡∏≤‡∏Ñ‡∏ú‡∏ô‡∏ß‡∏Å"

        # 2. Keywords Preparation
        require_phases = self.get_rule_content(sub_id, level, "require_phase") or ['P', 'D']
        query_syn = self.get_rule_content(sub_id, level, "query_synonyms") or ""
        
        raw_kws = self.get_rule_content(sub_id, level, "must_include_keywords") or []
        phase_map = {"P": "plan_keywords", "D": "do_keywords", "C": "check_keywords", "A": "act_keywords"}
        
        # ‡∏î‡∏∂‡∏á Default Keywords ‡∏à‡∏≤‡∏Å Enabler (KM)
        defaults = getattr(self, 'contextual_rules_map', {}).get("_enabler_defaults", {})
        for phase in require_phases:
            kw_key = phase_map.get(phase)
            if kw_key:
                raw_kws.extend(defaults.get(kw_key, []))

        clean_kws = " ".join(sorted(set(str(k).strip() for k in raw_kws if k))[:3])
        
        # Clean Statement (‡∏ï‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô '‡πÄ‡∏ä‡πà‡∏ô' ‡∏≠‡∏≠‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏°‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Search)
        clean_stmt = statement_text.split("‡πÄ‡∏ä‡πà‡∏ô", 1)[0].strip()
        clean_stmt = re.sub(r'[^\w\s]', '', clean_stmt)[:60]

        queries: List[str] = []

        # 3. Strategy: Multi-Angle Retrieval
        # A: Core Strategy (Synonyms + Statement)
        queries.append(f"{id_anchor} {query_syn} {clean_stmt}")

        # B: Evidence Type Strategy (‡∏ï‡∏≤‡∏°‡∏£‡∏∞‡∏î‡∏±‡∏ö Maturity)
        if level <= 2:
            queries.append(f"{tenant_name} ‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏® ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á ‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° ‡∏°‡∏ï‡∏¥‡∏ö‡∏≠‡∏£‡πå‡∏î ‡∏•‡∏á‡∏ô‡∏≤‡∏° {id_anchor} {query_syn}")
        else:
            queries.append(f"{tenant_name} ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏• KPI ‡∏ú‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏ú‡∏• {id_anchor} {query_syn} {neg_strict}")

        # C: Phase-Specific Strategy
        for phase in require_phases:
            queries.append(f"{id_anchor} {query_syn} {phase} {clean_kws} {neg_strict}")

        # D: Fallback Core
        if len(queries) < 4:
            queries.append(f"{tenant_name} {id_anchor} {clean_stmt} {clean_kws}")

        # 4. Post-process: Dedup, Truncate, and Shuffle
        final_queries = []
        seen = set()
        import random
        
        for q in queries:
            # Normalize ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡πÉ‡∏´‡πâ‡∏û‡∏≠‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Vector Search (18-24 ‡∏Ñ‡∏≥)
            q_clean = self._normalize_thai_text(q)
            words = q_clean.split()
            q_trunc = " ".join(words[:22])
            
            # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Query ‡∏ã‡πâ‡∏≥
            q_key = " ".join(words[:15])
            if q_trunc and q_key not in seen:
                final_queries.append(q_trunc)
                seen.add(q_key)

        random.shuffle(final_queries)
        
        logger.info(f"üöÄ [Query Gen v2026.1.25] {sub_id} L{level} | Final Queries: {len(final_queries[:8])} | Neg: {neg_strict}")
        return final_queries[:8]
    

    def _get_level_aware_queries(self, criteria_id: str, level_key: str) -> List[str]:
        """
        [REVISED v2026.1.25 - PRECISION EVIDENCE]
        """
        criteria_rules = self.contextual_rules_map.get(criteria_id, {})
        level_rule = criteria_rules.get(level_key, {})
        synonyms = level_rule.get("query_synonyms", "")
        
        tenant = getattr(self.config, 'tenant', 'PEA').upper()
        prefix = f"{tenant} {self.enabler} {criteria_id}"
        
        # ‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå‡∏ï‡∏≤‡∏° Synonyms ‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å JSON
        generated_queries = [
            f"{prefix} {synonyms}", # ‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á‡∏´‡∏•‡∏±‡∏Å
            f"{prefix} {synonyms} ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏° ‡∏°‡∏ï‡∏¥‡∏≠‡∏ô‡∏∏‡∏°‡∏±‡∏ï‡∏¥ ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÅ‡∏ï‡πà‡∏á‡∏ï‡∏±‡πâ‡∏á", # ‡∏á‡∏≤‡∏ô‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£
            f"{prefix} {synonyms} ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° ‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡πÉ‡∏ä‡πâ ‡∏•‡∏á‡∏ô‡∏≤‡∏°‡∏≠‡∏ô‡∏∏‡∏°‡∏±‡∏ï‡∏¥", # ‡∏á‡∏≤‡∏ô‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢
            f"{prefix} {synonyms} ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô ‡∏™‡∏£‡∏∏‡∏õ‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£ ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏•" # ‡∏á‡∏≤‡∏ô‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥
        ]
        
        return [self._normalize_thai_text(q) for q in generated_queries]

    def relevance_score_fn(self, evidence: Dict[str, Any], sub_id: str, level: int) -> float:
        """
        [ULTIMATE REVISED v2026.01.25]
        - 45% Rerank + 35% Keyword + 20% Contextual Bonuses
        - Optimized Code: ‡πÉ‡∏ä‡πâ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏•‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô
        - ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á Metadata ‡πÉ‡∏´‡πâ Robust ‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡∏≤‡∏° Header core/seam_assessment.py
        """
        if not evidence or not isinstance(evidence, dict):
            return 0.0

        # 1. Rerank Score Processing (45%)
        # ‡∏î‡∏∂‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏î‡∏¥‡∏ö‡∏à‡∏≤‡∏Å VectorStore ‡∏´‡∏£‡∏∑‡∏≠ Reranker
        raw_val = evidence.get('rerank_score') or evidence.get('score') or 0.0
        normalized_rerank = min(max(float(raw_val), 0.0), 1.0)

        # 2. ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô (‡πÉ‡∏ä‡πâ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏à‡∏≤‡∏Å Header)
        # ‡∏î‡∏∂‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô
        text = str(evidence.get('text') or evidence.get('page_content') or '').lower().strip()
        meta = evidence.get('metadata') or {}
        if not isinstance(meta, dict): meta = {}
        
        # ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏ó‡∏≥ Source Grading
        filename = str(meta.get('source') or meta.get('source_filename') or evidence.get('source') or '').lower()
        cum_rules = self.get_cumulative_rules_cached(sub_id, level)

        # 3. Source Grading Bonus (‡∏ñ‡πà‡∏ß‡∏á‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠‡∏Ç‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå)
        source_bonus = 0.0
        primary_docs = ["‡∏°‡∏ï‡∏¥", "‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å", "‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á", "‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®", "‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢", "‡πÅ‡∏ú‡∏ô‡πÅ‡∏°‡πà‡∏ö‡∏ó", "‡∏°‡∏ï‡∏¥‡∏ö‡∏≠‡∏£‡πå‡∏î"]
        secondary_docs = ["assessment report", "‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô", "‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•", "‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏•", "kpi"]
        
        if any(p in filename for p in primary_docs):
            source_bonus = 0.20
        elif any(s in filename for s in secondary_docs):
            source_bonus = 0.10

        # 4. Keyword Score (35%) - ‡πÉ‡∏´‡πâ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏ï‡∏≤‡∏°‡∏£‡∏∞‡∏î‡∏±‡∏ö Maturity
        target_kws = set()
        if level <= 2:
            target_kws.update(cum_rules.get('plan_keywords', []) + cum_rules.get('do_keywords', []))
        else:
            target_kws.update(cum_rules.get('check_keywords', []) + cum_rules.get('act_keywords', []))

        keyword_score = 0.0
        if target_kws:
            match_count = sum(1 for kw in target_kws if str(kw).lower() in text)
            if match_count > 0:
                expected = max(1, len(target_kws) * 0.3)
                # ‡πÉ‡∏ä‡πâ Power function ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Å‡∏≤‡∏£‡πÄ‡∏à‡∏≠ Keyword ‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡πÑ‡∏°‡πà‡∏Å‡∏µ‡πà‡∏Ñ‡∏≥‡∏Å‡πá‡πÑ‡∏î‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÇ‡∏î‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤
                keyword_score = min((match_count / expected) ** 0.6, 1.0)
                keyword_score = max(keyword_score, 0.20) # Floor ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏à‡∏≠‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 1 ‡∏Ñ‡∏≥

        # 5. PDCA Tag Bonus (High Priority ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ü‡∏™‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£)
        pdca_bonus = 0.0
        pdca_tag = str(evidence.get('pdca_tag') or meta.get('pdca_tag') or "").upper()
        required_phases = cum_rules.get('required_phases', [])
        
        if pdca_tag in required_phases:
            pdca_bonus = 0.30
        elif pdca_tag in {'P', 'D', 'C', 'A'}:
            pdca_bonus = 0.15

        # 6. Contextual Bonuses (Neighbors & Specific Rules)
        neighbor_bonus = 0.15 if evidence.get('is_neighbor') or meta.get('is_neighbor') else 0.0
        
        rule_bonus = 0.0
        specific_rule = str(cum_rules.get('specific_contextual_rule', '')).lower()
        if specific_rule and any(word in text for word in specific_rule.split()[:10]):
            rule_bonus = 0.15

        # 7. ‡∏ú‡∏•‡∏£‡∏ß‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏∏‡∏ó‡∏ò‡∏¥ (Final Weighted Score)
        final_score = (
            (0.45 * normalized_rerank) + 
            (0.35 * keyword_score) + 
            source_bonus + pdca_bonus + neighbor_bonus + rule_bonus
        )

        # 8. High-Confidence Min Floor
        # ‡∏ñ‡πâ‡∏≤ Rerank ‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏°‡∏≤‡∏Å (0.8+) ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô
        if normalized_rerank > 0.80:
            final_score = max(final_score, 0.45)

        final_score = min(max(final_score, 0.0), 1.0)

        # üéØ Logging ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ Debug (INFO Level ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£)
        self.logger.info(
            f"üîé [REL-CHECK] {sub_id} L{level} | Final: {final_score:.3f} | "
            f"Rerank: {normalized_rerank:.2f} | KW: {keyword_score:.2f} | Tag: {pdca_tag} | File: {os.path.basename(filename)[:30]}"
        )

        return float(final_score)

    def _perform_adaptive_retrieval(
        self,
        sub_id: str,
        level: int,
        stmt: str,
        vectorstore_manager: Any,
    ) -> Tuple[List[Dict], float]:
        """
        [ULTIMATE REVISED v2026.01.25]
        - Clean Code: ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ Global Variables (RETRIEVAL_*) ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á ‡πÑ‡∏°‡πà Assign ‡∏ã‡πâ‡∏≥
        - High Performance: ‡πÉ‡∏ä‡πâ Early Exit ‡πÅ‡∏•‡∏∞ High-Rerank Threshold ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î Latency
        - Robustness: ‡∏°‡∏µ Safe Scoring ‡πÅ‡∏•‡∏∞ Recovery Sweep ‡∏Å‡∏£‡∏ì‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠
        """
        start_time = time.time()
        if not stmt or not isinstance(stmt, str):
            return [], 0.0

        candidates: List[Dict] = []
        used_uuids = set()
        final_max_score = 0.0
        level_key = f"L{level}"
        tenant = getattr(self.config, "tenant", "PEA").upper()

        def safe_relevance_score(evidence: Dict) -> float:
            """ Safe wrapper ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏™‡∏£‡∏¥‡∏°‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏†‡∏≤‡∏¢‡πÉ‡∏ô """
            try:
                return self.relevance_score_fn(evidence, sub_id, level)
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è [SAFE-SCORE] {sub_id} L{level}: {e}")
                return float(evidence.get('rerank_score') or evidence.get('score') or 0.0)

        # --- STEP 1: PRIORITY MAPPING (‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà Mapping ‡πÑ‡∏ß‡πâ‡∏•‡πà‡∏ß‡∏á‡∏´‡∏ô‡πâ‡∏≤) ---
        try:
            _, priority_docs = self._get_mapped_uuids_and_priority_chunks(
                sub_id=sub_id, level=level, statement_text=stmt, vectorstore_manager=vectorstore_manager
            ) or (set(), [])
            
            for p in priority_docs:
                uid = p.get("chunk_uuid")
                if not uid or uid in used_uuids: continue
                
                p["source"] = os.path.basename(p.get("source") or "Unknown")
                # ‡∏â‡∏µ‡∏î‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡πÉ‡∏´‡πâ Priority Docs (‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥ 0.90)
                p["score"] = max(safe_relevance_score(p), 0.90) 
                
                used_uuids.add(uid)
                candidates.append(p)
                final_max_score = max(final_max_score, p["score"])
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Priority mapping skip: {e}")

        # --- STEP 2: HYBRID QUERY GENERATION ---
        json_queries = self._get_level_aware_queries(sub_id, level_key)
        legacy_queries = self.enhance_query_for_statement(stmt, sub_id, f"{sub_id}.L{level}", level)
        active_queries = list(dict.fromkeys(json_queries + legacy_queries))[:10]

        # --- STEP 3: ITERATIVE RETRIEVAL LOOP (‡∏û‡∏£‡πâ‡∏≠‡∏° EARLY EXIT) ---
        
        for i, q in enumerate(active_queries):
            # üéØ [EARLY EXIT] ‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£ Global ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á Assign ‡∏ã‡πâ‡∏≥)
            if len(candidates) >= RETRIEVAL_EARLY_EXIT_COUNT and final_max_score >= RETRIEVAL_EARLY_EXIT_SCORE_THRESHOLD:
                self.logger.info(f"üéØ [EARLY-EXIT] {sub_id} L{level} | Found {len(candidates)} docs | Max: {final_max_score:.4f}")
                break
            
            try:
                res = self.rag_retriever(
                    self._normalize_thai_text(q), self.doc_type, sub_id=sub_id, level=level,
                    vectorstore_manager=vectorstore_manager
                ) or {}
                
                for d in (res.get("top_evidences") or []):
                    uid = d.get("chunk_uuid")
                    score = float(d.get("score", 0.0))
                    
                    # ‡∏Å‡∏£‡∏≠‡∏á‡∏î‡πâ‡∏ß‡∏¢ RETRIEVAL_RERANK_FLOOR ‡∏à‡∏≤‡∏Å Global
                    if uid and uid not in used_uuids and score >= RETRIEVAL_RERANK_FLOOR:
                        d["source"] = os.path.basename(d.get("source") or "Unknown")
                        
                        # ‡∏â‡∏µ‡∏î‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ä‡∏¥‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û (High Rerank)
                        if score > RETRIEVAL_HIGH_RERANK_THRESHOLD:
                            d["score"] = max(score, safe_relevance_score(d))
                        else:
                            d["score"] = score  
                        
                        used_uuids.add(uid)
                        candidates.append(d)
                        final_max_score = max(final_max_score, d["score"])
            except Exception as e:
                self.logger.error(f"‚ùå Query Loop {i+1} failed: {e}")

        # --- STEP 4: RECOVERY SWEEP (‡∏ñ‡πâ‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÑ‡∏°‡πà‡∏û‡∏≠‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ï‡πà‡∏≥‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ) ---
        if final_max_score < RETRIEVAL_RELEVANCE_THRESHOLD or len(candidates) < 5:
            self.logger.info(f"üö® [RECOVERY] Insufficient evidence (Max:{final_max_score:.4f}). Triggering sweep...")
            self._execute_recovery_sweep(sub_id, level, stmt, tenant, used_uuids, candidates, vectorstore_manager)
            
            # Re-calculating Final Max Score ‡∏´‡∏•‡∏±‡∏á Recovery
            if candidates:
                for c in candidates:
                    if c.get("is_recovery"):
                        c["score"] = max(c.get("score", 0.0), safe_relevance_score(c))
                final_max_score = max([float(c.get("score", 0.0)) for c in candidates])

        # --- STEP 5: FINAL SORT & TRIM ---
        candidates.sort(key=lambda x: x.get("score", 0.0), reverse=True)
        # ‡πÉ‡∏ä‡πâ ANALYSIS_FINAL_K ‡∏à‡∏≤‡∏Å Global ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
        final_docs = candidates[:ANALYSIS_FINAL_K]
        
        elapsed = time.time() - start_time
        self.logger.info(f"üèÅ [COMPLETE] {sub_id} L{level} | Final Docs: {len(final_docs)} | Max: {final_max_score:.4f} | {elapsed:.2f}s")
        
        return final_docs, float(final_max_score)

    def _execute_recovery_sweep(self, sub_id, level, stmt, tenant, used_uuids, candidates, vectorstore_manager):
        """ [ULTIMATE REVISED] ‡∏£‡∏∞‡∏ö‡∏ö‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö‡∏Å‡∏ß‡πâ‡∏≤‡∏á (Broad Search) ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏à‡∏≤‡∏Å Global Header """
        try:
            # ‡∏î‡∏∂‡∏á‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏à‡∏≤‡∏Å Contextual Rules (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
            rule = getattr(self, 'contextual_rules_map', {}).get(sub_id, {}).get(f"L{level}", {})
            keywords = rule.get("must_include_keywords", [])[:4]
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á Query ‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡πÜ: ‡πÉ‡∏ä‡πâ tenant, keywords ‡πÅ‡∏•‡∏∞‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô
            recovery_query = self._normalize_thai_text(
                f"{sub_id} {tenant} {' '.join(keywords)} {stmt[:30]}"
            )
            
            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ rag_retriever (‡πÉ‡∏ä‡πâ self.doc_type ‡πÅ‡∏•‡∏∞ vectorstore_manager)
            res_fb = self.rag_retriever(
                recovery_query, 
                self.doc_type, 
                sub_id=sub_id, 
                level=level,  # ‡∏™‡πà‡∏á level ‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ retriever ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Ç‡∏∂‡πâ‡∏ô
                vectorstore_manager=vectorstore_manager
            ) or {}
            
            for d in (res_fb.get("top_evidences") or []):
                uid = d.get("chunk_uuid")
                score = float(d.get("score", 0.0))
                
                # ‡πÉ‡∏ä‡πâ RETRIEVAL_RERANK_FLOOR ‡∏à‡∏≤‡∏Å Global ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£ Hard-coded
                if uid and uid not in used_uuids and score >= RETRIEVAL_RERANK_FLOOR:
                    d["source"] = os.path.basename(d.get("source") or "Unknown")
                    d["is_recovery"] = True
                    used_uuids.add(uid)
                    candidates.append(d)
        except Exception as e:
            self.logger.error(f"‚ùå Recovery sweep failed: {e}")

    def _log_pdca_status(self, sub_id, name, level, blocks, req_phases, sources_count, score, conf_level, **kwargs):
        """ [FULL REVISED] ‡∏û‡πà‡∏ô Dashboard ‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ PDCA ‡πÅ‡∏ö‡∏ö Real-time """
        try:
            actual_counts = kwargs.get('pdca_breakdown', {}) 
            is_safety_pass = kwargs.get('is_safety_pass', False)
            status_parts = []
            source_errors = 0
            
            # Mapping Key ‡∏à‡∏≤‡∏Å LLM Output ‡∏Å‡∏±‡∏ö PDCA Phase
            mapping = [("Extraction_P", "P"), ("Extraction_D", "D"), ("Extraction_C", "C"), ("Extraction_A", "A")]

            for full_key, short in mapping:
                count = float(actual_counts.get(short, 0.0))
                content = str(blocks.get(full_key, "")).strip()
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ AI ‡πÄ‡∏à‡∏≠‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏£‡∏¥‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                ai_found = bool(content and content.lower() not in ["-", "n/a", "none", "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"])
                has_source = "[Source:" in content and "]" in content
                
                if ai_found and not has_source:
                    source_errors += 1

                # Logic ‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Icon ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏™‡∏î‡∏á Maturity Gap
                if count >= 1.0:
                    icon = "‚úÖ" if has_source else "‚ö†Ô∏è" # ‡∏ú‡πà‡∏≤‡∏ô/‡∏ú‡πà‡∏≤‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∑‡∏°‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á
                elif is_safety_pass and short in req_phases:
                    icon = "üõ°Ô∏è" # ‡∏ú‡πà‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏£‡∏∞‡∏ö‡∏ö Safety Pass
                elif ai_found: 
                    icon = "üî∑" # ‡∏û‡∏ö‡∏£‡πà‡∏≠‡∏á‡∏£‡∏≠‡∏¢‡πÅ‡∏ï‡πà‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÑ‡∏°‡πà‡πÅ‡∏ô‡πà‡∏ô‡∏û‡∏≠
                elif short not in req_phases: 
                    icon = "‚ûñ" # ‡πÄ‡∏ü‡∏™‡∏ô‡∏µ‡πâ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡∏µ‡πâ
                else: 
                    icon = "‚ùå" # ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•

                status_parts.append(f"{short}:{icon}({count:.1f})")

            display_score = float(score or 0.0)
            alert_msg = f" üö®[REF-ERR:{source_errors}]" if source_errors > 0 else ""
            pass_label = " üõ°Ô∏è[SAFETY-PASS]" if is_safety_pass else ""
            
            # ‡∏û‡πà‡∏ô Dashboard ‡∏≠‡∏≠‡∏Å Console
            self.logger.info(
                f"üìä [PDCA-DASHBOARD] {sub_id} L{level} | {str(name)[:60]}...\n"
                f"   Maturity Status: {' '.join(status_parts)}{pass_label}{alert_msg}\n"
                f"   Summary: Score={display_score:.2f} | Evidence={sources_count} chunks | AI-Conf={str(conf_level).upper()}"
            )
        except Exception as e:
            self.logger.error(f"‚ùå Dashboard Logging Failed: {str(e)}")

    
    def _summarize_evidence_list_short(self, evidences: list, max_sentences: int = 3) -> str:
        """ [REVISED] ‡πÅ‡∏õ‡∏•‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ AI ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏á‡πà‡∏≤‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô """
        if not evidences: 
            return "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÄ‡∏ä‡∏¥‡∏á‡∏õ‡∏£‡∏∞‡∏à‡∏±‡∏Å‡∏©‡πå‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö"
        
        parts = []
        # ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏£‡∏¥‡∏á
        valid_evidences = [ev for ev in evidences if isinstance(ev, dict) and (ev.get("text") or ev.get("content", "")).strip()]
        
        for ev in valid_evidences[:max_sentences]:
            # ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå (‡πÉ‡∏ä‡πâ os.path.basename ‡∏à‡∏≤‡∏Å Header)
            filename = os.path.basename(ev.get("file_name") or ev.get("source") or "Unknown_Document")
            page = ev.get("page", "N/A")
            
            # ‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏•‡∏∞‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î Token
            raw_text = ev.get("text") or ev.get("content") or ""
            clean_text = " ".join(raw_text[:150].split()).strip()
            
            parts.append(f"‚Ä¢ [{filename}, ‡∏´‡∏ô‡πâ‡∏≤ {page}]: \"{clean_text}...\"")

        return "\n".join(parts)
    
    def _run_expert_re_evaluation(
        self,
        sub_id: str,
        level: int,
        statement_text: str,
        context: str,
        first_attempt_reason: str,
        missing_tags: Union[List[str], Set[str]],
        highest_rerank_score: float,
        sub_criteria_name: str,
        llm_evaluator_to_use: Any,
        base_kwargs: Dict[str, Any],
        audit_instruction: str = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        [JUDICIAL REVIEW - FINAL POLISH v2026]
        ‡∏£‡∏∞‡∏ö‡∏ö‡∏≠‡∏∏‡∏ó‡∏ò‡∏£‡∏ì‡πå‡∏£‡∏≠‡∏ö‡∏™‡∏≠‡∏á: ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏¥‡∏î Format ‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡πÄ‡∏Å‡∏ì‡∏ë‡πå Substance over Form
        """
        log_prefix = f"Sub:{sub_id} L{level}"
        self.logger.info(f"‚öñÔ∏è [EXPERT-APPEAL] Starting for {log_prefix} (Max Rerank: {highest_rerank_score:.4f})")

        # 1. ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Missing Tags ‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤
        missing_set = set(missing_tags) if isinstance(missing_tags, (list, set)) else set()
        missing_str = ", ".join(sorted(missing_set)) if missing_set else "‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤ PDCA"

        # 2. ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á Instruction ‡πÉ‡∏´‡πâ‡∏î‡∏∏‡∏î‡∏±‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô AI ‡∏Ç‡∏µ‡πâ‡πÄ‡∏Å‡∏µ‡∏¢‡∏à‡∏ï‡∏£‡∏ß‡∏à)
        enabler_header = f"--- [ENABLER RULES] ---\n{audit_instruction}\n" if audit_instruction else ""
        hint_msg = f"""
### üö® EXPERT JUDICIAL REVIEW - SECOND CHANCE üö®
{enabler_header}
[ROUND 1 FAILURE]: "{first_attempt_reason[:150]}..."
[CRITICAL HINT]: ‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å ({highest_rerank_score:.4f}) ‡πÉ‡∏ô‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠: {missing_str}

MANDATORY AUDIT RULES:
1. **Substance over Form**: ‡∏´‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÉ‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏£‡∏∞‡∏ö‡∏∏‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°‡∏à‡∏£‡∏¥‡∏á ‡πÅ‡∏°‡πâ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏•‡∏≤‡∏¢‡πÄ‡∏ã‡πá‡∏ô "‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏´‡πâ‡∏ú‡πà‡∏≤‡∏ô"
2. **Specific Defense**: ‡∏£‡∏∞‡∏ö‡∏∏‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡∏∞‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏Ñ‡∏≥‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏ô‡∏ä‡πà‡∏≠‡∏á reason
"""

        # 3. ‚ú® [SAFE INJECTION] ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ pdca_blocks ‡πÉ‡∏´‡πâ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏∏‡∏Å‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        # ‡∏î‡∏∂‡∏á‡∏Ç‡∏≠‡∏á‡πÄ‡∏Å‡πà‡∏≤‡∏≠‡∏≠‡∏Å‡∏°‡∏≤ ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ List ‡∏ß‡πà‡∏≤‡∏á
        original_blocks = base_kwargs.get("pdca_blocks", [])
        
        if isinstance(original_blocks, list):
            # ‡∏Å‡πä‡∏≠‡∏õ‡∏õ‡∏µ‡πâ‡∏°‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏° (Side Effect)
            expert_pdca_blocks = list(original_blocks) 
            expert_pdca_blocks.append({
                "type": "judicial_review_instruction",
                "content": hint_msg,
                "metadata": {"priority": "highest", "is_appeal": True}
            })
        else:
            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô String ‡∏´‡∏£‡∏∑‡∏≠ Format ‡∏≠‡∏∑‡πà‡∏ô ‡πÉ‡∏´‡πâ‡∏ï‡πà‡∏≠‡∏ó‡πâ‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö Text
            expert_pdca_blocks = f"{str(original_blocks)}\n\n{hint_msg}"

        # 4. ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏£‡πà‡∏≤‡∏á Arguments ‡πÉ‡∏´‡∏°‡πà
        # ‡πÉ‡∏ä‡πâ .copy() ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡πÑ‡∏õ‡πÅ‡∏Å‡πâ base_kwargs ‡∏ï‡∏±‡∏ß‡∏à‡∏£‡∏¥‡∏á‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏ñ‡∏π‡∏Å‡πÉ‡∏ä‡πâ‡∏ã‡πâ‡∏≥‡πÉ‡∏ô Loop ‡∏≠‡∏∑‡πà‡∏ô
        expert_kwargs = base_kwargs.copy()
        expert_kwargs.update({
            "pdca_blocks": expert_pdca_blocks,
            "sub_id": sub_id,
            "level": level,
            "is_expert_mode": True # ‡∏™‡πà‡∏á Flag ‡πÉ‡∏´‡πâ Prompt ‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏´‡∏°‡∏î‡∏ï‡∏£‡∏ß‡∏à‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
        })

        # 5. Execute LLM Call (‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏£‡∏∞‡∏ö‡∏ö‡∏Å‡∏±‡∏ô‡∏ï‡∏≤‡∏¢)
        re_eval_result = None
        try:
            # ‡∏•‡∏≠‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ 1 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏™‡∏π‡∏á (Expert Mode)
            re_eval_result = llm_evaluator_to_use(**expert_kwargs)
        except Exception as e:
            self.logger.error(f"‚ùå [APPEAL-FATAL] LLM Call failed: {e}")
            return {"is_passed": False, "score": 0.0, "reason": f"Appeal system error: {str(e)}"}

        # 6. ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏≠‡∏∏‡∏ó‡∏ò‡∏£‡∏ì‡πå
        if not isinstance(re_eval_result, dict):
            return {"is_passed": False, "score": 0.0, "reason": "Appeal result format error"}

        is_passed_now = bool(re_eval_result.get("is_passed", False))
        
        if is_passed_now:
            self.logger.info(f"üõ°Ô∏è [OVERRIDE-SUCCESS] {log_prefix} | ‡∏ú‡∏•‡∏≠‡∏∏‡∏ó‡∏ò‡∏£‡∏ì‡πå: ‡∏ú‡πà‡∏≤‡∏ô")
            re_eval_result.update({
                "is_safety_pass": True,
                "appeal_status": "GRANTED",
                "reason": f"üåü [EXPERT OVERRIDE]: {re_eval_result.get('reason', '')}"
            })
        else:
            self.logger.info(f"‚ùå [APPEAL-DENIED] {log_prefix} | ‡∏ú‡∏•‡∏≠‡∏∏‡∏ó‡∏ò‡∏£‡∏ì‡πå: ‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô")
            re_eval_result["appeal_status"] = "DENIED"

        return re_eval_result

    def _build_multichannel_context_for_level( # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô Private Method
        self, # ‡πÄ‡∏û‡∏¥‡πà‡∏° self
        level: int,
        top_evidences: List[Dict[str, Any]],
        previous_levels_map: Optional[Dict[str, Any]] = None,
        previous_levels_evidence: Optional[List[Dict[str, Any]]] = None,
        max_main_context_tokens: int = 3000, 
        max_summary_sentences: int = 4,
        max_context_length: Optional[int] = None, 
        **kwargs
    ) -> Dict[str, Any]:
        """
        [ULTIMATE OPTIMIZED v2026.8 - REFACTORED AS CLASS METHOD]
        """
        K_MAIN = 5
        # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å Config ‡πÉ‡∏ô Class ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢‡∏ñ‡πâ‡∏≤‡∏°‡∏µ ‡πÄ‡∏ä‡πà‡∏ô self.config.l1_threshold
        MIN_RELEVANCE_FOR_AUX = 0.15 if level == 1 else 0.4 

        # 1. Baseline Summary
        baseline_evidence = previous_levels_evidence or []
        if previous_levels_map:
            for lvl_ev in previous_levels_map.values():
                baseline_evidence.extend(lvl_ev)

        # Normalize list (‡∏ï‡∏≤‡∏° Logic ‡πÄ‡∏î‡∏¥‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡∏ö‡∏±‡πä‡∏Å Slice ‡πÅ‡∏•‡πâ‡∏ß)
        baseline_evidence_list = []
        if isinstance(baseline_evidence, list):
            baseline_evidence_list = baseline_evidence
        elif isinstance(baseline_evidence, dict):
            baseline_evidence_list = list(baseline_evidence.values())
        
        summarizable_baseline = [
            item for item in baseline_evidence_list[:40] 
            if isinstance(item, dict) and (item.get("text") or item.get("content", "")).strip()
        ]

        if not summarizable_baseline:
            baseline_summary = "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ (‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏ó‡∏µ‡πà Level 1)"
        else:
            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ú‡πà‡∏≤‡∏ô self
            baseline_summary = self._summarize_evidence_list_short(
                summarizable_baseline,
                max_sentences=max_summary_sentences
            )

        # 2. Direct + Aux Separation
        direct, aux_candidates = [], []

        for idx, ev in enumerate(top_evidences[:40], 1):
            if not isinstance(ev, dict): continue

            tag = (ev.get("pdca_tag") or ev.get("PDCA") or "Other").upper()
            relevance = ev.get("rerank_score") or ev.get("score", 0.0)
            text_preview = (ev.get('text', '')[:80] + "...") if ev.get('text') else "[No text]"

            # ‡πÉ‡∏ä‡πâ self.logger ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢
            self.logger.debug(f"[TAG-CHECK L{level} #{idx}] Rel: {relevance:.3f} | Tag: {tag} | Preview: {text_preview}")

            if tag in {"P", "PLAN", "D", "DO", "C", "CHECK", "A", "ACT"}:
                direct.append(ev)
            elif relevance >= MIN_RELEVANCE_FOR_AUX:
                aux_candidates.append(ev)

        # 3. Logic ‡∏Å‡∏≤‡∏£‡∏¢‡πâ‡∏≤‡∏¢ Aux ‡πÅ‡∏•‡∏∞ Fallback (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏° ‡πÅ‡∏ï‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô self.logger)
        if len(direct) < K_MAIN:
            need = K_MAIN - len(direct)
            moved = aux_candidates[:need]
            direct.extend(moved)
            aux_candidates = aux_candidates[need:]
            self.logger.info(f"[DIRECT-FILL] Moved {need} aux chunks to direct (total direct: {len(direct)})")

        if level == 1 and len(direct) == 0 and top_evidences:
            need = min(K_MAIN, len(top_evidences))
            forced_chunks = sorted(top_evidences, key=lambda e: e.get("rerank_score", 0) or e.get("score", 0), reverse=True)[:need]
            direct.extend(forced_chunks)
            self.logger.warning(f"[L1-ULTRA-FALLBACK] No PDCA tag at all ‚Üí Forced top {need} chunks to direct")

        # 5. ‡∏™‡∏£‡πâ‡∏≤‡∏á aux_summary (‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ú‡πà‡∏≤‡∏ô self)
        aux_summary = self._summarize_evidence_list_short(aux_candidates, max_sentences=3) if aux_candidates else \
            "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏£‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠"

        # 6. Return ‡∏û‡∏£‡πâ‡∏≠‡∏° Debug Meta
        self.logger.info(f"Context L{level} ‚Üí Direct:{len(direct)} | Aux:{len(aux_candidates)} | Baseline:{len(summarizable_baseline)}")

        return {
            "baseline_summary": baseline_summary,
            "direct_context": "", 
            "aux_summary": aux_summary,
            "debug_meta": {
                "level": level,
                "direct_count": len(direct),
                "aux_count": len(aux_candidates),
                "top_relevance": max((ev.get("rerank_score", 0) for ev in top_evidences), default=0)
            },
        }

    def _apply_diversity_filter(
        self,
        evidences: List[Dict[str, Any]],
        max_per_source: int = 3,
        max_total: int = 40,
    ):
        """
        Diversity & dedup filter for evidence chunks

        Rules:
        - Preserve priority chunks
        - Deduplicate by chunk_uuid
        - Limit chunks per source file
        - Sort by (priority, rerank_score)
        - Stable / deterministic
        """

        if not evidences:
            return []

        # --------------------------------------------------
        # 1Ô∏è‚É£ Deduplicate by chunk_uuid (or content hash)
        # --------------------------------------------------
        unique = {}
        for d in evidences:
            if not isinstance(d, dict):
                continue

            uid = d.get("chunk_uuid")
            if not uid:
                uid = hashlib.sha256(
                    str(d.get("page_content", "")).encode()
                ).hexdigest()

            if uid not in unique:
                unique[uid] = d

        docs = list(unique.values())

        # --------------------------------------------------
        # 2Ô∏è‚É£ Sort: priority first, then score
        # --------------------------------------------------
        docs = sorted(
            docs,
            key=lambda x: (
                bool(x.get("is_priority", False)),
                self.get_actual_score(x),
            ),
            reverse=True,
        )

        # --------------------------------------------------
        # 3Ô∏è‚É£ Enforce per-source diversity
        # --------------------------------------------------
        source_counter = {}
        diversified = []

        for d in docs:
            src = (
                d.get("source")
                or d.get("metadata", {}).get("source")
                or "unknown"
            )

            count = source_counter.get(src, 0)
            if count >= max_per_source:
                continue

            diversified.append(d)
            source_counter[src] = count + 1

            if len(diversified) >= max_total:
                break

        return diversified

    
    def _load_evidence_map(self, is_for_merge: bool = False) -> Dict[str, Any]:
        """
        [REVISED v2026.1.24]
        - ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏Å‡πà‡∏≤ (List) ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà (UI-Ready)
        - ‡∏î‡∏∂‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà User ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å (is_selected) ‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å
        """
        if hasattr(self, '_evidence_cache') and self._evidence_cache is not None:
            return deepcopy(self._evidence_cache)

        try:
            path = get_evidence_mapping_file_path(
                tenant=self.config.tenant, year=self.config.year, enabler=self.enabler
            )
        except: return {}

        if not os.path.exists(path):
            return {}

        try:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)

            processed_map = {}
            for key, content in data.items():
                # üîÑ Auto-Convert: ‡∏ñ‡πâ‡∏≤‡∏Ç‡∏≠‡∏á‡πÄ‡∏Å‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô List ‡πÉ‡∏´‡πâ‡∏¢‡∏±‡∏î‡πÉ‡∏™‡πà‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà
                if isinstance(content, list):
                    evidences = content
                    status = "pending"
                else:
                    evidences = content.get("evidences", [])
                    status = content.get("status", "pending")
                
                cleaned = []
                for e in evidences:
                    if not isinstance(e, dict): continue
                    # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏ü‡∏¥‡∏•‡∏î‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö UI
                    e["is_selected"] = e.get("is_selected", True)
                    e["source_type"] = e.get("source_type", "ai_found")
                    cleaned.append(e)
                
                processed_map[key] = {"status": status, "evidences": cleaned}

            self._evidence_cache = deepcopy(processed_map)
            return processed_map
        except Exception as e:
            self.logger.error(f"‚ùå Load failed: {e}")
            return {}


    # ------------------------------------------------------------------------------------------
    # [FIXED] üß© Persistence Helper: Update Internal Evidence
    # ------------------------------------------------------------------------------------------
    def _update_internal_evidence_map(self, merged_evidence: Dict[str, Any]):
        """
        [FINAL REVISED v2026.01.25 - THE PERSISTENCE GUARD]
        - üîÑ Live Sync: ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ Normalize ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        """
        if not hasattr(self, 'evidence_map') or self.evidence_map is None:
            self.evidence_map = {}
            
        if not isinstance(merged_evidence, dict): return

        def get_stable_hash(text: str) -> str:
            if not text: return ""
            target = f"{text[:250]}...{text[-250:]}" if len(text) > 500 else text
            return hashlib.md5(target.encode('utf-8')).hexdigest()

        for key, incoming_data in merged_evidence.items():
            new_ev_list = incoming_data.get("evidences", []) if isinstance(incoming_data, dict) else incoming_data
            if not isinstance(new_ev_list, list): continue
                
            if key not in self.evidence_map or not isinstance(self.evidence_map[key], dict):
                self.evidence_map[key] = {"status": "pending", "evidences": []}
            
            target_bucket = self.evidence_map[key]
            existing_hashes = {get_stable_hash(str(e.get('content') or e.get('text', ''))) for e in target_bucket["evidences"]}
            
            for ev in new_ev_list:
                if not isinstance(ev, dict): continue
                content_str = str(ev.get('content') or ev.get('text') or "").strip()
                if not content_str: continue 
                
                if get_stable_hash(content_str) not in existing_hashes:
                    # üéØ [POINT OF CHANGE]: ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô Normalize ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Manual Dict
                    normalized_batch = self._normalize_evidence_metadata([ev])
                    if normalized_batch:
                        clean_ev = normalized_batch[0]
                        # ‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏ß‡πà‡∏≤ Content ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏´‡∏≤‡∏¢‡πÑ‡∏õ
                        clean_ev["content"] = content_str
                        target_bucket["evidences"].append(clean_ev)
                        existing_hashes.add(get_stable_hash(content_str))

        self.logger.info(f"‚úÖ Sync complete. Total Groups: {len(self.evidence_map)}")

    # evidence map structure (for ai understanding)
    # {
    # ¬† "1.1_L1": {
    # ¬† ¬† "status": "reviewed",
    # ¬† ¬† "evidences": [
    # ¬† ¬† ¬† {
    # ¬† ¬† ¬† ¬† "doc_id": "15f0060f-674d-551e-b855-3b7e335450a8",
    # ¬† ¬† ¬† ¬† "filename": "KM1.1L502 Learning Form ‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£.pdf",
    # ¬† ¬† ¬† ¬† "page": "11",
    # ¬† ¬† ¬† ¬† "source_type": "human_map",
    # ¬† ¬† ¬† ¬† "is_selected": true,
    # ¬† ¬† ¬† ¬† "relevance_score": 0.95,
    # ¬† ¬† ¬† ¬† "note": "‡∏´‡πâ‡∏≤‡∏°‡∏•‡∏ö! ‡πÉ‡∏ä‡πâ‡∏¢‡∏±‡∏ô‡∏Ç‡πâ‡∏≠ 1.1 ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞"
    # ¬† ¬† ¬† },
    # ¬† ¬† ¬† {
    # ¬† ¬† ¬† ¬† "doc_id": "ai-file-999",
    # ¬† ¬† ¬† ¬† "filename": "KM_Policy_2567_Final.pdf",
    # ¬† ¬† ¬† ¬† "page": "1",
    # ¬† ¬† ¬† ¬† "source_type": "system_gen",
    # ¬† ¬† ¬† ¬† "relevance_score": 0.98
    # ¬† ¬† ¬† }
    # ¬† ¬† ]
    # ¬† }
    # }

    def _save_evidence_map(self, map_to_save: Optional[Dict[str, Any]] = None, clear_existing: bool = False):
        """
        [ULTIMATE REVISE v2026.1.25 - THE STABLE ATOMIC BUILD]
        - üéØ ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Nested Level Key: "1.1_L1", "1.1_L2"
        - üõ°Ô∏è Atomic Write: ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏û‡∏±‡∏á‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ tempfile + shutil.move
        - üßπ Post-Merge: ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
        """
        try:
            # 1. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Path ‡πÅ‡∏•‡∏∞ Folder
            map_file_path = get_evidence_mapping_file_path(
                tenant=self.config.tenant, year=self.config.year, enabler=self.enabler
            )
            os.makedirs(os.path.dirname(map_file_path), exist_ok=True)
            
            # 2. ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏°‡∏°‡∏≤‡∏ï‡∏±‡πâ‡∏á‡∏ï‡πâ‡∏ô (‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô‡∏™‡∏±‡πà‡∏á‡∏•‡πâ‡∏≤‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á)
            final_map = {} if clear_existing else self._load_evidence_map(is_for_merge=True)
            
            # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏°‡∏≤ ‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô Class memory
            incoming = map_to_save if map_to_save is not None else getattr(self, 'evidence_map', {})

            # 3. ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô Merge ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            for key, evidence_data in incoming.items():
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö Key (‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô 1.1_L1)
                if "_L" not in key:
                    self.logger.warning(f"‚ö†Ô∏è [EVIDENCE-MAP] Key format mismatch: '{key}' should be like '1.1_L1'")
                
                target_bucket = final_map.setdefault(key, {"status": "pending", "evidences": []})
                existing_evs = target_bucket["evidences"]
                
                # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ list ‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô
                new_evs = evidence_data.get("evidences", []) if isinstance(evidence_data, dict) else evidence_data
                if not isinstance(new_evs, list): continue

                for new_e in new_evs:
                    if not isinstance(new_e, dict): continue
                    
                    doc_id = new_e.get("doc_id") or new_e.get("chunk_uuid")
                    if not doc_id: continue

                    page = str(new_e.get("page") or new_e.get("page_label", "0"))
                    idx_key = f"{doc_id}_{page}"
                    
                    # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ß‡πà‡∏≤‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ‡πÉ‡∏ô bucket ‡∏ô‡∏µ‡πâ‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á (Deduplicate ‡∏£‡∏≤‡∏¢ Level)
                    match = next((e for e in existing_evs if f"{e.get('doc_id')}_{e.get('page')}" == idx_key), None)

                    if match:
                        # --- UPDATE EXISTING ---
                        match["relevance_score"] = float(new_e.get("relevance_score", match.get("relevance_score", 0.0)))
                        match["is_selected"] = new_e.get("is_selected", match.get("is_selected", True))
                        
                        if new_e.get("source_type") == "human_map":
                            match["source_type"] = "human_map"
                        if new_e.get("note"):
                            match["note"] = new_e["note"]
                    else:
                        # --- INSERT NEW ---
                        if not new_e.get("filename"):
                            new_e["filename"] = getattr(self, 'document_map', {}).get(doc_id, "Unknown File")
                        
                        new_node = {
                            "doc_id": doc_id,
                            "filename": new_e.get("filename"),
                            "page": page,
                            "source_type": new_e.get("source_type", "system_gen"),
                            "is_selected": new_e.get("is_selected", True),
                            "relevance_score": float(new_e.get("relevance_score", new_e.get("rerank_score", 0.0))),
                            "note": new_e.get("note", "")
                        }
                        existing_evs.append(new_node)

            # 4. üßπ Post-Processing: Sorting & Status Update
            for k in final_map:
                evs = final_map[k]["evidences"]
                # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏à‡∏≤‡∏Å‡∏™‡∏π‡∏á‡πÑ‡∏õ‡∏ï‡πà‡∏≥‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ UI/AI ‡πÄ‡∏´‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Å‡πà‡∏≠‡∏ô
                evs.sort(key=lambda x: x.get("relevance_score", 0.0), reverse=True)
                
                # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠
                has_human = any(e.get("source_type") == "human_map" for e in evs)
                final_map[k]["status"] = "reviewed" if has_human else "ai_generated"

            # 5. üõ°Ô∏è Atomic Saving: ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏ö‡∏ö‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
            temp_dir = os.path.dirname(map_file_path)
            with tempfile.NamedTemporaryFile(mode='w', delete=False, dir=temp_dir, suffix='.tmp', encoding="utf-8") as tmp:
                json.dump(final_map, tmp, indent=4, ensure_ascii=False)
                tmp_path = tmp.name
            
            # ‡∏¢‡πâ‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå temp ‡πÑ‡∏õ‡∏ó‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏£‡∏¥‡∏á (Atomic Operation ‡πÉ‡∏ô OS ‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡∏ç‡πà)
            shutil.move(tmp_path, map_file_path)
            
            # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï Cache ‡πÉ‡∏ô‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥‡∏î‡πâ‡∏ß‡∏¢
            self.evidence_map = final_map
            self.logger.info(f"‚úÖ [EVIDENCE-MAP] Save Successful: {map_file_path}")

        except Exception as e:
            self.logger.error(f"‚ùå [EVIDENCE-MAP] Fatal Save Error: {str(e)}")
            # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏•‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏¢‡∏∞‡∏ñ‡πâ‡∏≤‡πÄ‡∏Å‡∏¥‡∏î Error
            if 'tmp_path' in locals() and os.path.exists(tmp_path):
                os.remove(tmp_path)

    def merge_evidence_mappings(self, results_list: List[Any]) -> Dict[str, Any]:
        """
        [FIXED v2026.1.25] - ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Nested Level Key
        """
        merged_mapping = {}
        
        for item in results_list:
            if not item: continue
            
            # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏à‡∏≤‡∏Å tuple (res, worker_mem)
            res_data = item[0] if isinstance(item, tuple) else item
            worker_ev_map = item[1] if isinstance(item, tuple) and len(item) > 1 else {}

            # ‡∏Å‡∏£‡∏ì‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏≤‡∏à‡∏≤‡∏Å _run_single_assessment ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á (‡∏ú‡πà‡∏≤‡∏ô Parallel)
            if 'evidence_sources' in res_data:
                sub_id = res_data.get('sub_id')
                level = res_data.get('level')
                level_key = f"{sub_id}_L{level}"
                
                if level_key not in merged_mapping:
                    merged_mapping[level_key] = {"status": "pending", "evidences": []}
                
                # ‡∏ô‡∏≥‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà list
                merged_mapping[level_key]["evidences"].extend(res_data['evidence_sources'])
            
            # ‡∏£‡∏ß‡∏°‡∏à‡∏≤‡∏Å temp_map (worker_mem) ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
            if isinstance(worker_ev_map, dict):
                for l_key, ev_list in worker_ev_map.items():
                    if l_key not in merged_mapping:
                        merged_mapping[l_key] = {"status": "pending", "evidences": []}
                    
                    # ‡∏ñ‡πâ‡∏≤ l_key ‡πÄ‡∏õ‡πá‡∏ô "1.1" ‡πÄ‡∏â‡∏¢‡πÜ ‡πÉ‡∏´‡πâ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô "1.1_L?" (‡∏ñ‡πâ‡∏≤‡∏ó‡∏≥‡πÑ‡∏î‡πâ) ‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏õ
                    # ‡πÅ‡∏ï‡πà‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö‡πÉ‡∏´‡∏°‡πà Worker ‡∏Ñ‡∏ß‡∏£‡∏™‡πà‡∏á 1.1_L1 ‡∏°‡∏≤‡πÄ‡∏•‡∏¢
                    target_list = merged_mapping[l_key]["evidences"]
                    target_list.extend(ev_list if isinstance(ev_list, list) else [])

        # Deduplicate ‡πÅ‡∏ï‡πà‡∏•‡∏∞ Level Key ‡∏ó‡∏¥‡πâ‡∏á‡∏ó‡πâ‡∏≤‡∏¢
        for k in merged_mapping:
            merged_mapping[k]["evidences"] = self._deduplicate_list(merged_mapping[k]["evidences"])

        return merged_mapping
    
    def _deduplicate_list(self, items: List[Dict]) -> List[Dict]:
        """
        [ULTIMATE REVISED v2026.1.25 - SMART DEDUPE]
        - ‚öñÔ∏è ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ä‡∏¥‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
        - üß¨ ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö ID ‡∏ó‡∏∏‡∏Å‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏à‡∏≤‡∏Å VectorStore
        """
        if not items: return []
        unique_map = {}
        for item in items:
            if not isinstance(item, dict): continue
            raw_id = str(item.get('doc_id') or item.get('stable_doc_uuid') or item.get('chunk_uuid') or "unknown")
            doc_id = raw_id.replace("-", "").lower().strip()
            page = str(item.get('page') or item.get('page_label', '0')).strip()
            uid = f"{doc_id}_pg{page}"
            
            score = float(item.get('relevance_score') or item.get('rerank_score') or 0.0)
            if uid not in unique_map or score > float(unique_map[uid].get('relevance_score') or 0.0):
                unique_map[uid] = item
        
        res = list(unique_map.values())
        res.sort(key=lambda x: float(x.get('relevance_score') or x.get('rerank_score') or 0.0), reverse=True)
        return res
    
    def _merge_worker_results(self, sub_result: Dict[str, Any], temp_map: Dict[str, Any]):
        """
        [ULTIMATE REVISE v2026.1.25 - NESTED & PARALLEL SAFE]
        - üõ°Ô∏è Data Persistence: ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏ã‡∏¥‡∏á‡∏Ñ‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà Global State ‡πÄ‡∏™‡∏°‡∏≠
        - üß¨ Evidence Integrity: ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏π‡∏ç‡∏´‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á Audit Trail ‡πÅ‡∏°‡πâ‡∏ú‡∏•‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô 0
        - ‚öñÔ∏è Resilience: ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á‡∏û‡∏±‡∏á ‡πÅ‡∏ï‡πà‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏•‡πà‡∏≤‡∏á‡∏ú‡πà‡∏≤‡∏ô (Partial Pass)
        """
        if not sub_result:
            self.logger.warning("‚ö†Ô∏è Received empty sub_result in merge process.")
            return None

        # 1. üîç Identity & Type Setup
        sub_id = str(sub_result.get('sub_id', 'Unknown'))
        # ‡∏î‡∏∂‡∏á Level ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô (‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏•‡πÄ‡∏ß‡∏•‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏≠‡∏¢‡∏π‡πà)
        raw_lvl = sub_result.get('level') or sub_result.get('highest_full_level', 0)
        try:
            level_received = int(raw_lvl)
        except (ValueError, TypeError):
            level_received = 0
            
        # 2. üõ°Ô∏è Evidence Mapping Sync (The Audit Trail Guard)
        if temp_map and isinstance(temp_map, dict):
            if not hasattr(self, 'evidence_map'): self.evidence_map = {}
            
            for level_key, evidence_list in temp_map.items():
                if not evidence_list: continue
                
                # ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô Key: sub_id_L{level}
                formatted_key = level_key if "_L" in level_key else f"{sub_id}_L{level_received}"
                
                # ‡∏´‡∏≤‡∏Å‡πÄ‡∏õ‡πá‡∏ô L0 ‡πÉ‡∏´‡πâ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÄ‡∏î‡∏≤‡∏à‡∏≤‡∏Å level_received ‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏õ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á
                if "_L0" in formatted_key and level_received > 0:
                    formatted_key = f"{sub_id}_L{level_received}"

                target_node = self.evidence_map.setdefault(formatted_key, {"status": "completed", "evidences": []})
                existing_evs = target_node["evidences"]
                
                # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î Unique ID ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ã‡πâ‡∏≥ (doc_id + page)
                existing_uids = {f"{e.get('doc_id')}_{e.get('page')}" for e in existing_evs}
                
                for ev in evidence_list:
                    if not isinstance(ev, dict) or not ev: continue
                    
                    doc_id = ev.get('doc_id') or ev.get('stable_doc_uuid')
                    page = str(ev.get('page') or ev.get('page_label', '0'))
                    uid = f"{doc_id}_{page}"
                    
                    if uid not in existing_uids and doc_id not in [None, "na", "n/a", "none"]:
                        # Mapping filename ‡∏à‡∏≤‡∏Å document_map ‡∏Å‡∏•‡∏≤‡∏á
                        if hasattr(self, 'document_map') and doc_id in self.document_map:
                            ev['filename'] = self.document_map.get(doc_id)
                        
                        existing_evs.append(ev)
                        existing_uids.add(uid)

        # 3. üèóÔ∏è Final Sub-criteria Results Aggregation
        if not hasattr(self, 'final_subcriteria_results'):
            self.final_subcriteria_results = []

        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Object ‡πÄ‡∏î‡∏¥‡∏°‡πÉ‡∏ô List ‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ
        target = next((r for r in self.final_subcriteria_results if str(r.get('sub_id')) == sub_id), None)
        if not target:
            target = {
                "sub_id": sub_id,
                "sub_criteria_name": sub_result.get('sub_criteria_name') or f"Criteria {sub_id}",
                "weight": float(sub_result.get('weight', 4.0)),
                "level_details": {},
                "highest_full_level": 0,
                "weighted_score": 0.0,
                "is_passed": False,
                "audit_stop_reason": "Initialized",
                "pdca_overall": {"P": 0.0, "D": 0.0, "C": 0.0, "A": 0.0}
            }
            self.final_subcriteria_results.append(target)

        # 4. üß© Atomic Update (Merge level details)
        new_details = sub_result.get('level_details', {})
        if isinstance(new_details, dict) and new_details:
            # ‡∏ã‡∏¥‡∏á‡∏Ñ‡πå‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏£‡∏≤‡∏¢‡πÄ‡∏•‡πÄ‡∏ß‡∏• (L1, L2, L3...)
            target['level_details'].update(new_details)
        else:
            # Fallback ‡∏´‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏∏‡∏î‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡πâ‡∏≠‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
            if level_received > 0:
                target['level_details'][str(level_received)] = sub_result

        # 5. ‚öñÔ∏è Step-Ladder Maturity Calculation (Robust Logic)
        current_highest = 0
        stop_reason = "Assessment complete"
        pdca_sums = {"P": 0.0, "D": 0.0, "C": 0.0, "A": 0.0}
        passed_lv_count = 0
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô L1 -> L5 (‡∏ï‡πâ‡∏≠‡∏á‡∏ú‡πà‡∏≤‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏•‡πà‡∏≤‡∏á‡∏Å‡πà‡∏≠‡∏ô‡∏ñ‡∏∂‡∏á‡∏à‡∏∞‡∏ô‡∏±‡∏ö‡∏Ç‡∏±‡πâ‡∏ô‡∏ö‡∏ô)
        for l in range(1, 6):
            l_str = str(l)
            l_data = target['level_details'].get(l_str)
            
            if l_data and isinstance(l_data, dict):
                score_val = float(l_data.get('score', 0.0))
                # ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡∏ú‡πà‡∏≤‡∏ô: AI Flag ‡∏ß‡πà‡∏≤‡∏ú‡πà‡∏≤‡∏ô ‡∏´‡∏£‡∏∑‡∏≠ ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô >= 0.7
                is_lv_passed = (l_data.get('is_passed') is True or score_val >= 0.7)
                
                if is_lv_passed:
                    current_highest = l
                    l_data['is_passed'] = True # Force sync flag
                    
                    # ‡∏™‡∏∞‡∏™‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô PDCA ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°
                    bd = l_data.get('pdca_breakdown', {})
                    for phase in pdca_sums:
                        pdca_sums[phase] += float(bd.get(phase, 0.0))
                    passed_lv_count += 1
                else:
                    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏ó‡∏µ‡πà‡∏´‡∏¢‡∏∏‡∏î‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ï‡πà‡∏≠ (Gap ‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å)
                    stop_reason = f"Stopped at L{l}: {l_data.get('reason', 'Insufficient evidence')[:60]}..."
                    break
            else:
                # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏•‡πÄ‡∏ß‡∏•‡∏ô‡∏µ‡πâ ‡πÉ‡∏´‡πâ‡∏´‡∏¢‡∏∏‡∏î‡∏ô‡∏±‡∏ö (Chain broken)
                if l <= level_received: # ‡∏Å‡∏£‡∏ì‡∏µ‡πÄ‡∏•‡πÄ‡∏ß‡∏•‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡∏à‡∏∞‡∏°‡∏µ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ
                    stop_reason = f"Data missing at L{l}"
                    break
                break

        # 6. üí∞ Score & Status Finalization
        target['highest_full_level'] = current_highest
        target['is_passed'] = (current_highest >= 1)
        target['weighted_score'] = round(current_highest * target['weight'], 2)
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ PDCA ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏•‡πÄ‡∏ß‡∏•‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô
        if passed_lv_count > 0:
            target['pdca_overall'] = {k: round(v / passed_lv_count, 2) for k, v in pdca_sums.items()}
            
        target['audit_stop_reason'] = stop_reason if current_highest < 5 else "Maximum maturity level reached"
        
        # üõ°Ô∏è ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: Force update ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏ó‡∏µ‡πà class attribute ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Score: 0 ‡πÉ‡∏ô‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏£‡∏∏‡∏õ
        self.final_subcriteria_results = [
            target if str(r.get('sub_id')) == sub_id else r 
            for r in getattr(self, 'final_subcriteria_results', [])
        ]

        self.logger.info(f"üèÅ [MERGE-DONE] {sub_id} | Maturity: L{current_highest} | Weighted Score: {target['weighted_score']}")
        return target
    
    def run_assessment(
        self,
        target_sub_id: str = "all",
        export: bool = False,
        vectorstore_manager: Optional['VectorStoreManager'] = None,
        sequential: bool = False,
        document_map: Optional[Dict[str, str]] = None,
        record_id: str = None,
    ) -> Dict[str, Any]:
        """
        [FINAL REVISED v2026.01.25 - THE PERSISTENCE MASTER]
        - üõ°Ô∏è ‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à 100% ‡∏ß‡πà‡∏≤ Evidence ‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏´‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£ Sync State ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡∏ó‡∏µ‡πà‡∏à‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠
        - üß© ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏à‡∏∏‡∏î‡∏≠‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏ô Summary
        - üíæ ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö Save Mapping ‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
        """
        start_ts = time.time()
        self.is_sequential = sequential
        self.current_record_id = record_id or self.record_id
        
        # 1. Setup Document Map ‡πÅ‡∏•‡∏∞ Internal State
        if document_map:
            self.document_map.update(document_map)
        
        if not hasattr(self, 'evidence_map') or self.evidence_map is None:
            self.evidence_map = {}

        # üìÇ ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô
        self.flattened_rubric = self._flatten_rubric_to_statements()
        grouped_sub_criteria = self._group_statements_by_sub_criteria(self.flattened_rubric)

        is_all = str(target_sub_id).lower() == "all"
        sub_criteria_list = list(grouped_sub_criteria.values()) if is_all else [grouped_sub_criteria.get(target_sub_id)]
        
        if not all(sub_criteria_list):
            return self._create_failed_result(self.current_record_id, f"Criteria '{target_sub_id}' not found", start_ts)

        total_subs = len(sub_criteria_list)
        results_list = []

        # üß† 2. ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô (Core Engine)
        if is_all and not sequential:
            # [MODE A] PARALLEL
            max_workers = int(os.environ.get("MAX_PARALLEL_WORKERS", 4))
            worker_args = [self._prepare_worker_tuple(sub, self.document_map) for sub in sub_criteria_list]
            
            ctx = multiprocessing.get_context("spawn")
            with ctx.Pool(processes=max_workers) as pool:
                for idx, res_tuple in enumerate(pool.imap_unordered(_static_worker_process, worker_args)):
                    results_list.append(res_tuple)
                    
                    # üéØ CRITICAL FIX: ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö Merge Evidence ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡∏ó‡∏µ‡πà Worker ‡∏™‡πà‡∏á‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤
                    # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏¢‡∏±‡∏î‡πÄ‡∏Ç‡πâ‡∏≤ self.evidence_map ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡πÉ‡∏ô‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ
                    self._merge_worker_results(res_tuple[0], res_tuple[1])
                    
                    self.db_update_task_status(
                        progress=15 + int(((idx+1)/total_subs) * 65), 
                        message=f"üß† ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô {res_tuple[0].get('sub_id', '?')} ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à"
                    )
        else:
            # [MODE B] SEQUENTIAL
            if not vectorstore_manager: self._initialize_vsm_if_none()
            vsm = vectorstore_manager or self.vectorstore_manager

            for idx, sub_criteria in enumerate(sub_criteria_list):
                sub_id = str(sub_criteria.get("sub_id", "Unknown"))
                
                # Baseline Hydration
                prev_map = self._collect_previous_level_evidences(sub_id=sub_id, current_level=1)
                initial_baseline = [ev for evs in prev_map.values() for ev in evs]
                
                # Run Worker
                res, worker_mem = self._run_sub_criteria_assessment_worker(sub_criteria, vsm, initial_baseline)
                results_list.append((res, worker_mem))
                
                # üéØ CRITICAL FIX: ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï State ‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
                self._merge_worker_results(res, worker_mem)


        # -------------------------------------------------------
        # üß© 3. ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô (The Evidence Guard)
        # -------------------------------------------------------
        self.db_update_task_status(progress=85, message="üß© ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏à‡∏±‡∏î‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô")
        
        # ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà self.evidence_map
        full_raw_mapping = self.merge_evidence_mappings(results_list)
        self._update_internal_evidence_map(full_raw_mapping)
        
        # [CRITICAL FIX] ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏•‡πâ‡∏≤‡∏á‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πä‡∏∞‡∏Å‡πà‡∏≠‡∏ô‡∏ô‡∏±‡∏ö
        total_evidence_found = 0
        for key in list(self.evidence_map.keys()):
            bucket = self.evidence_map[key]
            if isinstance(bucket, dict) and "evidences" in bucket:
                # Deduplicate ‡∏ó‡∏¥‡πâ‡∏á‡∏ó‡πâ‡∏≤‡∏¢‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏£‡∏≠‡∏ö
                bucket["evidences"] = self._deduplicate_list(bucket["evidences"])
                count = len(bucket["evidences"])
                total_evidence_found += count
                if count > 0:
                    bucket["status"] = "ai_generated"
            else:
                # ‡∏ñ‡πâ‡∏≤‡∏´‡∏•‡∏∏‡∏î‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô list ‡πÉ‡∏´‡πâ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
                ev_list = self._deduplicate_list(bucket if isinstance(bucket, list) else [])
                self.evidence_map[key] = {"status": "ai_generated", "evidences": ev_list}
                total_evidence_found += len(ev_list)

        self.logger.info(f"üìä Sanitized Evidence Total: {total_evidence_found} items")
        self._save_evidence_map(map_to_save=self.evidence_map)

        # -------------------------------------------------------
        # üèÅ 4. ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏• (Final Response & Export)
        # -------------------------------------------------------
        master_roadmap_data = None
        if is_all and len(self.final_subcriteria_results) > 0:
            master_roadmap_data = self.synthesize_strategic_roadmap(
                sub_criteria_results=self.final_subcriteria_results,
                enabler_name=self.enabler,
                llm_executor=self.llm
            )

        overall_stats = self._calculate_overall_stats(target_sub_id)
        if not overall_stats:
            overall_stats = {"efficiency": 0.0, "score": 0.0, "passed_count": 0, "total_count": 0}
            
        # [FIX 4] ‡∏¢‡∏±‡∏î‡∏Ñ‡πà‡∏≤‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ô‡∏±‡∏ö‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á‡∏•‡∏á‡πÉ‡∏ô Summary
        overall_stats["evidence_used_count"] = total_evidence_found

        final_response = {
            "record_id": self.current_record_id,
            "status": "COMPLETED",
            "enabler": self.enabler,
            "summary": overall_stats,
            "sub_criteria_results": self.final_subcriteria_results,
            "evidence_audit_trail": self.evidence_map, # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡πÅ‡∏•‡πâ‡∏ß
            "strategic_roadmap": master_roadmap_data,
            "run_time_seconds": round(time.time() - start_ts, 2)
        }

        if export:
            # ‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå JSON
            final_response["export_path"] = self._export_results(final_response, target_sub_id)

        self.db_update_task_status(progress=100, message="‚úÖ ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå", status="COMPLETED")
        return final_response
    
    # ------------------------------------------------------------------
    # üèõÔ∏è [TIER-3 METHOD] synthesize_strategic_roadmap - FINAL PRODUCTION
    # ------------------------------------------------------------------
    def synthesize_strategic_roadmap(
        self,
        sub_criteria_results: List[Dict[str, Any]],
        enabler_name: str,
        llm_executor: Any
    ) -> Dict[str, Any]:
        """
        [TIER-3 STRATEGIC ORCHESTRATOR - v2026.3.26]
        ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏£‡∏≤‡∏¢‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏°‡∏≤‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ú‡∏ô‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏° (Master Roadmap)
        - ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤ JSON Malformed ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£ Quote ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ã‡πâ‡∏≠‡∏ô
        - ‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏° Action Plans ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏ö‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏î‡πâ
        """
        self.logger.info(f"üåê [TIER-3] Starting Master Strategic Roadmap Synthesis for {enabler_name}")
        
        if not sub_criteria_results:
            self.logger.warning("‚ö†Ô∏è No sub-criteria results available for synthesis")
            return {"status": "INCOMPLETE", "overall_strategy": "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏ú‡∏ô"}

        # 1. üìÇ Data Collection (Gap Aggregation)
        aggregated_insights = []
        for res in sub_criteria_results:
            sub_id = res.get("sub_id", "Unknown")
            sub_name = res.get("sub_criteria_name", "N/A")
            highest_lv = res.get("highest_full_level", 0)
            level_details = res.get("level_details", {})
            
            gap_recs = []
            # ‡πÄ‡∏Å‡πá‡∏ö Insight ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà "‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô" ‡∏´‡∏£‡∏∑‡∏≠ "‡∏ú‡πà‡∏≤‡∏ô‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏≤‡∏ö‡πÄ‡∏™‡πâ‡∏ô (Score < 0.7)"
            for lvl_idx in range(1, 6):
                lv_str = str(lvl_idx)
                detail = level_details.get(lv_str, {})
                score = float(detail.get("score", 0))
                is_passed = detail.get("is_passed", False)
                
                if not is_passed or score < 0.7:
                    # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î Quote ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ JSON ‡∏û‡∏±‡∏á‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï
                    insight = str(detail.get("coaching_insight") or "").replace('"', "'").strip()
                    if insight and insight not in gap_recs and len(insight) > 5:
                        gap_recs.append(f"[L{lv_str}] {insight}")

            summary_text = " | ".join(gap_recs[:3]) if gap_recs else "‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á (‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á)"
            aggregated_insights.append(f"üìå [{sub_id}] {sub_name} (Highest: L{highest_lv}): {summary_text}")

        # 2. üß† LLM Orchestration
        formatted_insights_text = "\n".join(aggregated_insights)
        
        # üí° ‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤ Prompt ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏∏‡∏ü‡∏≠‡∏£‡πå‡πÅ‡∏°‡∏ï JSON ‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
        final_prompt = MASTER_ROADMAP_PROMPT.format(
            sub_id="OVERALL",
            sub_criteria_name=enabler_name, 
            enabler=enabler_name, 
            aggregated_insights=formatted_insights_text
        )

        try:
            # ‡πÉ‡∏ä‡πâ Temperature 0.2 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏´‡∏ï‡∏∏‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏•
            response = llm_executor.generate(
                system=SYSTEM_MASTER_ROADMAP_PROMPT, 
                prompts=[final_prompt], 
                temperature=0.2
            )
            
            raw_text = getattr(response, 'content', str(response)).strip()
            
            # 3. üßπ Robust JSON Extraction
            # ‡πÉ‡∏ä‡πâ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏≠‡∏±‡∏õ‡πÄ‡∏Å‡∏£‡∏î‡∏Å‡∏±‡∏ô‡πÑ‡∏õ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ô‡∏µ‡πâ
            strategic_plan = _robust_extract_json(raw_text)
            
            # 4. üõ°Ô∏è Result Normalization (Ensure standard keys for Exporter)
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏´‡∏≤ Roadmap ‡πÉ‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡πÜ Key ‡∏ó‡∏µ‡πà AI ‡∏°‡∏±‡∏Å‡∏à‡∏∞‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏≤
            final_roadmap = (
                strategic_plan.get("roadmap") or 
                strategic_plan.get("strategic_roadmap") or 
                strategic_plan.get("action_plan") or []
            )

            # üö® Fallback: ‡∏ñ‡πâ‡∏≤‡∏™‡∏Å‡∏±‡∏î Roadmap ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ ‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á 1 Step ‡πÉ‡∏´‡∏ç‡πà‡∏à‡∏≤‡∏Å Raw Text
            if not final_roadmap and len(raw_text) > 20:
                final_roadmap = [{
                    "phase": "Strategic Improvement",
                    "target_levels": "Overall",
                    "main_objective": "‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏¢‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°",
                    "key_actions": [raw_text[:200] + "..."],
                    "expected_outcome": "‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡πÉ‡∏ô‡∏£‡∏≠‡∏ö‡∏ñ‡∏±‡∏î‡πÑ‡∏õ"
                }]

            return {
                "status": "SUCCESS",
                "overall_strategy": (strategic_plan.get("overall_strategy") or 
                                    strategic_plan.get("summary") or 
                                    f"‡πÅ‡∏ú‡∏ô‡∏Ç‡∏±‡∏ö‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå {enabler_name}"),
                "roadmap": final_roadmap,
                "metadata": {
                    "generated_at": datetime.now().isoformat(),
                    "input_sub_count": len(sub_criteria_results),
                    "enabler": enabler_name
                }
            }

        except Exception as e:
            self.logger.error(f"üí• Master Roadmap Critical Error: {str(e)}", exc_info=True)
            return {
                "status": "ERROR", 
                "overall_strategy": "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏ú‡∏ô‡πÑ‡∏î‡πâ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•",
                "roadmap": [],
                "reason": str(e)
            }

    def create_atomic_action_plan(
        self, 
        insight: str, 
        level: int, 
        level_criteria: str = "", 
        **kwargs
    ) -> List[Dict[str, Any]]:
        """
        [ULTIMATE REVISED v2026.01.25 - FINAL STABLE]
        - FIXED: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô system_msg ‡πÄ‡∏õ‡πá‡∏ô system_prompt ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö llm_data_utils.py
        - Clean Code: ‡πÉ‡∏ä‡πâ‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏à‡∏≤‡∏Å Header (re, SEAM_ENABLER_FULL_NAME_TH, ‡∏Ø‡∏•‡∏Ø)
        - Anti-IT Ghost: ‡∏Å‡∏£‡∏≠‡∏á‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏î‡πâ‡∏≤‡∏ô IT ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö L1-L3 ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£
        """
        try:
            # 1. Validation & Data Sanitization
            clean_insight = str(insight or "").strip()
            if not clean_insight or clean_insight.lower() in ["-", "n/a", "none", "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", ""]:
                return []

            # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏Å‡∏ì‡∏ë‡πå (Criteria)
            actual_criteria = level_criteria or kwargs.get('level_statement') or "‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô SE-AM"
            
            # 2. ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô f-string formatting error (Double Braces Escape)
            safe_insight = clean_insight.replace('"', "'").replace('{', '{{').replace('}', '}}')
            safe_criteria = str(actual_criteria).replace('"', "'").replace('{', '{{').replace('}', '}}')

            # 3. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£ Enabler (Mapping Name & Code)
            enabler_code = str(getattr(self, 'enabler', 'UNKNOWN')).upper()
            enabler_name_th = SEAM_ENABLER_FULL_NAME_TH.get(enabler_code, f"‡∏î‡πâ‡∏≤‡∏ô {enabler_code}")

            # 4. Packaging ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Prompt
            prompt_payload = {
                "coaching_insight": safe_insight,
                "level": level,
                "enabler": enabler_code,
                "enabler_name_th": enabler_name_th,
                "level_criteria": safe_criteria
            }

            try:
                # ‡πÉ‡∏ä‡πâ Template ‡∏ó‡∏µ‡πà Import ‡∏°‡∏≤‡∏à‡∏≤‡∏Å Header
                human_prompt = ATOMIC_ACTION_PROMPT.format(**prompt_payload)
                system_prompt_content = SYSTEM_ATOMIC_ACTION_PROMPT.format(
                    enabler_name_th=enabler_name_th, 
                    enabler=enabler_code
                )
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è [FORMAT-ERROR] {e} -> Use raw backup format")
                system_prompt_content = f"Expert Action Plan Generator for {enabler_name_th}"
                human_prompt = f"Insight: {safe_insight}\nLevel: {level}"

            # 5. LLM Execution (FIXED Parameter Name: system_prompt)
            # 
            raw_response = _fetch_llm_response(
                system_prompt=system_prompt_content, # ‚úÖ ‡πÅ‡∏Å‡πâ‡∏à‡∏≤‡∏Å system_msg ‡πÄ‡∏õ‡πá‡∏ô system_prompt ‡πÅ‡∏•‡πâ‡∏ß
                user_prompt=human_prompt,
                llm_executor=self.llm
            )

            # 6. Robust Extraction (JSON -> Regex Fallback)
            actions = []
            try:
                # ‡πÉ‡∏ä‡πâ helper _robust_extract_json ‡∏à‡∏≤‡∏Å Header
                parsed = _robust_extract_json(raw_response)
                if isinstance(parsed, list):
                    actions = parsed
                elif isinstance(parsed, dict):
                    actions = parsed.get("actions", [parsed])
            except:
                pass

            # Regex Scavenger (‡∏ñ‡πâ‡∏≤ JSON ‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤) - ‡πÉ‡∏ä‡πâ re ‡∏à‡∏≤‡∏Å Header
            if not actions:
                matches = re.findall(r'["\']action["\']\s*:\s*["\']([^"\']+)["\']', raw_response)
                for m in matches:
                    actions.append({"action": m, "target_evidence": "‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô"})

            # 7. Post-Processing & Anti-IT Ghost (L1-L3 Safety)
            final_actions = []
            it_ghost_terms = r"(‡∏£‡∏∞‡∏ö‡∏ö‡∏™‡∏≤‡∏£‡∏™‡∏ô‡πÄ‡∏ó‡∏®|KMS|Software|Automation|‡πÅ‡∏≠‡∏õ‡∏û‡∏•‡∏¥‡πÄ‡∏Ñ‡∏ä‡∏±‡∏ô|IT System|‡πÅ‡∏û‡∏•‡∏ï‡∏ü‡∏≠‡∏£‡πå‡∏°‡∏î‡∏¥‡∏à‡∏¥‡∏ó‡∏±‡∏•|‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå)"
            
            for item in actions:
                if not isinstance(item, dict): continue
                
                act_text = (item.get("action") or "").strip()
                if len(act_text) < 5: continue
                
                # ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ IT ‡∏≠‡∏≠‡∏Å‡∏ñ‡πâ‡∏≤‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏£‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô L1-L3 (‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏á‡∏≤‡∏ô‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£/‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°)
                if level <= 3:
                    act_text = re.sub(it_ghost_terms, "‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥/‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô/‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏°‡∏µ‡∏™‡πà‡∏ß‡∏ô‡∏£‡πà‡∏ß‡∏°", act_text, flags=re.IGNORECASE)
                
                final_actions.append({
                    "action": act_text,
                    "target_evidence": item.get("target_evidence", "‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö/‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°"),
                    "level": level
                })

            # 8. Emergency Fallback
            if not final_actions:
                final_actions = [{
                    "action": f"‡∏à‡∏±‡∏î‡∏ó‡∏≥‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô‡πÉ‡∏´‡πâ‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏£‡∏∞‡∏î‡∏±‡∏ö {level}",
                    "target_evidence": "‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô",
                    "level": level
                }]

            self.logger.info(f"‚úÖ [ATOMIC-PLAN] {enabler_code} L{level} Success (Output: {len(final_actions[:2])})")
            return final_actions[:2]

        except Exception as e:
            self.logger.error(f"üõë [ATOMIC-PLAN-CRITICAL] {str(e)}", exc_info=True)
            return [{"action": "‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏á‡∏≤‡∏ô‡∏ï‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞‡∏Ç‡∏≠‡∏á‡πÄ‡∏Å‡∏ì‡∏ë‡πå", "target_evidence": "‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÄ‡∏ä‡∏¥‡∏á‡∏õ‡∏£‡∏∞‡∏à‡∏±‡∏Å‡∏©‡πå", "level": level}]
        
    # ------------------------------------------------------------------
    # üèõÔ∏è [TIER-3 METHOD] generate_master_roadmap - FULL REVISE v2026.1.23
    # ------------------------------------------------------------------
    def generate_master_roadmap(self, sub_id, sub_criteria_name, enabler, aggregated_insights):
        """
        [TIER-3 STRATEGIC SYNTHESIS - v2026.01.24]
        ‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Roadmap ‡πÇ‡∏î‡∏¢‡∏Ñ‡∏≥‡∏ô‡∏∂‡∏á‡∏ñ‡∏∂‡∏á Maturity Capping ‡πÅ‡∏•‡∏∞ Step-Ladder Logic
        - üß© Maturity Aware: ‡πÅ‡∏¢‡∏Å‡πÅ‡∏¢‡∏∞‡πÄ‡∏•‡πÄ‡∏ß‡∏•‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á (Continuous) ‡∏Å‡∏±‡∏ö‡πÄ‡∏•‡πÄ‡∏ß‡∏•‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡πÅ‡∏ï‡πà‡πÇ‡∏î‡∏ô Cap
        - üõ†Ô∏è Strategic Alignment: ‡∏™‡∏±‡πà‡∏á LLM ‡πÉ‡∏´‡πâ‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏ã‡πà‡∏≠‡∏°‡πÅ‡∏ã‡∏° "‡∏£‡∏≠‡∏¢‡∏ï‡πà‡∏≠‡∏ó‡∏µ‡πà‡∏Ç‡∏≤‡∏î" ‡∏Å‡πà‡∏≠‡∏ô‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏™‡πà‡∏ß‡∏ô‡∏õ‡∏•‡∏≤‡∏¢
        """
        
        self.logger.info(f"üîÆ [MASTER-ROADMAP] Starting synthesis for {sub_id} ({sub_criteria_name})")

        if not aggregated_insights:
            self.logger.warning(f"‚ö†Ô∏è No insights for {sub_id} - Using emergency fallback")
            return self._get_emergency_fallback_plan(sub_id, sub_criteria_name, "No insights provided")

        # 1. üìÇ Data Condensing & Maturity Tagging
        condensed_insights = []
        highest_continuous = 0
        has_gap_before = False
        
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏Ç‡∏≤‡∏î‡∏ä‡πà‡∏ß‡∏á (Maturity Gap)
        for item in aggregated_insights:
            lv = int(item.get('level', 0))
            is_passed = item.get('status') == "PASSED"
            is_capped = item.get('is_capped', False)
            
            # ‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ LLM
            if is_passed and not has_gap_before:
                status_text = "‚úÖ PASSED (Maturity ‡∏ô‡∏±‡∏ö)"
                highest_continuous = lv
            elif is_passed and has_gap_before:
                status_text = "‚ö†Ô∏è PASSED (CAPPED - ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÅ‡∏ô‡πà‡∏ô)"
            else:
                status_text = "‚ùå FAILED (GAP)"
                has_gap_before = True # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏Å‡∏¥‡∏î‡∏£‡∏≠‡∏¢‡∏ï‡πà‡∏≠‡∏ó‡∏µ‡πà‡∏Ç‡∏≤‡∏î

            insight = item.get('insight_summary') or item.get('reason') or '‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î'
            condensed_insights.append(f"Level {lv} [{status_text}]: {insight[:250]}")

        summary_context = "\n".join(condensed_insights)
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏° Metadata ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏∏‡∏°‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á LLM
        strategic_focus = f"‡∏£‡∏∞‡∏î‡∏±‡∏ö Maturity ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏´‡∏¢‡∏∏‡∏î‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà‡πÄ‡∏•‡πÄ‡∏ß‡∏• {highest_continuous} "
        if has_gap_before:
            strategic_focus += "‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö‡∏£‡∏≠‡∏¢‡∏ï‡πà‡∏≠ (Gap) ‡πÉ‡∏ô‡πÄ‡∏•‡πÄ‡∏ß‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏ã‡πà‡∏≠‡∏°‡πÅ‡∏ã‡∏°‡∏£‡∏≠‡∏¢‡∏ï‡πà‡∏≠‡∏ô‡∏µ‡πâ"
        else:
            strategic_focus += "‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏£‡∏∞‡∏î‡∏±‡∏ö‡πÅ‡∏•‡∏∞‡∏ï‡πà‡∏≠‡∏¢‡∏≠‡∏î‡∏™‡∏π‡πà‡πÄ‡∏•‡πÄ‡∏ß‡∏•‡∏ñ‡∏±‡∏î‡πÑ‡∏õ"

        # 2. üìù Prompt Construction
        # ‡∏™‡πà‡∏á‡∏Ñ‡πà‡∏≤ strategic_focus ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô Prompt ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏Å‡∏î‡πå AI
        try:
            formatted_prompt = MASTER_ROADMAP_PROMPT.format(
                sub_id=sub_id,
                sub_criteria_name=sub_criteria_name,
                enabler=enabler,
                aggregated_insights=summary_context,
                strategic_focus=strategic_focus # üëà ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÑ‡∏Å‡∏î‡πå‡πÑ‡∏•‡∏ô‡πå‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á Maturity
            )
        except Exception as fe:
            self.logger.error(f"‚ùå Prompt formatting error: {fe}")
            formatted_prompt = f"Summarize roadmap for {sub_criteria_name} (Focus: {strategic_focus}): {summary_context}"

        # 3. üß† LLM Execution & 4. üßπ Extraction (Logic ‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß)
        try:
            raw_json_str = _fetch_llm_response(
                system_prompt=SYSTEM_MASTER_ROADMAP_PROMPT,
                user_prompt=formatted_prompt,
                max_retries=3,
                llm_executor=self.llm 
            )

            master_data = _robust_extract_json(raw_json_str)
            
            if not master_data:
                return self._get_emergency_fallback_plan(sub_id, sub_criteria_name, "Hollow JSON")

            # 5. üèóÔ∏è UI-Ready Normalization
            final_strategy = master_data.get("overall_strategy") or master_data.get("summary") or "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡πÑ‡∏î‡πâ"
            raw_phases = master_data.get("phases") or master_data.get("roadmap") or []

            normalized_phases = []
            for i, p in enumerate(raw_phases, 1):
                if isinstance(p, dict):
                    # ‡πÄ‡∏ï‡∏¥‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ç‡∏±‡πâ‡∏ô‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
                    p["step_label"] = f"Phase {i}"
                    normalized_phases.append(p)
                else:
                    normalized_phases.append({"step": f"Phase {i}", "action": str(p)})

            self.logger.info(f"‚úÖ [MASTER-ROADMAP] Synthesis Success for {sub_id} (Maturity: {highest_continuous})")
            
            return {
                "sub_id": sub_id,
                "sub_criteria_name": sub_criteria_name,
                "highest_maturity_level": highest_continuous, # üõ°Ô∏è ‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏à‡∏£‡∏¥‡∏á
                "overall_strategy": final_strategy,
                "phases": normalized_phases,
                "status": "SUCCESS",
                "is_gap_detected": has_gap_before,
                "generated_at": datetime.now().isoformat()
            }

        except Exception as e:
            self.logger.error(f"üí• Critical error in Master Roadmap {sub_id}: {str(e)}")
            return self._get_emergency_fallback_plan(sub_id, sub_criteria_name, str(e))
            
    def _get_emergency_fallback_plan(self, sub_id, name, error_msg=""):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡∏™‡∏≥‡∏£‡∏≠‡∏á‡∏Å‡∏£‡∏ì‡∏µ LLM ‡∏û‡∏±‡∏á ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡∏´‡∏¢‡∏∏‡∏î‡∏ó‡∏≥‡∏á‡∏≤‡∏ô"""
        return {
            "overall_strategy": "‡πÅ‡∏ú‡∏ô‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô (Fallback Mode)",
            "phases": [
                {
                    "phase": "Quick Win",
                    "goal": f"‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ç‡πâ‡∏≠‡∏ö‡∏Å‡∏û‡∏£‡πà‡∏≠‡∏á‡πÉ‡∏ô {name}",
                    "actions": [{"action": "‡∏™‡∏≠‡∏ö‡∏ó‡∏≤‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Gap ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô", "priority": "High"}]
                }
            ],
            "status": "FALLBACK",
            "error_context": error_msg[:100]
        }
    

    def _apply_evidence_cap(self, evidence_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        [NEW v2026.1.24] ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Cap ‡∏Ç‡∏ô‡∏≤‡∏î‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏™‡∏∞‡∏™‡∏°‡∏ï‡∏≤‡∏° Strategy ‡∏ó‡∏µ‡πà‡∏ï‡∏±‡πâ‡∏á‡πÑ‡∏ß‡πâ
        """
        if not evidence_list:
            return []

        # 1. Deduplicate ‡∏Å‡πà‡∏≠‡∏ô
        unique_evidences = self._deduplicate_list(evidence_list)

        # 2. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        if EVIDENCE_SELECTION_STRATEGY == "score":
            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô Rerank ‡∏à‡∏≤‡∏Å‡∏™‡∏π‡∏á‡πÑ‡∏õ‡∏ï‡πà‡∏≥ ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏±‡∏ß‡∏ó‡πá‡∏≠‡∏õ
            sorted_list = sorted(
                unique_evidences, 
                key=lambda x: x.get('rerank_score', 0) if isinstance(x, dict) else 0, 
                reverse=True
            )
        else:
            # ‡πÅ‡∏ö‡∏ö‡πÄ‡∏î‡∏¥‡∏°: ‡πÄ‡∏≠‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠‡πÉ‡∏´‡∏°‡πà‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î (‡∏ï‡∏±‡∏î‡∏ó‡πâ‡∏≤‡∏¢)
            sorted_list = unique_evidences

        # 3. Cap ‡∏Ç‡∏ô‡∏≤‡∏î‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡∏±‡πâ‡∏á‡πÑ‡∏ß‡πâ‡πÉ‡∏ô global_vars
        return sorted_list[:EVIDENCE_CUMULATIVE_CAP]
    

    # ------------------------------------------------------------------------------------------
    # üß† [TIER-1 & TIER-2 WORKER] Sequential Assessment (HYDRATED) - FULL REVISED v2026.1.25
    # ------------------------------------------------------------------------------------------
    def _run_sub_criteria_assessment_worker(
        self,
        sub_criteria: Dict[str, Any],
        vectorstore_manager: Optional[Any] = None,
        initial_baseline: Optional[List[Dict[str, Any]]] = None,
    ) -> Tuple[Dict[str, Any], Dict[str, List[Dict[str, Any]]]]:
        """
        [PATCHED v2026.01.25] ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏£‡∏≤‡∏¢ Sub-Criteria ‡πÅ‡∏ö‡∏ö‡πÄ‡∏à‡∏≤‡∏∞‡∏•‡∏∂‡∏Å 1-5 ‡πÄ‡∏•‡πÄ‡∏ß‡∏• 
        - Fix Parameter Mismatch (level_criteria)
        - Add Atomic-Plan Isolation (try-except)
        """
        sub_id = str(sub_criteria.get("sub_id", "Unknown"))
        sub_name = sub_criteria.get("sub_criteria_name", "No Name")
        sub_weight = float(sub_criteria.get("weight", 0.0))
        target_limit = getattr(self.config, "target_level", 5)

        vsm = vectorstore_manager or getattr(self, "vectorstore_manager", None)
        level_details = {}
        roadmap_input_bundle = []
        
        highest_continuous_level = 0
        is_still_continuous = True 
        cumulative_baseline = list(initial_baseline or [])
        levels = sorted(sub_criteria.get("levels", []), key=lambda x: x.get("level", 0))

        for stmt in levels:
            level = int(stmt.get("level", 0))
            if level == 0 or level > target_limit: continue

            # --- üéØ 1. PER-LEVEL MAP HYDRATION ---
            level_key = f"{sub_id}_L{level}"
            map_data = self.evidence_map.get(level_key, {})
            saved_evidences = map_data.get("evidences", []) if isinstance(map_data, dict) else map_data
            priority_items = [e for e in saved_evidences if e.get("is_selected", True)] if saved_evidences else []

            # --- üß† 2. CORE ASSESSMENT ---
            current_baseline = self._deduplicate_list(cumulative_baseline + priority_items)
            res = self._run_single_assessment(
                sub_id=sub_id, level=level,
                criteria={"name": sub_name, "statement": stmt.get("statement", ""), "sub_criteria_name": sub_name},
                keyword_guide=stmt.get("keywords", []),
                baseline_evidences=current_baseline,
                vectorstore_manager=vsm,
            )

            # [SYNC STATE] ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤ Memory
            self._update_internal_evidence_map({f"{sub_id}_L{level}": res.get("top_chunks_data", [])})
            
            is_passed_by_llm = bool(res.get("is_passed", False))
            
            # --- ‚öñÔ∏è 3. STEP-LADDER MATURITY LOGIC ---
            if is_passed_by_llm:
                new_found = res.get("top_chunks_data", [])
                cumulative_baseline.extend(new_found)
                cumulative_baseline = self._apply_evidence_cap(cumulative_baseline)
                if is_still_continuous: highest_continuous_level = level
            else:
                is_still_continuous = False

            # --- üõ†Ô∏è 4. ATOMIC ACTION PLAN (Isolated Call) ---
            try:
                # [FIXED] ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô level_statement ‡πÄ‡∏õ‡πá‡∏ô level_criteria ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö def ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô
                atomic_actions = self.create_atomic_action_plan(
                    insight=res.get("coaching_insight", ""),
                    level=level,
                    level_criteria=stmt.get("statement", "")
                )
            except Exception as e:
                # ‡∏ñ‡πâ‡∏≤‡∏û‡∏±‡∏á ‡πÉ‡∏´‡πâ Log Error ‡πÅ‡∏ï‡πà‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏î‡πâ
                self.logger.error(f"[ATOMIC-PLAN-ERROR] L{level} for {sub_id}: {str(e)}", exc_info=True)
                atomic_actions = []

            level_details[str(level)] = {
                "level": level, 
                "is_passed": is_passed_by_llm, 
                "is_maturity_capped": (is_passed_by_llm and not is_still_continuous),
                "score": float(res.get("score", 0.0)) if is_still_continuous else 0.25,
                "reason": res.get("reason", ""),
                "coaching_insight": res.get("coaching_insight", ""),
                "atomic_action_plan": atomic_actions, 
                "pdca_breakdown": res.get("pdca_breakdown", {}),
                "evidence_sources": res.get("top_chunks_data", []),
                "judicial_review_applied": res.get("is_safety_pass", False)
            }

            roadmap_input_bundle.append({
                "level": level, "status": "PASSED" if is_passed_by_llm else "FAILED",
                "is_capped": (is_passed_by_llm and not is_still_continuous),
                "insight_summary": res.get("coaching_insight", "")[:200]
            })

        # --- üîÆ 5. MASTER ROADMAP SYNTHESIS ---
        master_roadmap = self.generate_master_roadmap(
            sub_id=sub_id, sub_criteria_name=sub_name,
            enabler=getattr(self, "enabler", "KM"), aggregated_insights=roadmap_input_bundle
        )

        return {
            "sub_id": sub_id, 
            "sub_criteria_name": sub_name, 
            "highest_full_level": highest_continuous_level, 
            "weighted_score": round(highest_continuous_level * sub_weight, 2),
            "is_passed": highest_continuous_level >= 1,
            "level_details": level_details, 
            "strategic_roadmap": master_roadmap
        }, self.evidence_map

    def _get_level_constraint_prompt(self, sub_id: str, level: int, req_phases: list = None, spec_rule: str = None) -> str:
        """
        [ULTIMATE REVISED v2026.1.25 - SCOPE GUARD ENABLED]
        """
        enabler = getattr(self, 'enabler', 'KM').upper()
        enabler_name = "‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ (KM)"
        
        level_goal = get_pdca_goal_for_level(level)
        level_focus = PDCA_PHASE_MAP.get(level, "‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô")

        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Mandatory Phases ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏£‡∏¥‡∏á‡∏Ç‡∏≠‡∏á Maturity
        required_phases = req_phases or []
        if not required_phases:
            if level >= 4: required_phases = ['P', 'D', 'C']
            elif level >= 2: required_phases = ['P', 'D']
            else: required_phases = ['P']

        req_str = " + ".join(required_phases)

        lines = [
            f"\n### üõ°Ô∏è [AUDIT GUIDELINE: {enabler} - LEVEL {level}] ###",
            f"üéØ ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô: {enabler_name} | {sub_id}",
            f"üö© ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡∏µ‡πâ: {level_goal}",
            f"üîç ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö (Mandatory PDCA): {req_str}",
            f"üìå ‡∏à‡∏∏‡∏î‡πÄ‡∏ô‡πâ‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: {level_focus}",
            f"üí° [‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÉ‡∏ô‡∏£‡∏π‡∏ö‡∏£‡∏¥‡∏Å]: {spec_rule}" if spec_rule else "",
            "\n‚ö†Ô∏è [‡∏Å‡∏é‡πÄ‡∏´‡∏•‡πá‡∏Å‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô - Strict Rules]:",
            f"1. [Maturity Scope Guard] ‡∏´‡πâ‡∏≤‡∏°‡∏ô‡∏≥‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á (‡πÄ‡∏ä‡πà‡∏ô AI, Automation, Benchmarking) ‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡πÉ‡∏´‡πâ '‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô' ‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö {level} ‡∏´‡∏≤‡∏Å‡∏£‡∏π‡∏ö‡∏£‡∏¥‡∏Å‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏∞‡∏ö‡∏∏‡πÑ‡∏ß‡πâ",
            "2. [L1-L2 Policy Check] ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ß‡∏≤‡∏á‡∏£‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô ‡∏´‡∏≤‡∏Å‡∏û‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ '‡πÅ‡∏ú‡∏ô‡πÅ‡∏°‡πà‡∏ö‡∏ó' ‡∏´‡∏£‡∏∑‡∏≠ '‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®' ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏•‡∏≤‡∏¢‡πÄ‡∏ã‡πá‡∏ô‡∏≠‡∏ô‡∏∏‡∏°‡∏±‡∏ï‡∏¥ ‡πÉ‡∏´‡πâ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå 'P' ‡πÅ‡∏•‡∏∞ 'D' ‡πÉ‡∏ô‡πÄ‡∏ä‡∏¥‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ó‡∏±‡∏ô‡∏ó‡∏µ",
            f"3. {GLOBAL_EVIDENCE_INSTRUCTION}",
            "4. [Substance over Form] ‡πÄ‡∏ô‡πâ‡∏ô‡∏î‡∏π‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ß‡πà‡∏≤‡∏ï‡∏≠‡∏ö‡πÇ‡∏à‡∏ó‡∏¢‡πå‡∏ß‡∏¥‡∏™‡∏±‡∏¢‡∏ó‡∏±‡∏®‡∏ô‡πå‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡∏î‡∏π‡πÅ‡∏Ñ‡πà‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå",
            "5. [Coaching Insight] ‡∏´‡∏≤‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ß‡πà‡∏≤ '‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô' ‡∏ï‡πâ‡∏≠‡∏á‡∏ö‡∏≠‡∏Å‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏Ç‡∏≤‡∏î‡∏´‡∏≤‡∏¢‡πÑ‡∏õ‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô‡πÑ‡∏õ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å‡∏à‡∏∏‡∏î"
        ]
        return "\n".join(filter(None, lines))


    # ------------------------------------------------------------------------------------------
    # üß† [TIER-1 CORE] _run_single_assessment (GOVERNANCE-LOCKED & FULL AUDIT TRACE)
    # ------------------------------------------------------------------------------------------
    def _run_single_assessment(
        self,
        sub_id: str,
        level: int,
        criteria: Dict[str, Any],
        keyword_guide: List[str],
        baseline_evidences: List[Dict[str, Any]],
        vectorstore_manager: Optional[Any] = None,
    ) -> Dict[str, Any]:
        """
        [ULTIMATE REVISED v2026.1.25]
        - üõ°Ô∏è Governance: ‡∏â‡∏µ‡∏î audit_instruction ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏∏‡∏°‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à (Scope Guard)
        - üß¨ Retrieval Expansion: ‡∏ú‡∏™‡∏≤‡∏ô Adaptive Retrieval ‡∏Å‡∏±‡∏ö Neighbor Expansion
        - ‚öñÔ∏è Resilience: ‡∏£‡∏∞‡∏ö‡∏ö Judicial Review (‡∏≠‡∏∏‡∏ó‡∏ò‡∏£‡∏ì‡πå) ‡πÄ‡∏°‡∏∑‡πà‡∏≠ Rerank Score ‡∏™‡∏π‡∏á‡πÅ‡∏ï‡πà LLM ‡∏°‡∏≠‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏´‡πá‡∏ô
        - üìä Integrity: ‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏Å‡∏≤‡∏£ Log PDCA Status ‡πÅ‡∏•‡∏∞ Multichannel Context ‡∏ï‡∏≤‡∏°‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡πÄ‡∏î‡∏¥‡∏°
        """
        log_prefix = f"Sub:{sub_id} L{level}"
        sub_name = criteria.get('name', 'Unknown Sub-item')
        statement_text = criteria.get('statement', 'No statement defined')
        
        self.logger.info(f"üîç [START-ASSESSMENT] {log_prefix} | {sub_name}")
        self.logger.info(f"üìã [CRITERIA] Level {level}: \"{statement_text}\"")

        # --- [STEP 1: GOVERNANCE & RULES] ---
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á "‡πÉ‡∏ö‡∏™‡∏±‡πà‡∏á‡∏á‡∏≤‡∏ô" (Scope Guard) ‡∏à‡∏≤‡∏Å‡πÄ‡∏Å‡∏ì‡∏ë‡πå Maturity
        audit_instruction = self._get_level_constraint_prompt(sub_id, level)
        current_rules = getattr(self, 'contextual_rules_map', {}).get(sub_id, {}).get(f"L{level}", {})

        # --- [STEP 2: ADAPTIVE RETRIEVAL & EXPANSION] ---
        # 1. ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö Adaptive (Vector + Rerank)
        retrieved_chunks, max_rerank = self._perform_adaptive_retrieval(
            sub_id=sub_id,
            level=level,
            stmt=statement_text,
            vectorstore_manager=vectorstore_manager,
        )

        # 2. ‚ú® Neighbor Expansion: ‡∏Å‡∏π‡πâ‡∏Ñ‡∏∑‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á (‡∏´‡∏ô‡πâ‡∏≤‡∏•‡∏á‡∏ô‡∏≤‡∏°/‡∏´‡∏ô‡πâ‡∏≤‡∏ú‡∏ô‡∏ß‡∏Å)
        if retrieved_chunks:
            enabler_key = str(getattr(self, 'enabler', 'km')).lower()
            collection_name = f"evidence_{enabler_key}"
            retrieved_chunks = self._expand_context_with_neighbor_pages(
                top_evidences=retrieved_chunks, 
                collection_name=collection_name
            )

        # 3. Diversity Filter: ‡∏Å‡∏£‡∏≠‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        retrieved_chunks = self._apply_diversity_filter(retrieved_chunks, level)

        # --- [STEP 3: EVIDENCE FUSION & METADATA] ---
        # ‡∏ú‡∏™‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ö Baseline ‡∏™‡∏∞‡∏™‡∏°
        evidences = (baseline_evidences or []) + (retrieved_chunks or [])

        # ‡πÅ‡∏¢‡∏Å‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ï‡∏≤‡∏° PDCA Tags
        pdca_blocks = self._get_pdca_blocks_from_evidences(
            evidences=evidences,
            baseline_evidences=baseline_evidences,
            level=level,
            sub_id=sub_id,
            contextual_rules_map=getattr(self, 'contextual_rules_map', {})
        )

        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Audit Confidence (‡πÉ‡∏ä‡πâ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡πÉ‡∏ô Report)
        audit_confidence = self.calculate_audit_confidence(
            matched_chunks=retrieved_chunks,
            sub_id=sub_id,
            level=level,
        )
        self.current_audit_meta = audit_confidence # üõ°Ô∏è ‡πÄ‡∏Å‡πá‡∏ö Metadata ‡πÄ‡∏î‡∏¥‡∏°‡πÑ‡∏ß‡πâ

        # --- [STEP 4: MULTICHANNEL LLM EXECUTION] ---
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Multichannel Context (Current + Historical)
        llm_context = self._build_multichannel_context_for_level(
            level=level,
            top_evidences=retrieved_chunks,
            previous_levels_evidence=baseline_evidences
        )

        # ‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ LLM ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏î‡πâ‡∏ß‡∏¢‡πÉ‡∏ö‡∏™‡∏±‡πà‡∏á‡∏á‡∏≤‡∏ô‡∏Ñ‡∏∏‡∏°‡∏Å‡∏é
        llm_raw = self.evaluate_pdca(
            pdca_blocks=pdca_blocks,
            sub_id=sub_id,
            level=level,
            audit_confidence=audit_confidence,
            audit_instruction=audit_instruction # üëà ‡∏â‡∏µ‡∏î‡πÉ‡∏ö‡∏™‡∏±‡πà‡∏á‡∏á‡∏≤‡∏ô‡∏Ñ‡∏∏‡∏°‡∏Å‡∏é
        )
        if not isinstance(llm_raw, dict): llm_raw = {}

        # --- [STEP 5: SMART NORMALIZATION] ---
        result = self.post_process_llm_result(
            llm_output=llm_raw,
            level=level,
            sub_id=sub_id,
            contextual_config=current_rules,
            top_evidences=retrieved_chunks
        )

        # --- [STEP 6: JUDICIAL REVIEW (RESCUE LOGIC)] ---
        is_safety_pass = False
        # ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô Rerank ‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å‡πÅ‡∏ï‡πà AI ‡πÉ‡∏´‡πâ‡∏ï‡∏Å ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏≠‡∏∏‡∏ó‡∏ò‡∏£‡∏ì‡πå‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
        if not result.get("is_passed") and max_rerank >= 0.70:
            self.logger.info(f"‚öñÔ∏è [TRIGGER-APPEAL] {log_prefix} | Rerank {max_rerank:.4f} is high. Re-evaluating...")

            appeal_result = self._run_expert_re_evaluation(
                sub_id=sub_id,
                level=level,
                statement_text=statement_text,
                context=str(llm_context.get("full_context", "")),
                first_attempt_reason=result.get("reason", "‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÑ‡∏°‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô"),
                missing_tags=result.get("missing_phases", []),
                highest_rerank_score=max_rerank,
                sub_criteria_name=sub_name,
                llm_evaluator_to_use=self.evaluate_pdca,
                audit_instruction=audit_instruction,
                base_kwargs={
                    "pdca_blocks": pdca_blocks,
                    "contextual_config": current_rules,
                    "top_evidences": retrieved_chunks
                }
            )

            if appeal_result and appeal_result.get("appeal_status") == "GRANTED":
                self.logger.info(f"‚úÖ [APPEAL-GRANTED] {log_prefix} | Expert Rescue Successful.")
                final_appeal = self.post_process_llm_result(
                    llm_output=appeal_result,
                    level=level,
                    sub_id=sub_id,
                    contextual_config=current_rules,
                    top_evidences=retrieved_chunks
                )
                result.update(final_appeal)
                result["is_passed"] = True
                result["score"] = max(result.get("score", 0.0), 1.0)
                is_safety_pass = True

        # --- [STEP 7: FINAL INSIGHTS & LOGGING] ---
        final_insight = (result.get("coaching_insight") or result.get("reason") or "").strip()
        final_insight = f"[{'STRENGTH' if result.get('is_passed') else 'GAP'}] {final_insight}"

        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ Logger ‡πÄ‡∏î‡∏¥‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏Å‡πá‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ PDCA ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        if hasattr(self, "_log_pdca_status"):
            self._log_pdca_status(
                sub_id=sub_id,
                name=sub_name,
                level=level,
                blocks=llm_raw,
                req_phases=result.get("required_phases", []),
                sources_count=len(retrieved_chunks),
                score=result.get("score", 0.0),
                conf_level=audit_confidence.get("level", "LOW"),
                pdca_breakdown=result.get("pdca_breakdown", {}),
                tagging_result=audit_confidence.get("pdca_found", []),
                is_safety_pass=is_safety_pass
            )

        return {
            "is_passed": bool(result.get("is_passed", False)),
            "score": float(result.get("score", 0.0)),
            "reason": result.get("reason", ""),
            "coaching_insight": final_insight,
            "pdca_breakdown": result.get("pdca_breakdown", {}),
            "audit_confidence": audit_confidence,
            "top_chunks_data": retrieved_chunks,
            "is_safety_pass": is_safety_pass,
            "debug_meta": {
                "max_rerank": max_rerank,
                "evidence_count": len(evidences),
                "judicial_review": is_safety_pass,
                "neighbor_expansion": True,
                "audit_instruction_applied": True
            }
        }