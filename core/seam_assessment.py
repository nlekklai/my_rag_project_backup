# core/seam_assessment.py

import sys
import json
import logging
import time
import os
from typing import List, Dict, Any, Optional, Union, Tuple, Set, Final, Literal
from collections import defaultdict
from datetime import datetime
from dataclasses import dataclass, field
import multiprocessing # NEW: Import for parallel execution
from functools import partial
import pathlib, uuid
from langchain_core.documents import Document as LcDocument
from core.retry_policy import RetryPolicy, RetryResult
from copy import deepcopy
import tempfile
import shutil
# from json_extractor import _robust_extract_json
from .json_extractor import _robust_extract_json
from filelock import FileLock  # ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á: pip install filelock
import re
import hashlib
import copy
from database import init_db
from database import db_update_task_status as update_db

import random  # Added for shuffle

# -------------------- PATH SETUP & IMPORTS --------------------
try:
    PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
    if PROJECT_ROOT not in sys.path:
        sys.path.insert(0, PROJECT_ROOT)

    # 1. Import Constants ‡∏à‡∏≤‡∏Å global_vars
    from config.global_vars import (
        MAX_LEVEL,
        EVIDENCE_DOC_TYPES,
        RERANK_THRESHOLD,
        MAX_EVI_STR_CAP,
        DEFAULT_LLM_MODEL_NAME,
        LLM_TEMPERATURE,
        MIN_RETRY_SCORE,
        BASE_PDCA_KEYWORDS,
        MAX_PARALLEL_WORKERS,
        PDCA_PRIORITY_ORDER,
        TARGET_DEVICE,
        PDCA_PHASE_MAP,
        INITIAL_TOP_K,
        FINAL_K_RERANKED,
        MAX_CHUNKS_PER_FILE,
        MAX_CHUNKS_PER_BLOCK
    )
    
    # 2. Import Logic Functions
    from core.llm_data_utils import ( 
        create_structured_action_plan, evaluate_with_llm,
        retrieve_context_with_filter, retrieve_context_for_low_levels,
        evaluate_with_llm_low_level, LOW_LEVEL_K, 
        set_mock_control_mode as set_llm_data_mock_mode,
        create_context_summary_llm,
        retrieve_context_by_doc_ids,
        _fetch_llm_response,
        build_multichannel_context_for_level
    )
    from core.vectorstore import VectorStoreManager, load_all_vectorstores, get_global_reranker 
    from core.action_plan_schema import ActionPlanActions

    # 3. üéØ Import Path Utilities
    from utils.path_utils import (
        get_mapping_file_path, 
        get_evidence_mapping_file_path, 
        get_contextual_rules_file_path,
        get_doc_type_collection_key,
        get_assessment_export_file_path,
        get_export_dir,
        get_rubric_file_path,
        load_evidence_mapping,
        _n
    )

    import assessments.seam_mocking as seam_mocking 
    
except ImportError as e:
    # -------------------- Modernized Fallback Code --------------------
    print(f"‚ö†Ô∏è WARNING: Import failed, using dynamic fallback for Mac. Error: {e}", file=sys.stderr)
    
    # Fallback Constants
    EXPORTS_DIR = "exports"
    MAX_LEVEL = 5
    INITIAL_LEVEL = 1
    QA_FINAL_K = 3
    RUBRIC_FILENAME_PATTERN = "{tenant}_{enabler}_rubric.json"
    DEFAULT_ENABLER = "KM"
    EVIDENCE_DOC_TYPES = "evidence"
    INITIAL_TOP_K = 10

    # üìå Placeholder functions for path_utils (‡∏ä‡∏µ‡πâ‡πÄ‡∏Ç‡πâ‡∏≤ data_store ‡∏ï‡∏£‡∏á‡πÜ)
    def _n(s): return str(s).lower().strip()

    def get_mapping_file_path(doc_type, tenant, year=None, enabler=None):
        t = _n(tenant)
        if _n(doc_type) == "evidence":
            return f"data_store/{t}/mapping/{year}/{t}_{year}_{_n(enabler)}_doc_id_mapping.json"
        return f"data_store/{t}/mapping/{t}_{_n(doc_type)}_doc_id_mapping.json"

    def get_evidence_mapping_file_path(tenant, year, enabler):
        t = _n(tenant)
        return f"data_store/{t}/mapping/{year}/{t}_{year}_{_n(enabler)}_evidence_mapping.json"

    def get_contextual_rules_file_path(tenant, enabler):
        t = _n(tenant)
        return f"data_store/{t}/config/{t}_{_n(enabler)}_contextual_rules.json"

    def get_rubric_file_path(tenant, enabler):
        t = _n(tenant)
        return f"data_store/{t}/config/{t}_{_n(enabler)}_rubric.json"

    def load_evidence_mapping(tenant="pea", year=2568, enabler="KM"):
        path = get_evidence_mapping_file_path(tenant, year, enabler)
        if os.path.exists(path):
            try:
                with open(path, "r", encoding="utf-8") as f: return json.load(f)
            except: return {}
        return {}

    # Mock Logic Functions
    def create_structured_action_plan(*args, **kwargs): return []
    def evaluate_with_llm(*args, **kwargs): return {"score": 0, "reason": "Import Error Fallback", "is_passed": False}
    def retrieve_context_with_filter(*args, **kwargs): return {"top_evidences": [], "aggregated_context": ""}
    def retrieve_context_for_low_levels(*args, **kwargs): return {"top_evidences": [], "aggregated_context": ""}
    def evaluate_with_llm_low_level(*args, **kwargs): return {"score": 0, "is_passed": False}
    def set_llm_data_mock_mode(mode): pass
    def build_multichannel_context_for_level(*args, **kwargs): return ""
    
    class VectorStoreManager: pass
    def load_all_vectorstores(*args, **kwargs): return None
    
    PDCA_PHASE_MAP = {1: "Plan", 2: "Do", 3: "Check", 4: "Act", 5: "Sustainability"}

    class seam_mocking:
        @staticmethod
        def set_mock_control_mode(mode): pass

    if "FATAL ERROR" in str(e):
        pass 
# ----------------------------------------------------------------------

logger = logging.getLogger(__name__)
if not logger.handlers:
    logging.basicConfig(level=logging.DEBUG, format="%(asctime)s - %(levelname)s - %(message)s")

def classify_by_keyword(
    text: str, 
    sub_id: str = None, 
    level: int = None,
    contextual_rules_map: dict = None,
    chunk_metadata: dict = None
) -> str:
    """
    [ULTIMATE PDCA CLASSIFIER v2026.4 - FULL REVISED FOR NEW BRANCH]
    --------------------------------------------------
    - Metadata & Filename Awareness: ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
    - JSON v2 Compatibility: ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á 'require_phase' ‡πÅ‡∏•‡∏∞ 'specific_contextual_rule'
    - L1/L2 High-Pass: ‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö‡∏°‡∏ï‡∏¥‡∏ö‡∏≠‡∏£‡πå‡∏î (D) ‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ö‡∏•‡πá‡∏≠‡∏Å‡∏î‡πâ‡∏ß‡∏¢ Must-include
    """
    if not text: return 'Other'

    # --- 0. METADATA OVERRIDE (‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î) ---
    if chunk_metadata:
        meta_tag = chunk_metadata.get("pdca_tag") or chunk_metadata.get("PDCA")
        if meta_tag and str(meta_tag).upper() in {"P", "D", "C", "A"}:
            return str(meta_tag).upper()

    if not contextual_rules_map: return 'Other'
    text_lower = text.lower()

    def keyword_match(text_to_search: str, keywords_input, anchor: str = None) -> bool:
        """
        ‡∏´‡∏±‡∏ß‡πÉ‡∏à‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Keywords ‡∏ó‡∏±‡πâ‡∏á‡πÉ‡∏ô '‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤' ‡πÅ‡∏•‡∏∞ '‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå'
        """
        filename = ""
        if chunk_metadata and isinstance(chunk_metadata, dict):
            # ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å 'source' ‡∏´‡∏£‡∏∑‡∏≠ 'filename' (ChromaDB ‡∏°‡∏±‡∏Å‡πÄ‡∏Å‡πá‡∏ö‡πÉ‡∏ô source)
            filename = chunk_metadata.get("source", "") or chunk_metadata.get("filename", "")
            filename = os.path.basename(filename)

        # ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ (‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ KM1.1L106.pdf ‡∏Å‡∏•‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ä‡∏±‡πâ‡∏ô‡∏î‡∏µ)
        search_scope = (text_to_search + " " + filename).lower()

        # 1. ANCHOR CHECK (‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏£‡∏≤‡∏¢‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠)
        if anchor:
            # ‡πÉ‡∏ä‡πâ regex ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡πÄ‡∏•‡∏Ç‡∏Ç‡πâ‡∏≠ (‡πÄ‡∏ä‡πà‡∏ô 1.1) ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏ï‡∏¥‡∏î‡πÄ‡∏•‡∏Ç‡∏≠‡∏∑‡πà‡∏ô (‡πÄ‡∏ä‡πà‡∏ô 1.11)
            anchor_pattern = rf'(?<!\d){re.escape(anchor)}(?!\d)'
            if not re.search(anchor_pattern, search_scope):
                return False

        # 2. KEYWORD NORMALIZATION
        kws = keywords_input if isinstance(keywords_input, list) else \
              [k.strip() for k in str(keywords_input).split(",") if k.strip()]
        
        # 3. REGEX MATCHING
        for kw in kws:
            kw_clean = str(kw).strip().lower()
            if not kw_clean: continue
            
            is_thai = any("\u0e00" <= c <= "\u0e7f" for c in kw_clean)
            if is_thai:
                pattern = re.escape(kw_clean)
            else:
                pattern = r'\b{}\b'.format(re.escape(kw_clean))
                
            if re.search(pattern, search_scope, re.IGNORECASE):
                return True
                
        return False

    def check_pdca_sequence(rules_dict: dict, anchor: str = None) -> Optional[str]:
        mapping = {
            "plan_keywords": "P", 
            "do_keywords": "D", 
            "check_keywords": "C", 
            "act_keywords": "A"
        }
        for json_key, tag in mapping.items():
            if keyword_match(text_lower, rules_dict.get(json_key, []), anchor=anchor):
                return tag
        return None

    # --- EXECUTION STEPS ---
    
    # Step 1: Specific Level Rules (‡∏à‡∏≤‡∏Å JSON ‡πÉ‡∏´‡∏°‡πà ‡πÄ‡∏ä‡πà‡∏ô 1.1 -> L1)
    rules = contextual_rules_map.get(sub_id, {})
    current_l_rules = rules.get(f"L{level}", {})
    
    if isinstance(current_l_rules, dict) and current_l_rules:
        # ‡∏î‡∏∂‡∏á Phase ‡∏ó‡∏µ‡πà JSON ‡∏£‡∏∞‡∏ö‡∏∏‡∏ß‡πà‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô (‡πÄ‡∏ä‡πà‡∏ô ["P", "D"])
        required_phases = current_l_rules.get("require_phase", [])
        
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Tag ‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô
        tag = check_pdca_sequence(current_l_rules, anchor=None)
        
        if tag:
            # [LOGIC OVERRIDE] ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Level 1-2 ‡∏ñ‡πâ‡∏≤‡πÄ‡∏à‡∏≠ 'D' (‡∏°‡∏ï‡∏¥/‡∏≠‡∏ô‡∏∏‡∏°‡∏±‡∏ï‡∏¥) 
            # ‡πÉ‡∏´‡πâ‡∏ú‡πà‡∏≤‡∏ô‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö "‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ô‡∏≥‡πÑ‡∏õ‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πâ‡∏ô"
            if tag == "D" and level <= 2:
                return "D"

            # Must-include / Avoid Guard ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
            if rules.get("must_include_keywords"):
                if not keyword_match(text_lower, rules["must_include_keywords"]):
                    return 'Other'
            
            if rules.get("avoid_keywords"):
                if keyword_match(text_lower, rules["avoid_keywords"]):
                    return 'Other'
                    
            return tag

    # Step 2: JSON Sub-ID Fallback (‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Keywords ‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏±‡πâ‡∏ô‡πÜ)
    # ‡πÉ‡∏ä‡πâ Anchor=sub_id ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Ç‡πâ‡∏≠‡∏à‡∏£‡∏¥‡∏á
    tag = check_pdca_sequence(rules, anchor=sub_id)
    if tag: return tag

    # Step 3: Default Enabler Rules (‡πÄ‡∏ä‡πà‡∏ô KM ‡∏Å‡∏•‡∏≤‡∏á)
    defaults = contextual_rules_map.get("_enabler_defaults", {})
    tag = check_pdca_sequence(defaults, anchor=sub_id)
    if tag: return tag

    # Step 4: Global System Fallback (‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡∏≠‡∏∞‡πÑ‡∏£‡πÄ‡∏•‡∏¢)
    try:
        from config.global_vars import PDCA_PRIORITY_ORDER, BASE_PDCA_KEYWORDS
        tag_map = {"Plan": "P", "Do": "D", "Check": "C", "Act": "A"}
        for full_tag in PDCA_PRIORITY_ORDER:
            if keyword_match(text_lower, BASE_PDCA_KEYWORDS.get(full_tag, []), anchor=sub_id):
                return tag_map.get(full_tag, 'Other')
    except ImportError:
        pass

    return 'Other'

def get_actual_score(ev: dict) -> float:
    """
    [v2026.1 - ROBUST SCORING]
    - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤ '0.0 or score' ‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ Logic ‡πÄ‡∏û‡∏µ‡πâ‡∏¢‡∏ô
    - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á Dict ‡πÅ‡∏•‡∏∞ Langchain Document Object
    """
    if not ev:
        return 0.0

    # 1. ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö Top-level
    # ‡πÉ‡∏ä‡πâ next(...) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏Ñ‡πà‡∏≤‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà None
    score_keys = ["relevance_score", "rerank_score", "score"]
    
    # ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å Dict ‡∏´‡∏£‡∏∑‡∏≠ Object attribute
    val = None
    for key in score_keys:
        val = ev.get(key) if isinstance(ev, dict) else getattr(ev, key, None)
        if val is not None:
            return float(val)

    # 2. Fallback ‡πÑ‡∏õ‡∏ó‡∏µ‡πà Metadata (‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡πÉ‡∏ô ChromaDB)
    meta = ev.get("metadata", {}) if isinstance(ev, dict) else getattr(ev, "metadata", {})
    if meta:
        for key in score_keys:
            val = meta.get(key)
            if val is not None:
                return float(val)

    return 0.0

def merge_evidence_mappings(results_list: List[Union[Tuple, Dict[str, Any]]]) -> Dict[str, List[Dict[str, Any]]]:
    """
    ‡∏£‡∏ß‡∏° evidence_mapping ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å Worker (‡∏ã‡∏∂‡πà‡∏á‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô Tuple) ‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô
    """
    merged_mapping = defaultdict(list)
    
    for item in results_list:
        # 1. ‡∏Å‡∏£‡∏ì‡∏µ Worker ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô Tuple (Standard Engine Return)
        # item[0] ‡∏Ñ‡∏∑‡∏≠ sub_result, item[1] ‡∏Ñ‡∏∑‡∏≠ temp_map (evidence)
        if isinstance(item, tuple) and len(item) == 2:
            worker_evidence_map = item[1]
            if isinstance(worker_evidence_map, dict):
                for level_key, evidence_list in worker_evidence_map.items():
                    if isinstance(evidence_list, list):
                        merged_mapping[level_key].extend(evidence_list)
        
        # 2. ‡∏Å‡∏£‡∏ì‡∏µ Worker ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô Dict (Fallback ‡∏´‡∏£‡∏∑‡∏≠ Error case)
        elif isinstance(item, dict) and 'evidence_mapping' in item:
            worker_evidence_map = item['evidence_mapping']
            for level_key, evidence_list in worker_evidence_map.items():
                merged_mapping[level_key].extend(evidence_list)
                
    return dict(merged_mapping)

def get_pdca_keywords_str(phase: str) -> str:
    """
    ‡∏î‡∏∂‡∏á Keywords ‡∏à‡∏≤‡∏Å Global Vars ‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î Regex 
    ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ LLM ‡∏ô‡∏≥‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Extraction
    """
    # ‡∏î‡∏∂‡∏á list ‡∏ï‡∏≤‡∏° phase (Plan, Do, Check, Act)
    raw_keywords = BASE_PDCA_KEYWORDS.get(phase, [])
    
    # ‡∏•‡πâ‡∏≤‡∏á‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏Ç‡∏≠‡∏á Regex ‡∏≠‡∏≠‡∏Å (‡πÄ‡∏ä‡πà‡∏ô r"", \, ^, $)
    clean_keywords = []
    for kw in raw_keywords:
        # ‡∏•‡∏ö escape characters ‡πÅ‡∏•‡∏∞‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå regex ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        k = re.sub(r'[\\^$r"\']', '', kw)
        if k not in clean_keywords:
            clean_keywords.append(k)
            
    # ‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô‡πÄ‡∏õ‡πá‡∏ô string ‡∏Ç‡∏±‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏à‡∏∏‡∏•‡∏†‡∏≤‡∏Ñ (‡πÄ‡∏≠‡∏≤‡πÅ‡∏Ñ‡πà 10 ‡∏Ñ‡∏≥‡πÅ‡∏£‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î Token)
    return ", ".join(clean_keywords[:10])

def _static_worker_process(worker_input_tuple: Tuple) -> Any:
    """
    [ULTIMATE WORKER v2026.3] Isolated Execution for Parallel Assessment
    ---------------------------------------------------------------------
    - ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ Process (Zero Memory Leak)
    - ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏ä‡πâ Context ‡∏à‡∏≤‡∏Å‡πÅ‡∏°‡πà‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥
    - ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏£‡∏∞‡∏ö‡∏ö‡∏û‡∏±‡∏á‡∏î‡πâ‡∏ß‡∏¢ Fallback Dictionary
    """

    # 1. üìÇ PATH SETUP
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏≠‡∏á‡πÄ‡∏´‡πá‡∏ô‡πÇ‡∏°‡∏î‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Import AssessmentConfig ‡πÅ‡∏•‡∏∞ Engine ‡πÑ‡∏î‡πâ
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
    if project_root not in sys.path:
        sys.path.append(project_root)
        
    worker_logger = logging.getLogger(f"Worker_{os.getpid()}")

    # 2. üì¶ ROBUST UNPACKING
    try:
        (
            sub_criteria_data, enabler, target_level, mock_mode, 
            evidence_map_path, model_name, temperature,
            min_retry_score, max_retrieval_attempts, document_map, 
            action_plan_model, year, tenant
        ) = worker_input_tuple
        
        sub_id = sub_criteria_data.get('sub_id', 'UNKNOWN')
        worker_logger.info(f"‚öôÔ∏è PID:{os.getpid()} | Starting: {sub_id} ({tenant}/{year})")
        
    except Exception as e:
        return {"error": f"Worker unpacking failed: {str(e)}", "status": "critical_failure"}
        
    # 3. üèóÔ∏è RECONSTRUCT ISOLATED ENGINE
    try:
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Config ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Worker ‡∏ï‡∏±‡∏ß‡∏ô‡∏µ‡πâ
        worker_config = AssessmentConfig(
            enabler=enabler,
            tenant=tenant,
            year=int(year) if year else None,     
            target_level=target_level,
            mock_mode=mock_mode,
            model_name=model_name, 
            temperature=temperature,
            min_retry_score=min_retry_score,            
            max_retrieval_attempts=max_retrieval_attempts 
        )

        # ‡∏Ñ‡∏∑‡∏ô‡∏ä‡∏µ‡∏û Engine (‡∏à‡∏∞‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà __init__ ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏á Patch ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡πÑ‡∏õ)
        worker_instance = SEAMPDCAEngine(
            config=worker_config, 
            evidence_map_path=evidence_map_path, 
            llm_instance=None,              
            vectorstore_manager=None,       
            logger_instance=worker_logger,
            document_map=document_map,      
            ActionPlanActions=action_plan_model
        )
    except Exception as e:
        worker_logger.error(f"‚ùå Worker initialization failed for {sub_id}: {e}")
        return {"sub_id": sub_id, "error": f"Init Error: {str(e)}", "status": "failed"}

    # 4. ‚ö° EXECUTE & TIME TRACKING
    try:
        start_time = time.time()
        
        # ‡∏£‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏£‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠ (Core Logic)
        result = worker_instance._run_sub_criteria_assessment_worker(sub_criteria_data)
        
        elapsed = time.time() - start_time
        worker_logger.info(f"‚úÖ PID:{os.getpid()} | Finished: {sub_id} in {elapsed:.2f}s")
        
        return result
        
    except Exception as e:
        worker_logger.error(f"‚ùå Execution error for {sub_id}: {str(e)}")
        return {
            "sub_id": sub_id,
            "error": str(e),
            "status": "failed",
            "execution_time": 0
        }
    
# =================================================================
# Configuration Class
# =================================================================
@dataclass
class AssessmentConfig:
    """Configuration for the SEAM PDCA Assessment Run."""
    
    # ------------------ 1. Assessment Context ------------------
    enabler: str = None
    tenant: str = None
    year: int = None  # üëà ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å DEFAULT_YEAR ‡πÄ‡∏õ‡πá‡∏ô None
    target_level: int = MAX_LEVEL
    mock_mode: str = "none" # 'none', 'random', 'control'
    force_sequential: bool = field(default=False) # Flag ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏£‡∏±‡∏ô‡πÅ‡∏ö‡∏ö Sequential

    # ------------------ 2. LLM Configuration (Configurable) ------------------
    # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Default ‡∏à‡∏≤‡∏Å global_vars.py
    model_name: str = DEFAULT_LLM_MODEL_NAME 
    temperature: float = LLM_TEMPERATURE

    # ------------------ 3. Adaptive RAG Retrieval Configuration ------------------
    # üü¢ NEW: ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô Rerank ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Adaptive Loop (MIN_RETRY_SCORE)
    # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Default 0.65 ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏û‡∏ö‡πÉ‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î Logic
    min_retry_score: float = MIN_RETRY_SCORE
    # üü¢ NEW: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≠‡∏ö‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á Adaptive RAG Loop (MAX_RETRIEVAL_ATTEMPTS)
    # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Default 3
    max_retrieval_attempts: int = 3
    
    # ------------------ 4. Export Configuration ------------------
    export_output: bool = field(default=False) # Flag ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏¥‡∏î/‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£ Export ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    export_path: str = "" # Path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå Export (‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å)


# =================================================================
# SEAM Assessment Engine (PDCA Focused) - Full Revise v2026
# =================================================================
class SEAMPDCAEngine:
    
    def __init__(
        self, 
        config: AssessmentConfig,
        llm_instance: Any = None, 
        logger_instance: logging.Logger = None,
        rag_retriever_instance: Any = None,
        doc_type: str = None, 
        vectorstore_manager: Optional['VectorStoreManager'] = None,
        evidence_map_path: Optional[str] = None,
        document_map: Optional[Dict[str, str]] = None,
        is_parallel_all_mode: bool = False,
        sub_id: str = 'all',
        **kwargs  
    ):
        """
        [ULTIMATE REVISE v2026.3] SEAM Assessment Engine Constructor
        ‡πÄ‡∏ô‡πâ‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏ô‡∏ó‡∏≤‡∏ô (Resilience), ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢ (Sanity Check) ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        """
        # -------------------------------------------------------
        # 1. Logger Setup (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö 1 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£ Trace Error)
        # -------------------------------------------------------
        self.doc_type = doc_type or getattr(config, 'doc_type', EVIDENCE_DOC_TYPES)
        clean_dt = str(self.doc_type).strip().lower()
        log_year = config.year if clean_dt == EVIDENCE_DOC_TYPES.lower() else "general"

        if logger_instance is not None:
            self.logger = logger_instance
        else:
            self.logger = logging.getLogger(__name__).getChild(
                f"Engine|{config.enabler}|{config.tenant}/{log_year}"
            )

        self.logger.info(f"üöÄ Initializing SEAMPDCAEngine: {config.enabler} ({config.tenant}/{log_year})")

        # -------------------------------------------------------
        # 2. Patch: Sanity Check & Core Configuration
        # -------------------------------------------------------
        self.config = config
        
        # [CRITICAL PATCH] ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏á‡∏≤‡∏ô
        if not self.config.enabler or not self.config.tenant:
            self.logger.critical("‚ùå Mandatory Config Missing: enabler and tenant must be provided!")
            raise ValueError("Enabler and Tenant are required for SEAMPDCAEngine.")

        self.enabler_id = config.enabler
        self.target_level = config.target_level
        self.sub_id = sub_id
        self.llm = llm_instance
        self.vectorstore_manager = vectorstore_manager
        self.is_parallel_all_mode = is_parallel_all_mode
        self.is_sequential = getattr(config, 'force_sequential', True)
        self.results = {}

        # -------------------------------------------------------
        # 3. Database & System Warm-up
        # -------------------------------------------------------
        try:
            init_db()  # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô 'no such table' ‡∏´‡∏£‡∏∑‡∏≠ Schema mismatch
            self.logger.info("üìÇ Database Schema verified/initialized.")
        except Exception as e:
            self.logger.error(f"‚ö†Ô∏è DB Init Warning: {e} (Check if tables already exist)")

        # -------------------------------------------------------
        # 4. Data Loading (Rubric, Rules & Policies)
        # -------------------------------------------------------
        # ‡πÇ‡∏´‡∏•‡∏î Rubric ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô AttributeError ‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô
        self.rubric = self._load_rubric()
        
        # ‡πÇ‡∏´‡∏•‡∏î‡∏Å‡∏é Contextual Rules ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö PDCA Logic
        self.contextual_rules_map = self._load_contextual_rules_map()
        
        self.retry_policy = RetryPolicy(
            max_attempts=3,
            base_delay=2.0,
            jitter=True,
            exponential_backoff=True,
        )

        # ‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô (Global Constants)
        self.base_pdca_keywords = BASE_PDCA_KEYWORDS
        self.RERANK_THRESHOLD = RERANK_THRESHOLD
        self.MAX_EVI_STR_CAP = MAX_EVI_STR_CAP

        # -------------------------------------------------------
        # 5. Mapping & Evidence Persistence Setup
        # -------------------------------------------------------
        # 5.1 Evidence Mapping (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏∑‡∏ö‡∏Ñ‡πâ‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á)
        self.evidence_map = {}
        if clean_dt == EVIDENCE_DOC_TYPES.lower():
            self.evidence_map_path = evidence_map_path or get_evidence_mapping_file_path(
                tenant=self.config.tenant, 
                year=self.config.year, 
                enabler=self.enabler_id
            )
            self.evidence_map = self._load_evidence_map()
            self.logger.info(f"üìä Evidence Mapping: Loaded {len(self.evidence_map)} keys.")

        # 5.2 Document Mapping (ID -> Filename ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏≥ Report/Audit)
        loaded_map = document_map or {}
        if not loaded_map:
            is_evi_mode = (clean_dt == EVIDENCE_DOC_TYPES.lower())
            mapping_path = get_mapping_file_path(
                self.doc_type, 
                tenant=self.config.tenant, 
                year=self.config.year if is_evi_mode else None,
                enabler=self.enabler_id if is_evi_mode else None
            )

            if os.path.exists(mapping_path):
                try:
                    with open(mapping_path, 'r', encoding='utf-8') as f:
                        raw_data = json.load(f)
                    loaded_map = {k: v.get("file_name", k) for k, v in raw_data.items()}
                    self.logger.info(f"üéØ Document Mapping: Loaded {len(loaded_map)} entries.")
                except Exception as e:
                    self.logger.error(f"‚ùå Error parsing mapping file: {e}")

        self.doc_id_to_filename_map = loaded_map
        self.document_map = loaded_map
        self.temp_map_for_save = {}

        # -------------------------------------------------------
        # 6. Lazy Engine Initialization (VSM & LLM)
        # -------------------------------------------------------
        if self.llm is None: self._initialize_llm_if_none()
        if self.vectorstore_manager is None: self._initialize_vsm_if_none()

        # ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ ID Mapping ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô VectorStore (‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô)
        if self.vectorstore_manager:
            try:
                self.vectorstore_manager._load_doc_id_mapping()
            except: pass

        # -------------------------------------------------------
        # 7. Function Registry (Pointers)
        # -------------------------------------------------------
        self.llm_evaluator = evaluate_with_llm
        self.rag_retriever = retrieve_context_with_filter
        self.create_structured_action_plan = create_structured_action_plan
        self.ActionPlanActions = ActionPlanActions

        self.logger.info(f"‚úÖ Engine Initialized: Ready for Assessment (Sub-ID: {self.sub_id})")
    

    # =================================================================
    # DB Proxy Methods
    # =================================================================
    def db_update_task_status(self, record_id: str, progress: int, message: str, status: str = "RUNNING"):
        """
        Wrapper ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏ú‡πà‡∏≤‡∏ô Database Module
        - record_id: ID ‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÉ‡∏ô‡∏£‡∏≠‡∏ö‡∏ô‡∏±‡πâ‡∏ô
        - progress: 0-100
        - message: ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞
        """
        if not record_id: return
        try:
            # ‡πÉ‡∏ä‡πâ update_db ‡∏ó‡∏µ‡πà alias ‡∏°‡∏≤‡∏à‡∏≤‡∏Å database.db_update_task_status
            update_db(record_id, progress, message, status=status)
            self.logger.debug(f"[DB-PROGRESS] {record_id}: {progress}% - {message}")
        except Exception as e:
            self.logger.error(f"‚ùå DB Update Error: {e}")


    def get_rule_content(self, sub_id: str, level: int, key_type: str):
        """
        [ULTIMATE RULE ENGINE v2026.3]
        ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏à‡∏≤‡∏Å Contextual Rules ‡πÅ‡∏ö‡∏ö‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ä‡∏±‡πâ‡∏ô
        - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Priority: Specific Level > Sub-ID Root > Global Defaults
        """
        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏é‡∏Ç‡∏≠‡∏á Sub-ID ‡∏ô‡∏±‡πâ‡∏ô‡πÜ
        rule = self.contextual_rules_map.get(sub_id, {})
        level_key = f"L{level}"
        
        # 1. ü•á Priority 1: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏∞‡∏î‡∏±‡∏ö Level ‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á (e.g., 1.1 -> L3)
        level_data = rule.get(level_key, {})
        if key_type in level_data:
            return level_data[key_type]
        
        # 2. ü•à Priority 2: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏∞‡∏î‡∏±‡∏ö Sub-ID Root (‡∏Å‡∏é‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô‡∏ó‡∏∏‡∏Å Level ‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏ô‡∏±‡πâ‡∏ô)
        if key_type in rule:
            return rule[key_type]
        
        # 3. ü•â Priority 3: Global Defaults (‡∏Ñ‡πà‡∏≤‡∏Å‡∏•‡∏≤‡∏á‡∏Ç‡∏≠‡∏á Enabler)
        # ‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏•‡∏∏‡πà‡∏° keywords ‡πÅ‡∏•‡∏∞ required_phases ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô
        defaults = self.contextual_rules_map.get("_enabler_defaults", {})
        if key_type in defaults:
            return defaults[key_type]
            
        # 4. üõ°Ô∏è Fallback: ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        if key_type == "require_phase":
            return None # ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ caller ‡πÄ‡∏ä‡πá‡∏Ñ if phase is not None ‡πÑ‡∏î‡πâ
        if "keywords" in key_type:
            return [] # ‡∏Ñ‡∏∑‡∏ô list ‡∏ß‡πà‡∏≤‡∏á‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô loop ‡∏û‡∏±‡∏á
            
        return ""

    def enhance_query_for_statement(
        self,
        statement_text: str,
        sub_id: str,
        statement_id: str,
        level: int,
        focus_hint: str,
    ) -> List[str]:
        """
        [Revised 2026 - STABLE ANCHOR VERSION]
        ‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ Sub-ID ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏™‡∏°‡∏≠ (Anchor) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πâ‡∏≤‡∏°‡∏Ç‡πâ‡∏≠
        """
        logger = logging.getLogger(__name__)
        enabler_id = self.enabler_id
        cum_rules = self.get_cumulative_rules(sub_id, level)
        
        # üü¢ [ANCHOR] ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡∏£‡∏∞‡∏ö‡∏∏‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà‡πÅ‡∏Ç‡πá‡∏á‡πÅ‡∏£‡∏á
        # ‡πÄ‡∏ä‡πà‡∏ô "KM 1.1" ‡∏´‡∏£‡∏∑‡∏≠ "1.1 KM"
        id_anchor = f"{enabler_id} {sub_id}"
        
        # ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏° Keywords
        plan_kws = cum_rules.get('plan_keywords', [])
        do_kws = cum_rules.get('do_keywords', [])
        check_kws = cum_rules.get('check_keywords', [])
        act_kws = cum_rules.get('act_keywords', [])
        all_kws = list(set(plan_kws + do_kws + check_kws + act_kws))
        keywords_str = " ".join(all_kws[:8]) 

        queries = []

        # Query 1: Direct Matching (‡πÄ‡∏ô‡πâ‡∏ô‡∏Ç‡πâ‡∏≠ + ‡πÉ‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå) - ‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏ô‡πâ‡∏≤‡∏™‡∏∏‡∏î
        # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: "KM 1.1 ‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ß‡∏¥‡∏™‡∏±‡∏¢‡∏ó‡∏±‡∏®‡∏ô‡πå ‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á"
        queries.append(f"{id_anchor} {statement_text}")

        # Query 2: Document Type Anchor (‡πÄ‡∏ô‡πâ‡∏ô‡∏´‡∏≤‡∏ä‡∏ô‡∏¥‡∏î‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏ô‡∏±‡πâ‡∏ô‡πÜ)
        if level <= 2:
            # ‡πÄ‡∏ô‡πâ‡∏ô‡∏£‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô: ‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏® ‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢ ‡πÅ‡∏ú‡∏ô
            queries.append(f"{id_anchor} ‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏® ‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢ ‡πÅ‡∏ú‡∏ô‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£ ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÅ‡∏ï‡πà‡∏á‡∏ï‡∏±‡πâ‡∏á {keywords_str}")
        else:
            # ‡πÄ‡∏ô‡πâ‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÄ‡∏ä‡∏¥‡∏á‡∏õ‡∏£‡∏∞‡∏à‡∏±‡∏Å‡∏©‡πå: ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•
            queries.append(f"{id_anchor} ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏• ‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏° ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° {keywords_str}")

        # Query 3: Maturity Specific (‡πÄ‡∏à‡∏≤‡∏∞‡πÄ‡∏ü‡∏™ PDCA)
        # ‡πÄ‡∏û‡∏¥‡πà‡∏° Query ‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á Maturity Rules ‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏°‡∏≤
        specific_rule = cum_rules.get('instruction', '')
        if specific_rule:
            queries.append(f"{id_anchor} {specific_rule}")

        # Query 4: Cross-Check (‡πÉ‡∏ä‡πâ Keyword ‡∏•‡πâ‡∏ß‡∏ô‡πÜ ‡πÅ‡∏ï‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ ID Anchor ‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤)
        # ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ BM25 ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
        if level >= 3:
            ca_kws = " ".join(list(set(check_kws + act_kws))[:5])
            queries.append(f"{id_anchor} ‡∏ú‡∏•‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏° ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á {ca_kws}")

        # --- ‡∏Å‡∏£‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å ---
        final_queries = []
        seen = set()
        for q in queries:
            q_strip = q.strip()
            if q_strip and q_strip not in seen:
                final_queries.append(q_strip)
                seen.add(q_strip)

        return final_queries[:5]

    def _initialize_llm_if_none(self):
        """Initializes LLM instance if self.llm is None."""
        if self.llm is None:
            self.logger.warning("‚ö†Ô∏è Initializing LLM: model=%s, temperature=%s", 
                                self.config.model_name, self.config.temperature)
            try:
                # üü¢ FIX: Import ‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ create_llm_instance
                from models.llm import create_llm_instance 
                self.llm = create_llm_instance( 
                    model_name=self.config.model_name,
                    temperature=self.config.temperature
                )
                self.logger.info("‚úÖ LLM Instance created successfully: %s (Temp: %s)", 
                                 self.config.model_name, self.config.temperature)
            except Exception as e:
                self.logger.error(f"FATAL: Could not initialize LLM: {e}")
                raise

    def _initialize_vsm_if_none(self):
        """
        Initializes VectorStoreManager with Smart Year Selection.
        - If Evidence: Priority 1 = Specific Year, Priority 2 = Root Fallback.
        - If Document: Priority 1 = Root (General), Priority 2 = Specific Year Fallback.
        """
        if self.vectorstore_manager is not None:
            return

        # 1. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ç‡∏≠‡∏á DocType
        clean_dt = str(self.doc_type or getattr(self.config, 'doc_type', EVIDENCE_DOC_TYPES)).strip().lower()
        is_evidence = (clean_dt == EVIDENCE_DOC_TYPES.lower())
        
        self.logger.info(f"üöÄ Loading vectorstore(s) for DocType: '{clean_dt}' (Mode: {'Evidence' if is_evidence else 'General'})")

        try:
            target_enabler = str(self.enabler_id).lower() if self.enabler_id else None
            
            # üéØ [SMART SELECTION] ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å
            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô evidence ‡πÉ‡∏´‡πâ‡∏´‡∏≤‡∏ï‡∏≤‡∏°‡∏õ‡∏µ (2568) ‡∏Å‡πà‡∏≠‡∏ô ‡πÅ‡∏ï‡πà‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô document ‡πÉ‡∏´‡πâ‡∏´‡∏≤‡∏ó‡∏µ‡πà Root (None) ‡∏Å‡πà‡∏≠‡∏ô
            primary_year = self.config.year if is_evidence else None
            secondary_year = None if is_evidence else self.config.year

            # 2. First Attempt: ‡πÇ‡∏´‡∏•‡∏î‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
            self.vectorstore_manager = load_all_vectorstores(
                doc_types=[clean_dt], 
                enabler_filter=target_enabler, 
                tenant=self.config.tenant, 
                year=primary_year       
            )
            
            def count_retrievers(vsm):
                if vsm and hasattr(vsm, '_multi_doc_retriever') and vsm._multi_doc_retriever:
                    return len(vsm._multi_doc_retriever._all_retrievers)
                return 0

            len_retrievers = count_retrievers(self.vectorstore_manager)
            
            # 3. Second Attempt (Fallback): ‡∏ñ‡πâ‡∏≤‡∏´‡∏≤‡πÅ‡∏ö‡∏ö‡πÅ‡∏£‡∏Å‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ ‡πÉ‡∏´‡πâ‡∏™‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏´‡∏≤‡∏≠‡∏µ‡∏Å‡πÅ‡∏ö‡∏ö
            if len_retrievers == 0:
                lookup_label = f"year {secondary_year}" if secondary_year else "tenant root"
                self.logger.info(f"‚ö†Ô∏è No collections found in primary path, searching in {lookup_label}...")
                
                self.vectorstore_manager = load_all_vectorstores(
                    doc_types=[clean_dt], 
                    enabler_filter=target_enabler, 
                    tenant=self.config.tenant, 
                    year=secondary_year       
                )
                len_retrievers = count_retrievers(self.vectorstore_manager)

            # 4. Post-Load Process
            if self.vectorstore_manager and len_retrievers > 0:
                self.vectorstore_manager._load_doc_id_mapping() 
                self.logger.info(f"‚úÖ MultiDocRetriever loaded with {len_retrievers} collections.") 
            else:
                # 5. Final Error Handling
                expected_p = f"data_store/{self.config.tenant}/vectorstore/{primary_year or 'root'}"
                self.logger.error(f"‚ùå FATAL: 0 vector store collections loaded. Please check folder: {expected_p}")
                raise ValueError(f"No vector collections found for '{target_enabler}' in {self.config.tenant}")

        except Exception as e:
            self.logger.error(f"‚ùå FATAL: Could not initialize VectorStoreManager: {str(e)}")
            raise

    def get_cumulative_rules(self, sub_id: str, current_level: int) -> Dict[str, Any]:
        """
        [FINAL POLISHED v2026.7] - Cumulative Maturity Rules Engine
        ------------------------------------------------------------
        - ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏Å‡∏é‡∏™‡∏∞‡∏™‡∏°‡∏à‡∏≤‡∏Å L1 ‡∏ñ‡∏∂‡∏á current_level (Maturity Core)
        - ‡∏ú‡∏™‡∏° Generic Defaults ‡∏à‡∏≤‡∏Å _enabler_defaults ‡∏Ç‡∏≠‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£
        - ‡πÉ‡∏ä‡πâ set ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô duplication ‡∏Ç‡∏≠‡∏á keywords ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î token usage
        - ‡πÅ‡∏¢‡∏Å instructions ‡πÄ‡∏õ‡πá‡∏ô list ‡∏ï‡∏≤‡∏° level ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠‡πÉ‡∏´‡πâ LLM ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏î‡πâ‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥
        - Logging ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ debug ‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö Production
        """
        import logging
        logger = logging.getLogger("AssessmentApp")

        # 1. ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Defaults (Global PDCA Keywords ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå JSON)
        defaults = self.contextual_rules_map.get('_enabler_defaults', {})
        
        # ‡πÉ‡∏ä‡πâ set ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÇ‡∏î‡∏¢‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
        cum_plan = set(defaults.get('plan_keywords', []))
        cum_do   = set(defaults.get('do_keywords', []))
        cum_check = set(defaults.get('check_keywords', []))
        cum_act  = set(defaults.get('act_keywords', []))

        required_phases = set()
        instructions = []

        # 2. ‡∏™‡∏∞‡∏™‡∏°‡∏Å‡∏é‡∏à‡∏≤‡∏Å Level 1 ‡∏à‡∏ô‡∏ñ‡∏∂‡∏á current_level (‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î Maturity ‡∏™‡∏∞‡∏™‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô)
        sub_rules = self.contextual_rules_map.get(sub_id, {})
        
        for lv in range(1, current_level + 1):
            lv_key = f"L{lv}"
            level_rule = sub_rules.get(lv_key, {})
            
            # ‡∏Ç‡πâ‡∏≤‡∏°‡πÄ‡∏•‡πÄ‡∏ß‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏ô‡∏¥‡∏¢‡∏≤‡∏°‡∏Å‡∏é‡πÑ‡∏ß‡πâ (Graceful Handling)
            if not level_rule:
                continue

            # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï Keywords ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡∏≠‡∏á‡πÄ‡∏•‡πÄ‡∏ß‡∏•‡∏ô‡∏±‡πâ‡∏ô‡πÜ ‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏™‡∏∞‡∏™‡∏°
            cum_plan.update(level_rule.get('plan_keywords', []))
            cum_do.update(level_rule.get('do_keywords', []))
            cum_check.update(level_rule.get('check_keywords', []))
            cum_act.update(level_rule.get('act_keywords', []))

            # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï Phase ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô (‡πÄ‡∏ä‡πà‡∏ô L1 ‡∏≠‡∏≤‡∏à‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏Ñ‡πà P, D ‡πÅ‡∏ï‡πà L4 ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ P, D, C, A)
            if 'require_phase' in level_rule:
                required_phases.update(level_rule.get('require_phase', []))

            # ‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏£‡∏≤‡∏¢‡πÄ‡∏•‡πÄ‡∏ß‡∏• (‡πÄ‡∏ä‡πà‡∏ô "L1: ‡∏°‡∏ï‡∏¥‡∏£‡∏±‡∏ö‡∏ó‡∏£‡∏≤‡∏ö = ‡∏ú‡πà‡∏≤‡∏ô")
            specific = level_rule.get('specific_contextual_rule')
            if specific:
                instructions.append(f"L{lv}: {specific.strip()}")

        # 3. ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ Engine ‡∏™‡πà‡∏ß‡∏ô‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
        result = {
            "plan_keywords": list(cum_plan),
            "do_keywords": list(cum_do),
            "check_keywords": list(cum_check),
            "act_keywords": list(cum_act),
            "required_phases": sorted(list(required_phases)),
            "instructions": instructions,  # ‡∏™‡πà‡∏á‡πÅ‡∏ö‡∏ö List ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ LLM ‡πÅ‡∏¢‡∏Å‡πÅ‡∏¢‡∏∞‡πÑ‡∏î‡πâ‡∏á‡πà‡∏≤‡∏¢
            "cumulative_instruction": "\n".join(instructions) if instructions else ""
        }

        # 4. Logging ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô (Monitoring)
        logger.debug(
            f"[RULE_CUMULATIVE] {sub_id} L{current_level} | "
            f"Keywords: P={len(result['plan_keywords'])} | "
            f"D={len(result['do_keywords'])} | "
            f"C={len(result['check_keywords'])} | "
            f"A={len(result['act_keywords'])} | "
            f"Phases={result['required_phases']} | "
            f"Instructions={len(instructions)}"
        )

        return result


    def validate_accumulative_pass(self, llm_result: Dict[str, Any], target_phases: List[str]) -> Tuple[bool, str]:
        """
        ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Phase ‡∏ó‡∏µ‡πà AI ‡∏ï‡∏£‡∏ß‡∏à‡πÄ‡∏à‡∏≠ ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà Level ‡∏ô‡∏±‡πâ‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏∞‡∏™‡∏°‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        """
        # ‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Phase ‡∏ó‡∏µ‡πà AI ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô > 0 (‡∏Ñ‡∏∑‡∏≠‡πÄ‡∏à‡∏≠‡∏à‡∏£‡∏¥‡∏á)
        pdca_breakdown = llm_result.get('pdca_breakdown', {})
        found_phases = {phase for phase, score in pdca_breakdown.items() if score > 0}
        required_set = set(target_phases)

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô Subset (Required ‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô Found)
        is_subset = required_set.issubset(found_phases)
        
        if not is_subset:
            missing = required_set - found_phases
            error_msg = f"‡∏Ç‡∏≤‡∏î‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏ô‡πÄ‡∏ü‡∏™‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: {', '.join(missing)} (‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏™‡∏∞‡∏™‡∏° Maturity)"
            return False, error_msg
        
        return True, ""
    

    def _check_maturity_consistency(
        self, 
        sub_id: str, 
        current_level: int, 
        top_evidences: List[Dict[str, Any]],
        llm_pdca_breakdown: Dict[str, float]
    ) -> Dict[str, Any]:
        """
        ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡πÄ‡∏ä‡∏¥‡∏á Maturity:
        1. Phase ‡∏ï‡πâ‡∏≠‡∏á‡∏Ñ‡∏£‡∏ö‡∏ï‡∏≤‡∏° JSON require_phase (‡∏™‡∏∞‡∏™‡∏°)
        2. ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô (Rerank) ‡∏Ç‡∏≠‡∏á Phase ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏ï‡πâ‡∏≠‡∏á‡∏ñ‡∏∂‡∏á‡πÄ‡∏Å‡∏ì‡∏ë‡πå
        """
        # ‡∏î‡∏∂‡∏á‡∏Å‡∏é‡∏™‡∏∞‡∏™‡∏°‡∏à‡∏≤‡∏Å JSON
        cum_rules = self.get_cumulative_rules(sub_id, current_level)
        required_phases = set(cum_rules['phases'])
        
        # ‡∏î‡∏∂‡∏á Phase ‡∏ó‡∏µ‡πà AI ‡∏ï‡∏£‡∏ß‡∏à‡πÄ‡∏à‡∏≠‡∏à‡∏£‡∏¥‡∏á (score > 0)
        found_phases = {p for p, score in llm_pdca_breakdown.items() if score > 0}
        
        # 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Phase Gap
        missing_phases = required_phases - found_phases
        
        # 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏£‡∏≤‡∏¢ Phase (Critical Evidence Check)
        # ‡πÄ‡∏ä‡πà‡∏ô ‡∏ñ‡πâ‡∏≤ L3 require 'D' ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô D ‡∏ó‡∏µ‡πà Rerank Score ‡∏™‡∏π‡∏á‡∏û‡∏≠
        low_quality_phases = []
        for phase in required_phases:
            max_score = max([
                doc.get('rerank_score', 0.0) 
                for doc in top_evidences 
                if doc.get('pdca_tag') == phase
            ], default=0.0)
            
            if max_score < 0.4: # Threshold ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà "‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠‡πÑ‡∏î‡πâ"
                low_quality_phases.append(phase)

        return {
            "is_consistent": len(missing_phases) == 0 and len(low_quality_phases) == 0,
            "missing_phases": list(missing_phases),
            "low_quality_phases": low_quality_phases,
            "required_phases": list(required_phases)
        }


    def post_process_llm_result(self, llm_output: Dict[str, Any], level: int) -> Dict[str, Any]:
        """
        [REVISED v2026.3] - ‡∏ú‡∏™‡∏≤‡∏ô Hard-Fail Logic ‡πÅ‡∏•‡∏∞ Heuristic ‡∏à‡∏≤‡∏Å‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏Ñ‡∏∏‡∏ì
        ‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ö Class Engine ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
        """
        if not llm_output:
            return {"is_passed": False, "score": 0.0, "reason": "No Output from LLM"}

        # 1. ‡∏ô‡∏¥‡∏¢‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå (Mapping Keys)
        # ‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏´‡πâ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡πá‡∏°‡πÅ‡∏•‡∏∞‡∏ä‡∏∑‡πà‡∏≠‡∏¢‡πà‡∏≠‡∏ó‡∏µ‡πà LLM ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏Ñ‡∏∑‡∏ô‡∏°‡∏≤
        extraction_map = {
            "Extraction_P": ["P_Plan_Score", "score_p"],
            "Extraction_D": ["D_Do_Score", "score_d"],
            "Extraction_C": ["C_Check_Score", "score_c"],
            "Extraction_A": ["A_Act_Score", "score_a"]
        }
        phase_map = {"Extraction_P": "Plan", "Extraction_D": "Do", "Extraction_C": "Check", "Extraction_A": "Act"}
        reason_text = llm_output.get("reason", "")
        is_consistent = llm_output.get("consistency_check", True)

        for ext_key, score_keys in extraction_map.items():
            val = llm_output.get(ext_key, "-") or "-"
            raw_val = str(val).strip()
            
            # --- ‡∏Å. ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ---
            content_only = re.sub(r'[^\u0e00-\u0e7fa-zA-Z0-9]', '', raw_val)
            is_negative = raw_val in ["-", "N/A", "n/a", "‡πÑ‡∏°‡πà‡∏û‡∏ö", "‡πÑ‡∏°‡πà‡∏°‡∏µ", "‡πÑ‡∏°‡πà‡∏õ‡∏£‡∏≤‡∏Å‡∏è", "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏"]
            is_empty = (not content_only) or is_negative
            
            # ‡∏î‡∏∂‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô (‡∏•‡∏≠‡∏á‡∏î‡∏π‡∏ó‡∏∏‡∏Å key ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ)
            current_score = 0.0
            target_key = score_keys[0]
            for sk in score_keys:
                if sk in llm_output:
                    current_score = float(llm_output.get(sk, 0))
                    target_key = sk
                    break

            if is_empty and current_score > 0:
                # --- ‡∏Ç. [DYNAMIC HEURISTIC OVERRIDE] ---
                # ‡∏î‡∏∂‡∏á Keywords ‡∏à‡∏≤‡∏Å self.global_pdca_keywords (‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡πÉ‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á)
                phase_name = phase_map.get(ext_key)
                # ‡∏î‡∏∂‡∏á keywords ‡∏°‡∏≤‡πÄ‡∏ä‡πá‡∏Ñ‡∏ã‡πâ‡∏≥‡πÉ‡∏ô reason
                raw_keywords = getattr(self, 'global_pdca_keywords', {}).get(phase_name, [])
                
                found_keyword = any(kw in reason_text for kw in raw_keywords if len(kw) > 1)
                
                if found_keyword:
                    self.logger.info(f" üõ°Ô∏è [Heuristic Pass] L{level}: {ext_key} empty but found keyword in Reason.")
                    continue 
                    
                # ‡∏£‡∏¥‡∏ö‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô Extraction field
                self.logger.warning(f" üö® [Revoke] L{level}: {target_key} revoked. No evidence in {ext_key}")
                llm_output[target_key] = 0.0

        # 2. Normalize ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏≤‡∏¢‡πÄ‡∏ü‡∏™ (Max 2.0 ‡∏ï‡πà‡∏≠‡πÄ‡∏ü‡∏™)
        def get_sc(keys):
            for k in keys:
                if k in llm_output: return float(llm_output[k])
            return 0.0

        p = round(min(get_sc(["P_Plan_Score", "score_p"]), 2.0), 1)
        d = round(min(get_sc(["D_Do_Score", "score_d"]), 2.0), 1)
        c = round(min(get_sc(["C_Check_Score", "score_c"]), 2.0), 1)
        a = round(min(get_sc(["A_Act_Score", "score_a"]), 2.0), 1)
        pdca_sum = round(p + d + c + a, 1)
        
        # 3. SE-AM Threshold & Hard-Fail Logic
        threshold_map = {1: 1, 2: 2, 3: 4, 4: 6, 5: 8}
        threshold = threshold_map.get(level, 2)
        is_passed = pdca_sum >= threshold

        # ‡∏Å‡∏é‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏ï‡∏Å (Hard-Fail)
        fail_reason = ""
        if is_passed:
            if not is_consistent:
                is_passed = False
                fail_reason = "‡∏û‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ç‡∏±‡∏î‡πÅ‡∏¢‡πâ‡∏á‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô (Consistency Fail)"
            elif level == 3 and c <= 0:
                is_passed = False
                fail_reason = "‡∏£‡∏∞‡∏î‡∏±‡∏ö 3 ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏±‡∏î (C > 0)"
            elif level == 4 and a <= 0:
                is_passed = False
                fail_reason = "‡∏£‡∏∞‡∏î‡∏±‡∏ö 4 ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á (A > 0)"
            elif level == 5 and (c < 2.0 or a < 2.0):
                is_passed = False
                fail_reason = "‡∏£‡∏∞‡∏î‡∏±‡∏ö 5 ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô C ‡πÅ‡∏•‡∏∞ A ‡πÄ‡∏ï‡πá‡∏° (2.0)"

        # 4. Final Object Update
        llm_output.update({
            "score": pdca_sum,
            "pdca_breakdown": {"P": p, "D": d, "C": c, "A": a},
            "is_passed": is_passed,
            "fail_reason": fail_reason,
            "consistency_check": is_consistent,
            "pass_threshold": threshold
        })

        return llm_output

    def _check_contextual_rule_condition(
        self, 
        condition: Dict[str, Any], 
        sub_id: str, 
        level: int, 
        top_evidences: List[Dict[str, Any]]
    ) -> bool:
        """
        [Revised 2026] ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏ï‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å Maturity Accumulation
        - ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏Ç‡∏≠‡∏á Level (Sequential Pass)
        - ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô (Rerank Quality) ‡∏ï‡∏≤‡∏° Phase ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏ô JSON
        """
        self.logger.info(f"üîç [Maturity Check] Verifying rules for {sub_id} L{level}...")
        
        # 1. ‡∏î‡∏∂‡∏á‡∏Å‡∏é‡∏™‡∏∞‡∏™‡∏°‡∏à‡∏≤‡∏Å JSON ‡πÉ‡∏´‡∏°‡πà‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤
        cum_rules = self.get_cumulative_rules(sub_id, level)
        required_phases = cum_rules.get('phases', [])

        # 2. Sequential Check: ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà L1 ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡πÄ‡∏•‡πÄ‡∏ß‡∏•‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ "‡∏ú‡πà‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á" ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if level > 1:
            for prev_lv in range(1, level):
                if not self._is_previous_level_passed(sub_id, prev_lv):
                    self.logger.warning(f"‚ùå Maturity Gap: {sub_id} L{prev_lv} must pass before L{level}.")
                    return False

        # 3. Evidence Quality Check: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö 'require_phase' ‡∏ó‡∏µ‡πà‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏ô JSON
        # ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏Ç‡πâ‡∏°‡∏á‡∏ß‡∏î‡∏Å‡∏±‡∏ö Phase ‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏‡πÑ‡∏ß‡πâ‡πÉ‡∏ô JSON ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ Phase ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
        for phase_to_check in required_phases:
            # ‡∏î‡∏∂‡∏á‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô JSON ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ Global Threshold)
            # ‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Ç‡∏¢‡∏≤‡∏¢ JSON ‡πÉ‡∏´‡πâ‡πÉ‡∏™‡πà min_rerank ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏ü‡∏™‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï
            threshold = globals().get('CRITICAL_CA_THRESHOLD', 0.60) if phase_to_check in ['C', 'A'] else 0.40
            
            found_valid_evidence = any(
                doc.get('pdca_tag') == phase_to_check and 
                doc.get('rerank_score', 0.0) >= threshold
                for doc in top_evidences
            )
            
            if not found_valid_evidence:
                self.logger.warning(f"‚ùå Evidence Gap: Required Phase '{phase_to_check}' not found or quality too low (Threshold: {threshold}).")
                return False

        # 4. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç 'and' ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° (‡∏´‡∏≤‡∏Å‡πÉ‡∏ô JSON ‡∏¢‡∏±‡∏á‡∏°‡∏µ Logic ‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏´‡∏•‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏≠‡∏¢‡∏π‡πà)
        if 'and' in condition:
            # ... (‡∏£‡∏±‡∏Å‡∏©‡∏≤ Logic ‡∏Å‡∏≤‡∏£‡∏ß‡∏ô Loop ‡πÄ‡∏ä‡πá‡∏Ñ 'and' ‡πÄ‡∏î‡∏¥‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏ß‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° Backward Compatible) ...
            pass

        self.logger.info(f"‚úÖ [Maturity Check] {sub_id} L{level} passed all rule conditions.")
        return True

    def _is_previous_level_passed(self, sub_id: str, level: int) -> bool:
        """
        Helper: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡∏ú‡πà‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≤‡∏Å self.assessment_results_map
        (‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö check_passed_levels ‡πÉ‡∏ô Contextual Rule)
        """
        # Key ‡πÉ‡∏ô assessment_results_map ‡∏°‡∏±‡∏Å‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô '3.1.L1', '3.1.L2'
        key = f"{sub_id}.L{level}"
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏≠‡∏á Level ‡∏ô‡∏±‡πâ‡∏ô‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà ‡πÅ‡∏•‡∏∞‡∏°‡∏µ is_passed ‡πÄ‡∏õ‡πá‡∏ô True
        result = self.assessment_results_map.get(key)
        
        return result is not None and result.get('is_passed', False)

    def _expand_context_with_neighbor_pages(self, top_evidences: List[Any], collection_name: str) -> List[Any]:
        """
        [SMART EXPAND v3 - Low Log] ‡∏î‡∏∂‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏ñ‡∏±‡∏î‡πÑ‡∏õ (‡πÅ‡∏•‡∏∞‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á 1 ‡∏´‡∏ô‡πâ‡∏≤) ‡∏´‡∏≤‡∏Å‡∏û‡∏ö Check
        - ‡∏•‡∏î log ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏ô‡πâ‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á debug ‡πÑ‡∏î‡πâ
        - ‡∏£‡∏ß‡∏° summary log ‡∏ï‡∏≠‡∏ô‡∏à‡∏ö
        """
        if not self.vectorstore_manager or not top_evidences:
            return top_evidences

        expanded_evidences = list(top_evidences)
        seen_keys = set()
        added_pages = 0
        added_chunks = 0
        failed_pages = set()  # ‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ü‡∏•‡πå+‡∏´‡∏ô‡πâ‡∏≤ ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà log ‡∏ã‡πâ‡∏≥

        check_triggers = [
            "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏∂‡∏á‡∏û‡∏≠‡πÉ‡∏à", "‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô", "‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•", "‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô", "score", "kpi", "3.41",
            "‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•", "‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏•", "‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î", "‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå", "‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô", "‡∏ú‡∏•‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô"
        ]

        for doc in top_evidences:
            text = (doc.get('text') or doc.get('page_content') or "").lower()
            meta = doc.metadata if hasattr(doc, 'metadata') else doc.get('metadata', {})
            page_label = meta.get("page_label")
            doc_uuid = meta.get("stable_doc_uuid")

            try:
                current_page = int("".join(filter(str.isdigit, str(page_label))))
            except (ValueError, TypeError):
                continue

            if not any(k in text for k in check_triggers):
                continue

            # ‡∏î‡∏∂‡∏á ¬±1 ‡πÅ‡∏•‡∏∞ +2 ‡∏´‡∏ô‡πâ‡∏≤ (‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á + ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ)
            for offset in [-1, 1, 2]:
                target_page = current_page + offset
                if target_page < 1:
                    continue  # ‡∏Ç‡πâ‡∏≤‡∏°‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡∏¥‡∏î‡∏•‡∏ö
                cache_key = f"{doc_uuid}_{target_page}"

                if cache_key in seen_keys:
                    continue

                neighbor_chunks = self.vectorstore_manager.get_chunks_by_page(
                    collection_name=collection_name,
                    stable_doc_uuid=doc_uuid,
                    page_label=str(target_page)
                )

                if neighbor_chunks:
                    for nc in neighbor_chunks:
                        new_doc = {
                            "text": f"[Act Context - Page {target_page} (‡∏à‡∏≤‡∏Å Check ‡∏ó‡∏µ‡πà‡∏´‡∏ô‡πâ‡∏≤ {current_page})]:\n{nc.page_content}",
                            "page_content": nc.page_content,
                            "metadata": nc.metadata,
                            "pdca_tag": "Act",
                            "is_supplemental": True,
                            "rerank_score": doc.get('rerank_score', 0.0)
                        }
                        expanded_evidences.append(new_doc)
                    seen_keys.add(cache_key)
                    added_pages += 1
                    added_chunks += len(neighbor_chunks)
                else:
                    fail_key = f"{doc_uuid}_{target_page}"
                    if fail_key not in failed_pages:
                        self.logger.debug(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏ô‡πâ‡∏≤ {target_page} ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå {doc_uuid}")
                        failed_pages.add(fail_key)

        # Summary log ‡πÅ‡∏Ñ‡πà‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
        if added_pages > 0:
            self.logger.info(f"Act-Hook completed: Added {added_pages} pages ({added_chunks} chunks) from Check triggers")
        else:
            self.logger.debug("Act-Hook: No additional neighbor pages found")

        return expanded_evidences
        
    def _resolve_evidence_filenames(self, evidence_entries: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        [ULTIMATE FIX] ‡∏ú‡∏™‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡∏Ç‡∏≠‡∏á‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏î‡∏¥‡∏° + ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏â‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Trace ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà
        """
        resolved_entries = []
        for entry in evidence_entries:
            resolved_entry = deepcopy(entry)
            doc_id = resolved_entry.get("doc_id", "")
            content_raw = resolved_entry.get('content')
            level_origin = resolved_entry.get('level', 'N/A')
            
            # ‡∏î‡∏∂‡∏á‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏° (Page Label ‡∏à‡∏≤‡∏Å Ingest ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤‡∏î‡∏¥‡∏ö)
            page_label = resolved_entry.get("page_label") or resolved_entry.get("page") or "N/A"

            # 1. AI Generated Reference (‡∏°‡πÇ‡∏ô‡∏°‡∏≤)
            if str(doc_id).startswith("UNKNOWN-"):
                resolved_entry["filename"] = "AI-GENERATED-REF"
                resolved_entry["page"] = "N/A"
                resolved_entries.append(resolved_entry)
                continue

            # 2. ‡πÄ‡∏Ñ‡∏™‡∏õ‡∏Å‡∏ï‡∏¥: ‡∏°‡∏µ doc_id ‡πÅ‡∏•‡∏∞‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô Map
            if doc_id and doc_id in self.doc_id_to_filename_map:
                mapped_name = self.doc_id_to_filename_map[doc_id]
                resolved_entry["filename"] = mapped_name
                resolved_entry["display_source"] = f"{mapped_name} (‡∏´‡∏ô‡πâ‡∏≤ {page_label})"
            
            # 3. ‡πÄ‡∏Ñ‡∏™‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡πÉ‡∏ô Map (Fallback)
            elif doc_id:
                resolved_entry["filename"] = f"DOC-{str(doc_id)[:8]}"
                resolved_entry["display_source"] = f"‡∏£‡∏´‡∏±‡∏™ {str(doc_id)[:8]} (‡∏´‡∏ô‡πâ‡∏≤ {page_label})"

            # 4. ‡πÄ‡∏Ñ‡∏™‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏≤‡∏¢ (The "Skipping" Case)
            else:
                if not content_raw:
                    # üü¢ ‡∏£‡∏∞‡∏ö‡∏∏ Level ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡∏Ñ‡∏ß‡∏£‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡πÅ‡∏Å‡πâ PDF ‡∏ó‡∏µ‡πà‡∏Ç‡πâ‡∏≠‡πÑ‡∏´‡∏ô
                    self.logger.warning(f"‚ö†Ô∏è [Data Gap] Level {level_origin}: Entry ‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤ (Skipped)")
                    continue 
                
                resolved_entry["filename"] = "UNMAPPED-DOCUMENT"
                preview = str(content_raw)[:30].replace('\n', ' ')
                self.logger.debug(f"üîç Unmapped Content Preview: {preview}...")

            resolved_entries.append(resolved_entry)
        return resolved_entries
        
    
    # -------------------- Contextual Rules Handlers (FIXED) --------------------
    def _load_contextual_rules_map(self) -> Dict[str, Any]:
        """
        [FINAL REVISED v2026.5] ‡πÇ‡∏´‡∏•‡∏î Contextual Rules ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
        - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö multi-tenant ‡πÅ‡∏•‡∏∞ multi-enabler ‡πÄ‡∏ï‡πá‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö
        - ‡∏°‡∏µ fallback ‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
        - ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Maturity (L1-L5) ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏Ç‡πâ‡∏°‡∏á‡∏ß‡∏î
        - Logging ‡∏ó‡∏µ‡πà‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏ä‡πà‡∏ß‡∏¢ debug
        - ‡πÑ‡∏°‡πà raise exception (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ engine ‡∏•‡πâ‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏£‡∏∞‡∏ö‡∏ö)
        """
        # 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á path ‡∏ï‡∏≤‡∏° tenant + enabler
        try:
            filepath = get_contextual_rules_file_path(
                tenant=self.config.tenant,
                enabler=self.enabler_id
            )
            self.logger.debug(f"üîç Attempting to load contextual rules from: {filepath}")
        except Exception as e:
            self.logger.error(f"‚ùå FATAL: Failed to generate rules file path: {e}")
            return {"_enabler_defaults": {}}

        # 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if not os.path.exists(filepath):
            self.logger.warning(
                f"‚ö†Ô∏è Contextual Rules file not found: {filepath}\n"
                f"   ‚Üí Using only global defaults (if available from fallback)."
            )
            return {"_enabler_defaults": {}}

        # 3. ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞ parse JSON
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
        except json.JSONDecodeError as e:
            self.logger.error(f"‚ùå JSON Decode Error in {filepath}: {e} (line {e.lineno}, col {e.colno})")
            return {"_enabler_defaults": {}}
        except Exception as e:
            self.logger.error(f"‚ùå Unexpected error reading {filepath}: {e}")
            return {"_enabler_defaults": {}}

        # 4. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        if not isinstance(data, dict):
            self.logger.error(f"‚ùå Invalid rules format in {filepath}: Expected dict, got {type(data)}")
            return {"_enabler_defaults": {}}

        # 5. ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô criteria ‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Maturity Structure
        sub_criteria_keys = [k for k in data.keys() if not k.startswith("_")]
        num_criteria = len(sub_criteria_keys)

        if num_criteria == 0:
            self.logger.warning(f"‚ö†Ô∏è No sub-criteria found in {filepath} (only defaults).")
        else:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ Maturity Levels (L1, L2, ...) ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            sample_sub = sub_criteria_keys[0]
            sub_data = data[sample_sub]
            level_keys = [k for k in sub_data.keys() if k.startswith("L") and len(k) >= 2]
            
            if level_keys:
                detected_levels = sorted(level_keys)
                self.logger.info(
                    f"‚úÖ Maturity-Based Rules loaded successfully!\n"
                    f"   File: {filepath}\n"
                    f"   Criteria: {num_criteria} (e.g., {sample_sub})\n"
                    f"   Levels detected: {', '.join(detected_levels)}"
                )
            else:
                self.logger.warning(
                    f"‚ö†Ô∏è Rules for '{sample_sub}' in {filepath} do not contain Maturity Levels (L1-L5).\n"
                    f"   Falling back to flat structure or defaults."
                )

        # 6. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö _enabler_defaults
        if "_enabler_defaults" in data:
            defaults = data["_enabler_defaults"]
            if isinstance(defaults, dict) and any(k.endswith("_keywords") for k in defaults.keys()):
                self.logger.info("‚úÖ Global PDCA Keywords (_enabler_defaults) loaded.")
            else:
                self.logger.warning("‚ö†Ô∏è _enabler_defaults exists but has invalid structure.")
        else:
            self.logger.info("‚ÑπÔ∏è No _enabler_defaults section found. Using empty defaults.")

        # 7. ‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô data ‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
        self.logger.info(f"‚úÖ Contextual Rules fully loaded from {filepath} ({num_criteria} criteria).")
        return data

    # ----------------------------------------------------------------------
    # üéØ FINAL FIX 2.3: Manual Map Reload Function (inside SEAMPDCAEngine)
    # ----------------------------------------------------------------------
    def _collect_previous_level_evidences(self, sub_id: str, current_level: int) -> Dict[str, List[Dict]]:
        """
        ‡∏î‡∏∂‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏à‡∏≤‡∏Å Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ (L1 ‚Üí L2, L2 ‚Üí L3 ‡∏Ø‡∏•‡∏Ø) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô Baseline Context
        
        [FINAL REVISED 2026] - Robust Hydration & UUID Normalization
        - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö UUID ‡∏ó‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö‡∏°‡∏µ‡∏Ç‡∏µ‡∏î (-) ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡∏µ‡∏î
        - ‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏¢‡∏∞ (fallback_doc_id, N/A) ‡∏≠‡∏≠‡∏Å‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£
        - ‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á Metadata (PDCA Tag, Source, Page)
        """
        
        # 1. ‡∏Ç‡πâ‡∏≤‡∏° Hydration ‡πÉ‡∏ô Full Parallel Mode ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß
        if getattr(self, 'is_parallel_all_mode', False):
            self.logger.info("FULL PARALLEL MODE: Skipping hydration")
            return {}

        # 2. ‡∏Ñ‡∏±‡∏î‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á Level ‡∏ó‡∏µ‡πà‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡πÉ‡∏ô Sub-Criteria ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
        collected = {}
        for key, ev_list in self.evidence_map.items():
            if key.startswith(f"{sub_id}.L") and isinstance(ev_list, list) and ev_list:
                try:
                    level_num = int(key.split(".L")[-1])
                    if level_num < current_level:
                        collected[key] = ev_list
                except (ValueError, IndexError):
                    continue

        if not collected:
            self.logger.info(f"No previous level evidences found for {sub_id} L{current_level}")
            return {}

        # 3. ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏° IDs ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á (Valid IDs only)
        stable_ids = set()
        for ev_list in collected.values():
            for ev in ev_list:
                sid = ev.get("stable_doc_uuid") or ev.get("doc_id")
                # ‡∏Å‡∏£‡∏≠‡∏á‡∏Ñ‡πà‡∏≤‡∏Ç‡∏¢‡∏∞‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏´‡∏•‡∏∏‡∏î‡∏°‡∏≤‡∏à‡∏≤‡∏Å LLM Response ‡πÉ‡∏ô‡∏£‡∏≠‡∏ö‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤
                if sid and isinstance(sid, str) and sid not in ["N/A", "fallback_doc_id", "None", ""]:
                    stable_ids.add(sid)

        if not stable_ids:
            self.logger.warning(f"‚ö†Ô∏è No valid IDs to hydrate for {sub_id} L{current_level}")
            return collected

        # 4. ‡∏î‡∏∂‡∏á Full Text Chunks ‡∏à‡∏≤‡∏Å Vector Store (Bulk Hydration)
        vsm = self.vectorstore_manager
        try:
            # ‡∏î‡∏∂‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏°‡∏≤‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥ Mapping ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
            full_chunks = vsm.get_documents_by_id(list(stable_ids), self.doc_type, self.enabler_id) 
            self.logger.info(f"HYDRATION: Retrieved {len(full_chunks)} chunks from VSM for mapping")
        except Exception as e:
            self.logger.error(f"Hydration failed in VSM call: {e}")
            return collected

        # 5. ‡∏™‡∏£‡πâ‡∏≤‡∏á Chunk Map (Key Optimization: ‡πÄ‡∏Å‡πá‡∏ö‡∏ó‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö‡∏°‡∏µ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡∏µ‡∏î)
        chunk_map = {}
        for chunk in full_chunks:
            meta = getattr(chunk, "metadata", {})
            # ‡πÄ‡∏Å‡πá‡∏ö UUID ‡∏ó‡∏∏‡∏Å‡∏•‡∏π‡∏Å‡πÄ‡∏•‡πà‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Match ‡πÄ‡∏à‡∏≠ 100%
            potential_ids = [
                meta.get("chunk_uuid"),
                meta.get("stable_doc_uuid"),
                meta.get("doc_id")
            ]
            for pid in potential_ids:
                if pid and isinstance(pid, str):
                    chunk_map[pid] = {"text": chunk.page_content, "metadata": meta}
                    chunk_map[pid.replace("-", "")] = {"text": chunk.page_content, "metadata": meta}

        # 6. ‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏ü‡∏∑‡πâ‡∏ô‡∏ü‡∏π‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ (Hydration Loop)
        hydrated = {}
        restored_count = 0
        total_items = sum(len(v) for v in collected.values())

        for key, ev_list in collected.items():
            new_list = []
            for ev in ev_list:
                new_ev = ev.copy()
                data = None
                
                # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° IDs ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô Map
                sid_raw = ev.get("stable_doc_uuid") or ev.get("doc_id") or ""
                sid_clean = sid_raw.replace("-", "")
                cid_raw = ev.get("chunk_uuid") or ""
                cid_clean = cid_raw.replace("-", "")

                # TRY 1: Match ‡∏î‡πâ‡∏ß‡∏¢ Chunk UUID (‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î)
                data = chunk_map.get(cid_raw) or chunk_map.get(cid_clean)

                # TRY 2: Match ‡∏î‡πâ‡∏ß‡∏¢ Stable Doc UUID / Doc ID (Fallback)
                if not data:
                    data = chunk_map.get(sid_raw) or chunk_map.get(sid_clean)

                if data:
                    new_ev["text"] = data["text"]
                    # Merge Metadata ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ
                    merged_meta = data["metadata"].copy()
                    merged_meta.update(new_ev) # ‡πÉ‡∏´‡πâ‡∏Ñ‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏°‡πÉ‡∏ô Evidence ‡πÄ‡∏Å‡πà‡∏á‡∏Å‡∏ß‡πà‡∏≤
                    new_ev = merged_meta
                    
                    new_ev["is_baseline"] = True 
                    restored_count += 1
                else:
                    # ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô Error ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô ID ‡∏à‡∏£‡∏¥‡∏á‡πÅ‡∏ï‡πà‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠
                    if sid_raw not in ["N/A", "fallback_doc_id", ""]:
                        self.logger.error(f"‚ùå HYDRATION FAILURE: {sid_raw[:8]}... (File: {ev.get('source_filename', 'Unknown')})")
                    new_ev["is_baseline"] = False
                    new_ev["page_label"] = ev.get("page_label") or ev.get("page") or "N/A"
                
                new_list.append(new_ev)
            hydrated[key] = new_list
                
        self.logger.info(f"‚úÖ BASELINE HYDRATED: {restored_count}/{total_items} chunks restored successfully")
        return hydrated

    def _get_contextual_rules_prompt(self, sub_id: str, level: int) -> str:
        """
        Retrieves the specific Contextual Rule prompt for a given Sub-Criteria and Level,
        ‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á‡∏Å‡∏≤‡∏£ Inject ‡∏Å‡∏é L5 ‡∏û‡∏¥‡πÄ‡∏®‡∏© ‡∏´‡∏≤‡∏Å Level == 5
        """
        sub_id_rules = self.contextual_rules_map.get(sub_id)
        rule_text = ""
        
        # 1. ‡∏î‡∏∂‡∏á‡∏Å‡∏é‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏¢‡πà‡∏≠‡∏¢ (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
        if sub_id_rules:
            level_key = f"L{level}"
            specific_rule = sub_id_rules.get(level_key)
            if specific_rule:
                rule_text += f"\n--- ‡∏Å‡∏é‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏¢‡πà‡∏≠‡∏¢ ({sub_id} L{level}) ---\n‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Ñ‡∏ß‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ: {specific_rule}\n"
        
        # 2. **INJECT L5 SPECIAL RULE (Safe Injection)**
        # ‡πÉ‡∏™‡πà‡∏Å‡∏é Bonus 2.0 ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô L5 ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏ö‡∏Å‡∏ß‡∏ô L3/L4
        if level == 5:
            l5_bonus_rule = """
            \n--- L5 SPECIAL RULE (Innovation & Sustainability) ---
            * **L5 PASS Condition (‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö):** ‡∏´‡∏≤‡∏Å Level ‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠ **L5** ‡∏ó‡πà‡∏≤‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏° PDCA (P+D+C+A) ‡∏ï‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô L3/L4 ‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏ô‡∏û‡∏ö‡∏Å‡πà‡∏≠‡∏ô (‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î 8.0)
            * **‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç Bonus 2.0:** ‡∏´‡∏≤‡∏Å‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏° PDCA ‡πÑ‡∏î‡πâ **‚â• 7.0** **‡πÅ‡∏•‡∏∞** ‡∏ó‡πà‡∏≤‡∏ô‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠ **‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 1 ‡∏ä‡∏¥‡πâ‡∏ô** ‡πÉ‡∏ô Context ‡∏ó‡∏µ‡πà‡πÅ‡∏™‡∏î‡∏á‡∏ñ‡∏∂‡∏á:
                * (a) ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏• KM / ‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏° (Innovation Award)
                * (b) ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡∏ó‡∏µ‡πà‡∏ß‡∏±‡∏î‡πÑ‡∏î‡πâ (ROI, Productivity, Cost Saving)
                * (c) ‡∏Å‡∏≤‡∏£‡πÄ‡∏ú‡∏¢‡πÅ‡∏û‡∏£‡πà/‡∏Å‡∏≤‡∏£‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å (External Recognition/Publication)
            * **‡πÉ‡∏´‡πâ‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡πÉ‡∏´‡πâ Bonus Score 2.0 ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ **Score ‚â• 9.0** ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡πâ‡∏á **is_passed=true** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏∞‡∏ó‡πâ‡∏≠‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏•‡∏¥‡∏® (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£ Reset ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô)
            """
            rule_text += l5_bonus_rule
            
        return rule_text

    def _load_rubric(self) -> Dict[str, Any]:
        """ Loads the SEAM rubric JSON file using path_utils. """
        
        # üéØ FIX: ‡πÉ‡∏ä‡πâ get_rubric_file_path ‡∏à‡∏≤‡∏Å path_utils ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Path ‡πÄ‡∏≠‡∏á
        filepath = None # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô UnboundLocalError
        
        try:
            # 1. ‡∏£‡∏±‡∏ö Path ‡∏à‡∏≤‡∏Å path_utils ‡∏ã‡∏∂‡πà‡∏á‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏ä‡∏µ‡πâ‡πÑ‡∏õ‡∏ó‡∏µ‡πà 'config/' ‡πÅ‡∏•‡πâ‡∏ß
            filepath = get_rubric_file_path(
                tenant=self.config.tenant,
                enabler=self.enabler_id
            )
        except Exception as e:
            # ‡∏î‡∏±‡∏Å‡∏à‡∏±‡∏ö Exception ‡∏´‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏î‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô path_utils
            self.logger.error(f"‚ùå FATAL: Error calling get_rubric_file_path: {e}")
            return {} 

        # 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if filepath is None or not os.path.exists(filepath):
            self.logger.error(f"‚ö†Ô∏è Rubric file not found at expected path: {filepath}")
            return {}

        # 3. ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå JSON
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
            self.logger.info(f"‚úÖ Rubric loaded successfully from: {filepath}")
            return data
        except json.JSONDecodeError:
            self.logger.error(f"‚ùå Error decoding Rubric JSON. File might be corrupted: {filepath}")
            return {}
        except Exception as e:
            self.logger.error(f"‚ùå Error loading Rubric file from {filepath}: {e}")
            return {}
    
    # -------------------- Helper Function for Map Processing --------------------
    def _get_level_order_value(self, level_str: str) -> int:
        """Converts Level string ('L1', 'L5') to an integer for comparison."""
        try:
            return int(level_str.upper().replace('L', ''))
        except:
            return 0

    # -------------------- Persistent Mapping Handlers (FIXED) --------------------
    def _process_temp_map_to_final_map(self, temp_map: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Converts the temporary map into the final map format for saving, 
        and filters out temporary/unresolvable evidence IDs.
        """
        working_map = temp_map or self.temp_map_for_save or {}
        final_map_for_save = {}
        total_cleaned_items = 0

        for sub_level_key, evidence_list in working_map.items():
            if isinstance(evidence_list, dict):
                evidence_list = [evidence_list]
            elif not isinstance(evidence_list, list):
                logger.warning(f"[EVIDENCE] Skipping {sub_level_key}: not a list or dict")
                continue

            clean_list = []
            seen_ids = set()
            for ev in evidence_list:
                doc_id = ev.get("doc_id")
                
                if not doc_id:
                    continue
                
                # 1. FIX: ‡∏Å‡∏£‡∏≠‡∏á ID ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß (TEMP-) ‡∏≠‡∏≠‡∏Å
                if doc_id.startswith("TEMP-"):
                    # ID ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏õ‡∏•‡∏á‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô Stable Document ID ‡πÑ‡∏î‡πâ ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÉ‡∏ä‡πâ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ
                    logger.debug(f"[EVIDENCE] Filtering out unresolvable TEMP- ID: {doc_id} for {sub_level_key}.")
                    continue 
                
                # 2. Logic ‡πÄ‡∏î‡∏¥‡∏°: ‡∏Å‡∏£‡∏≠‡∏á HASH- (Placeholder) ‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ã‡πâ‡∏≥
                if doc_id.startswith("HASH-") or doc_id in seen_ids:
                    continue
                    
                seen_ids.add(doc_id)
                clean_list.append(ev)
                total_cleaned_items += 1 

            if clean_list:
                final_map_for_save[sub_level_key] = clean_list

        logger.info(f"[EVIDENCE] Processed {len(final_map_for_save)} sub-level keys with total {total_cleaned_items} evidence items")
        return final_map_for_save

    def _clean_map_for_json(self, data: Union[Dict, List, Set, Any]) -> Union[Dict, List, Any]:
        """Recursively converts objects that cannot be serialized (like sets) into lists."""
        if isinstance(data, dict):
            return {k: self._clean_map_for_json(v) for k, v in data.items()}
        elif isinstance(data, list):
            return [self._clean_map_for_json(v) for v in data]
        elif isinstance(data, set):
            return [self._clean_map_for_json(v) for v in data]
        return data

    def _clean_temp_entries(self, evidence_map: Dict[str, List[Any]]) -> Dict[str, List[Dict]]:
        """
        ‡∏Å‡∏£‡∏≠‡∏á TEMP-, HASH-, ‡πÅ‡∏•‡∏∞ Unknown ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å evidence map ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô 'str' object has no attribute 'get' ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ã‡πà‡∏≠‡∏°‡πÅ‡∏ã‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        """
        if not evidence_map or not isinstance(evidence_map, dict):
            return {}

        cleaned_map = {}
        total_removed = 0
        total_unknown_fixed = 0
        total_invalid_type = 0

        for key, entries in evidence_map.items():
            if not isinstance(entries, list):
                continue
                
            valid_entries = []
            for entry in entries:
                # üõ°Ô∏è Defense: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
                if not isinstance(entry, dict):
                    if isinstance(entry, str) and entry.strip():
                        # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏Å‡∏π‡πâ‡∏Ñ‡∏∑‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô string
                        entry = {"doc_id": entry, "filename": "Unknown", "relevance_score": 0.0}
                    else:
                        total_invalid_type += 1
                        continue

                doc_id = entry.get("doc_id")
                if doc_id is None:
                    total_removed += 1
                    continue
                
                doc_id_str = str(doc_id)

                # 1. ‡∏Å‡∏£‡∏≠‡∏á TEMP- ‡πÅ‡∏•‡∏∞ HASH-
                if doc_id_str.startswith("TEMP-") or doc_id_str.startswith("HASH-"):
                    total_removed += 1
                    continue

                # 2. ‡∏Å‡∏£‡∏≠‡∏á Unknown
                if not doc_id_str or doc_id_str.lower() == "unknown":
                    total_removed += 1
                    continue

                # 3. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Filename
                filename = str(entry.get("filename", "")).strip()
                if not filename or filename.lower() in ["unknown", "none", "unknown_file.pdf", "n/a"]:
                    short_id = doc_id_str[:8]
                    entry["filename"] = f"‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á_{short_id}.pdf"
                    total_unknown_fixed += 1
                else:
                    try:
                        entry["filename"] = os.path.basename(filename)
                    except:
                        entry["filename"] = filename

                valid_entries.append(entry)

            if valid_entries:
                cleaned_map[key] = valid_entries

        return cleaned_map

    def _save_evidence_map(self, map_to_save: Optional[Dict[str, List[Dict[str, Any]]]] = None):
        """
        ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å evidence map ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢ 100% 
        [REVISED 2026] - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö UUID v5, ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô ID 'fallback', ‡πÅ‡∏•‡∏∞‡∏ó‡∏≥ Atomic Write
        """
        try:
            map_file_path = get_evidence_mapping_file_path(
                tenant=self.config.tenant,
                year=self.config.year,
                enabler=self.enabler_id
            )
        except Exception as e:
            self.logger.critical(f"[EVIDENCE] FATAL: ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Path ‡πÑ‡∏î‡πâ: {e}")
            raise

        lock_path = map_file_path + ".lock"
        tmp_path = None

        self.logger.info(f"[EVIDENCE] Preparing atomic save ‚Üí {map_file_path}")

        try:
            os.makedirs(os.path.dirname(map_file_path), exist_ok=True)

            with FileLock(lock_path, timeout=60):
                # 1. ‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å (Merge Logic)
                if map_to_save is not None:
                    final_map_to_write = map_to_save
                else:
                    # ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤‡∏à‡∏≤‡∏Å Disk ‡∏°‡∏≤ Merge ‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏ô Memory
                    existing_map = self._load_evidence_map(is_for_merge=True) or {}
                    runtime_map = deepcopy(self.evidence_map)
                    final_map_to_write = existing_map

                    for key, new_entries in runtime_map.items():
                        current_entries = final_map_to_write.setdefault(key, [])
                        
                        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Index ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ä‡πá‡∏Ñ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô (Key: Cleaned UUID)
                        entry_map = {}
                        for e in current_entries:
                            if isinstance(e, dict):
                                # üü¢ [FIX] ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î ID ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô Key ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö
                                raw_id = e.get("chunk_uuid") or e.get("doc_id") or "N/A"
                                clean_id = str(raw_id).replace("-", "").lower()
                                entry_map[clean_id] = e
                        
                        for new_entry in new_entries:
                            if not isinstance(new_entry, dict): continue
                            
                            # ‡∏î‡∏∂‡∏á ID ‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏´‡∏°‡πà
                            raw_new_id = new_entry.get("chunk_uuid") or new_entry.get("doc_id") or "N/A"
                            clean_new_id = str(raw_new_id).replace("-", "").lower()

                            # üü¢ [FIX] ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏¢‡∏∞‡∏´‡∏£‡∏∑‡∏≠ 'fallback' ‡∏´‡∏•‡∏∏‡∏î‡πÄ‡∏Ç‡πâ‡∏≤‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
                            if clean_new_id in ["na", "n/a", "fallback", "none", ""]:
                                continue

                            new_score = new_entry.get("relevance_score", 0.0)

                            # ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ ID ‡∏ô‡∏µ‡πâ‡πÉ‡∏ô Database ‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡∏±‡∏ß‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏î‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤ -> ‡πÉ‡∏´‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å
                            if clean_new_id not in entry_map:
                                entry_map[clean_new_id] = new_entry
                            else:
                                old_entry = entry_map[clean_new_id]
                                old_score = old_entry.get("relevance_score", 0.0)
                                
                                # ‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Metadata ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏´‡∏≤‡∏Å‡∏ï‡∏±‡∏ß‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ (‡πÄ‡∏ä‡πà‡∏ô ‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤)
                                if "page" not in new_entry or new_entry["page"] in ["N/A", None]:
                                    new_entry["page"] = old_entry.get("page")
                                if "page_label" not in new_entry:
                                    new_entry["page_label"] = old_entry.get("page_label")
                                        
                                if new_score >= old_score:
                                    entry_map[clean_new_id] = new_entry

                        final_map_to_write[key] = list(entry_map.values())

                if not final_map_to_write:
                    self.logger.warning("[EVIDENCE] Nothing to save (empty map).")
                    return

                # 2. ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå
                final_map_to_write = self._clean_temp_entries(final_map_to_write)
                for key, entries in final_map_to_write.items():
                    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á (‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏≠‡∏¢‡∏π‡πà‡∏ö‡∏ô)
                    entries.sort(key=lambda x: x.get("relevance_score", 0.0), reverse=True)

                # 3. üõ°Ô∏è ATOMIC WRITE (‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏û‡∏±‡∏á‡∏Ç‡∏ì‡∏∞‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô)
                with tempfile.NamedTemporaryFile(
                    mode='w', delete=False, encoding="utf-8", dir=os.path.dirname(map_file_path)
                ) as tmp_file:
                    cleaned_data = self._clean_map_for_json(final_map_to_write)
                    json.dump(cleaned_data, tmp_file, indent=4, ensure_ascii=False)
                    tmp_path = tmp_file.name

                # ‡∏¢‡πâ‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß‡∏°‡∏≤‡∏ó‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏£‡∏¥‡∏á (‡πÄ‡∏õ‡πá‡∏ô Atomic Operation ‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö OS)
                shutil.move(tmp_path, map_file_path)
                tmp_path = None

                self.logger.info(f"[EVIDENCE] SAVED SUCCESSFULLY! Total Keys: {len(final_map_to_write)}")

        except Exception as e:
            self.logger.critical("[EVIDENCE] FATAL ERROR DURING ATOMIC SAVE")
            self.logger.exception(e)
            raise
        finally:
            # ‡πÄ‡∏Å‡πá‡∏ö‡∏Å‡∏ß‡∏≤‡∏î‡πÑ‡∏ü‡∏•‡πå Lock ‡πÅ‡∏•‡∏∞‡πÑ‡∏ü‡∏•‡πå Temp ‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏≤‡∏á‡∏≠‡∏¢‡∏π‡πà
            if os.path.exists(lock_path):
                try: os.unlink(lock_path)
                except: pass
            if tmp_path and os.path.exists(tmp_path):
                try: os.unlink(tmp_path)
                except: pass

    def _load_evidence_map(self, is_for_merge: bool = False) -> Dict[str, List[Dict[str, Any]]]:
        """
        ‡πÇ‡∏´‡∏•‡∏î evidence map ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
        is_for_merge = True ‚Üí ‡πÑ‡∏°‡πà log "No existing map" (‡πÉ‡∏ä‡πâ‡πÉ‡∏ô save)
        """
        try:
            path = get_evidence_mapping_file_path(
                tenant=self.config.tenant,
                year=self.config.year,
                enabler=self.enabler_id
            )
        except Exception as e:
            self.logger.error(f"[EVIDENCE] FATAL: ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏î‡πâ: {e}")
            return {}

        if not os.path.exists(path):
            if not is_for_merge:
                self.logger.info("[EVIDENCE] No existing evidence map found ‚Äì starting fresh.")
            return {}

        try:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
            if not is_for_merge:
                total_items = sum(len(v) for v in data.values() if isinstance(v, list))
                self.logger.info(f"[EVIDENCE] Loaded evidence map: {len(data)} keys, {total_items} items from {path}")
            return data
        except Exception as e:
            self.logger.error(f"[EVIDENCE] Failed to load evidence map from {path}: {e}")
            return {}


    def _set_mock_handlers(self, mode: str):
        """Replaces real LLM/RAG functions with mock versions."""
        if mode == "control" or mode == "random":
            if hasattr(seam_mocking, 'evaluate_with_llm_CONTROLLED_MOCK'):
                self.llm_evaluator = seam_mocking.evaluate_with_llm_CONTROLLED_MOCK
            if hasattr(seam_mocking, 'retrieve_context_with_filter_MOCK'):
                self.rag_retriever = seam_mocking.retrieve_context_with_filter_MOCK
            if hasattr(seam_mocking, 'create_structured_action_plan_MOCK'):
                self.action_plan_generator = seam_mocking.create_structured_action_plan_MOCK
            if hasattr(seam_mocking, 'set_mock_control_mode'):
                seam_mocking.set_mock_control_mode(mode == "control") 

        logger.warning(f"Engine is running in MOCK mode: {mode}")

    def _get_pdca_phase(self, level: int) -> str:
        """Helper to get the PDCA phase string from the map."""
        return PDCA_PHASE_MAP.get(level, f"Level {level} Requirement")

    def _get_level_constraint_prompt(self, sub_id: str, level: int) -> str:
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á Prompt Constraint ‡πÅ‡∏ö‡∏ö Dynamic ‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏‡πÉ‡∏ô Contextual Rules
        [REVISED 2026] - ‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô‡∏ï‡∏≤‡∏° Sub-ID ‡πÅ‡∏•‡∏∞ Level ‡∏à‡∏£‡∏¥‡∏á
        """
        # 1. ‡∏î‡∏∂‡∏á‡∏Å‡∏é‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡∏≠‡∏á‡πÄ‡∏•‡πÄ‡∏ß‡∏•‡∏ô‡∏±‡πâ‡∏ô‡∏à‡∏≤‡∏Å JSON
        required_phases = self.get_rule_content(sub_id, level, "require_phase") or []
        specific_rule = self.get_rule_content(sub_id, level, "specific_contextual_rule") or ""
        
        # ‡πÅ‡∏õ‡∏•‡∏á‡∏ï‡∏±‡∏ß‡∏¢‡πà‡∏≠ P, D, C, A ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡πá‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ö‡∏≠‡∏Å AI
        phase_map = {"P": "‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô (Plan)", "D": "‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥ (Do)", "C": "‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö (Check)", "A": "‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á (Act)"}
        full_phase_names = [phase_map.get(p, p) for p in required_phases]

        # 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á Prompt ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        constraint_msg = f"--- ‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ß‡∏∏‡∏í‡∏¥‡∏†‡∏≤‡∏ß‡∏∞ (Level {level}) ---\n"
        
        if full_phase_names:
            constraint_msg += f"üéØ ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏ü‡∏™: {', '.join(full_phase_names)} ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å\n"
        
        # 3. ‡πÉ‡∏™‡πà‡∏Å‡∏é‡∏û‡∏¥‡πÄ‡∏®‡∏© (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
        if specific_rule:
            constraint_msg += f"‚ö†Ô∏è ‡∏Å‡∏é‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ: {specific_rule}\n"
        
        # 4. ‡πÉ‡∏™‡πà Logic ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Maturity Level
        if level >= 3:
            constraint_msg += (
                "üí° ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö L3 ‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏õ: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏™‡∏î‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏´‡πá‡∏ô‡∏ñ‡∏∂‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á "
                "‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏°‡∏≤‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏à‡∏£‡∏¥‡∏á ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡πÅ‡∏Ñ‡πà‡∏°‡∏µ‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô\n"
            )
            
        return constraint_msg
        

    # -------------------- Statement Preparation & Filtering Helpers --------------------
    def _flatten_rubric_to_statements(self) -> List[Dict[str, Any]]:
        """
        Transforms the hierarchical rubric structure loaded in self.rubric
        into a flat list of statements ready for assessment.
        """
        if not self.rubric:
            self.logger.warning("Cannot flatten rubric: self.rubric is empty.")
            return []
            
        data = deepcopy(self.rubric)
        extracted_list = []
        
        if not isinstance(data, dict):
             self.logger.error("Rubric data structure is invalid (expected dict of criteria).")
             return []
             
        # for criteria_id, criteria_data in data.items():
        for criteria_id, criteria_data in data.get('criteria', {}).items():
            # üéØ FIX 1: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Criteria Data
            if not isinstance(criteria_data, dict):
                self.logger.warning(f"Skipping malformed criteria entry: {criteria_id} (not a dict).")
                continue
                
            sub_criteria_map = criteria_data.get('subcriteria', {})
            criteria_name = criteria_data.get('name')
            
            # üéØ FIX 2: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Sub-criteria Map
            if not isinstance(sub_criteria_map, dict):
                 self.logger.warning(f"Skipping criteria {criteria_id}: 'subcriteria' is not a dictionary.")
                 continue

            for sub_id, sub_data in sub_criteria_map.items():
                
                # üéØ FIX 3: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ sub_data ‡πÄ‡∏õ‡πá‡∏ô Dictionary ‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô TypeError)
                if not isinstance(sub_data, dict):
                    self.logger.warning(
                        f"Skipping malformed sub-criteria entry: {criteria_id}.{sub_id} "
                        f"is not a dictionary (found type: {type(sub_data).__name__})."
                    )
                    continue
                
                # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤ Metadata ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
                sub_data['criteria_id'] = criteria_id
                sub_data['criteria_name'] = criteria_name
                sub_data['sub_id'] = sub_id 
                sub_data['sub_criteria_name'] = sub_data.get('name', criteria_name + ' sub')
                if 'weight' not in sub_data:
                    sub_data['weight'] = criteria_data.get('weight', 0)
                extracted_list.append(sub_data)

        # Re-check and re-sort levels
        final_list = []
        for sub_criteria in extracted_list: 
            
            # üéØ FIX 4: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á Level ‡∏à‡∏≤‡∏Å Dict ‡∏ó‡∏µ‡πà‡∏°‡∏µ Key ‡πÄ‡∏õ‡πá‡∏ô String ‡πÄ‡∏õ‡πá‡∏ô List
            if "levels" in sub_criteria and isinstance(sub_criteria["levels"], dict):
                levels_list = []
                for level_str, statement in sub_criteria["levels"].items():
                    try:
                        # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÅ‡∏õ‡∏•‡∏á Level Key ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô Integer
                        level_int = int(level_str)
                        if isinstance(statement, str):
                            levels_list.append({"level": level_int, "statement": statement})
                        else:
                            self.logger.warning(f"Level {level_str} statement in {sub_criteria.get('sub_id')} is not a string.")
                    except ValueError:
                        self.logger.error(f"Invalid level key '{level_str}' found in {sub_criteria.get('sub_id', 'UNKNOWN_ID')}. Skipping.")
                        continue
                        
                sub_criteria["levels"] = levels_list
            
            # ‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏£‡∏∞‡∏î‡∏±‡∏ö
            if "levels" in sub_criteria and isinstance(sub_criteria["levels"], list):
                 sub_criteria["levels"].sort(key=lambda x: x.get("level", 0))
                 final_list.append(sub_criteria)
            else:
                 self.logger.warning(f"Sub-criteria {sub_criteria.get('sub_id', 'UNKNOWN_ID')} missing 'levels' list.")


        return final_list

    
    # -------------------- Evidence Classification Helper (Full Fixed 2026) --------------------
    def _get_mapped_uuids_and_priority_chunks(
        self,
        sub_id: str,
        level: int,
        statement_text: str,
        level_constraint: str,
        vectorstore_manager: Optional['VectorStoreManager']
    ) -> Tuple[List[str], List[Dict]]:
        """
        [DYNAMIC CONTINUITY v2026.5] - No Manual UUIDs
        ----------------------------------------------
        - Auto-History: ‡∏î‡∏∂‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà AI ‡πÄ‡∏Ñ‡∏¢‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ß‡πâ‡πÉ‡∏ô Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ê‡∏≤‡∏ô (Baseline)
        - Semantic Hinting: ‡πÉ‡∏ä‡πâ Keywords ‡∏à‡∏≤‡∏Å Rules ‡πÑ‡∏õ‡∏ä‡πà‡∏ß‡∏¢‡∏™‡∏Å‡∏±‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏•‡πà‡∏ß‡∏á‡∏´‡∏ô‡πâ‡∏≤
        - Zero Manual Input: ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏£‡∏≠‡∏Å UUID ‡πÄ‡∏≠‡∏á ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ ID ‡πÉ‡∏ô Memory ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        """
        from copy import deepcopy
        priority_chunks = []
        mapped_stable_ids = []

        # 1. üß† [AUTO-HISTORY] ‡∏î‡∏∂‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥‡∏à‡∏≤‡∏Å Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
        # ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡∏à‡∏≥ UUID ‡∏ó‡∏µ‡πà‡∏°‡∏±‡∏ô‡πÄ‡∏Ñ‡∏¢‡πÉ‡∏ä‡πâ‡∏ï‡∏£‡∏ß‡∏à‡∏ú‡πà‡∏≤‡∏ô‡πÉ‡∏ô L1 ‡∏°‡∏≤‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠‡πÉ‡∏´‡πâ L2 ‡πÄ‡∏≠‡∏á‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
        for key, evidences in self.evidence_map.items():
            if key.startswith(f"{sub_id}.L") and isinstance(evidences, list):
                try:
                    prev_level = int(key.split(".L")[-1])
                    if prev_level < level:
                        history_items = deepcopy(evidences)
                        for item in history_items:
                            item["is_baseline"] = True
                            item["rerank_score"] = max(item.get("rerank_score", 0.0), 0.85)
                        priority_chunks.extend(history_items)
                except (ValueError, IndexError):
                    continue

        # 2. üîç [SEMANTIC HINTING] ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö L1 ‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏û‡∏¢‡∏∏‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥ (‡πÄ‡∏ä‡πà‡∏ô L1) ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏à‡∏≤‡∏Å Rule ‡πÑ‡∏õ‡∏ó‡∏≥ 'Pre-Search' ‡∏™‡∏±‡πâ‡∏ô‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏•‡∏±‡∏Å
        if not priority_chunks:
            rule_config = self.contextual_rules_map.get(sub_id, {}).get(str(level), {})
            # ‡∏î‡∏∂‡∏á‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏à‡∏≤‡∏Å plan_keywords ‡∏´‡∏£‡∏∑‡∏≠ do_keywords ‡πÉ‡∏ô JSON ‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô
            hints = rule_config.get("plan_keywords", [])[:2] # ‡πÄ‡∏≠‡∏≤‡πÅ‡∏Ñ‡πà 2 ‡∏Ñ‡∏≥‡∏´‡∏•‡∏±‡∏Å
            if hints and vectorstore_manager:
                self.logger.info(f"üîé L1 Discovery: Searching for anchors using hints: {hints}")
                discovery_result = vectorstore_manager.quick_search(
                    query=f"{sub_id} {' '.join(hints)}",
                    top_k=5 # ‡∏î‡∏∂‡∏á‡∏°‡∏≤‡πÅ‡∏Ñ‡πà 5 ‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏°‡∏≠‡πÄ‡∏£‡∏∑‡∏≠
                )
                for chunk in discovery_result:
                    chunk["rerank_score"] = 0.85 # ‡∏ö‡∏π‡∏™‡∏ï‡πå‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤
                    priority_chunks.append(chunk)

        if not priority_chunks:
            return [], []

        # 3. üíß [ROBUST HYDRATION] ‡πÄ‡∏ï‡∏¥‡∏°‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏ï‡πá‡∏°
        try:
            priority_chunks = self._robust_hydrate_documents_for_priority_chunks(
                chunks_to_hydrate=priority_chunks,
                vsm=vectorstore_manager
            )
        except Exception as e:
            self.logger.error(f"‚ùå Hydration failed: {e}")

        # 4. üéØ [ID SYNC] ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏° IDs ‡∏™‡πà‡∏á‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡πÄ‡∏õ‡πá‡∏ô Filter ‡πÉ‡∏´‡πâ Main RAG
        seen_ids = set()
        for chunk in priority_chunks:
            sid = chunk.get("stable_doc_uuid") or chunk.get("doc_id")
            if sid and isinstance(sid, str):
                if sid not in seen_ids and len(sid) >= 32:
                    mapped_stable_ids.append(sid)
                    seen_ids.add(sid)

        self.logger.info(f"‚úÖ Continuity Ready: {len(priority_chunks)} priority chunks | {len(mapped_stable_ids)} IDs")
        return mapped_stable_ids, priority_chunks
    
    # -------------------- Calculation Helpers (ADDED) --------------------
    def _calculate_weighted_score(self, highest_full_level: int, weight: int) -> float:
        """
        Calculates the weighted score based on the highest full level achieved.
        Score is calculated by: (Level / MAX_LEVEL) * Weight
        """
        # üéØ ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å MAX_LEVEL_CALC ‡πÄ‡∏õ‡πá‡∏ô MAX_LEVEL ‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏°‡∏≤‡∏à‡∏≤‡∏Å global_vars
        from config.global_vars import MAX_LEVEL  
        
        if highest_full_level <= 0:
            return 0.0
        
        # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô (‡πÄ‡∏ä‡πà‡∏ô ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ data ‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î)
        level_for_calc = min(highest_full_level, MAX_LEVEL)
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ñ‡πà‡∏ß‡∏á‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏ï‡∏≤‡∏°‡πÄ‡∏û‡∏î‡∏≤‡∏ô‡πÄ‡∏•‡πÄ‡∏ß‡∏•‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
        score = (level_for_calc / MAX_LEVEL) * weight
        return score

    def _calculate_overall_stats(self, target_sub_id: str):
        """
        [ULTIMATE STATS v2026.3] Weighted Maturity Calculation
        ------------------------------------------------------
        - ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Maturity Score (0-5) ‡∏ï‡∏≤‡∏°‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏Å‡∏ì‡∏ë‡πå
        - ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Progress % ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ú‡πà‡∏≤‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö L5 ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        - ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Maturity Level Label ‡πÇ‡∏î‡∏¢‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏à‡∏£‡∏¥‡∏á (Conservative Approach)
        """
        from config.global_vars import MAX_LEVEL
        results = self.final_subcriteria_results
        
        # 1. üõ°Ô∏è Safety Guard: ‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        if not results:
            self.total_stats = {
                "overall_avg_score": 0.0,
                "overall_level_label": "L0",
                "progress_percent": 0.0,
                "total_weighted_score": 0.0,
                "total_possible_weight": 0.0,
                "record_id": self.current_record_id,
                "status": "No Data"
            }
            return

        # 2. ‚öñÔ∏è ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ñ‡πà‡∏ß‡∏á‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å (Weighted Summation)
        # weighted_score = (Level / MAX_LEVEL) * weight ‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏ô‡∏±‡πâ‡∏ô‡πÜ
        total_weighted_score_achieved = sum(r.get('weighted_score', 0.0) for r in results)
        total_possible_weight = sum(r.get('weight', 0.0) for r in results)

        # 3. üìä ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Maturity Score (Scale 0.0 - 5.0)
        overall_avg_score = 0.0
        if total_possible_weight > 0:
            # ‡∏™‡∏π‡∏ï‡∏£: (‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ / ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏£‡∏ß‡∏°) * 5
            overall_avg_score = round((total_weighted_score_achieved / total_possible_weight) * MAX_LEVEL, 2)
        
        # 4. üìà ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Progress Percentage (0-100%)
        # ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏ï‡πá‡∏°‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ (‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏£‡∏ß‡∏° * 5)
        max_possible_points = total_possible_weight * MAX_LEVEL
        progress_percent = 0.0
        if max_possible_points > 0:
            progress_percent = round((total_weighted_score_achieved / max_possible_points) * 100, 2)

        # 5. üè∑Ô∏è ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Maturity Level Label (Audit Logic)
        # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô "‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô" (Highest Full Level) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå SE-AM
        avg_full_level = sum(r.get('highest_full_level', 0) for r in results) / len(results)
        final_level = int(avg_full_level) # ‡πÉ‡∏ä‡πâ Floor (‡∏ï‡∏±‡∏î‡πÄ‡∏®‡∏©) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° Conservative ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ round() ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
        overall_level_label = f"L{min(max(final_level, 0), MAX_LEVEL)}"
        
        # 6. üìù ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏™‡∏£‡∏∏‡∏õ (Comprehensive Object)
        self.total_stats = {
            "overall_avg_score": min(overall_avg_score, float(MAX_LEVEL)),
            "overall_level_label": overall_level_label,
            "total_weighted_score": round(total_weighted_score_achieved, 2),
            "total_possible_weight": total_possible_weight,
            "progress_percent": progress_percent,
            "gap_to_full": round(total_possible_weight - total_weighted_score_achieved, 2),
            "assessed_count": len(results),
            "total_subcriteria_in_rubric": len(self._flatten_rubric_to_statements()),
            "enabler": self.config.enabler,
            "target_sub_id": target_sub_id,
            "record_id": self.current_record_id,
            "assessed_at": datetime.now().isoformat()
        }

        # 7. üì¢ Logging ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°
        self.logger.info(f"--- üèÅ ASSESSMENT COMPLETE (ID: {self.current_record_id}) ---")
        self.logger.info(f"üèÜ Overall Level: {overall_level_label} | Score: {overall_avg_score}/{MAX_LEVEL}")
        self.logger.info(f"üìä Progress: {progress_percent}% | Weighted Score: {self.total_stats['total_weighted_score']}/{total_possible_weight}")
        self.logger.info(f"----------------------------------------------------------")
       
    def _export_results(self, results: dict, sub_criteria_id: str, **kwargs) -> str:
        """
        [ULTIMATE EXPORTER v2026.3]
        ---------------------------
        - üìÇ Hierarchical Storage: ‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏° Tenant/Year/Enabler ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö
        - üìë Self-Contained Metadata: ‡∏ù‡∏±‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ö‡∏£‡∏¥‡∏ö‡∏ó (Model, Target, ID) ‡∏•‡∏á‡πÉ‡∏ô JSON
        - üìä Audit Summary: ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô‡∏•‡∏á‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡∏≠‡∏∑‡πà‡∏ô‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢
        - üõ°Ô∏è Path Resilience: ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Path ‡∏ó‡∏µ‡πà‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Folder
        """
        import os
        import json
        from datetime import datetime

        # 1. ‚öôÔ∏è ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        record_id = kwargs.get("record_id", getattr(self, "current_record_id", "no_id"))
        enabler = self.config.enabler
        tenant = self.config.tenant
        year = str(self.config.year)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # 2. üìÅ ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Path (Enterprise Directory Structure)
        # ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á: data_store/{tenant}/exports/{year}/{enabler}/
        export_dir = os.path.join("data_store", tenant, "exports", year, enabler)
        
        try:
            os.makedirs(export_dir, exist_ok=True)
            file_name = f"assessment_{enabler}_{record_id}_{sub_criteria_id}_{timestamp}.json"
            full_path = os.path.join(export_dir, file_name)
        except Exception as e:
            self.logger.error(f"‚ùå Directory creation failed: {e}")
            # Fallback ‡πÑ‡∏õ‡∏¢‡∏±‡∏á root data_store ‡∏Å‡∏£‡∏ì‡∏µ Permission ‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤
            full_path = f"assessment_fallback_{record_id}.json"

        # 3. üìä ‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Summary (‡∏î‡∏∂‡∏á Logic ‡∏à‡∏≤‡∏Å Origin ‡∏°‡∏≤‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô)
        if 'summary' not in results:
            results['summary'] = {}
        
        summary = results['summary']
        sub_res_list = results.get('sub_criteria_results', [])

        # ‡∏ù‡∏±‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏∞‡∏ö‡∏∏‡∏ï‡∏±‡∏ß‡∏ï‡∏ô (Identity Metadata)
        results['metadata'] = {
            "record_id": record_id,
            "tenant": tenant,
            "year": year,
            "enabler": enabler,
            "model_used": getattr(self.config, "model_name", "unknown"),
            "target_level": self.config.target_level,
            "export_at": datetime.now().isoformat()
        }

        # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô (Single vs All)
        if str(sub_criteria_id).lower() != "all" and len(sub_res_list) > 0:
            main_res = sub_res_list[0]
            summary.update({
                "highest_pass_level": main_res.get('highest_full_level', 0),
                "achieved_weight": round(main_res.get('weighted_score', 0.0), 2),
                "total_weight": main_res.get('weight', 0.0),
                "is_target_achieved": main_res.get('target_level_achieved', False)
            })
        else:
            all_pass_levels = [r.get('highest_full_level', 0) for r in sub_res_list]
            total_achieved = sum(r.get('weighted_score', 0.0) for r in sub_res_list)
            total_possible = sum(r.get('weight', 0.0) for r in sub_res_list)
            
            summary.update({
                "highest_pass_level_overall": max(all_pass_levels) if all_pass_levels else 0,
                "total_achieved_weight": round(total_achieved, 2),
                "total_possible_weight": round(total_possible, 2),
                "overall_percentage": round((total_achieved / total_possible * 100), 2) if total_possible > 0 else 0.0,
                "total_subcriteria_assessed": len(sub_res_list)
            })

        # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Action Plan ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        summary['total_action_plan_items'] = sum(len(r.get('action_plan', [])) for r in sub_res_list if isinstance(r.get('action_plan'), list))

        # 4. üíæ ‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÑ‡∏ü‡∏•‡πå (Writing Process)
        try:
            with open(full_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=4)
            
            self.logger.info(f"üíæ EXPORT SUCCESS: {full_path}")
            
            # ‡∏™‡∏£‡∏∏‡∏õ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡πÉ‡∏ô Log
            final_lvl = summary.get('highest_pass_level', summary.get('highest_pass_level_overall', 0))
            self.logger.info(f"üìä [FINAL REPORT] Record: {record_id} | Level: L{final_lvl} | Score: {summary.get('total_achieved_weight', summary.get('achieved_weight', 0))}")
            
            return full_path
            
        except Exception as e:
            self.logger.error(f"‚ùå Export failed: {str(e)}")
            return ""
        
    def _create_error_result(
        self,
        level: int,
        error_message: str,
        start_time: float,
        retrieval_duration: float,
        sub_id: str,
        statement_id: str,
        statement_text: str,
        llm_duration: float = 0.0,
    ) -> Dict[str, Any]:
        """
        Generates a standard error dictionary for when RAG or LLM fails.
        """
        # NOTE: Assumes self._get_pdca_phase is defined
        try:
            pdca_phase = self._get_pdca_phase(level) 
        except Exception:
            pdca_phase = "N/A"
            
        pdca_breakdown = {p: 0 for p in ['P', 'D', 'C', 'A']}
        total_duration = time.time() - start_time

        self.logger.error(f"FATAL ERROR RESULT for {sub_id} L{level}: {error_message}")

        return {
            "sub_criteria_id": sub_id,
            "statement_id": statement_id,
            "level": level,
            "statement": statement_text,
            "pdca_phase": pdca_phase,
            "llm_score": 0.0,
            "pdca_breakdown": pdca_breakdown,
            "is_passed": False,
            "status": "FAIL",
            "score": 0.0,
            "llm_result_full": {"error": error_message, "details": "Assessment skipped due to critical failure."},
            "retrieval_duration_s": round(retrieval_duration, 2),
            "llm_duration_s": round(llm_duration, 2),
            "top_evidences_ref": [],
            "temp_map_for_level": [],
            "evidence_strength": 0.0,
            "ai_confidence": "LOW",
            "evidence_count": 0,
            "pdca_coverage": 0.0,
            "direct_evidence_count": 0,
            "rag_query": statement_text,
            "full_context_meta": {"error_type": "Critical Failure"},
            # üü¢ NEW: Relevant Score Gate Metadata (Set to default error values)
            "max_relevant_score": 0.0,
            "max_relevant_source": "ERROR_HANDLING",
            "is_evidence_strength_capped": False,
            "max_evidence_strength_used": 0.0,
            "total_run_time_s": round(total_duration, 2)
        }

    def _save_level_evidences_and_calculate_strength(
        self, 
        level_temp_map: List[Dict[str, Any]], 
        sub_id: str, 
        level: int, 
        llm_result: Dict[str, Any],
        highest_rerank_score: float = 0.0
    ) -> float:
        """
        [ULTIMATE SAVER v2026.3] Evidence Persistence & Strength Calculation
        --------------------------------------------------------------------
        - üõ°Ô∏è Robust Extraction: ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á Dict ‡πÅ‡∏•‡∏∞ LangChain Document
        - üîó Unique Identification: ‡∏£‡∏∞‡∏ö‡∏ö Fallback ID ‡∏î‡πâ‡∏ß‡∏¢ Hash (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Data Loss)
        - üìä Precision Scoring: ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ú‡πà‡∏≤‡∏ô Scoring Gate
        - üß© UI Ready: ‡∏à‡∏±‡∏î‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Metadata ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏•‡∏∞‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô
        """
        import hashlib
        map_key = f"{sub_id}.L{level}"
        new_evidence_list: List[Dict[str, Any]] = []
        
        self.logger.info(f"üíæ [EVI SAVE] Starting save for {map_key} ({len(level_temp_map)} potential chunks)")

        for chunk in level_temp_map:
            # üéØ 1. üì¶ ‡∏Å‡∏≤‡∏£‡∏™‡∏Å‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢)
            if isinstance(chunk, dict):
                meta = chunk.get("metadata", {}) or {}
                # ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: Direct Key > Metadata > ID
                chunk_uuid = chunk.get("chunk_uuid") or chunk.get("id") or meta.get("chunk_uuid")
                stable_doc_uuid = (chunk.get("stable_doc_uuid") or chunk.get("doc_id") or 
                                  meta.get("stable_doc_uuid") or meta.get("doc_id"))
                source = chunk.get("source") or meta.get("source") or "N/A"
                text_content = chunk.get("page_content") or chunk.get("text") or ""
            else:
                # ‡∏Å‡∏£‡∏ì‡∏µ‡πÄ‡∏õ‡πá‡∏ô LangChain Document Object
                meta = getattr(chunk, "metadata", {}) or {}
                chunk_uuid = meta.get("chunk_uuid") or getattr(chunk, "id", None)
                stable_doc_uuid = meta.get("stable_doc_uuid") or meta.get("doc_id")
                source = meta.get("source", "N/A")
                text_content = getattr(chunk, "page_content", "")

            # üéØ 2. üõ°Ô∏è Validation & Fallback (‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Error "ID Missing")
            if not stable_doc_uuid or not chunk_uuid:
                if text_content and len(str(text_content)) > 10:
                    # ‡∏™‡∏£‡πâ‡∏≤‡∏á Hash ‡∏à‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ ID ‡∏ó‡∏µ‡πà‡∏Ñ‡∏á‡∏ó‡∏µ‡πà (Deterministic)
                    content_hash = hashlib.md5(str(text_content).encode()).hexdigest()
                    chunk_uuid = chunk_uuid or f"hash-{content_hash[:16]}"
                    stable_doc_uuid = stable_doc_uuid or f"doc-{content_hash[16:32]}"
                    self.logger.warning(f"‚ö†Ô∏è [EVI SAVE] Generated Hash-ID for source: {source}")
                else:
                    self.logger.error(f"‚ùå [EVI SAVE] Critical ID Missing & No Content! Skipping source: {source}")
                    continue

            # üéØ 3. üìÑ ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤ (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏ö Label ‡∏Ç‡∏≠‡∏á PDF)
            page_val = (
                meta.get("page_label") or 
                meta.get("page") or 
                meta.get("page_number") or 
                (chunk.get("page") if isinstance(chunk, dict) else "N/A")
            )

            # üéØ 4. üìù ‡∏™‡∏£‡πâ‡∏≤‡∏á Evidence Entry ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ô‡∏≥‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô JSON Report)
            evidence_entry = {
                "sub_id": sub_id,
                "level": level,
                "relevance_score": float(chunk.get("rerank_score") or chunk.get("score") or 0.5 if isinstance(chunk, dict) else 0.5),
                "doc_id": str(stable_doc_uuid),
                "stable_doc_uuid": str(stable_doc_uuid),
                "chunk_uuid": str(chunk_uuid),
                "source": source,
                "source_filename": meta.get("source_filename") or meta.get("file_name") or os.path.basename(str(source)),
                "page": str(page_val),
                "pdca_tag": (chunk.get("pdca_tag") if isinstance(chunk, dict) else meta.get("pdca_tag")) or "Other", 
                "status": "PASS" if llm_result.get("is_passed") else "FAIL", 
                "timestamp": datetime.now().isoformat(),
            }
            new_evidence_list.append(evidence_entry)

        # üéØ 5. ‚öñÔ∏è ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏°‡∏Ç‡πâ‡∏ô‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô (Evidence Strength)
        # ‡∏™‡πà‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏Å‡∏£‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏Ç‡πâ‡∏≤ Score Gate ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏ß‡πà‡∏≤‡∏à‡∏∞ Cap ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô AI ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        evi_cap_data = self._calculate_evidence_strength_cap(
            top_evidences=new_evidence_list,
            level=level,
            highest_rerank_score=highest_rerank_score
        )
        final_evi_str = evi_cap_data.get('max_evi_str_for_prompt', 0.0)

        # üéØ 6. üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á Memory Map (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á Main Process ‡πÅ‡∏•‡∏∞ Worker)
        if new_evidence_list:
            self.evidence_map.setdefault(map_key, []).extend(new_evidence_list)
            self.temp_map_for_save.setdefault(map_key, []).extend(new_evidence_list)
            self.logger.info(f"‚úÖ [EVIDENCE SAVED] {map_key}: {len(new_evidence_list)} chunks | Strength: {final_evi_str}")
        else:
            self.logger.warning(f"‚ö†Ô∏è [EVI SAVE] No valid evidence entries for {map_key}")
            final_evi_str = 0.0
        
        return final_evi_str
    
    def calculate_audit_confidence(self, matched_chunks: List[Any]) -> Dict[str, Any]:
        """
        [ULTIMATE AUDIT CONFIDENCE v2026.3]
        - Quality Gate: ‡∏Å‡∏£‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà Score ‡∏ï‡πà‡∏≥‡∏ó‡∏¥‡πâ‡∏á
        - Independence: ‡πÄ‡∏ä‡πá‡∏Ñ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏ï‡∏≠‡∏ö‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á)
        - Coverage: ‡πÄ‡∏ä‡πá‡∏Ñ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡∏ß‡∏á‡∏à‡∏£ PDCA
        - Traceability: ‡πÄ‡∏ä‡πá‡∏Ñ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ç‡∏≠‡∏á Metadata (‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå/‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤)
        """
        if not matched_chunks:
            return {
                "level": "NONE", "reason": "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö",
                "source_count": 0, "coverage_ratio": 0, "traceability_score": 0
            }

        # 0. üõ°Ô∏è Quality Filter (‡πÉ‡∏ä‡πâ Threshold 0.40 ‡∏ï‡∏≤‡∏°‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô SE-AM)
        valid_chunks = []
        for doc in matched_chunks:
            # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á Dict ‡πÅ‡∏•‡∏∞ Langchain Document
            meta = doc.metadata if hasattr(doc, 'metadata') else doc.get('metadata', {})
            score = meta.get('rerank_score') or meta.get('score') or 1.0
            
            if score >= 0.40:
                valid_chunks.append(doc)
        
        if not valid_chunks:
            return {
                "level": "LOW", "reason": "‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ï‡πà‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á (Relevance) ‡∏ï‡πà‡∏≥‡πÄ‡∏Å‡∏¥‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠‡πÑ‡∏î‡πâ",
                "source_count": 0, "coverage_ratio": 0, "traceability_score": 0
            }

        # 1. üìÇ Source Independence (‡∏ô‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô)
        unique_sources = set()
        for doc in valid_chunks:
            meta = doc.metadata if hasattr(doc, 'metadata') else doc.get('metadata', {})
            # ‡πÉ‡∏ä‡πâ Priority ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô _normalize_meta
            src = (meta.get('source_filename') or meta.get('filename') or 
                   meta.get('file_name') or meta.get('source'))
            if src:
                unique_sources.add(os.path.basename(str(src)))
        
        independence_score = len(unique_sources)
        
        # 2. üß© PDCA Coverage (‡πÄ‡∏ä‡πá‡∏Ñ‡∏°‡∏¥‡∏ï‡∏¥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô)
        pdca_map = {"P": False, "D": False, "C": False, "A": False}
        for doc in valid_chunks:
            meta = doc.metadata if hasattr(doc, 'metadata') else doc.get('metadata', {})
            tag = str(meta.get('pdca_tag', '')).upper()
            if tag in pdca_map:
                pdca_map[tag] = True
        
        found_tags_count = sum(pdca_map.values())
        coverage_ratio = found_tags_count / 4
        
        # 3. üîç Traceability (‡∏Ñ‡∏ß‡∏≤‡∏°‡πÇ‡∏õ‡∏£‡πà‡∏á‡πÉ‡∏™‡∏Ç‡∏≠‡∏á‡πÅ‡∏´‡∏•‡πà‡∏á‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á)
        traceable_count = 0
        for doc in valid_chunks:
            meta = doc.metadata if hasattr(doc, 'metadata') else doc.get('metadata', {})
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏´‡∏ô‡πâ‡∏≤ 0 ‡∏´‡∏≤‡∏¢)
            has_page = any([
                meta.get('page_label') is not None,
                meta.get('page') is not None,
                meta.get('page_number') is not None
            ])
            has_file = any([meta.get('source_filename'), meta.get('filename'), meta.get('file_name')])
            
            if has_page and has_file:
                traceable_count += 1
        
        traceability_score = traceable_count / len(valid_chunks) if valid_chunks else 0

        # 4. ‚öñÔ∏è Decision Matrix (Gated Audit Logic)
        if coverage_ratio <= 0.25 or independence_score < 1:
            confidence = "LOW"
            desc = "‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏±‡πà‡∏ô‡∏ï‡πà‡∏≥: ‡∏Ç‡∏≤‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏´‡∏£‡∏∑‡∏≠‡∏°‡∏µ‡∏°‡∏¥‡∏ï‡∏¥ PDCA ‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠"
        elif independence_score < 3 or coverage_ratio < 0.75:
            confidence = "MEDIUM"
            desc = "‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏±‡πà‡∏ô‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á: ‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏™‡∏ô‡∏±‡∏ö‡∏™‡∏ô‡∏∏‡∏ô‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏´‡∏•‡πà‡∏á ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡∏Ç‡∏≤‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡πÉ‡∏ô‡∏°‡∏¥‡∏ï‡∏¥ PDCA"
        else:
            confidence = "HIGH"
            desc = "‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏±‡πà‡∏ô‡∏™‡∏π‡∏á: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏£‡∏ö‡∏ß‡∏á‡∏à‡∏£ PDCA ‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏à‡∏≤‡∏Å‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢ (Cross-Check)"

        # üö® Penalty: ‡∏•‡∏î‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏´‡∏≤‡∏Å‡∏£‡∏∞‡∏ö‡∏∏‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á (‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤) ‡πÑ‡∏°‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
        if traceability_score < 0.50 and confidence != "LOW":
            confidence = "MEDIUM" if confidence == "HIGH" else "LOW"
            desc += " (‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Å‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ï‡πà‡∏≥)"

        return {
            "level": confidence, 
            "reason": desc, 
            "source_count": independence_score,
            "coverage_ratio": coverage_ratio,
            "traceability_score": round(traceability_score, 2),
            "pdca_found": [k for k, v in pdca_map.items() if v]
        }

    def _calculate_evidence_strength_cap(
        self,
        top_evidences: List[Union[Dict[str, Any], Any]],
        level: int,
        highest_rerank_score: Optional[float] = None
    ) -> Dict[str, Any]:
        """
        [ULTIMATE REVISE v2026.3] Relevant Score Gate
        --------------------------------------------
        - Fixes: UnboundLocalError & Metadata Extraction
        - Scoring: Metadata (P1) > Adaptive Loop (P2) > Regex Tail (P3)
        - Logic: ‡∏´‡∏≤‡∏Å‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î < Threshold ‡∏à‡∏∞‡∏ó‡∏≥‡∏Å‡∏≤‡∏£ Cap ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô '‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à' ‡∏Ç‡∏≠‡∏á AI
        """
        import re
        score_keys = [
            "rerank_score", "score", "relevance_score",
            "_rerank_score_force", "_rerank_score",
            "Score", "RelevanceScore"
        ]

        # 1. ‚öôÔ∏è Configuration (‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å Class ‡∏´‡∏£‡∏∑‡∏≠ Global Vars)
        threshold = getattr(self, "RERANK_THRESHOLD", 0.35) 
        cap_value = getattr(self, "MAX_EVI_STR_CAP", 5.0)

        # 2. üìç Baseline Score (‡∏à‡∏≤‡∏Å Adaptive RAG Loop ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ß‡∏ô‡∏´‡∏≤‡∏°‡∏≤)
        max_score_found = highest_rerank_score if highest_rerank_score is not None else 0.0
        max_score_source = "Adaptive_RAG_Loop" if highest_rerank_score is not None else "N/A"

        for idx, doc in enumerate(top_evidences, 1):
            current_doc_source = "Unknown_Source"
            current_score = 0.0
            page_content = ""
            metadata = {}

            # 3. üì¶ Object Extraction (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á Dict ‡πÅ‡∏•‡∏∞ Langchain Document)
            if isinstance(doc, dict):
                metadata = doc.get("metadata", {}) or {}
                page_content = doc.get("page_content", "") or doc.get("text", "") or doc.get("content", "")
                current_doc_source = (
                    metadata.get("file_name") or metadata.get("source_filename") or 
                    metadata.get("filename") or metadata.get("source") or 
                    doc.get("source") or f"Doc_ID_{idx}"
                )
            else:
                metadata = getattr(doc, "metadata", {}) or {}
                page_content = getattr(doc, "page_content", "") or getattr(doc, "text", "")
                current_doc_source = (
                    metadata.get("file_name") or metadata.get("source_filename") or 
                    getattr(doc, "source", "Unknown_Document")
                )

            # 4. üîç Priority 1: ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏à‡∏≤‡∏Å Metadata/Top-level
            for key in score_keys:
                score_val = metadata.get(key)
                if score_val is None and isinstance(doc, dict):
                    score_val = doc.get(key)

                if score_val is not None:
                    try:
                        temp_score = float(score_val)
                        if 0.0 < temp_score <= 1.0:
                            current_score = temp_score
                            break
                    except (ValueError, TypeError): continue

            # 5. üìë Priority 2: Regex Fallback (‡∏Å‡∏£‡∏ì‡∏µ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ñ‡∏π‡∏Å‡∏ù‡∏±‡∏á‡πÉ‡∏ô Text)
            if current_score <= 0.0 and page_content and isinstance(page_content, str):
                tail = page_content[-1200:].replace('\n', ' ')
                patterns = [
                    r"Relevance[ :]+([0-9]*\.?[0-9]+)",
                    r"Score[ :]+([0-9]*\.?[0-9]+)",
                    r"\[Score: ([0-9]*\.?[0-9]+)\]",
                    r"rerank_score['\"]?[\s:=]+([0-9]*\.?[0-9]+)",
                    r"\|\s*([0-9]\.[0-9]+)\s*\|" # ‡∏î‡∏±‡∏Å‡∏à‡∏±‡∏ö Markdown Table Score
                ]
                for pat in patterns:
                    m = re.search(pat, tail, re.IGNORECASE)
                    if m:
                        try:
                            ts = float(m.group(1))
                            if 0.0 < ts <= 1.0:
                                current_score = ts
                                break
                        except: continue

            # 6. üõ°Ô∏è Clamp & Protection
            if current_score > 1.0:
                self.logger.warning(f"üö® Score Clamp L{level}: {current_score} > 1.0 ‡∏à‡∏≤‡∏Å '{current_doc_source}' (Scaled to 0.0)")
                current_score = 0.0

            # 7. üèÜ ‡∏Ñ‡∏±‡∏î‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
            if current_score > max_score_found:
                max_score_found = current_score
                max_score_source = current_doc_source

        # 8. ‚öñÔ∏è Decision Gate (Capping Logic)
        # ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏¢‡∏±‡∏á‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏Å‡∏ì‡∏ë‡πå (Threshold) -> AI ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏™‡∏±‡πà‡∏á‡πÉ‡∏´‡πâ "‡∏£‡∏∞‡∏°‡∏±‡∏î‡∏£‡∏∞‡∏ß‡∏±‡∏á" ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏ú‡πà‡∏≤‡∏ô
        is_capped = max_score_found < threshold
        max_evi_str_for_prompt = cap_value if is_capped else 10.0

        status_icon = "üö®" if is_capped else "‚úÖ"
        self.logger.info(
            f"{status_icon} Evi Str {'CAPPED' if is_capped else 'FULL'} L{level}: "
            f"Best {max_score_found:.4f} from '{max_score_source}' (Threshold: {threshold})"
        )

        return {
            "is_capped": is_capped,
            "max_evi_str_for_prompt": max_evi_str_for_prompt,
            "highest_rerank_score": round(float(max_score_found), 4),
            "max_score_source": str(max_score_source),
        }

    def _robust_hydrate_documents_for_priority_chunks(
        self,
        chunks_to_hydrate: List[Dict],
        vsm: Optional['VectorStoreManager'],
        current_sub_id: Optional[str] = None,
        level: Optional[int] = None
    ) -> List[Dict]:
        """
        [ULTIMATE HYDRATION v2026.3]
        - ‡∏î‡∏∂‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏ï‡πá‡∏° (Full Text) ‡∏Ç‡∏≠‡∏á Priority Chunks ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ AI ‡πÄ‡∏´‡πá‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        - ‡∏£‡∏∞‡∏ö‡∏ö Fallback Scoring (1.0 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à, 0.85 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß)
        - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏∞‡∏ö‡∏ö Deduplication ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô‡πÉ‡∏ô Prompt
        """
        active_sub_id = current_sub_id or getattr(self, 'sub_id', 'unknown')
        if not chunks_to_hydrate:
            self.logger.debug(f"‚ÑπÔ∏è [HYDRATION] No chunks to hydrate for L{level} {active_sub_id}")
            return []

        TAG_ABBREV = {
            "PLAN": "P", "DO": "D", "CHECK": "C", "ACT": "A",
            "P": "P", "D": "D", "C": "C", "A": "A"
        }

        # 1. üè∑Ô∏è Helper: ‡∏à‡∏±‡∏î‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà PDCA ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ñ‡∏π‡∏Å‡πÄ‡∏ï‡∏¥‡∏°
        def _safe_classify(text: str) -> str:
            try:
                raw = classify_by_keyword(
                    text=text, sub_id=active_sub_id, level=level,
                    contextual_rules_map=self.contextual_rules_map
                )
                if not raw: return "Other"
                return TAG_ABBREV.get(str(raw).upper(), "Other")
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è PDCA classify failed in hydration: {e}")
                return "Other"

        # 2. üìè Helper: ‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡πÉ‡∏´‡πâ Chunk
        def _standardize_chunk(chunk: Dict, score: float):
            chunk.setdefault("is_baseline", True)
            text = chunk.get("text", "").strip()
            if text:
                chunk["pdca_tag"] = _safe_classify(text)
                # Boost ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÉ‡∏´‡πâ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Å‡∏•‡∏∏‡πà‡∏° Priority ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ AI ‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
                chunk["rerank_score"] = max(chunk.get("rerank_score", 0.0), score)
                chunk["score"] = max(chunk.get("score", 0.0), score)
            return chunk

        # 3. üîë Extract IDs ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Database
        stable_ids = {
            sid for c in chunks_to_hydrate
            if (sid := (c.get("stable_doc_uuid") or c.get("doc_id") or c.get("chunk_uuid")))
        }

        if not stable_ids or not vsm:
            self.logger.warning(f"‚ö†Ô∏è [HYDRATION] No IDs or VSM available ‚Üí Using partial content")
            boosted = [_standardize_chunk(c.copy(), 0.9) for c in chunks_to_hydrate]
            return self._guarantee_text_key(boosted)

        # 4. üõ∞Ô∏è Fetch Full Documents from VSM
        stable_id_map = defaultdict(list)
        try:
            retrieved_docs = vsm.get_documents_by_id(
                list(stable_ids), doc_type=self.doc_type, enabler=self.config.enabler
            )
            self.logger.info(f"üõ∞Ô∏è [HYDRATION] Retrieved {len(retrieved_docs)} full docs from VSM")

            for doc in retrieved_docs:
                sid = doc.metadata.get("stable_doc_uuid") or doc.metadata.get("doc_id")
                if sid:
                    stable_id_map[sid].append({"text": doc.page_content, "metadata": doc.metadata})
        except Exception as e:
            self.logger.error(f"‚ùå [HYDRATION] VSM Fetch Error: {e}")
            fallback = [_standardize_chunk(c.copy(), 0.9) for c in chunks_to_hydrate]
            return self._guarantee_text_key(fallback)

        # 5. üíß Hydrate & Deduplicate
        hydrated_priority_docs = []
        restored_count = 0
        seen_signatures = set()
        SAFE_META_KEYS = {"source", "file_name", "page", "page_label", "page_number", 
                          "enabler", "tenant", "year", "sub_topic"}

        for chunk in chunks_to_hydrate:
            new_chunk = chunk.copy()
            sid = new_chunk.get("stable_doc_uuid") or new_chunk.get("doc_id")

            hydrated = False
            if sid and sid in stable_id_map:
                # ‡πÉ‡∏ä‡πâ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ï‡∏±‡∏ß‡πÄ‡∏ï‡πá‡∏°‡∏à‡∏≤‡∏Å VSM ‡πÅ‡∏ó‡∏ô Snippet ‡πÄ‡∏î‡∏¥‡∏°
                best_match = stable_id_map[sid][0]
                new_chunk["text"] = best_match["text"]
                # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï Metadata ‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
                meta = best_match.get("metadata", {})
                new_chunk.update({k: v for k, v in meta.items() if k in SAFE_META_KEYS})
                hydrated = True
                restored_count += 1

            new_chunk = _standardize_chunk(new_chunk, score=1.0 if hydrated else 0.85)

            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô (Dedup)
            signature = (sid, new_chunk.get("chunk_uuid"), new_chunk.get("text", "")[:200])
            if signature in seen_signatures:
                continue
            seen_signatures.add(signature)
            hydrated_priority_docs.append(new_chunk)

        self.logger.info(
            f"‚úÖ [HYDRATION SUMMARY] Restored {restored_count}/{len(chunks_to_hydrate)} chunks | "
            f"Unique: {len(hydrated_priority_docs)}"
        )

        return self._guarantee_text_key(hydrated_priority_docs)

    def _guarantee_text_key(
        self,
        chunks: List[Dict],
        total_count: int = 0,
        restored_count: int = 0
    ) -> List[Dict]:
        """
        Guarantee 'text' key exists in every chunk
        """
        final_chunks = []

        for chunk in chunks:
            if "text" not in chunk or not isinstance(chunk["text"], str):
                chunk["text"] = ""
                cid = str(chunk.get("chunk_uuid", "N/A"))
                self.logger.debug(f"Guaranteed 'text' key for chunk (ID: {cid[:8]})")
            final_chunks.append(chunk)

        if total_count > 0:
            baseline_count = sum(1 for c in final_chunks if c.get("is_baseline"))
            self.logger.info(
                f"HYDRATION FINAL: Restored {restored_count}/{total_count} "
                f"(Baseline={baseline_count}, Total final={len(final_chunks)})"
            )

        return final_chunks


    def _normalize_meta(self, c: Dict) -> Tuple[str, str]:
        """
        [REVISED] ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡∏∞‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏£‡∏∞‡∏ö‡∏ö Priority Fallback
        - ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤ 0 ‡∏´‡∏≤‡∏¢ (Index-based)
        - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏à‡∏≤‡∏Å Ingest ‡πÉ‡∏´‡∏°‡πà ‡πÅ‡∏•‡∏∞ Reranker Output
        - ‡πÄ‡∏ô‡πâ‡∏ô page_label ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏ö‡∏ô UI
        """
        if not isinstance(c, dict):
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡πá‡∏ô LangChain Document ‡πÉ‡∏´‡πâ‡∏î‡∏∂‡∏á metadata ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤
            if hasattr(c, "metadata"):
                meta = c.metadata
                # ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á dict ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÇ‡∏Ñ‡πâ‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏•‡πà‡∏≤‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏î‡πâ
                c = {"metadata": meta}
            else:
                return "Unknown Source", "N/A"

        # ‡∏î‡∏∂‡∏á metadata ‡∏°‡∏≤‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÑ‡∏ß‡πâ (‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏Å‡∏£‡∏ì‡∏µ None ‡∏´‡∏£‡∏∑‡∏≠ Dict)
        meta = c.get("metadata") or {}

        # 1. üîç ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå (Source Name Priority)
        source_priority = [
            c.get("source_filename"),
            meta.get("source_filename"),
            c.get("filename"),
            meta.get("source"),
            meta.get("file_name"),
            c.get("id") 
        ]
        
        source = "Unknown"
        for s in source_priority:
            if s and str(s).strip():
                source = str(s).strip()
                break

        # 2. üîç ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤ (Page Label Priority)
        page_keys = ["page_label", "page", "page_number"]
        found_page = None
        for key in page_keys:
            val_root = c.get(key)
            if val_root is not None and str(val_root).strip().lower() != "n/a":
                found_page = val_root
                break
            val_meta = meta.get(key)
            if val_meta is not None and str(val_meta).strip().lower() != "n/a":
                found_page = val_meta
                break

        # 3. ‚ú® Final Cleaning & UI Formatting
        clean_source = os.path.basename(source) if "/" in source or "\\" in source else source
        if found_page is not None:
            page_str = str(found_page).strip()
            clean_page = page_str if page_str.lower() != "n/a" else "N/A"
        else:
            clean_page = "N/A"

        return clean_source, clean_page
    
    def _get_pdca_blocks_from_evidences(
        self,
        evidences: List[Dict],
        baseline_evidences: Dict[str, List[Dict]],
        level: int,
        sub_id: str,
        contextual_rules_map: Dict[str, Any],
        record_id: str = None
    ) -> Tuple[str, str, str, str, str]:

        # 1. üßπ Data Integration (‡∏Ñ‡∏á‡πÄ‡∏î‡∏¥‡∏°)
        all_chunks = []
        for c in (evidences or []):
            if isinstance(c, dict) and c.get("text", "").strip():
                chunk = c.copy()
                chunk["is_baseline"] = False
                all_chunks.append(chunk)

        # ‡∏õ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á Baseline ‡πÉ‡∏´‡πâ‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á Key ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô Level ‡∏ï‡∏£‡∏á‡πÜ ‡∏´‡∏£‡∏∑‡∏≠ ID ‡πÄ‡∏ï‡πá‡∏°)
        target_baseline = baseline_evidences.get(str(level)) or baseline_evidences.get(f"{sub_id}.L{level}") or []
        for b in target_baseline:
            if isinstance(b, dict) and b.get("text", "").strip():
                b_copy = b.copy()
                b_copy["is_baseline"] = True
                all_chunks.append(b_copy)

        if not all_chunks:
            return "", "", "", "", ""

        # 2. üè∑Ô∏è Classification & Smart Sorting
        pdca_groups = defaultdict(list)
        for chunk in all_chunks:
            tag = classify_by_keyword(
                text=chunk["text"], sub_id=sub_id, level=level,
                contextual_rules_map=contextual_rules_map,
                chunk_metadata=chunk.get('metadata') # ‡∏™‡πà‡∏á Meta ‡πÑ‡∏õ‡∏ä‡πà‡∏ß‡∏¢‡∏à‡∏±‡∏î‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó
            )
            final_tag = tag if tag in {"P", "D", "C", "A"} else "Other"
            
            # L1 Logic: ‡πÄ‡∏ô‡πâ‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
            if level == 1 and final_tag == "Other":
                final_tag = "P"
            
            # [NEW] ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Priority Score ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Sorting
            # ‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Å‡∏±‡∏ö: 1. ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô Relevancy ‡πÉ‡∏´‡∏°‡πà  2. ‡πÑ‡∏ü‡∏•‡πå‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á
            rel_score = float(chunk.get("relevance_score_custom") or 0.0)
            rerank_score = float(chunk.get("rerank_score") or chunk.get("score") or 0.0)
            chunk["priority_score"] = (rel_score * 0.7) + (rerank_score * 0.3)

            label = {"P":"Plan","D":"Do","C":"Check","A":"Act"}.get(final_tag, "Other")
            pdca_groups[label].append(chunk)

        # 3. üé≠ Diverse Block Generator (Enhanced)
        def _create_block(tag: str, chunks: List[Dict]) -> str:
            if not chunks: return ""
            
            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏° priority_score ‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏°‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á Source Grading ‡πÑ‡∏ß‡πâ‡πÅ‡∏•‡πâ‡∏ß
            sorted_chunks = sorted(chunks, key=lambda x: x.get("priority_score", 0), reverse=True)
            
            diverse_list = []
            file_counts = {}
            for doc in sorted_chunks:
                # ‡∏™‡∏Å‡∏±‡∏î‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥ Diversity Check
                meta = doc.get('metadata') or {}
                raw_source = doc.get("source_filename") or meta.get("source_filename") or meta.get("source") or "Unknown"
                fname = os.path.basename(str(raw_source))
                
                # ‡∏à‡∏≥‡∏Å‡∏±‡∏î Chunks ‡∏ï‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ AI ‡πÄ‡∏´‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏´‡∏•‡πà‡∏á
                if file_counts.get(fname, 0) < MAX_CHUNKS_PER_FILE:
                    diverse_list.append(doc)
                    file_counts[fname] = file_counts.get(fname, 0) + 1
                
                if len(diverse_list) >= MAX_CHUNKS_PER_BLOCK: # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ Block ‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ (LLM Context Optimization)
                    break

            # Build Formatted String
            parts = []
            for i, c in enumerate(diverse_list, start=1):
                meta = c.get('metadata') or {}
                fname = os.path.basename(str(c.get("source_filename") or meta.get("source") or "Unknown"))
                pnum = c.get("page_label") or meta.get("page_label") or "N/A"
                
                baseline_tag = " [üìú ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏£‡∏∞‡∏î‡∏±‡∏ö‡πÄ‡∏î‡∏¥‡∏°]" if c.get("is_baseline") else ""
                parts.append(
                    f"### [{tag} Evidence {i}/{len(diverse_list)}]{baseline_tag}\n"
                    f"{c['text'].strip()}\n"
                    f"[‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á: {fname}, ‡∏´‡∏ô‡πâ‡∏≤: {pnum}, Relevancy: {c.get('priority_score', 0):.4f}]"
                )
            
            return "\n\n---\n\n".join(parts)

        # 4. Construct Final Outputs
        p_text = _create_block("Plan",  pdca_groups["Plan"])
        d_text = _create_block("Do",    pdca_groups["Do"])
        c_text = _create_block("Check", pdca_groups["Check"])
        a_text = _create_block("Act",   pdca_groups["Act"])
        o_text = _create_block("Other", pdca_groups["Other"])

        # Logging ‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞
        status = " | ".join([f"{k}:{'‚úÖ' if v else '‚ùå'}" for k, v in 
                            {"P":p_text, "D":d_text, "C":c_text, "A":a_text}.items()])
        self.logger.info(f"üìä [PDCA Blocks Ready] {sub_id} L{level} -> {status}")

        return p_text, d_text, c_text, a_text, o_text
    
    # ----------------------------------------------------------------------
    # üöÄ CORE WORKER: Assessment Execution (REVISED v2026.3)
    # ----------------------------------------------------------------------
    def _run_sub_criteria_assessment_worker(
        self,
        sub_criteria: Dict[str, Any],
        vectorstore_manager: Optional['VectorStoreManager'] = None
    ) -> Tuple[Dict[str, Any], Dict[str, List[Dict[str, Any]]]]:
        """
        [ADVANCED AUDITOR MODE v2026.5]
        - ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏£‡∏ö L1-L5 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥ Gap Analysis ‡πÅ‡∏•‡∏∞‡πÄ‡∏Å‡πá‡∏ö Evidence Roadmap
        - Sequential Integrity: ‡∏Å‡∏£‡∏≠‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏Ç‡∏≠‡∏á SE-AM
        - Gap Type Classification: ‡πÅ‡∏¢‡∏Å‡πÅ‡∏¢‡∏∞‡∏à‡∏∏‡∏î‡∏ï‡∏Å‡∏à‡∏£‡∏¥‡∏á (Primary) ‡∏Å‡∏±‡∏ö‡∏à‡∏∏‡∏î‡∏ï‡∏¥‡∏î Sequential (Gap)
        """

        # 1. INITIALIZATION
        MAX_RETRY_ATTEMPTS = 2
        sub_id = sub_criteria['sub_id']
        sub_criteria_name = sub_criteria['sub_criteria_name']
        sub_weight = sub_criteria.get('weight', 0)
        
        current_enabler = getattr(self.config, 'enabler', 'KM')
        vsm = vectorstore_manager or getattr(self, 'vectorstore_manager', None)
        
        current_sequential_pass_level = 0 
        first_failure_level = None # ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ä‡∏µ‡πâ‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà "‡∏ï‡∏Å‡∏à‡∏£‡∏¥‡∏á" ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å
        raw_results_for_sub_seq: List[Dict[str, Any]] = []
        start_ts = time.time() 

        self.logger.info(f"üßµ [WORKER START] Full Gap-Analysis Mode | {sub_id}")
        
        # ‡∏î‡∏∂‡∏á‡∏Å‡∏é‡∏ó‡∏µ‡πà‡∏â‡∏µ‡∏î‡πÑ‡∏ß‡πâ‡πÉ‡∏ô Class
        all_rules_for_sub = getattr(self, 'contextual_rules_map', {}).get(sub_id, {})

        # -----------------------------------------------------------
        # 2. EVALUATION LOOP (L1 ‚Üí Target Level)
        # -----------------------------------------------------------
        for statement_data in sub_criteria.get('levels', []):
            level = statement_data.get('level')
            if level is None or level > self.config.target_level:
                continue
            
            # --- [PREPARATION] ---
            level_key = f"L{level}"
            specific_rules = all_rules_for_sub.get(level_key, {})
            
            # ‡∏â‡∏µ‡∏î Keywords ‡πÅ‡∏•‡∏∞ Rules
            extra_keys = set(specific_rules.get('plan_keywords', []) + specific_rules.get('do_keywords', []))
            focus_hint = specific_rules.get('specific_contextual_rule', "")
            
            enhanced_statement = statement_data.get('statement', '')
            if extra_keys:
                enhanced_statement += f" (Keywords: {', '.join(extra_keys)})"
            
            # üéØ [BASELINE SYNC]
            past_summaries = [f"L{p['level']}: {'PASS' if p.get('raw_is_passed') else 'FAIL'} - {p.get('reason', '')[:100]}..." for p in raw_results_for_sub_seq]
            baseline_context = "\n".join(past_summaries) if past_summaries else "‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤"

            # ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô
            def assessment_fn(attempt):
                return self._run_single_assessment(
                    sub_criteria=sub_criteria,
                    statement_data={**statement_data, 'statement': enhanced_statement, 'focus_hint': focus_hint},
                    vectorstore_manager=vsm,
                    attempt=attempt,
                    doc_type=self.doc_type,      
                    top_k=INITIAL_TOP_K,          
                    baseline_summary=baseline_context,
                    **specific_rules 
                )

            # --- [EXECUTION] ---
            level_result = {}
            for attempt_num in range(1, MAX_RETRY_ATTEMPTS + 1):
                level_result = assessment_fn(attempt_num)
                if level_result.get('is_passed', False): 
                    break

            # --- [SEQUENTIAL & GAP LOGIC] ---
            # ‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡πà‡∏≤‡∏ú‡∏•‡∏ï‡∏£‡∏ß‡∏à‡∏î‡∏¥‡∏ö‡∏à‡∏≤‡∏Å LLM ‡πÑ‡∏ß‡πâ‡∏Å‡πà‡∏≠‡∏ô
            is_passed_llm = level_result.get('is_passed', False)
            level_result['raw_is_passed'] = is_passed_llm # ‡∏à‡∏≥‡πÑ‡∏ß‡πâ‡∏ß‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á‡πÜ ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ú‡πà‡∏≤‡∏ô‡πÑ‡∏´‡∏°

            if not is_passed_llm and first_failure_level is None:
                # ‡∏à‡∏∏‡∏î‡∏ï‡∏Å‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏∏‡∏î‡πÅ‡∏£‡∏Å (Primary Gap)
                first_failure_level = level
                level_result["display_status"] = "FAILED"
                level_result["gap_type"] = "PRIMARY_GAP"
            
            elif is_passed_llm and first_failure_level is not None:
                # ‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÅ‡∏ï‡πà "‡∏ï‡∏¥‡∏î Sequential" (Capped)
                level_result["display_status"] = "PASSED (CAPPED)"
                level_result["gap_type"] = "SEQUENTIAL_GAP"
                level_result["is_passed"] = False # ‡πÉ‡∏ô‡∏ó‡∏≤‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏ô‡∏±‡∏ö‡∏ú‡πà‡∏≤‡∏ô
            
            elif not is_passed_llm and first_failure_level is not None:
                # ‡∏ï‡∏Å‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô
                level_result["display_status"] = "FAILED (GAP)"
                level_result["gap_type"] = "COMPOUND_GAP"
            else:
                # ‡∏ú‡πà‡∏≤‡∏ô‡πÅ‡∏ö‡∏ö‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏õ‡∏Å‡∏ï‡∏¥
                current_sequential_pass_level = level
                level_result["display_status"] = "PASSED"
                level_result["gap_type"] = "NONE"

            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô (Evidence) ‡∏ó‡∏∏‡∏Å‡∏•‡∏≥‡∏î‡∏±‡∏ö
            highest_rerank = level_result.get('max_relevant_score', 0.0)
            self._save_level_evidences_and_calculate_strength(
                level_temp_map=level_result.get("temp_map_for_level", []),
                sub_id=sub_id, level=level, llm_result=level_result, highest_rerank_score=highest_rerank 
            )

            raw_results_for_sub_seq.append(level_result)
            self.logger.info(f"‚úÖ L{level} Done | Status: {level_result['display_status']} | Gap: {level_result['gap_type']}")

        # -----------------------------------------------------------
        # 3. FINAL SYNTHESIS
        # -----------------------------------------------------------
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Action Plan ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Gap Type ‡πÑ‡∏õ‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥
        action_plan_result = self._generate_action_plan_safe(
            sub_id, sub_criteria_name, current_enabler, raw_results_for_sub_seq
        )

        # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö UI
        for r in raw_results_for_sub_seq:
            if "temp_map_for_level" in r:
                r["temp_map_for_level"] = self._resolve_evidence_filenames(r["temp_map_for_level"])

        # ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ñ‡πà‡∏ß‡∏á‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏ï‡∏≤‡∏° Level ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏£‡∏¥‡∏á
        weighted_score = round(self._calculate_weighted_score(current_sequential_pass_level, sub_weight), 2)
        final_temp_map = {k: v for k, v in self.evidence_map.items() if k.startswith(f"{sub_id}.")}

        return {
            "sub_criteria_id": sub_id,
            "sub_criteria_name": sub_criteria_name,
            "highest_full_level": current_sequential_pass_level,
            "weight": sub_weight,
            "weighted_score": weighted_score,
            "display_status": "PASSED" if current_sequential_pass_level >= self.config.target_level else "FAILED",
            "action_plan": action_plan_result, 
            "raw_results_ref": raw_results_for_sub_seq,
            "worker_duration_s": round(time.time() - start_ts, 2)
        }, final_temp_map

    def _generate_action_plan_safe(
        self, 
        sub_id: str, 
        name: str, 
        enabler: str, 
        results: List[Dict]
    ) -> Any:
        """
        Wrapper ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏£‡∏∞‡∏ö‡∏ö Action Plan ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
        """
        try:
            # 1. ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å (Import ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î Circular Dependency)
            # 2. ‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ (FAILED, WEAK_EVIDENCE, PDCA_INCOMPLETE)
            # ‡∏Å‡∏é: ‡∏ñ‡πâ‡∏≤‡∏ú‡πà‡∏≤‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö target ‡πÅ‡∏•‡πâ‡∏ß ‡πÅ‡∏•‡∏∞ strength > 3.0 ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Action Plan
            to_recommend = []
            for r in results:
                is_passed = r.get('is_passed', False)
                strength = r.get('evidence_strength', 10.0)
                
                if not is_passed or strength < 3.0:
                    to_recommend.append(r)

            if not to_recommend:
                return {"status": "EXCELLENT", "message": "‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡πÅ‡∏ú‡∏ô‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏î‡∏µ‡∏°‡∏≤‡∏Å"}

            # 3. ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å (‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏™‡πà‡∏á‡∏°‡∏≤‡πÉ‡∏ô‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°)
            return create_structured_action_plan(
                recommendation_statements=to_recommend,
                sub_id=sub_id,
                sub_criteria_name=name,
                enabler=enabler,
                target_level=self.config.target_level,
                llm_executor=self.llm,  # ‡∏™‡πà‡∏á‡∏ï‡∏±‡∏ß LLM ‡πÑ‡∏õ‡∏£‡∏±‡∏ô Prompt
                logger=self.logger
            )

        except Exception as e:
            self.logger.error(f"‚ö†Ô∏è Action Plan Generation Failed: {str(e)}")
            return {
                "status": "ERROR",
                "message": "‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥",
                "error_detail": str(e)
            }
    
    def _prepare_worker_tuple(self, sub_data: Dict, document_map: Optional[Dict]) -> Tuple:
        return (
            sub_data,                          # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏ì‡∏ë‡πå (1.1, 1.2...)
            self.config.enabler,               # KM / IT / ...
            self.config.target_level,          # ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ L5
            self.config.mock_mode,             # none/random
            self.evidence_map_path,            # ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Å‡πá‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô
            self.config.model_name,            # llama3.1:70b ‡∏´‡∏£‡∏∑‡∏≠ 8b
            self.config.temperature,           # 0.0
            getattr(self.config, 'min_retry_score', 0.65),
            getattr(self.config, 'max_retrieval_attempts', 3),
            document_map or self.document_map, # ‡πÅ‡∏ú‡∏ô‡∏ú‡∏±‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
            getattr(self, 'ActionPlanActions', None),
            self.config.year,                  # 2567
            self.config.tenant                 # pea
        )

    def _integrate_worker_results(self, sub_result: Dict, temp_map: Dict):
        # 1. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÅ‡∏ú‡∏ô‡∏ú‡∏±‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô (Evidence Map)
        if isinstance(temp_map, dict):
            for level_key, evidence_list in temp_map.items():
                # ‡πÅ‡∏õ‡∏•‡∏á UUID ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏£‡∏¥‡∏á ‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Metadata
                resolved_list = self._resolve_evidence_filenames(evidence_list)
                self._normalize_evidence_metadata(resolved_list)
                
                # ‡∏¢‡∏±‡∏î‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏£‡∏∞‡πÄ‡∏õ‡πã‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á Main Process
                if level_key not in self.evidence_map:
                    self.evidence_map[level_key] = []
                self.evidence_map[level_key].extend(resolved_list)
        
        # 2. ‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏î‡∏¥‡∏ö‡∏à‡∏≤‡∏Å LLM
        if sub_result:
            self.raw_llm_results.extend(sub_result.get("raw_results_ref", []))
            self.final_subcriteria_results.append(sub_result)

    def run_assessment(
        self,
        target_sub_id: str = "all",
        export: bool = False,
        vectorstore_manager: Optional['VectorStoreManager'] = None,
        sequential: bool = False,
        document_map: Optional[Dict[str, str]] = None,
        record_id: str = None,
    ) -> Dict[str, Any]:
        """
        [ULTIMATE ASSEMBLY v2026.3] ‚Äî Server L40 & Mac Optimized
        ------------------------------------------------------
        ‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏° (Orchestrator) ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô
        """
        start_ts = time.time()
        self.current_record_id = record_id 

        # 1. üìã ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå (Rubric Filtering)
        all_statements = self._flatten_rubric_to_statements()
        is_all = str(target_sub_id).lower() == "all"
        sub_criteria_list = all_statements if is_all else [
            s for s in all_statements if str(s.get('sub_id')).lower() == str(target_sub_id).lower()
        ]

        if not sub_criteria_list:
            return self._create_failed_result(record_id, f"Criteria '{target_sub_id}' not found", start_ts)

        # 2. ‚öôÔ∏è ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Parallel / Sequential (L40 vs Mac Detection)
        # ‡∏î‡∏∂‡∏á‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Worker ‡∏à‡∏≤‡∏Å .env (‡∏ö‡∏ô L40 ‡∏Ñ‡∏∑‡∏≠ 4, ‡∏ö‡∏ô Mac ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ 1 ‡∏ï‡∏≤‡∏° Log)
        env_workers = os.environ.get('NUM_WORKERS') or os.environ.get('MAX_PARALLEL_WORKERS')
        num_workers = 1 if (sequential or not is_all) else int(env_workers or 1)
        
        run_parallel = (num_workers > 1)

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Hardware ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏û‡πà‡∏ô Log ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
        import torch
        device_info = "NVIDIA CUDA (L40)" if torch.cuda.is_available() else "Apple Silicon (MPS)" if torch.backends.mps.is_available() else "CPU"

        self.logger.info(f"üéØ Target: {target_sub_id} | Mode: {'Parallel' if run_parallel else 'Sequential'}")
        self.logger.info(f"üöÄ Device Config: {device_info} | Active Workers: {num_workers}")

        self.raw_llm_results = []
        self.final_subcriteria_results = []
        self.evidence_map = {}

        # 3. üî• Execution Phase
        if run_parallel:
            # üìå ‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô‡∏Ç‡∏ô‡∏≤‡∏ô (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö L40)
            worker_args = [self._prepare_worker_tuple(s, document_map) for s in sub_criteria_list]
            try:
                # ‡πÉ‡∏ä‡πâ 'spawn' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡πâ‡∏≤‡∏á CUDA memory context ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á process ‡πÉ‡∏´‡∏°‡πà
                ctx = multiprocessing.get_context('spawn')
                with ctx.Pool(processes=num_workers) as pool:
                    results_list = pool.map(_static_worker_process, worker_args)
                
                for res in results_list:
                    if isinstance(res, tuple) and len(res) == 2:
                        sub_result, temp_map = res
                        self._merge_worker_results(sub_result, temp_map)
            except Exception as e:
                self.logger.critical(f"‚ùå Parallel execution failed: {e}")
                raise
        else:
            # üßµ ‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Mac/Specific Sub-ID)
            vsm = vectorstore_manager or self._init_local_vsm()
            for sub_criteria in sub_criteria_list:
                # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å Assessment Core (_run_single_assessment ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô)
                sub_result, final_temp_map = self._run_sub_criteria_assessment_worker(sub_criteria, vsm)
                self._merge_worker_results(sub_result, final_temp_map)

        # 4. üèÅ ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å (Calculate & Export)
        self._calculate_overall_stats(target_sub_id)
        
        final_results = {
            "record_id": record_id,
            "summary": self.total_stats,
            "sub_criteria_results": self.final_subcriteria_results,
            "run_time_seconds": round(time.time() - start_ts, 2),
            "timestamp": datetime.now().isoformat(),
            "device_used": device_info
        }

        if export:
            self._export_results(results=final_results, sub_criteria_id=target_sub_id, record_id=record_id)

        return final_results

    def _merge_worker_results(self, sub_result: Dict[str, Any], temp_map: Dict[str, List[Dict]]):
        """
        [INTERNAL HELPER] ‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å Worker ‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á Engine
        - ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£ Normalize Metadata ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡∏∞‡∏´‡∏ô‡πâ‡∏≤‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
        - ‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå LLM ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡πÇ‡∏¢‡∏á‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô (Evidence Map)
        """
        if not sub_result:
            return

        # 1. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Evidence Map (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏ô‡∏û‡∏ö)
        if temp_map and isinstance(temp_map, dict):
            for level_key, evidence_list in temp_map.items():
                if not evidence_list:
                    continue
                
                # Normalize Metadata ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ (‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå/‡∏´‡∏ô‡πâ‡∏≤) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö seam_prompts.py
                self._normalize_evidence_metadata(evidence_list)
                
                # ‡∏ô‡∏≥‡πÑ‡∏õ‡∏£‡∏ß‡∏°‡πÉ‡∏ô map ‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á Engine
                if level_key not in self.evidence_map:
                    self.evidence_map[level_key] = []
                self.evidence_map[level_key].extend(evidence_list)

        # 2. ‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏î‡∏¥‡∏ö‡∏à‡∏≤‡∏Å LLM (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Ref. ‡πÅ‡∏•‡∏∞ Debug)
        if "raw_results_ref" in sub_result:
            self.raw_llm_results.extend(sub_result["raw_results_ref"])

        # 3. ‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏£‡∏≤‡∏¢‡πÄ‡∏Å‡∏ì‡∏ë‡πå (Final Results per Sub-ID)
        self.final_subcriteria_results.append(sub_result)
        
        self.logger.debug(f"‚úÖ Merged results for {sub_result.get('sub_criteria_id', 'Unknown')}")
    
    # ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏™‡∏°‡∏∑‡∏≠‡∏ô "‡∏®‡∏≤‡∏•‡∏≠‡∏∏‡∏ó‡∏ò‡∏£‡∏ì‡πå" ‡∏´‡∏≤‡∏Å AI ‡∏ï‡∏£‡∏ß‡∏à‡∏£‡∏≠‡∏ö‡πÅ‡∏£‡∏Å‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏´‡πâ‡∏ï‡∏Å 
    # ‡πÅ‡∏ï‡πà‡∏£‡∏∞‡∏ö‡∏ö‡∏û‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á‡∏≠‡∏¢‡∏π‡πà ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏´‡πâ LLM ‡∏ï‡∏£‡∏ß‡∏à‡∏ã‡πâ‡∏≥‡∏î‡πâ‡∏ß‡∏¢‡∏°‡∏∏‡∏°‡∏°‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡∏Ç‡∏∂‡πâ‡∏ô (Diversity Focus)
    def _run_expert_re_evaluation(
        self,
        sub_id: str,
        level: int,
        statement_text: str,
        context: str,
        first_attempt_reason: str,
        missing_tags: Set[str],
        highest_rerank_score: float,
        sub_criteria_name: str,
        llm_evaluator_to_use: Any,
        base_kwargs: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        [EXPERT LOOP - DIVERSITY FORCED] ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏´‡πâ LLM ‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô
        - ‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ú‡∏•‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏£‡∏≠‡∏ö‡πÅ‡∏£‡∏Å‡∏ï‡∏Å‡πÅ‡∏ï‡πà‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô Rerank ‡∏™‡∏π‡∏á
        - ‡πÅ‡∏ó‡∏£‡∏Å‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏û‡∏¥‡πÄ‡∏®‡∏© (Expert Hint) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏≠‡∏Ñ‡∏ï‡∏¥‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•
        """
        self.logger.info(f"üïµÔ∏è [EXPERT RE-EVAL] Starting second pass for {sub_id} L{level} (Rerank: {highest_rerank_score:.4f})")

        # üü¢ ‡∏™‡∏£‡πâ‡∏≤‡∏á Expert Hint ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏£‡∏∞‡∏ï‡∏∏‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏∂‡∏Å
        missing_info = f"‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏°‡∏¥‡∏ï‡∏¥ {', '.join(missing_tags)}" if missing_tags else "‡πÉ‡∏ô‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°"
        
        hint_msg = f"""
--- üö® EXPERT AUDITOR OVERRIDE (‡∏£‡∏≠‡∏ö‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏û‡∏¥‡πÄ‡∏®‡∏©) ---
‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏£‡∏≠‡∏ö‡πÅ‡∏£‡∏Å: ‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô (‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•: {first_attempt_reason})

‡∏Ç‡πâ‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏ö‡∏ö Search:
- ‡∏£‡∏∞‡∏ö‡∏ö‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏™‡∏π‡∏á (Score: {highest_rerank_score:.4f}) ‡πÅ‡∏ï‡πà‡∏Ñ‡∏∏‡∏ì‡∏≠‡∏≤‡∏à‡∏°‡∏≠‡∏á‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏õ
- ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏û‡∏¥‡πÄ‡∏®‡∏©: ‡πÇ‡∏õ‡∏£‡∏î‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡πÉ‡∏ô Context ‡∏ô‡∏≠‡∏Å‡πÄ‡∏´‡∏ô‡∏∑‡∏≠‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà‡∏≠‡πà‡∏≤‡∏ô‡∏£‡∏≠‡∏ö‡πÅ‡∏£‡∏Å
- ‡∏´‡∏≤‡∏Å‡∏û‡∏ö "‡∏£‡πà‡∏≠‡∏á‡∏£‡∏≠‡∏¢" (Implicit Evidence) ‡∏Ç‡∏≠‡∏á‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£ {missing_info} ‡πÅ‡∏°‡πâ Keyword ‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á 100% ‡πÅ‡∏ï‡πà‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏à‡∏£‡∏¥‡∏á ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏î‡∏∏‡∏•‡∏¢‡∏û‡∏¥‡∏ô‡∏¥‡∏à (Professional Discretion) ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô
- ‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ (Baseline) ‡∏´‡∏≤‡∏Å‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏î‡∏µ‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏™‡∏£‡∏¥‡∏° ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ò‡∏£‡∏£‡∏°
"""

        # ‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÄ‡∏î‡∏¥‡∏°‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï Context ‡πÉ‡∏´‡∏°‡πà
        expert_kwargs = base_kwargs.copy()
        expert_kwargs["context"] = f"{context}\n\n{hint_msg}"
        expert_kwargs["sub_criteria_name"] = f"{sub_criteria_name} (Expert Re-assessment Mode)"
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏à‡∏≥‡∏•‡∏≠‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ LLM ‡∏Å‡∏•‡πâ‡∏≤‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô
        expert_kwargs["ai_confidence"] = "High (Expert Override)"

        try:
            # ‡∏£‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ã‡πâ‡∏≥‡∏î‡πâ‡∏ß‡∏¢ Evaluator ‡∏ï‡∏±‡∏ß‡πÄ‡∏î‡∏¥‡∏° (‡πÅ‡∏ï‡πà Prompt ‡∏°‡∏µ Hint ‡πÉ‡∏´‡∏°‡πà)
            re_eval_result = llm_evaluator_to_use(**expert_kwargs)
            
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏ß‡πà‡∏≤‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ Expert Re-eval
            re_eval_result["is_expert_evaluated"] = True
            re_eval_result["original_fail_reason"] = first_attempt_reason
            
            if re_eval_result.get("is_passed"):
                self.logger.info(f"‚ú® [EXPERT SUCCESS] {sub_id} L{level} has been reversed to PASSED")
            else:
                self.logger.info(f"‚ùå [EXPERT CONFIRMED] {sub_id} L{level} is still FAILED after re-evaluation")
                
            return re_eval_result
            
        except Exception as e:
            self.logger.error(f"‚ùå Expert Re-evaluation crashed: {e}")
            # ‡∏Å‡∏£‡∏ì‡∏µ‡∏û‡∏±‡∏á ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏£‡∏≠‡∏ö‡πÅ‡∏£‡∏Å
            return {"is_passed": False, "reason": f"Expert Loop Error: {str(e)}", "score": 0.0}
    
    def _apply_diversity_filter(self, evidences: List[Dict], level: int) -> List[Dict]:
        """
        ‡∏Å‡∏£‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢ (Diversity) ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡πÄ‡∏¢‡∏≠‡∏∞‡∏à‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ (Context Window Management)
        - Level 1-2: ‡πÄ‡∏ô‡πâ‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ (Top 15 chunks)
        - Level 3 ‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏õ: ‡∏à‡∏≥‡∏Å‡∏±‡∏î chunks ‡∏ï‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î Bias
        """
        if not evidences:
            return []

        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ï‡∏≤‡∏° Score ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î (Rerank score)
        sorted_evidences = sorted(evidences, key=lambda x: x.get('rerank_score', 0), reverse=True)

        if level <= 2:
            # ‡πÄ‡∏•‡πÄ‡∏ß‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô ‡πÄ‡∏≠‡∏≤ Top 15 ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
            return sorted_evidences[:15]
        
        # ‡πÄ‡∏•‡πÄ‡∏ß‡∏•‡∏™‡∏π‡∏á: ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡πÉ‡∏´‡πâ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 3-4 chunks ‡∏ï‡πà‡∏≠‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ AI ‡πÄ‡∏´‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢
        diverse_results = []
        file_counts = {}
        per_file_limit = 4

        for ev in sorted_evidences:
            source = ev.get('metadata', {}).get('source_filename', 'Unknown')
            file_counts[source] = file_counts.get(source, 0) + 1
            
            if file_counts[source] <= per_file_limit:
                diverse_results.append(ev)
            
            # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Context ‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ (‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏£‡∏ß‡∏°‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 20-25 chunks)
            if len(diverse_results) >= 20:
                break
                
        return diverse_results
    
    def _normalize_evidence_metadata(self, evidence_list: List[Dict[str, Any]]):
        """
        ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÉ‡∏´‡πâ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á Flattened ‡πÅ‡∏•‡∏∞ Nested Metadata 
        ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô (Export) ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà Error
        """
        for ev in evidence_list:
            if not isinstance(ev, dict):
                continue
                
            # 1. ‡∏î‡∏∂‡∏á Metadata ‡∏´‡∏•‡∏±‡∏Å (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
            meta = ev.get("metadata", {}) if isinstance(ev.get("metadata"), dict) else {}
            
            # 2. ‡∏õ‡∏£‡∏±‡∏ö Source (‡πÄ‡∏ä‡πá‡∏Ñ‡∏ó‡∏±‡πâ‡∏á‡∏ô‡∏≠‡∏Å‡πÅ‡∏•‡∏∞‡πÉ‡∏ô meta)
            raw_source = ev.get("source") or meta.get("source") or ev.get("source_filename") or meta.get("source_filename")
            ev["source"] = os.path.basename(str(raw_source)) if raw_source else "Unknown_File"
            
            # 3. ‡∏õ‡∏£‡∏±‡∏ö Page (‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô String ‡πÄ‡∏™‡∏°‡∏≠‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô JSON Error)
            raw_page = ev.get("page") or meta.get("page") or meta.get("page_label") or "N/A"
            ev["page"] = str(raw_page)
            
            # 4. ‡∏õ‡∏£‡∏±‡∏ö Relevance Score (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏´‡∏•‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠ Key)
            raw_score = ev.get("relevance_score") or ev.get("score") or meta.get("rerank_score") or 0.0
            try:
                ev["relevance_score"] = float(raw_score)
            except (ValueError, TypeError):
                ev["relevance_score"] = 0.0
            
            # 5. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö ID ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏¢‡∏á‡πÉ‡∏¢ Database)
            if not ev.get("stable_doc_uuid"):
                ev["stable_doc_uuid"] = ev.get("doc_id") or meta.get("stable_doc_uuid") or "unknown_uuid"
            
            # 6. ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ü‡∏¥‡∏•‡∏î‡πå‡πÅ‡∏™‡∏î‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏ö‡∏ö‡∏™‡∏ß‡∏¢‡πÜ (Optional - ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏ä‡∏ß‡πå‡πÉ‡∏ô Log)
            ev["source_filename"] = ev["source"]

        return evidence_list

    def relevance_score_fn(self, evidence: Dict[str, Any], sub_id: str, level: int) -> float:
        """
        [REVISED v2026.4.8] 
        - ‡πÄ‡∏û‡∏¥‡πà‡∏° Source Grading: ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÑ‡∏ü‡∏•‡πå '‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å/‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á/‡∏°‡∏ï‡∏¥' ‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤ '‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô'
        - ‡∏õ‡∏£‡∏±‡∏ö Keyword Saturation: ‡πÄ‡∏ô‡πâ‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Å‡∏≤‡∏£ Match ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì
        - ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á Neighbor Context Handling ‡πÉ‡∏´‡πâ‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡∏Ç‡∏∂‡πâ‡∏ô
        """
        if not evidence:
            return 0.0

        # 1. Rerank Score (Weight 50%)
        rerank_score = evidence.get('rerank_score', evidence.get('score', 0.0))
        normalized_rerank = min(max(rerank_score, 0.0), 1.0)

        # 2. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        text = (evidence.get('text', '') or evidence.get('page_content', '')).lower()
        meta = evidence.get('metadata', {})
        filename = (meta.get('source', '') or meta.get('source_filename', '') or '').lower()

        # 3. ‡∏î‡∏∂‡∏á Rules ‡∏õ‡∏£‡∏∞‡∏à‡∏≥ Level
        cum_rules = self.get_cumulative_rules(sub_id, level)
        
        # 4. Source Grading Logic (‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏î‡∏∂‡∏á Assessment Report)
        source_bonus = 0.0
        # ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ö‡πà‡∏á‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ä‡∏±‡πâ‡∏ô‡∏ï‡πâ‡∏ô (High Priority)
        primary_evidence_patterns = ["‡∏°‡∏ï‡∏¥", "‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å", "‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á", "‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®", "‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢", "‡πÅ‡∏ú‡∏ô‡πÅ‡∏°‡πà‡∏ö‡∏ó"]
        # ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ö‡πà‡∏á‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏™‡∏£‡∏∏‡∏õ/‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô (Lower Priority ‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ)
        secondary_report_patterns = ["assessment report", "‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô", "‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•"]

        if any(p in filename for p in primary_evidence_patterns):
            source_bonus += 0.20  # Boost ‡πÉ‡∏´‡πâ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ï‡∏±‡∏ß‡∏à‡∏£‡∏¥‡∏á
        if any(p in filename for p in secondary_report_patterns):
            source_bonus -= 0.15  # Penalty ‡πÉ‡∏´‡πâ‡πÑ‡∏ü‡∏•‡πå‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡πÄ‡∏ö‡∏µ‡∏¢‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏£‡∏¥‡∏á

        # 5. Keyword Score (Weight 50%) - Saturation 
        target_kws = set()
        if level <= 2:
            target_kws.update(cum_rules.get('plan_keywords', []) + cum_rules.get('do_keywords', []))
        else:
            target_kws.update(cum_rules.get('check_keywords', []) + cum_rules.get('act_keywords', []))
        
        # ‡πÄ‡∏ô‡πâ‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà Match ‡∏à‡∏£‡∏¥‡∏á‡πÜ
        match_count = sum(1 for kw in target_kws if kw.lower() in text)
        # Saturation: ‡πÄ‡∏à‡∏≠ 2-3 ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å‡∏û‡∏≠‡πÅ‡∏•‡πâ‡∏ß (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏£‡∏≤‡∏ä‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏°‡πà‡∏¢‡∏≤‡∏ß‡∏°‡∏≤‡∏Å)
        keyword_score = min(match_count / 2.5, 1.0) 

        # 6. Act-Hook Bonus (Neighbor Boost)
        neighbor_bonus = 0.0
        is_neighbor = evidence.get('is_neighbor', False) or meta.get('is_neighbor', False)
        if is_neighbor:
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÅ‡∏ï‡πâ‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏´‡∏ô‡πâ‡∏≤‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏Å‡∏£‡∏≠‡∏á‡∏ó‡∏¥‡πâ‡∏á (Threshold ‡∏°‡∏±‡∏Å‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà 0.35)
            neighbor_bonus += 0.25 

        # 7. Specific Context Phrase Match
        specific_rule = cum_rules.get('specific_contextual_rule', '').lower()
        rule_bonus = 0.25 if specific_rule and specific_rule in text else 0.0

        # 8. ‡∏£‡∏ß‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô (50/50 Ratio + Bonuses)
        # ‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ 0.5/0.5 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Keyword Match ‡∏°‡∏µ‡∏ú‡∏•‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö Vector Search ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
        final_score = (0.5 * normalized_rerank) + (0.5 * keyword_score) + source_bonus + neighbor_bonus + rule_bonus
        final_score = min(max(final_score, 0.0), 1.0)

        self.logger.debug(f"[{sub_id} L{level}] RelScore: {final_score:.4f} | Rerank: {normalized_rerank:.4f} | KW: {keyword_score:.4f} | Src: {source_bonus}")
        return final_score

    def enhance_query_for_statement(
        self,
        statement_text: str,
        sub_id: str,
        statement_id: str,
        level: int,
        focus_hint: str = ""
    ) -> List[str]:
        """
        [ULTIMATE QUERY ENHANCER v2026.6 - Enhanced for all PDCA phases]
        - ‡πÄ‡∏û‡∏¥‡πà‡∏° keyword ‡πÅ‡∏•‡∏∞ query ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Check & Act
        - ‡∏Ç‡∏¢‡∏≤‡∏¢ synonym ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏£‡∏±‡∏ê‡∏ß‡∏¥‡∏™‡∏≤‡∏´‡∏Å‡∏¥‡∏à
        - ‡πÄ‡∏ô‡πâ‡∏ô Do ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö L1-L2, Check & Act ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö L3+
        - ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î 8 queries
        """
        logger = logging.getLogger(__name__)

        # 1. Anchor ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        enabler_id = str(self.enabler_id).upper()  # e.g., "KM", "CG"
        id_anchor = f"{enabler_id} {sub_id}"      # e.g., "KM 1.1"

        # 2. ‡∏î‡∏∂‡∏á cumulative rules
        cum_rules = self.get_cumulative_rules(sub_id, level)

        # 3. ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏° keywords ‡∏à‡∏≤‡∏Å rules
        plan_kws = cum_rules.get('plan_keywords', [])
        do_kws = cum_rules.get('do_keywords', [])
        check_kws = cum_rules.get('check_keywords', [])
        act_kws = cum_rules.get('act_keywords', [])

        # 4. ‡∏Ç‡∏¢‡∏≤‡∏¢ Synonym ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ (‡πÄ‡∏û‡∏¥‡πà‡∏° Check & Act)
        do_synonyms = [
            "‡∏°‡∏ï‡∏¥‡∏ö‡∏≠‡∏£‡πå‡∏î", "‡∏°‡∏ï‡∏¥‡∏Ñ‡∏ì‡∏∞‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£", "‡∏°‡∏ï‡∏¥‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°", "‡∏≠‡∏ô‡∏∏‡∏°‡∏±‡∏ï‡∏¥", "‡πÄ‡∏´‡πá‡∏ô‡∏ä‡∏≠‡∏ö",
            "‡∏£‡∏±‡∏ö‡∏ó‡∏£‡∏≤‡∏ö", "‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡πÅ‡∏•‡πâ‡∏ß", "‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡πÉ‡∏ä‡πâ", "‡∏•‡∏á‡∏°‡∏ï‡∏¥", "‡∏°‡∏µ‡∏°‡∏ï‡∏¥",
            "‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÅ‡∏ï‡πà‡∏á‡∏ï‡∏±‡πâ‡∏á", "‡∏≠‡∏ô‡∏∏‡∏°‡∏±‡∏ï‡∏¥‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£", "‡∏Ç‡∏≠‡∏≠‡∏ô‡∏∏‡∏°‡∏±‡∏ï‡∏¥", "‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡∏ô‡∏∏‡∏°‡∏±‡∏ï‡∏¥",
            "‡∏°‡∏ï‡∏¥‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£", "‡∏°‡∏ï‡∏¥ ‡∏Å‡∏ü‡∏†", "‡∏°‡∏ï‡∏¥‡∏Ñ‡∏ì‡∏∞‡∏ó‡∏≥‡∏á‡∏≤‡∏ô"
        ]

        check_synonyms = [
            "‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏•", "‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô", "‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î", "KPI", "‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•",
            "‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏ú‡∏•", "‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö", "‡∏ß‡∏±‡∏î‡∏ú‡∏•", "‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•", "‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå",
            "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡πâ‡∏≤‡∏ß‡∏´‡∏ô‡πâ‡∏≤", "‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ú‡∏•", "‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™"
        ]

        act_synonyms = [
            "‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á", "‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á", "‡∏ñ‡∏≠‡∏î‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô", "‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ",
            "‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç", "‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô", "‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°", "‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô", "‡∏¢‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö",
            "‡πÅ‡∏ú‡∏ô‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡∏°‡πà", "‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞", "‡∏°‡∏≤‡∏ï‡∏£‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç"
        ]

        # ‡∏£‡∏ß‡∏° keywords ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ query ‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô)
        all_kws = list(set(
            plan_kws + do_kws + check_kws + act_kws +
            do_synonyms + check_synonyms + act_synonyms
        ))
        keywords_str = " ".join(all_kws[:15])  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 12 ‡πÄ‡∏õ‡πá‡∏ô 15

        queries = []

        # Query 1: Direct + Anchor (‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î)
        queries.append(f"{id_anchor} {statement_text}")

        # Query 2: Maturity + Keywords
        if level <= 2:
            queries.append(f"{id_anchor} ‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢ ‡∏ß‡∏¥‡∏™‡∏±‡∏¢‡∏ó‡∏±‡∏®‡∏ô‡πå ‡∏°‡∏ï‡∏¥‡∏ö‡∏≠‡∏£‡πå‡∏î ‡∏≠‡∏ô‡∏∏‡∏°‡∏±‡∏ï‡∏¥ ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÅ‡∏ï‡πà‡∏á‡∏ï‡∏±‡πâ‡∏á {keywords_str}")
        else:
            queries.append(f"{id_anchor} ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏• ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏ú‡∏• ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á ‡∏ñ‡∏≠‡∏î‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô {keywords_str}")

        # Query 3: Specific Rule (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
        specific_rule = cum_rules.get('specific_contextual_rule', '')
        if specific_rule:
            queries.append(f"{id_anchor} {specific_rule[:80]}")  # ‡∏ï‡∏±‡∏î‡∏™‡∏±‡πâ‡∏ô‡∏•‡∏á‡∏≠‡∏µ‡∏Å‡∏ô‡∏¥‡∏î

        # Query 4: Do-Focused (L1-L2)
        if level <= 2:
            do_focus = f"‡∏°‡∏ï‡∏¥‡∏ö‡∏≠‡∏£‡πå‡∏î ‡∏≠‡∏ô‡∏∏‡∏°‡∏±‡∏ï‡∏¥ ‡πÄ‡∏´‡πá‡∏ô‡∏ä‡∏≠‡∏ö ‡∏£‡∏±‡∏ö‡∏ó‡∏£‡∏≤‡∏ö ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÅ‡∏ï‡πà‡∏á‡∏ï‡∏±‡πâ‡∏á {sub_id} {keywords_str}"
            queries.append(do_focus)

        # Query 5: Check-Focused (L3+)
        if level >= 3:
            check_focus = f"‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏• ‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î KPI ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏• ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏ú‡∏• {id_anchor} {keywords_str}"
            queries.append(check_focus)

        # Query 6: Act-Focused (L4+)
        if level >= 4:
            act_focus = f"‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á ‡∏ñ‡∏≠‡∏î‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç ‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏° {id_anchor} {keywords_str}"
            queries.append(act_focus)

        # Query 7: Broad/Global (‡πÑ‡∏°‡πà‡∏ï‡∏¥‡∏î anchor ‡πÄ‡∏ï‡πá‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö)
        broad_query = f"{enabler_id} {statement_text} {keywords_str}"
        queries.append(broad_query)

        # Query 8: Tenant + Enabler Context
        tenant_context = f"{self.config.tenant} {enabler_id} {statement_text}"
        queries.append(tenant_context)

        # Final Cleaning & Deduplication
        final_queries = []
        seen = set()
        for q in queries:
            q_strip = q.strip()
            if q_strip and q_strip not in seen and len(q_strip) > 5:
                # ‡∏ï‡∏±‡∏î‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Å‡∏±‡∏ö embedding model (30 ‡∏Ñ‡∏≥)
                clean_q = " ".join(q_strip.split()[:30])
                final_queries.append(clean_q)
                seen.add(clean_q)

        # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î 8
        final_queries = final_queries[:8]

        logger.debug(f"Generated {len(final_queries)} enhanced queries for {id_anchor} L{level}")
        return final_queries
    
    def _get_baseline_summary_text(self, sub_id: str, level: int) -> str:
        """ ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÄ‡∏•‡πÄ‡∏ß‡∏•‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏´‡πâ LLM (Fix AttributeError) """
        if level <= 1:
            return "‡∏£‡∏∞‡∏î‡∏±‡∏ö‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô (Starting Level): ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤"
        
        prev_res = self.results.get(f"{sub_id}.L{level-1}", {})
        if prev_res:
            status = "‡∏ú‡πà‡∏≤‡∏ô ‚úÖ" if prev_res.get('is_passed') else "‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô ‚ùå"
            return f"‡∏ú‡∏•‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô L{level-1}: {status} | ‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•: {prev_res.get('reason', 'N/A')[:200]}..."
        
        return f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• L{level-1}"
        
    def _run_single_assessment(
        self,
        sub_criteria: Dict[str, Any],
        statement_data: Dict[str, Any],
        vectorstore_manager: Optional['VectorStoreManager'],
        sequential_chunk_uuids: Optional[List[str]] = None,
        record_id: str = None,
        attempt: int = 1,
        **kwargs
    ) -> Dict[str, Any]:
        """
        [ULTIMATE PRODUCTION v2026.4.5] - Precision & Data-Driven Logic
        - ‡πÉ‡∏ä‡πâ top_k=INITIAL_TOP_K ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏Å‡∏©‡∏≤ Precision
        - ‡πÉ‡∏ä‡πâ Dynamic Query Expansion ‡∏à‡∏≤‡∏Å Rules ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£ Hardcode ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå
        - ‡∏£‡∏∞‡∏ö‡∏ö Relevance Filter + PDCA Tagging ‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á LLM
        """
        start_time = time.time()
        sub_id = sub_criteria['sub_id']
        level = statement_data['level']
        statement_text = statement_data['statement']
        sub_criteria_name = sub_criteria['sub_criteria_name']
        
        MIN_RETRY_SC = 0.70
        self.logger.info(f"üîç [ASSESSMENT] {sub_id} L{level} | Attempt: {attempt} (Server Mode)")

        # 1. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Contextual Rules ‡πÅ‡∏•‡∏∞ Level Constraints
        # ‡∏î‡∏∂‡∏á‡∏Å‡∏é‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö Rubrics ‡πÅ‡∏•‡∏∞ Keywords ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏ï‡∏±‡πâ‡∏á‡πÑ‡∏ß‡πâ‡πÉ‡∏ô JSON
        cum_rules = self.get_cumulative_rules(sub_id, level)
        level_constraint = self._get_level_constraint_prompt(sub_id, level)

        # 2. ADAPTIVE RAG LOOP + Dynamic Query Expansion
        mapped_stable_ids, priority_chunks = self._get_mapped_uuids_and_priority_chunks(
            sub_id=sub_id, level=level, statement_text=statement_text,
            level_constraint=level_constraint, vectorstore_manager=vectorstore_manager
        )

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Query List ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥: ‡∏£‡∏ß‡∏°‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏î‡∏¥‡∏° + ‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏à‡∏≤‡∏Å Rules
        rag_query_list = [statement_text]
        important_kws = cum_rules.get('plan_keywords', []) + cum_rules.get('do_keywords', [])
        if important_kws:
            # ‡∏î‡∏∂‡∏á‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á Query ‡πÄ‡∏™‡∏£‡∏¥‡∏° (‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 3-5 ‡∏Ñ‡∏≥‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ Query ‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ)
            rag_query_list.append(" OR ".join(important_kws[:3]))

        highest_rerank_score = -1.0
        final_top_evidences = []

        # Adaptive Loop ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î 4 ‡∏£‡∏≠‡∏ö)
        for loop_attempt in range(1, 5):
            # ‡∏£‡∏≠‡∏ö‡πÅ‡∏£‡∏Å‡πÉ‡∏ä‡πâ Full Query, ‡∏£‡∏≠‡∏ö‡∏ñ‡∏±‡∏î‡πÜ ‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏ô‡∏≥
            query_input = rag_query_list if loop_attempt == 1 else [statement_text]
            
            retrieval_result = self.rag_retriever(
                query=query_input,
                doc_type=self.doc_type,
                sub_id=sub_id, level=level,
                vectorstore_manager=vectorstore_manager,
                stable_doc_ids=mapped_stable_ids,
                top_k=INITIAL_TOP_K  # ‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏Ñ‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏°‡∏ï‡∏≤‡∏°‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
            )
            
            current_evidences = retrieval_result.get("top_evidences", [])
            all_candidates = current_evidences + priority_chunks
            
            if not all_candidates:
                continue

            current_max = max(
                (ev.get('rerank_score', ev.get('score', 0)) for ev in all_candidates),
                default=0.0
            )

            if current_max >= highest_rerank_score:
                highest_rerank_score = current_max
                final_top_evidences = all_candidates

            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏î‡πâ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà Rerank score ‡∏™‡∏π‡∏á‡∏û‡∏≠‡πÅ‡∏•‡πâ‡∏ß ‡πÉ‡∏´‡πâ‡∏´‡∏¢‡∏∏‡∏î Loop
            if highest_rerank_score >= MIN_RETRY_SC:
                break

        self.logger.info(f"Adaptive RAG completed | Highest score: {highest_rerank_score:.4f} | Raw evidences: {len(final_top_evidences)}")

        # 3. ACT-HOOK: CONTEXT EXPANSION (‡∏î‡∏∂‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏Ç‡πâ‡∏≤‡∏á‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏°‡∏≤‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏ö‡∏£‡∏¥‡∏ö‡∏ó)
        expanded_evidences = self._expand_context_with_neighbor_pages(
            top_evidences=final_top_evidences,
            collection_name=f"evidence_{self.enabler_id.lower()}"
        )

        # 4. ROBUST PDCA TAGGING (‡∏ï‡∏¥‡∏î‡∏õ‡πâ‡∏≤‡∏¢‡∏Å‡∏≥‡∏Å‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô)
        for doc in expanded_evidences:
            doc["pdca_tag"] = classify_by_keyword(
                text=doc.get("text", ""),
                sub_id=sub_id, level=level,
                contextual_rules_map=self.contextual_rules_map,
                chunk_metadata=doc.get('metadata')
            )

        # 5. DIVERSITY FILTER (‡∏Å‡∏£‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô)
        sorted_evidences = sorted(
            expanded_evidences,
            key=lambda x: x.get('rerank_score', 0),
            reverse=True
        )
        
        diverse_filtered = []
        file_counts = {}
        for doc in sorted_evidences:
            meta = doc.get('metadata', {})
            fname = os.path.basename(meta.get('source', 'unknown'))
            if file_counts.get(fname, 0) < 5:  # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Chunks ‡∏ï‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå
                diverse_filtered.append(doc)
                file_counts[fname] = file_counts.get(fname, 0) + 1
            if len(diverse_filtered) >= 30: # Max Context Chunks
                break

        self.logger.info(f"Diversity filter applied | Before relevance filter: {len(diverse_filtered)}")

        # 6. RELEVANCE FILTER (‡∏Å‡∏£‡∏≠‡∏á Noise ‡πÇ‡∏î‡∏¢‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å Rules ‡πÉ‡∏ô JSON)
        relevant_filtered = []
        # ‡πÉ‡∏ä‡πâ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏°‡∏á‡∏ß‡∏î‡∏ï‡∏≤‡∏° Maturity Level
        rel_threshold = 0.35 if level <= 2 else 0.45 
        
        for doc in diverse_filtered:
            rel_score = self.relevance_score_fn(doc, sub_id, level) # ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
            doc['relevance_score_custom'] = rel_score
            if rel_score >= rel_threshold:
                relevant_filtered.append(doc)

        # Fallback Mechanism: ‡∏´‡∏≤‡∏Å‡∏Å‡∏£‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÄ‡∏•‡∏¢ ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ Top 5 ‡∏à‡∏≤‡∏Å Rerank
        if not relevant_filtered and diverse_filtered:
            relevant_filtered = sorted(diverse_filtered, key=lambda x: x.get('rerank_score', 0), reverse=True)[:5]
            self.logger.warning(f"Relevance filter too strict for {sub_id} L{level} ‚Üí fallback to top 5")

        self.logger.info(f"After relevance filter: {len(relevant_filtered)} evidences sent to LLM")

        # 7. PDCA SYNTHESIS (‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LLM)
        previous_evidence = self._collect_previous_level_evidences(sub_id, level) if level > 1 else {}
        formatted_baseline_evi = {k.split(".L")[-1]: v for k, v in previous_evidence.items()}
        
        plan_b, do_b, check_b, act_b, other_b = self._get_pdca_blocks_from_evidences(
            evidences=relevant_filtered,
            baseline_evidences=formatted_baseline_evi,
            level=level, sub_id=sub_id,
            contextual_rules_map=self.contextual_rules_map,
            record_id=record_id
        )

        # 8. EVALUATION WITH LLM
        processed_lc_docs = [LcDocument(page_content=d['text'], metadata=d.get('metadata', d)) for d in relevant_filtered]
        confidence_result = self.calculate_audit_confidence(processed_lc_docs)
        
        synthesized_context = (
            f"--- EVIDENCE BLOCKS SYNTHESIS ---\n"
            f"<Plan_Evidence>\n{plan_b or '‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô'}\n</Plan_Evidence>\n"
            f"<Do_Evidence>\n{do_b or '‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô'}\n</Do_Evidence>\n"
            f"<Check_Evidence>\n{check_b or '‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö'}\n</Check_Evidence>\n"
            f"<Act_Evidence>\n{act_b or '‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á'}\n</Act_Evidence>\n"
            f"<General_Context>\n{other_b}\n</General_Context>"
        )

        llm_result = evaluate_with_llm(
            context=synthesized_context,
            baseline_summary=self._get_baseline_summary_text(sub_id, level),
            sub_id=sub_id,
            level=level,
            statement_text=statement_text,
            sub_criteria_name=sub_criteria_name,
            ai_confidence=confidence_result["level"],
            confidence_reason=confidence_result.get("reason", "N/A"),
            max_rerank_score=highest_rerank_score,
            llm_executor=self.llm
        )

        # 9. POST-PROCESSING & SAVE
        llm_result = self.post_process_llm_result(llm_result, level)
        max_evi_str = self._save_level_evidences_and_calculate_strength(
            level_temp_map=relevant_filtered,
            sub_id=sub_id,
            level=level,
            llm_result=llm_result,
            highest_rerank_score=highest_rerank_score
        )

        self.logger.info(f"üìä [RESULT] {sub_id} L{level} -> Score: {llm_result.get('score', 0.0):.4f} | Pass: {llm_result.get('is_passed')}")

        return {
            "sub_criteria_id": sub_id,
            "level": level,
            "is_passed": llm_result.get('is_passed', False),
            "score": float(llm_result.get('score', 0.0)),
            "audit_confidence": confidence_result,
            "pdca_breakdown": {
                "P": llm_result.get("P_Plan_Score", 0),
                "D": llm_result.get("D_Do_Score", 0),
                "C": llm_result.get("C_Check_Score", 0),
                "A": llm_result.get("A_Act_Score", 0)
            },
            "reason": llm_result.get('reason', "‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à"),
            "max_relevant_score": highest_rerank_score,
            "evidence_strength": max_evi_str,
            "temp_map_for_level": relevant_filtered,
            "duration": round(time.time() - start_time, 2)
        }