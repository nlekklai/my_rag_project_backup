# -*- coding: utf-8 -*-
# core/seam_assessment.py

import sys
import json
import logging
import time
from datetime import datetime, date
import os
from typing import List, Dict, Any, Optional, Union, Tuple, Set, Final, Literal
from collections import defaultdict, OrderedDict
from dataclasses import dataclass, field
import multiprocessing 
from functools import partial
import pathlib, uuid
from copy import deepcopy
import tempfile
import shutil
import re
import hashlib
import unicodedata 
import random

from core.json_extractor import _robust_extract_json, _robust_extract_json_list

# -------------------- 1. PROTECTIVE IMPORTS --------------------
# ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á FileLock ‡πÅ‡∏•‡∏∞ Database ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏™‡πà‡∏ß‡∏ô‡∏≠‡∏∑‡πà‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏ä‡∏±‡∏ß‡∏£‡πå‡πÜ
try:
    from filelock import FileLock
except ImportError:
    class FileLock:
        def __init__(self, *args, **kwargs): pass
        def __enter__(self): return self
        def __exit__(self, *args): pass
    print("‚ö†Ô∏è WARNING: 'filelock' not installed.")

try:
    from database import init_db, db_update_task_status
    update_db_core = db_update_task_status
except ImportError:
    def init_db(): pass
    def update_db_core(*args, **kwargs): pass

# -------------------- 2. PATH SETUP --------------------
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

# -------------------- 3. CORE LOGIC & CONFIG IMPORTS --------------------
try:
    # --- Configs ---
    from config.global_vars import (
        MAX_LEVEL, EVIDENCE_DOC_TYPES, RERANK_THRESHOLD, MAX_EVI_STR_CAP,
        DEFAULT_LLM_MODEL_NAME, LLM_TEMPERATURE, MIN_RETRY_SCORE,
        MAX_PARALLEL_WORKERS, PDCA_PRIORITY_ORDER, TARGET_DEVICE,
        PDCA_PHASE_MAP, INITIAL_TOP_K, FINAL_K_RERANKED,
        MAX_CHUNKS_PER_FILE, MAX_CHUNKS_PER_BLOCK, MATURITY_LEVEL_GOALS,
        SEAM_ENABLER_FULL_NAME_TH, SEAM_ENABLER_FULL_NAME_EN,
        SCORING_MODE, MAX_CHUNKS_LOW, MAX_CHUNKS_HIGH,
        CRITICAL_CA_THRESHOLD
    )
    
    # --- Utilities ---
    from core.llm_data_utils import ( 
        evaluate_with_llm, retrieve_context_with_filter, 
        action_plan_normalize_keys, evaluate_with_llm_low_level, 
        LOW_LEVEL_K, create_context_summary_llm, _fetch_llm_response,
        _check_and_handle_empty_context, set_mock_control_mode as set_llm_data_mock_mode
    )
    from core.vectorstore import VectorStoreManager, load_all_vectorstores
    from core.retry_policy import RetryPolicy, RetryResult  # <-- ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å
    from core.json_extractor import _robust_extract_json

    # --- Path Utils ---
    from utils.path_utils import (
        get_mapping_file_path, get_evidence_mapping_file_path, 
        get_contextual_rules_file_path, get_assessment_export_file_path,
        get_export_dir, get_rubric_file_path, _n
    )

    # --- Prompts ---
    try:
        from core.seam_prompts import (
            ATOMIC_ACTION_PROMPT, MASTER_ROADMAP_PROMPT,
            SYSTEM_ATOMIC_ACTION_PROMPT, SYSTEM_MASTER_ROADMAP_PROMPT
        )
    except ImportError:
        ATOMIC_ACTION_PROMPT = "Recommendation: {coaching_insight} Level: {level}"
        MASTER_ROADMAP_PROMPT = "Roadmap for {sub_criteria_name}: {aggregated_insights}"
        SYSTEM_ATOMIC_ACTION_PROMPT = "Assistant mode."
        SYSTEM_MASTER_ROADMAP_PROMPT = "Strategy mode."

except ImportError as e:
    # üö® EMERGENCY FALLBACK: ‡∏î‡πà‡∏≤‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ üö®
    print(f"‚ö†Ô∏è EMERGENCY: Core Module missing, initializing safety fallbacks: {e}")
    
    # Constants
    MAX_LEVEL = 5
    EVIDENCE_DOC_TYPES = "evidence"
    LOW_LEVEL_K = 5  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ï‡∏±‡∏ß‡∏ô‡∏µ‡πâ
    
    # [FIX] Class/Policy
    class RetryResult:
        def __init__(self, data=None): self.data = data or {}
        def get(self, k, d=None): return self.data.get(k, d)
    class RetryPolicy:
        def __init__(self, *args, **kwargs): pass
        def execute(self, func, *args, **kwargs): return RetryResult(func(*args, **kwargs))

    # [FIX] Path Functions
    def _n(s): return str(s).lower().strip()
    def get_rubric_file_path(tenant, enabler, **kwargs): 
        return f"data_store/{_n(tenant)}/config/{_n(tenant)}_{_n(enabler)}_rubric.json"
    def get_contextual_rules_file_path(tenant, enabler, **kwargs):
        return f"data_store/{_n(tenant)}/config/{_n(tenant)}_{_n(enabler)}_contextual_rules.json"
    def get_evidence_mapping_file_path(tenant, year, enabler, **kwargs):
        return f"data_store/{_n(tenant)}/mapping/{year}/{_n(tenant)}_{year}_{_n(enabler)}_evidence_mapping.json"
    def get_mapping_file_path(doc_type, tenant, year=None, enabler=None, **kwargs):
        return f"data_store/{_n(tenant)}/mapping/{year}/{_n(tenant)}_{year}_{_n(enabler)}_doc_id_mapping.json"
    def get_assessment_export_file_path(*args, **kwargs): return "exports/temp_report.json"
    def get_export_dir(*args, **kwargs): return "exports"

    # [FIX] Logic Functions (‡∏î‡πà‡∏≤‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏ö‡∏ñ‡∏≤‡∏°‡∏´‡∏≤)
    def evaluate_with_llm_low_level(*args, **kwargs): 
        return {"score": 0.0, "reason": "Fallback mode active", "is_passed": False}
    
    def evaluate_with_llm(*args, **kwargs): 
        return {"score": 0.0, "reason": "Fallback mode active", "is_passed": False}

    def retrieve_context_with_filter(*args, **kwargs): 
        return {"top_evidences": [], "aggregated_context": ""}


    # Placeholders
    def _fetch_llm_response(*args, **kwargs): return "{}"
    def _robust_extract_json(t): return {}
    def set_llm_data_mock_mode(m): pass
    def action_plan_normalize_keys(d): return d
    def create_context_summary_llm(*args, **kwargs): return {"summary": "N/A", "coaching": "N/A"}
    def _check_and_handle_empty_context(*args, **kwargs): return None, False

    ATOMIC_ACTION_PROMPT = "Level {level}: {coaching_insight}"
    MASTER_ROADMAP_PROMPT = "Roadmap: {aggregated_insights}"
    SYSTEM_ATOMIC_ACTION_PROMPT = "Assistant"
    SYSTEM_MASTER_ROADMAP_PROMPT = "Strategist"

# -------------------- 5. LOGGER SETUP --------------------
logger = logging.getLogger(__name__)
if not logger.handlers:
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

def get_enabler_full_name(enabler_code: str, lang: str = "th") -> str:
    """
    ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡πá‡∏°‡∏Ç‡∏≠‡∏á Enabler ‡∏ï‡∏≤‡∏°‡∏£‡∏´‡∏±‡∏™ (‡πÄ‡∏ä‡πà‡∏ô "KM" ‚Üí "‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ")
    
    Args:
        enabler_code (str): ‡∏£‡∏´‡∏±‡∏™ enabler ‡πÄ‡∏ä‡πà‡∏ô "KM", "CG", "SP"
        lang (str): "th" ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢, "en" ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏© (default: "th")
    
    Returns:
        str: ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡πá‡∏° ‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏∑‡∏ô‡∏£‡∏´‡∏±‡∏™‡πÄ‡∏î‡∏¥‡∏°‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏ö
    
    Example:
        get_enabler_full_name("KM") ‚Üí "‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ"
        get_enabler_full_name("CG", "en") ‚Üí "Corporate Governance"
    """
    code = str(enabler_code).upper().strip()
    if lang.lower() == "th":
        return SEAM_ENABLER_FULL_NAME_TH.get(code, code)
    return SEAM_ENABLER_FULL_NAME_EN.get(code, code)


def get_pdca_goal_for_level(level: int) -> str:
    """
    ‡∏î‡∏∂‡∏á‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á Maturity Level ‡∏ô‡∏±‡πâ‡∏ô ‡πÜ
    
    Args:
        level (int): ‡∏£‡∏∞‡∏î‡∏±‡∏ö 1-5
    
    Returns:
        str: ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ ‡∏´‡∏£‡∏∑‡∏≠ "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢" ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏ö
    
    Example:
        get_pdca_goal_for_level(5) ‚Üí "‡πÄ‡∏ô‡πâ‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏±‡πà‡∏á‡∏¢‡∏∑‡∏ô ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÄ‡∏ä‡∏¥‡∏á‡∏£‡∏∏‡∏Å...‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏ô‡πÅ‡∏ö‡∏ö (Role Model)"
    """
    return MATURITY_LEVEL_GOALS.get(int(level), "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢")
    
def _static_worker_process(worker_input_tuple: Tuple) -> Any:
    """
    [ULTIMATE WORKER v2026.1.23] Isolated Execution with Evidence Streaming
    ---------------------------------------------------------------------
    - ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°‡πÉ‡∏´‡∏°‡πà‡πÅ‡∏¢‡∏Å‡∏Ç‡∏≤‡∏î‡∏à‡∏≤‡∏Å‡∏Å‡∏±‡∏ô (Zero Memory Leak)
    - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô Evidence Map ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏¢‡∏±‡∏á Main Process ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏≤‡∏¢
    - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏∞‡∏ö‡∏ö Error Handling ‡πÅ‡∏ö‡∏ö‡∏Å‡πâ‡∏≠‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Object) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏û‡∏±‡∏á
    """
    # 1. üìÇ PATH & ENVIRONMENT SETUP
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
    if project_root not in sys.path:
        sys.path.append(project_root)
        
    worker_logger = logging.getLogger(f"Worker_{os.getpid()}")

    # 2. üì¶ UNPACKING ARGS
    try:
        (
            sub_criteria_data, enabler, target_level, mock_mode, 
            evidence_map_path, model_name, temperature,
            min_retry_score, max_retrieval_attempts, document_map, 
            action_plan_model, year, tenant
        ) = worker_input_tuple
        
        sub_id = sub_criteria_data.get('sub_id', 'UNKNOWN')
    except Exception as e:
        return {"error": f"Worker unpacking failed: {str(e)}", "status": "failure"}, {}

    # 3. üèóÔ∏è RECONSTRUCT ISOLATED ENGINE
    try:
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Config ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Å‡∏¥‡∏à
        worker_config = AssessmentConfig(
            enabler=enabler,
            tenant=tenant,
            year=int(year) if year else None,     
            target_level=target_level,
            mock_mode=mock_mode,
            model_name=model_name, 
            temperature=temperature,
            min_retry_score=min_retry_score,            
            max_retrieval_attempts=max_retrieval_attempts 
        )

        # ‡∏Ñ‡∏∑‡∏ô‡∏ä‡∏µ‡∏û Engine (‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤ Class SEAMPDCAEngine ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ö document_map)
        worker_instance = SEAMPDCAEngine(
            config=worker_config, 
            evidence_map_path=evidence_map_path, 
            llm_instance=None,              
            vectorstore_manager=None,       
            logger_instance=worker_logger,
            document_map=document_map,      
            ActionPlanActions=action_plan_model
        )
    except Exception as e:
        worker_logger.error(f"‚ùå Worker initialization failed: {e}")
        return {"sub_id": sub_id, "error": f"Init Error: {str(e)}"}, {}

    # 4. ‚ö° EXECUTE & STREAM BACK RESULTS
    try:
        # üéØ ‡∏à‡∏∏‡∏î‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: _run_sub_criteria_assessment_worker ‡∏ï‡πâ‡∏≠‡∏á‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ (result, evidence_mem)
        # evidence_mem ‡∏Ñ‡∏∑‡∏≠ dict ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö chunks/docs ‡∏ó‡∏µ‡πà AI ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ä‡πâ‡∏à‡∏£‡∏¥‡∏á
        result, worker_evidence_mem = worker_instance._run_sub_criteria_assessment_worker(sub_criteria_data)
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ result ‡∏°‡∏µ sub_id ‡∏ï‡∏¥‡∏î‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Merge ‡∏ñ‡∏π‡∏Å‡∏ï‡∏±‡∏ß
        if isinstance(result, dict) and 'sub_id' not in result:
            result['sub_id'] = sub_id

        return result, worker_evidence_mem
        
    except Exception as e:
        worker_logger.error(f"‚ùå Execution error for {sub_id}: {str(e)}")
        # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÅ‡∏ö‡∏ö Fallback ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Main Process ‡πÑ‡∏°‡πà‡∏Ñ‡πâ‡∏≤‡∏á
        return {
            "sub_id": sub_id,
            "error": str(e),
            "status": "failed",
            "is_passed": False,
            "score": 0.0,
            "reason": f"Worker Exception: {str(e)}"
        }, {}
        
# =================================================================
# Configuration Class
# =================================================================
@dataclass
class AssessmentConfig:
    """Configuration for the SEAM PDCA Assessment Run."""
    
    # ------------------ 1. Assessment Context ------------------
    enabler: str = None
    tenant: str = None
    year: int = None  # üëà ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å DEFAULT_YEAR ‡πÄ‡∏õ‡πá‡∏ô None
    target_level: int = MAX_LEVEL
    mock_mode: str = "none" # 'none', 'random', 'control'
    force_sequential: bool = field(default=False) # Flag ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏£‡∏±‡∏ô‡πÅ‡∏ö‡∏ö Sequential

    # ------------------ 2. LLM Configuration (Configurable) ------------------
    # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Default ‡∏à‡∏≤‡∏Å global_vars.py
    model_name: str = DEFAULT_LLM_MODEL_NAME 
    temperature: float = LLM_TEMPERATURE

    # ------------------ 3. Adaptive RAG Retrieval Configuration ------------------
    # üü¢ NEW: ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô Rerank ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Adaptive Loop (MIN_RETRY_SCORE)
    # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Default 0.65 ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏û‡∏ö‡πÉ‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î Logic
    min_retry_score: float = MIN_RETRY_SCORE
    # üü¢ NEW: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≠‡∏ö‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á Adaptive RAG Loop (MAX_RETRIEVAL_ATTEMPTS)
    # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Default 3
    max_retrieval_attempts: int = 3
    
    # ------------------ 4. Export Configuration ------------------
    export_output: bool = field(default=False) # Flag ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏¥‡∏î/‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£ Export ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    export_path: str = "" # Path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå Export (‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å)


# =================================================================
# SEAM Assessment Engine (PDCA Focused) - Full Revise v2026
# =================================================================
class SEAMPDCAEngine:
    
    def __init__(
        self, 
        config: AssessmentConfig,
        llm_instance: Any = None, 
        logger_instance: logging.Logger = None,
        rag_retriever_instance: Any = None,
        doc_type: str = None, 
        vectorstore_manager: Optional['VectorStoreManager'] = None,
        evidence_map_path: Optional[str] = None,
        document_map: Optional[Dict[str, str]] = None,
        is_parallel_all_mode: bool = False,
        sub_id: str = 'all',
        record_id: Optional[str] = None, # üëà ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ
        **kwargs  
    ):
        """
        [ULTIMATE REVISE v2026.3] SEAM Assessment Engine Constructor
        ‡πÄ‡∏ô‡πâ‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏ô‡∏ó‡∏≤‡∏ô (Resilience), ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢ (Sanity Check) ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        """
        # -------------------------------------------------------
        # 1. Logger Setup (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö 1 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£ Trace Error)
        # -------------------------------------------------------
        self.doc_type = doc_type or getattr(config, 'doc_type', EVIDENCE_DOC_TYPES)
        clean_dt = str(self.doc_type).strip().lower()
        log_year = config.year if clean_dt == EVIDENCE_DOC_TYPES.lower() else "general"

        if logger_instance is not None:
            self.logger = logger_instance
        else:
            self.logger = logging.getLogger(__name__).getChild(
                f"Engine|{config.enabler}|{config.tenant}/{log_year}"
            )

        self.logger.info(f"üöÄ Initializing SEAMPDCAEngine: {config.enabler} ({config.tenant}/{log_year})")

        # -------------------------------------------------------
        # 2. Patch: Sanity Check & Core Configuration
        # -------------------------------------------------------
        self.config = config
        
        # [CRITICAL PATCH] ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏á‡∏≤‡∏ô
        if not self.config.enabler or not self.config.tenant:
            self.logger.critical("‚ùå Mandatory Config Missing: enabler and tenant must be provided!")
            raise ValueError("Enabler and Tenant are required for SEAMPDCAEngine.")

        self.enabler = config.enabler
        self.tenant_id = config.tenant  # üëà ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ: ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ _get_semantic_tag ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ
        self.year = config.year        # üëà ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ: ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô
        self.target_level = config.target_level
        self.sub_id = sub_id
        self.llm = llm_instance
        self.vectorstore_manager = vectorstore_manager
        self.is_parallel_all_mode = is_parallel_all_mode
        self.is_sequential = getattr(config, 'force_sequential', True)
        self.results = {}

        # -------------------------------------------------------
        # 3. Database & System Warm-up
        # -------------------------------------------------------
        try:
            init_db()  # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô 'no such table' ‡∏´‡∏£‡∏∑‡∏≠ Schema mismatch
            self.logger.info("üìÇ Database Schema verified/initialized.")
        except Exception as e:
            self.logger.error(f"‚ö†Ô∏è DB Init Warning: {e} (Check if tables already exist)")

        self.record_id = record_id # üëà ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡πÉ‡∏ô instance

        # -------------------------------------------------------
        # 4. Data Loading (Rubric, Rules & Policies)
        # -------------------------------------------------------
        # ‡πÇ‡∏´‡∏•‡∏î Rubric ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô AttributeError ‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô
        self.rubric = self._load_rubric()
        
        # ‡πÇ‡∏´‡∏•‡∏î‡∏Å‡∏é Contextual Rules ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö PDCA Logic
        self.contextual_rules_map = self._load_contextual_rules_map()
        
        self.retry_policy = RetryPolicy(
            max_attempts=3,
            base_delay=2.0,
            jitter=True,
            exponential_backoff=True,
        )

        # ‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô (Global Constants)
        self.RERANK_THRESHOLD = RERANK_THRESHOLD
        self.MAX_EVI_STR_CAP = MAX_EVI_STR_CAP

        # -------------------------------------------------------
        # 5. Mapping & Evidence Persistence Setup
        # -------------------------------------------------------
        # 5.1 Evidence Mapping (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏∑‡∏ö‡∏Ñ‡πâ‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á)
        self.evidence_map = {}
        if clean_dt == EVIDENCE_DOC_TYPES.lower():
            self.evidence_map_path = evidence_map_path or get_evidence_mapping_file_path(
                tenant=self.config.tenant, 
                year=self.config.year, 
                enabler=self.enabler
            )
            self.evidence_map = self._load_evidence_map()
            self.logger.info(f"üìä Evidence Mapping: Loaded {len(self.evidence_map)} keys.")

        # 5.2 Document Mapping (ID -> Filename ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏≥ Report/Audit)
        loaded_map = document_map or {}
        if not loaded_map:
            is_evi_mode = (clean_dt == EVIDENCE_DOC_TYPES.lower())
            mapping_path = get_mapping_file_path(
                self.doc_type, 
                tenant=self.config.tenant, 
                year=self.config.year if is_evi_mode else None,
                enabler=self.enabler if is_evi_mode else None
            )

            if os.path.exists(mapping_path):
                try:
                    with open(mapping_path, 'r', encoding='utf-8') as f:
                        raw_data = json.load(f)
                    loaded_map = {k: v.get("file_name", k) for k, v in raw_data.items()}
                    self.logger.info(f"üéØ Document Mapping: Loaded {len(loaded_map)} entries.")
                except Exception as e:
                    self.logger.error(f"‚ùå Error parsing mapping file: {e}")

        self.doc_id_to_filename_map = loaded_map
        self.document_map = loaded_map
        self.temp_map_for_save = {}

        # -------------------------------------------------------
        # 6. Lazy Engine Initialization (VSM & LLM)
        # -------------------------------------------------------
        if self.llm is None: self._initialize_llm_if_none()
        if self.vectorstore_manager is None: self._initialize_vsm_if_none()

        # ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ ID Mapping ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô VectorStore (‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô)
        if self.vectorstore_manager:
            try:
                self.vectorstore_manager._load_doc_id_mapping()
            except: pass

        # -------------------------------------------------------
        # 7. Function Registry (Pointers)
        # -------------------------------------------------------
        self.standard_audit_agent = evaluate_with_llm        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö L3-L5 (‡πÄ‡∏Ç‡πâ‡∏°‡∏á‡∏ß‡∏î)
        self.foundation_coaching_agent = evaluate_with_llm_low_level # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö L1-L2 (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥)

        # ‡∏ú‡∏π‡∏Å‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à (The Router)
        self.assessment_router = self.evaluate_pdca
        
        # Registry ‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏ï‡∏≤‡∏°‡πÄ‡∏î‡∏¥‡∏°
        self.rag_retriever = retrieve_context_with_filter
        # --- [PATCH v2026.1.17] State Management Initialization ---
        self.final_subcriteria_results = []
        self.total_stats = {}
        self.raw_llm_results = []
        self.level_details_map = {} # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ L1-L5 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥ Gap Analysis

        # --- [CRITICAL PATCH v2026.2.20] Core Assessment States ---
        # ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ô‡∏µ‡πâ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ _is_previous_level_passed ‡∏à‡∏∞‡∏°‡∏≤‡∏≠‡πà‡∏≤‡∏ô‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà
        self.assessment_results_map = {} 
        
        # ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÄ‡∏Å‡πá‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏™‡∏∞‡∏™‡∏° ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÉ‡∏ô Step 2: Baseline Hydration
        self.previous_levels_evidence = [] 
        
        # ‡πÄ‡∏Å‡πá‡∏ö Mapping ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ Level ‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏° Sub-ID
        self.level_evidence_cache = {}
        self._cumulative_rules_cache: Dict[Tuple[str, int], Dict[str, Any]] = {}

        self.logger.info(f"‚úÖ Engine Initialized: Ready for Assessment (Sub-ID: {self.sub_id})")
    

    # =================================================================
    # DB Proxy Methods (Enhanced v2026)
    # =================================================================
    def db_update_task_status(self, message: str, progress: Optional[int] = None, status: str = "RUNNING"):
        """
        Enhanced Wrapper ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞
        - ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏™‡πà‡∏á record_id ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á (‡πÉ‡∏ä‡πâ self.current_record_id ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥)
        - ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏™‡πà‡∏á progress ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÄ‡∏â‡∏û‡∏≤‡∏∞ message (‡∏Ñ‡∏á‡∏Ñ‡πà‡∏≤ % ‡πÄ‡∏î‡∏¥‡∏°‡πÑ‡∏ß‡πâ)
        """
        # 1. ‡∏î‡∏∂‡∏á record_id ‡∏à‡∏≤‡∏Å instance ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≤‡∏°
        rid = getattr(self, 'current_record_id', None) or getattr(self, 'record_id', None)
        if not rid: 
            return

        try:
            # 2. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï
            # ‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà progress ‡πÄ‡∏õ‡πá‡∏ô None ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å memory ‡∏´‡∏£‡∏∑‡∏≠ database 
            # ‡πÅ‡∏ï‡πà‡πÇ‡∏î‡∏¢‡∏õ‡∏Å‡∏ï‡∏¥ database.db_update_task_status ‡∏Ñ‡∏ß‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≤‡∏°‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô None ‡πÉ‡∏´‡πâ‡πÄ‡∏≠
            
            # 3. ‡∏™‡πà‡∏á‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï
            update_db_core(
                record_id=rid, 
                progress=progress, 
                message=message, 
                status=status
            )
            
            self.logger.debug(f"[DB-PROGRESS] {rid}: {progress if progress is not None else 'KEEP'}% - {message}")
            
        except Exception as e:
            self.logger.error(f"‚ùå DB Update Error for {rid}: {e}")

    def _initialize_llm_if_none(self):
        """Initializes LLM instance if self.llm is None."""
        if self.llm is None:
            self.logger.warning("‚ö†Ô∏è Initializing LLM: model=%s, temperature=%s", 
                                self.config.model_name, self.config.temperature)
            try:
                # üü¢ FIX: Import ‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ create_llm_instance
                from models.llm import create_llm_instance 
                self.llm = create_llm_instance( 
                    model_name=self.config.model_name,
                    temperature=self.config.temperature
                )
                self.logger.info("‚úÖ LLM Instance created successfully: %s (Temp: %s)", 
                                 self.config.model_name, self.config.temperature)
            except Exception as e:
                self.logger.error(f"FATAL: Could not initialize LLM: {e}")
                raise

    def _initialize_vsm_if_none(self):
        """
        Initializes VectorStoreManager with Smart Year Selection.
        - If Evidence: Priority 1 = Specific Year, Priority 2 = Root Fallback.
        - If Document: Priority 1 = Root (General), Priority 2 = Specific Year Fallback.
        """
        if self.vectorstore_manager is not None:
            return

        # 1. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ç‡∏≠‡∏á DocType
        clean_dt = str(self.doc_type or getattr(self.config, 'doc_type', EVIDENCE_DOC_TYPES)).strip().lower()
        is_evidence = (clean_dt == EVIDENCE_DOC_TYPES.lower())
        
        self.logger.info(f"üöÄ Loading vectorstore(s) for DocType: '{clean_dt}' (Mode: {'Evidence' if is_evidence else 'General'})")

        try:
            target_enabler = str(self.enabler).lower() if self.enabler else None
            
            # üéØ [SMART SELECTION] ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å
            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô evidence ‡πÉ‡∏´‡πâ‡∏´‡∏≤‡∏ï‡∏≤‡∏°‡∏õ‡∏µ (2568) ‡∏Å‡πà‡∏≠‡∏ô ‡πÅ‡∏ï‡πà‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô document ‡πÉ‡∏´‡πâ‡∏´‡∏≤‡∏ó‡∏µ‡πà Root (None) ‡∏Å‡πà‡∏≠‡∏ô
            primary_year = self.config.year if is_evidence else None
            secondary_year = None if is_evidence else self.config.year

            # 2. First Attempt: ‡πÇ‡∏´‡∏•‡∏î‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
            self.vectorstore_manager = load_all_vectorstores(
                doc_types=[clean_dt], 
                enabler_filter=target_enabler, 
                tenant=self.config.tenant, 
                year=primary_year       
            )
            
            def count_retrievers(vsm):
                if vsm and hasattr(vsm, '_multi_doc_retriever') and vsm._multi_doc_retriever:
                    return len(vsm._multi_doc_retriever._all_retrievers)
                return 0

            len_retrievers = count_retrievers(self.vectorstore_manager)
            
            # 3. Second Attempt (Fallback): ‡∏ñ‡πâ‡∏≤‡∏´‡∏≤‡πÅ‡∏ö‡∏ö‡πÅ‡∏£‡∏Å‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ ‡πÉ‡∏´‡πâ‡∏™‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏´‡∏≤‡∏≠‡∏µ‡∏Å‡πÅ‡∏ö‡∏ö
            if len_retrievers == 0:
                lookup_label = f"year {secondary_year}" if secondary_year else "tenant root"
                self.logger.info(f"‚ö†Ô∏è No collections found in primary path, searching in {lookup_label}...")
                
                self.vectorstore_manager = load_all_vectorstores(
                    doc_types=[clean_dt], 
                    enabler_filter=target_enabler, 
                    tenant=self.config.tenant, 
                    year=secondary_year       
                )
                len_retrievers = count_retrievers(self.vectorstore_manager)

            # 4. Post-Load Process
            if self.vectorstore_manager and len_retrievers > 0:
                self.vectorstore_manager._load_doc_id_mapping() 
                self.logger.info(f"‚úÖ MultiDocRetriever loaded with {len_retrievers} collections.") 
            else:
                # 5. Final Error Handling
                expected_p = f"data_store/{self.config.tenant}/vectorstore/{primary_year or 'root'}"
                self.logger.error(f"‚ùå FATAL: 0 vector store collections loaded. Please check folder: {expected_p}")
                raise ValueError(f"No vector collections found for '{target_enabler}' in {self.config.tenant}")

        except Exception as e:
            self.logger.error(f"‚ùå FATAL: Could not initialize VectorStoreManager: {str(e)}")
            raise

    # -------------------- Contextual Rules Handlers (FIXED) --------------------
    def _load_contextual_rules_map(self) -> Dict[str, Any]:
        """
        [FINAL REVISED v2026.5] ‡πÇ‡∏´‡∏•‡∏î Contextual Rules ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
        - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö multi-tenant ‡πÅ‡∏•‡∏∞ multi-enabler ‡πÄ‡∏ï‡πá‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö
        - ‡∏°‡∏µ fallback ‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
        - ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Maturity (L1-L5) ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏Ç‡πâ‡∏°‡∏á‡∏ß‡∏î
        - Logging ‡∏ó‡∏µ‡πà‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏ä‡πà‡∏ß‡∏¢ debug
        - ‡πÑ‡∏°‡πà raise exception (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ engine ‡∏•‡πâ‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏£‡∏∞‡∏ö‡∏ö)
        """
        # 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á path ‡∏ï‡∏≤‡∏° tenant + enabler
        try:
            filepath = get_contextual_rules_file_path(
                tenant=self.config.tenant,
                enabler=self.enabler
            )
            self.logger.debug(f"üîç Attempting to load contextual rules from: {filepath}")
        except Exception as e:
            self.logger.error(f"‚ùå FATAL: Failed to generate rules file path: {e}")
            return {"_enabler_defaults": {}}

        # 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if not os.path.exists(filepath):
            self.logger.warning(
                f"‚ö†Ô∏è Contextual Rules file not found: {filepath}\n"
                f"   ‚Üí Using only global defaults (if available from fallback)."
            )
            return {"_enabler_defaults": {}}

        # 3. ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞ parse JSON
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
        except json.JSONDecodeError as e:
            self.logger.error(f"‚ùå JSON Decode Error in {filepath}: {e} (line {e.lineno}, col {e.colno})")
            return {"_enabler_defaults": {}}
        except Exception as e:
            self.logger.error(f"‚ùå Unexpected error reading {filepath}: {e}")
            return {"_enabler_defaults": {}}

        # 4. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        if not isinstance(data, dict):
            self.logger.error(f"‚ùå Invalid rules format in {filepath}: Expected dict, got {type(data)}")
            return {"_enabler_defaults": {}}

        # 5. ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô criteria ‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Maturity Structure
        sub_criteria_keys = [k for k in data.keys() if not k.startswith("_")]
        num_criteria = len(sub_criteria_keys)

        if num_criteria == 0:
            self.logger.warning(f"‚ö†Ô∏è No sub-criteria found in {filepath} (only defaults).")
        else:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ Maturity Levels (L1, L2, ...) ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            sample_sub = sub_criteria_keys[0]
            sub_data = data[sample_sub]
            level_keys = [k for k in sub_data.keys() if k.startswith("L") and len(k) >= 2]
            
            if level_keys:
                detected_levels = sorted(level_keys)
                self.logger.info(
                    f"‚úÖ Maturity-Based Rules loaded successfully!\n"
                    f"   File: {filepath}\n"
                    f"   Criteria: {num_criteria} (e.g., {sample_sub})\n"
                    f"   Levels detected: {', '.join(detected_levels)}"
                )
            else:
                self.logger.warning(
                    f"‚ö†Ô∏è Rules for '{sample_sub}' in {filepath} do not contain Maturity Levels (L1-L5).\n"
                    f"   Falling back to flat structure or defaults."
                )

        # 6. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö _enabler_defaults
        if "_enabler_defaults" in data:
            defaults = data["_enabler_defaults"]
            if isinstance(defaults, dict) and any(k.endswith("_keywords") for k in defaults.keys()):
                self.logger.info("‚úÖ Global PDCA Keywords (_enabler_defaults) loaded.")
            else:
                self.logger.warning("‚ö†Ô∏è _enabler_defaults exists but has invalid structure.")
        else:
            self.logger.info("‚ÑπÔ∏è No _enabler_defaults section found. Using empty defaults.")

        # 7. ‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô data ‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
        self.logger.info(f"‚úÖ Contextual Rules fully loaded from {filepath} ({num_criteria} criteria).")
        return data
    
    def get_rule_content(self, sub_id: str, level: int, key_type: str):
        """
        ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏à‡∏≤‡∏Å Contextual Rules ‡πÅ‡∏ö‡∏ö‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ä‡∏±‡πâ‡∏ô
        Priority: Specific Level > Sub-ID Root > Global Defaults > Fallback
        """
        rule = self.contextual_rules_map.get(sub_id, {})
        level_key = f"L{level}"

        # 1. Specific Level (e.g., 1.2 -> L1 -> query_synonyms)
        if key_type in rule.get(level_key, {}):
            return rule[level_key][key_type]

        # 2. Sub-ID Root (e.g., 1.2 -> query_synonyms)
        if key_type in rule:
            return rule[key_type]

        # 3. Global Defaults (e.g., _enabler_defaults -> plan_keywords)
        defaults = self.contextual_rules_map.get("_enabler_defaults", {})
        if key_type in defaults:
            return defaults[key_type]

        # 4. Fallback (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Error ‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ Key)
        fallbacks = {
            "require_phase": ["P", "D"], # Default ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
            "must_include_keywords": [],
            "plan_keywords": [],
            "do_keywords": [],
            "check_keywords": [],
            "act_keywords": [],
            "query_synonyms": ""
        }
        return fallbacks.get(key_type, "")

    def get_cumulative_rules_cached(self, sub_id: str, level: int) -> Dict[str, Any]:
        """
        Return cumulative rules with per-engine caching.
        Cache key: (sub_id, level)
        """
        key = (sub_id, level)

        if key not in self._cumulative_rules_cache:
            self._cumulative_rules_cache[key] = self.get_cumulative_rules(
                sub_id=sub_id,
                current_level=level
            )

        return self._cumulative_rules_cache[key]


    def get_cumulative_rules(self, sub_id: str, current_level: int) -> Dict[str, Any]:
        """
        [FINAL REVISED v2026.1.20] - SMART ACCUMULATION & ROBUST MAPPING
        ------------------------------------------------------------------
        - ‡∏™‡∏∞‡∏™‡∏° Keywords (PDCA) ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏à‡∏≤‡∏Å L1 ‡∏à‡∏ô‡∏ñ‡∏∂‡∏á‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
        - **Robust Synonym Split**: ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥‡∏î‡πâ‡∏ß‡∏¢ ‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á, ‡∏Ñ‡∏≠‡∏°‡∏°‡πà‡∏≤, ‡πÅ‡∏•‡∏∞‡πÄ‡∏ã‡∏°‡∏¥‡πÇ‡∏Ñ‡∏•‡∏≠‡∏ô
        - **Auto-Fallback Phases**: ‡πÄ‡∏ï‡∏¥‡∏° Required Phases ‡πÉ‡∏´‡πâ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡∏´‡∏≤‡∏Å Config ‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏
        - **Case-Insensitive**: ‡∏õ‡∏£‡∏±‡∏ö Keywords ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏û‡∏¥‡∏°‡∏û‡πå‡πÄ‡∏•‡πá‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Match
        """
        defaults = self.contextual_rules_map.get('_enabler_defaults', {})
        sub_rules = self.contextual_rules_map.get(sub_id, {})
        
        # 1. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° OrderedDict ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÅ‡∏•‡∏∞‡∏Å‡∏≥‡∏à‡∏±‡∏î‡∏Ñ‡∏≥‡∏ã‡πâ‡∏≥
        cum_keywords = {
            "plan": OrderedDict((k.lower(), None) for k in defaults.get('plan_keywords', [])),
            "do":   OrderedDict((k.lower(), None) for k in defaults.get('do_keywords', [])),
            "check": OrderedDict((k.lower(), None) for k in defaults.get('check_keywords', [])),
            "act":  OrderedDict((k.lower(), None) for k in defaults.get('act_keywords', []))
        }
        
        cum_must_include = OrderedDict()
        required_phases = set()
        level_specific_instructions = {}
        source_levels = []

        # 2. ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏™‡∏∞‡∏™‡∏°‡∏Å‡∏é‡∏à‡∏≤‡∏Å L1 ‡∏à‡∏ô‡∏ñ‡∏∂‡∏á‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
        for lv in range(1, current_level + 1):
            lv_key = f"L{lv}"
            level_rule = sub_rules.get(lv_key, {})
            
            if not level_rule:
                continue
            
            source_levels.append(lv)

            # A) ‡∏î‡∏∂‡∏á query_synonyms ‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô must_include (Robust Split)
            synonyms_str = level_rule.get('query_synonyms', "")
            if synonyms_str:
                # ‡πÉ‡∏ä‡πâ Regex ‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á "‡∏Ñ‡∏≥1 ‡∏Ñ‡∏≥2" ‡∏´‡∏£‡∏∑‡∏≠ "‡∏Ñ‡∏≥1,‡∏Ñ‡∏≥2" ‡∏´‡∏£‡∏∑‡∏≠ "‡∏Ñ‡∏≥1;‡∏Ñ‡∏≥2"
                words = re.split(r'[,\s;|]+', synonyms_str)
                for word in words:
                    clean_word = word.strip().lower()
                    if clean_word:
                        cum_must_include[clean_word] = None

            # B) ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï PDCA Keywords (‡∏™‡∏∞‡∏™‡∏°‡∏ï‡πà‡∏≠‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤)
            for phase, key_name in [("plan", "plan_keywords"), ("do", "do_keywords"),
                                ("check", "check_keywords"), ("act", "act_keywords")]:
                new_kws = level_rule.get(key_name, [])
                for kw in new_kws:
                    cum_keywords[phase][kw.lower()] = None
            
            # C) ‡∏™‡∏∞‡∏™‡∏° Required Phases
            if 'require_phase' in level_rule:
                # ‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ‡∏ó‡∏±‡πâ‡∏á List ["P", "D"] ‡∏´‡∏£‡∏∑‡∏≠ String "P,D"
                phases = level_rule['require_phase']
                if isinstance(phases, str):
                    phases = re.split(r'[,\s]+', phases)
                required_phases.update([p.upper() for p in phases if p])
            
            # D) ‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ (Specific Instructions)
            specific = level_rule.get('specific_contextual_rule')
            if specific:
                level_specific_instructions[lv] = specific.strip()

        # 3. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å (Finalize & Fallback)
        result_keywords = {phase: list(cum_keywords[phase].keys()) for phase in cum_keywords}
        
        # Smart Fallback ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Required Phases (‡∏ñ‡πâ‡∏≤‡πÉ‡∏ô Config ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏∞‡∏ö‡∏∏‡πÄ‡∏•‡∏¢)
        final_phases = sorted(list(required_phases))
        if not final_phases:
            if current_level <= 3: final_phases = ["P", "D"]
            elif current_level == 4: final_phases = ["P", "D", "C"]
            else: final_phases = ["P", "D", "C", "A"]

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Instructions String ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Prompt
        instructions_lines = [f"‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {sub_id} ‡∏£‡∏∞‡∏î‡∏±‡∏ö Maturity L{current_level}:"]
        for lv in sorted(level_specific_instructions.keys()):
            icon = "üéØ" if lv == current_level else "‚úÖ"
            instructions_lines.append(f"{icon} [Level {lv}]: {level_specific_instructions[lv]}")

        # 4. Logging & Return
        self.logger.info(
            f"üöÄ [RULE_CUMULATIVE] {sub_id} L{current_level} | "
            f"Must-Include: {len(cum_must_include)} words | "
            f"Phases: {final_phases}"
        )

        return {
            "plan_keywords": result_keywords["plan"],
            "do_keywords": result_keywords["do"],
            "check_keywords": result_keywords["check"],
            "act_keywords": result_keywords["act"],
            "required_phases": final_phases,
            "must_include_keywords": list(cum_must_include.keys()),
            "level_specific_instructions": level_specific_instructions,
            "all_instructions": "\n".join(instructions_lines),
            "source_summary": f"Accumulated from levels: {source_levels}"
        }

    def _check_contextual_rule_condition(
        self, 
        condition: Dict[str, Any], 
        sub_id: str, 
        level: int, 
        top_evidences: List[Dict[str, Any]]
    ) -> bool:
        """
        [SIMPLIFIED v2026] ‡πÅ‡∏Ñ‡πà log ‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô continuity + min evidence
        ‡πÑ‡∏°‡πà block ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô (return True ‡πÄ‡∏™‡∏°‡∏≠)
        """
        if level > 1:
            prev_level = level - 1
            is_prev_passed = False
            if hasattr(self, 'level_details_map') and str(prev_level) in self.level_details_map:
                is_prev_passed = self.level_details_map[str(prev_level)].get('is_passed', False)
            
            if not is_prev_passed:
                self.logger.warning(f"‚ö†Ô∏è [GAP DETECTED] L{prev_level} not passed for {sub_id} L{level} - may affect validity")

        min_docs = condition.get('min_evidences', 1)
        if len(top_evidences) < min_docs:
            self.logger.warning(f"‚ö†Ô∏è [LOW EVIDENCE] {sub_id} L{level}: {len(top_evidences)} docs (required: {min_docs})")

        return True  # ‡πÑ‡∏°‡πà block

    def post_process_llm_result(
        self,
        llm_output: Any,
        level: int,
        sub_id: str = None,
        contextual_config: Dict = {},
        top_evidences: List[Dict[str, Any]] = []
    ) -> Dict[str, Any]:
        """
        [FULL REVISED v2026.1.24 - OPTIMIZED & ROBUST]
        - Robust PDCA key extraction (handle _Do_Score, _Check_Score, etc.)
        - Explicit 'is_passed' boost to 1.2 if LLM confirms pass
        - Rerank Safety Net threshold reduced to 0.82 for better rescue
        - Enhanced debug logging for key matching & rescue
        """
        log_prefix = f"Sub:{sub_id or '??'} L{level}"

        # 1. JSON Repair
        if isinstance(llm_output, tuple):
            llm_output = llm_output[0] if len(llm_output) > 0 else {}
        
        if isinstance(llm_output, str):
            try:
                cleaned = re.sub(r'```json\s*|\s*```', '', llm_output)
                cleaned = re.sub(r',\s*([}\]])', r'\1', cleaned.strip())
                cleaned = cleaned.encode('utf-8', 'ignore').decode('utf-8')
                llm_output = json.loads(cleaned)
            except Exception as e:
                self.logger.error(f"‚ùå [JSON FAILED] {log_prefix}: {str(e)}")
                return {"is_passed": False, "score": 0.0, "reason": "AI Response Format Error"}

        if not isinstance(llm_output, dict):
            return {"is_passed": False, "score": 0.0, "reason": "Invalid Output Format"}

        # 2. Required Phases Setup
        required_phases = contextual_config.get("required_phases", [])
        if not required_phases:
            if level <= 3: required_phases = ["P", "D"]
            elif level == 4: required_phases = ["P", "D", "C"]
            else: required_phases = ["P", "D", "C", "A"]

        must_include_keywords = contextual_config.get("must_include_keywords", [])

        # 3. Robust PDCA Extraction + Smart Rescue
        pdca_results = {"P": 0.0, "D": 0.0, "C": 0.0, "A": 0.0}
        reason_content = str(llm_output.get('reason', '')).lower()

        for phase in ["P", "D", "C", "A"]:
            possible_keys = [
                f"{phase}_Plan_Score",
                f"{phase}_Do_Score",
                f"{phase}_Check_Score",
                f"{phase}_Act_Score",
                f"Extraction_{phase}_Score",
                f"score_{phase.lower()}",
                f"{phase}_Score"
            ]
            
            val = 0.0
            for k in possible_keys:
                if k in llm_output:
                    try:
                        val = float(llm_output[k])
                        self.logger.debug(f"üü¢ [KEY-FOUND] {log_prefix} Phase {phase}: {k} = {val}")
                        break
                    except ValueError:
                        continue
            
            score = min(val, 2.0)

            # Smart Rescue by keywords
            phase_kws = contextual_config.get(f"{phase.lower()}_keywords", [])
            all_critical = list(set(phase_kws + must_include_keywords))
            extraction_text = str(llm_output.get(f"Extraction_{phase}", "")).lower()
            
            if score < 1.0 and any(kw.lower() in (reason_content + extraction_text) for kw in all_critical):
                old_score = score
                score = 1.5
                self.logger.info(f"üõ°Ô∏è [RESCUE] {log_prefix} Phase {phase} boosted from {old_score} to {score} by keyword match.")

            pdca_results[phase] = score

        # 4. Adaptive Normalization + Explicit Pass Boost
        sum_required = sum(pdca_results[p] for p in required_phases)
        max_required = len(required_phases) * 2.0
        normalized_score = round((sum_required / max_required) * 2.0 if max_required > 0 else 0.0, 2)

        # üü¢ [ADJUSTED] Respect explicit 'is_passed' ‚Üí force to 1.2 if LLM confirms pass
        explicit_pass = llm_output.get("is_passed", False)
        if explicit_pass and normalized_score < 1.2:
            normalized_score = 1.2
            self.logger.info(f"üõ°Ô∏è [EXPLICIT-PASS BOOST] {log_prefix}: LLM says pass ‚Üí set score to 1.2")

        # 5. Rerank Safety Net (Adjusted threshold)
        max_rerank = max([ev.get('relevance_score', 0.0) for ev in top_evidences]) if top_evidences else 0.0
        # üü¢ [ADJUSTED] Lower threshold from 0.88 ‚Üí 0.82 for better rescue
        is_conflict = (normalized_score < 1.2) and (max_rerank > 0.82)

        if is_conflict:
            normalized_score = 1.2
            llm_output["is_force_pass"] = True
            self.logger.warning(f"üö® [CONFLICT RESOLVED] {log_prefix} Force Passed due to high Rerank ({max_rerank:.2f})")

        # 6. Final Packaging
        is_passed = normalized_score >= 1.2
        missing_phases = [p for p in required_phases if pdca_results[p] < 1.0]
        
        coaching = str(llm_output.get("coaching_insight") or llm_output.get("‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞") or "").strip()
        if missing_phases:
            coaching = f"‚ö†Ô∏è ‡∏Ç‡∏≤‡∏î‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏ô‡πÄ‡∏ü‡∏™: {', '.join(missing_phases)}. {coaching}"
        if is_conflict:
            coaching += " (‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏ú‡πà‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏™‡∏π‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©)"

        return {
            "score": normalized_score,
            "is_passed": is_passed,
            "pdca_breakdown": pdca_results,
            "reason": llm_output.get("reason", "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•"),
            "summary_thai": llm_output.get("summary_thai") or llm_output.get("‡∏ö‡∏ó‡∏™‡∏£‡∏∏‡∏õ") or "",
            "coaching_insight": coaching,
            "required_phases": required_phases,
            "missing_phases": missing_phases,
            "needs_human_review": is_conflict or llm_output.get("consistency_check") == False,
            "is_force_pass": is_conflict
        }

    def _expand_context_with_neighbor_pages(self, top_evidences: List[Any], collection_name: str) -> List[Dict[str, Any]]:
        """
        [REVISED v2026.3.5] - ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Type Mismatch & Standardize Output
        - ‡πÅ‡∏õ‡∏•‡∏á‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á (‡∏ó‡∏±‡πâ‡∏á‡∏Ç‡∏≠‡∏á‡πÄ‡∏Å‡πà‡∏≤‡πÅ‡∏•‡∏∞‡∏Ç‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà) ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô Dictionary ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô
        - ‡∏£‡∏±‡∏Å‡∏©‡∏≤ PDCA Rescue Tagging ‡πÉ‡∏´‡πâ‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠‡πÉ‡∏´‡πâ _get_pdca_blocks_from_evidences
        """
        if not self.vectorstore_manager or not top_evidences:
            return top_evidences

        # üõ°Ô∏è STEP A: Standardize ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏¥‡∏°‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô Dict ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Å‡πà‡∏≠‡∏ô
        standardized_evidences = []
        for d in top_evidences:
            if hasattr(d, 'page_content'):
                standardized_evidences.append({
                    "text": d.page_content,
                    "page_content": d.page_content,
                    "metadata": getattr(d, 'metadata', {}),
                    "rerank_score": getattr(d, 'metadata', {}).get('rerank_score', 0.5) # Fallback score
                })
            elif isinstance(d, dict):
                standardized_evidences.append(deepcopy(d))
            else:
                continue

        expanded_evidences = list(standardized_evidences)
        seen_keys = set()
        added_pages = 0
        MAX_PAGES_PER_SUB = 12
        
        # Triggers... (‡∏Ñ‡∏á‡πÄ‡∏î‡∏¥‡∏°)
        strategic_triggers = ["‡∏ß‡∏¥‡∏™‡∏±‡∏¢‡∏ó‡∏±‡∏®‡∏ô‡πå", "‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢", "‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á", "‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏Å", "‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå", "‡∏™‡∏≤‡∏£‡∏à‡∏≤‡∏Å", "‡∏Ñ‡∏≥‡∏ô‡∏≥"]
        check_triggers = ["‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏∂‡∏á‡∏û‡∏≠‡πÉ‡∏à", "‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô", "‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•", "‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î", "‡∏ú‡∏•‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô", "kpi", "score", "‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô"]
        action_triggers = ["‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£", "‡∏à‡∏±‡∏î‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°", "‡∏≠‡∏ö‡∏£‡∏°", "‡∏à‡∏±‡∏î‡∏ó‡∏≥", "‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà", "‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà", "‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏â‡∏ö‡∏±‡∏ö‡∏ó‡∏µ‡πà"]

        for doc in standardized_evidences: # ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏à‡∏≤‡∏Å‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà Standardized ‡πÅ‡∏•‡πâ‡∏ß
            if added_pages >= MAX_PAGES_PER_SUB: break

            meta = doc.get('metadata', {})
            text = (doc.get('text', '') or "").lower()
            
            filename = meta.get("source") or meta.get("source_filename") or "Unknown File"
            doc_uuid = meta.get("stable_doc_uuid") or meta.get("doc_id")
            if not doc_uuid: continue

            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤ (‡∏Ñ‡∏á‡πÄ‡∏î‡∏¥‡∏°)
            try:
                current_page_str = str(meta.get("page_label", meta.get("page", "1")))
                current_page = int("".join(filter(str.isdigit, current_page_str)))
            except: continue

            # Advanced Offset Strategy... (‡∏Ñ‡∏á‡πÄ‡∏î‡∏¥‡∏°)
            offsets = []
            if any(k in text for k in strategic_triggers): offsets.extend([-1, 1, 2])
            if any(k in text for k in check_triggers): offsets.extend([-2, -1, 1, 2, 3])
            if any(k in text for k in action_triggers): offsets.extend([-1, 1])
            if not offsets: offsets = [1]

            for offset in sorted(list(set(offsets))):
                target_page = current_page + offset
                if target_page < 1 or target_page == current_page: continue
                
                cache_key = f"{doc_uuid}_{target_page}"
                if cache_key in seen_keys: continue
                seen_keys.add(cache_key)

                neighbor_chunks = self.vectorstore_manager.get_chunks_by_page(
                    collection_name=collection_name,
                    stable_doc_uuid=doc_uuid,
                    page_label=str(target_page)
                )

                if neighbor_chunks:
                    for nc in neighbor_chunks:
                        nc_text = nc.page_content.lower()
                        assigned_tag = "Support" if offset < 0 else "Detail"
                        
                        # üè∑Ô∏è Smart PDCA Rescue Tagging
                        if any(k in nc_text for k in check_triggers): assigned_tag = "Check" # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô Act/Check ‡πÄ‡∏õ‡πá‡∏ô Check ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô
                        elif any(k in nc_text for k in action_triggers): assigned_tag = "Do"
                        elif any(k in nc_text for k in strategic_triggers): assigned_tag = "Plan"

                        # üõ°Ô∏è ‡∏™‡∏£‡πâ‡∏≤‡∏á Dict ‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡πÅ‡∏ö‡∏ö
                        fixed_metadata = nc.metadata.copy() if hasattr(nc, 'metadata') else {}
                        fixed_metadata.update({
                            "stable_doc_uuid": doc_uuid,
                            "page_label": str(target_page),
                            "source": filename,
                            "is_supplemental": True,
                            "pdca_tag": assigned_tag 
                        })

                        expanded_evidences.append({
                            "text": nc.page_content, # ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏û‡∏µ‡∏¢‡∏ß‡πÜ (‡πÑ‡∏°‡πà‡πÄ‡∏ï‡∏¥‡∏° Prefix ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ LLM ‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢)
                            "page_content": nc.page_content,
                            "metadata": fixed_metadata,
                            "pdca_tag": assigned_tag,
                            "is_supplemental": True,
                            "rerank_score": float(doc.get('rerank_score', 0.5)) * 0.9,
                            "source": filename # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏µ‡∏¢‡πå source ‡∏ï‡∏£‡∏á‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏ß
                        })
                    added_pages += 1

        return expanded_evidences
    
    def _resolve_evidence_filenames(self, evidence_entries: List[Any]) -> List[Dict[str, Any]]:
        """
        [ULTIMATE SAFE v2026.3] - ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Error 'str' has no attribute 'get'
        - ‡∏Å‡∏≤‡∏£‡∏±‡∏ô‡∏ï‡∏µ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ List of Dictionary ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
        - ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô String ‡∏´‡∏£‡∏∑‡∏≠ None
        """
        resolved_entries = []
        if not evidence_entries:
            return []

        for entry in evidence_entries:
            # üõ°Ô∏è GUARD 1: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô Dictionary ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            if not isinstance(entry, dict):
                # ‡∏ñ‡πâ‡∏≤‡∏´‡∏•‡∏∏‡∏î‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô String ‡∏´‡∏£‡∏∑‡∏≠ Langchain Document ‡πÉ‡∏´‡πâ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÅ‡∏õ‡∏•‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πâ‡∏≤‡∏°
                if hasattr(entry, 'page_content'): # ‡∏Å‡∏£‡∏ì‡∏µ‡πÄ‡∏õ‡πá‡∏ô Document Object
                    entry = {
                        "content": entry.page_content,
                        "metadata": getattr(entry, 'metadata', {}),
                        "doc_id": getattr(entry, 'metadata', {}).get('doc_id', str(hash(entry.page_content)))
                    }
                else:
                    self.logger.warning(f"‚ö†Ô∏è Skipping non-dict evidence: {type(entry)}")
                    continue

            resolved_entry = deepcopy(entry)
            doc_id = resolved_entry.get("doc_id", "")
            
            # üõ°Ô∏è GUARD 2: Metadata Check
            meta = resolved_entry.get("metadata", {})
            if not isinstance(meta, dict): meta = {}
            
            # ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏´‡∏•‡πà‡∏á
            meta_filename = meta.get("source") or meta.get("source_filename") or meta.get("filename")
            content_raw = resolved_entry.get('content') or resolved_entry.get('text', '')
            page_label = str(meta.get("page_label") or meta.get("page") or meta.get("page_number") or "N/A")

            # 1. AI Generated / Missing Content Case
            if not content_raw:
                # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ ‡πÅ‡∏ï‡πà‡∏°‡∏µ ID ‡πÉ‡∏´‡πâ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô Reference ‡∏ß‡πà‡∏≤‡∏á
                resolved_entry["filename"] = "MISSING-CONTENT"
                resolved_entry["display_source"] = f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ (ID: {doc_id})"
            
            # 2. Match ‡πÉ‡∏ô Map
            elif doc_id in self.doc_id_to_filename_map:
                mapped_name = self.doc_id_to_filename_map[doc_id]
                resolved_entry["filename"] = mapped_name
                resolved_entry["display_source"] = f"{os.path.basename(mapped_name)} (‡∏´‡∏ô‡πâ‡∏≤ {page_label})"
            
            # 3. Match ‡πÉ‡∏ô Metadata
            elif meta_filename:
                clean_name = os.path.basename(str(meta_filename))
                resolved_entry["filename"] = clean_name
                resolved_entry["display_source"] = f"{clean_name} (‡∏´‡∏ô‡πâ‡∏≤ {page_label})"

            # 4. Fallback ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
            else:
                short_id = str(doc_id)[:8] if doc_id else "UNKNOWN"
                resolved_entry["filename"] = f"DOC-{short_id}"
                resolved_entry["display_source"] = f"‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á {short_id} (‡∏´‡∏ô‡πâ‡∏≤ {page_label})"

            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ Key ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ñ‡∏£‡∏ö‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏û‡∏¥‡πà‡∏°
            if 'content' not in resolved_entry and content_raw:
                resolved_entry['content'] = content_raw

            resolved_entries.append(resolved_entry)
            
        return resolved_entries

    # ----------------------------------------------------------------------
    # üéØ FINAL FIX 2.3: Manual Map Reload Function (inside SEAMPDCAEngine)
    # ----------------------------------------------------------------------
    def _collect_previous_level_evidences(self, sub_id: str, current_level: int) -> Dict[str, List[Dict]]:
        """
        [REVISED v2026.1.18] - Robust Context Hydration
        - ‡∏î‡∏∂‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏ï‡πá‡∏°‡∏à‡∏≤‡∏Å VectorStore ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡πá‡∏ô Baseline ‡πÉ‡∏´‡πâ‡πÄ‡∏•‡πÄ‡∏ß‡∏•‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
        - ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏≤‡∏£ Match UUID ‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏ó‡∏∏‡∏Å Format (Strip dashes)
        """
        if getattr(self, 'is_parallel_all_mode', False):
            return {}

        collected = {}
        for key, ev_list in self.evidence_map.items():
            # ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏•‡πÄ‡∏ß‡∏•‡∏ó‡∏µ‡πà‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡πÉ‡∏ô Sub-Criteria ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
            if key.startswith(f"{sub_id}.L"):
                try:
                    level_num = int(key.split(".L")[-1])
                    if level_num < current_level:
                        collected[key] = ev_list
                except: continue

        if not collected: return {}

        # 1. ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏° Unique IDs (‡∏Ñ‡∏±‡∏î‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡∏¢‡∏∞)
        stable_ids = set()
        for ev_list in collected.values():
            for ev in ev_list:
                sid = ev.get("stable_doc_uuid") or ev.get("doc_id")
                if sid and str(sid).lower() not in ["n/a", "none", ""]:
                    stable_ids.add(str(sid))

        if not stable_ids: return collected

        # 2. Bulk Hydration (Query ‡∏ó‡∏µ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û)
        vsm = self.vectorstore_manager
        chunk_map = {}
        try:
            full_chunks = vsm.get_documents_by_id(list(stable_ids), self.doc_type, self.enabler)
            for chunk in full_chunks:
                m = chunk.metadata
                # ‡πÄ‡∏Å‡πá‡∏ö Map ‡∏ó‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö‡∏°‡∏µ‡∏Ç‡∏µ‡∏î‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡∏µ‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏ß‡∏£‡πå
                keys = [str(m.get(k)) for k in ["stable_doc_uuid", "doc_id", "chunk_uuid"] if m.get(k)]
                for k in keys:
                    chunk_map[k] = {"text": chunk.page_content, "metadata": m}
                    chunk_map[k.replace("-", "")] = {"text": chunk.page_content, "metadata": m}
        except Exception as e:
            self.logger.error(f"‚ùå Hydration VSM Error: {e}")
            return collected

        # 3. Restoration Loop
        restored_count = 0
        for key, ev_list in collected.items():
            for ev in ev_list:
                sid = str(ev.get("stable_doc_uuid") or ev.get("doc_id") or "")
                data = chunk_map.get(sid) or chunk_map.get(sid.replace("-", ""))

                if data:
                    ev.update({
                        "text": data["text"],
                        "metadata": data["metadata"],
                        "is_baseline": True
                    })
                    restored_count += 1
                else:
                    ev["is_baseline"] = False
                
        self.logger.info(f"‚úÖ Hydrated {restored_count} baseline chunks for {sub_id} L{current_level}")
        return collected

    def _get_contextual_rules_prompt(self, sub_id: str, level: int) -> str:
        """
        Retrieves the specific Contextual Rule prompt for a given Sub-Criteria and Level,
        ‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á‡∏Å‡∏≤‡∏£ Inject ‡∏Å‡∏é L5 ‡∏û‡∏¥‡πÄ‡∏®‡∏© ‡∏´‡∏≤‡∏Å Level == 5
        """
        sub_id_rules = self.contextual_rules_map.get(sub_id)
        rule_text = ""
        
        # 1. ‡∏î‡∏∂‡∏á‡∏Å‡∏é‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏¢‡πà‡∏≠‡∏¢ (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
        if sub_id_rules:
            level_key = f"L{level}"
            specific_rule = sub_id_rules.get(level_key)
            if specific_rule:
                rule_text += f"\n--- ‡∏Å‡∏é‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏¢‡πà‡∏≠‡∏¢ ({sub_id} L{level}) ---\n‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Ñ‡∏ß‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ: {specific_rule}\n"
        
        # 2. **INJECT L5 SPECIAL RULE (Safe Injection)**
        # ‡πÉ‡∏™‡πà‡∏Å‡∏é Bonus 2.0 ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô L5 ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏ö‡∏Å‡∏ß‡∏ô L3/L4
        if level == 5:
            l5_bonus_rule = """
            \n--- L5 SPECIAL RULE (Innovation & Sustainability) ---
            * **L5 PASS Condition (‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö):** ‡∏´‡∏≤‡∏Å Level ‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠ **L5** ‡∏ó‡πà‡∏≤‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏° PDCA (P+D+C+A) ‡∏ï‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô L3/L4 ‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏ô‡∏û‡∏ö‡∏Å‡πà‡∏≠‡∏ô (‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î 8.0)
            * **‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç Bonus 2.0:** ‡∏´‡∏≤‡∏Å‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏° PDCA ‡πÑ‡∏î‡πâ **‚â• 7.0** **‡πÅ‡∏•‡∏∞** ‡∏ó‡πà‡∏≤‡∏ô‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠ **‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 1 ‡∏ä‡∏¥‡πâ‡∏ô** ‡πÉ‡∏ô Context ‡∏ó‡∏µ‡πà‡πÅ‡∏™‡∏î‡∏á‡∏ñ‡∏∂‡∏á:
                * (a) ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏• KM / ‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏° (Innovation Award)
                * (b) ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡∏ó‡∏µ‡πà‡∏ß‡∏±‡∏î‡πÑ‡∏î‡πâ (ROI, Productivity, Cost Saving)
                * (c) ‡∏Å‡∏≤‡∏£‡πÄ‡∏ú‡∏¢‡πÅ‡∏û‡∏£‡πà/‡∏Å‡∏≤‡∏£‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å (External Recognition/Publication)
            * **‡πÉ‡∏´‡πâ‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡πÉ‡∏´‡πâ Bonus Score 2.0 ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ **Score ‚â• 9.0** ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡πâ‡∏á **is_passed=true** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏∞‡∏ó‡πâ‡∏≠‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏•‡∏¥‡∏® (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£ Reset ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô)
            """
            rule_text += l5_bonus_rule
            
        return rule_text

    def _load_rubric(self) -> Dict[str, Any]:
        """ Loads the SEAM rubric JSON file using path_utils. """
        
        # üéØ FIX: ‡πÉ‡∏ä‡πâ get_rubric_file_path ‡∏à‡∏≤‡∏Å path_utils ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Path ‡πÄ‡∏≠‡∏á
        filepath = None # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô UnboundLocalError
        
        try:
            # 1. ‡∏£‡∏±‡∏ö Path ‡∏à‡∏≤‡∏Å path_utils ‡∏ã‡∏∂‡πà‡∏á‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏ä‡∏µ‡πâ‡πÑ‡∏õ‡∏ó‡∏µ‡πà 'config/' ‡πÅ‡∏•‡πâ‡∏ß
            filepath = get_rubric_file_path(
                tenant=self.config.tenant,
                enabler=self.enabler
            )
        except Exception as e:
            # ‡∏î‡∏±‡∏Å‡∏à‡∏±‡∏ö Exception ‡∏´‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏î‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô path_utils
            self.logger.error(f"‚ùå FATAL: Error calling get_rubric_file_path: {e}")
            return {} 

        # 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if filepath is None or not os.path.exists(filepath):
            self.logger.error(f"‚ö†Ô∏è Rubric file not found at expected path: {filepath}")
            return {}

        # 3. ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå JSON
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
            self.logger.info(f"‚úÖ Rubric loaded successfully from: {filepath}")
            return data
        except json.JSONDecodeError:
            self.logger.error(f"‚ùå Error decoding Rubric JSON. File might be corrupted: {filepath}")
            return {}
        except Exception as e:
            self.logger.error(f"‚ùå Error loading Rubric file from {filepath}: {e}")
            return {}
    
    # -------------------- Helper Function for Map Processing --------------------
    def _get_level_order_value(self, level_str: str) -> int:
        """Converts Level string ('L1', 'L5') to an integer for comparison."""
        try:
            return int(level_str.upper().replace('L', ''))
        except:
            return 0

    def _clean_temp_entries(self, evidence_map: Dict[str, List[Any]]) -> Dict[str, List[Dict]]:
        """
        ‡∏Å‡∏£‡∏≠‡∏á TEMP-, HASH-, ‡πÅ‡∏•‡∏∞ Unknown ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å evidence map ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô 'str' object has no attribute 'get' ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ã‡πà‡∏≠‡∏°‡πÅ‡∏ã‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        """
        if not evidence_map or not isinstance(evidence_map, dict):
            return {}

        cleaned_map = {}
        total_removed = 0
        total_unknown_fixed = 0
        total_invalid_type = 0

        for key, entries in evidence_map.items():
            if not isinstance(entries, list):
                continue
                
            valid_entries = []
            for entry in entries:
                # üõ°Ô∏è Defense: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
                if not isinstance(entry, dict):
                    if isinstance(entry, str) and entry.strip():
                        # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏Å‡∏π‡πâ‡∏Ñ‡∏∑‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô string
                        entry = {"doc_id": entry, "filename": "Unknown", "relevance_score": 0.0}
                    else:
                        total_invalid_type += 1
                        continue

                doc_id = entry.get("doc_id")
                if doc_id is None:
                    total_removed += 1
                    continue
                
                doc_id_str = str(doc_id)

                # 1. ‡∏Å‡∏£‡∏≠‡∏á TEMP- ‡πÅ‡∏•‡∏∞ HASH-
                if doc_id_str.startswith("TEMP-") or doc_id_str.startswith("HASH-"):
                    total_removed += 1
                    continue

                # 2. ‡∏Å‡∏£‡∏≠‡∏á Unknown
                if not doc_id_str or doc_id_str.lower() == "unknown":
                    total_removed += 1
                    continue

                # 3. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Filename
                filename = str(entry.get("filename", "")).strip()
                if not filename or filename.lower() in ["unknown", "none", "unknown_file.pdf", "n/a"]:
                    short_id = doc_id_str[:8]
                    entry["filename"] = f"‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á_{short_id}.pdf"
                    total_unknown_fixed += 1
                else:
                    try:
                        entry["filename"] = os.path.basename(filename)
                    except:
                        entry["filename"] = filename

                valid_entries.append(entry)

            if valid_entries:
                cleaned_map[key] = valid_entries

        return cleaned_map

    
    def _clean_temp_entries(self, evidence_map: Dict[str, List[Any]]) -> Dict[str, List[Dict]]:
        """
        [ULTIMATE SANITIZER v2026.1.23]
        ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î Evidence Map ‡πÅ‡∏ö‡∏ö‡πÄ‡∏ö‡πá‡∏î‡πÄ‡∏™‡∏£‡πá‡∏à:
        1. ‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡∏¢‡∏∞ (TEMP-, HASH-, Unknown)
        2. ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ã‡πâ‡∏≥ (Deduplication) ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô Level ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
        3. ‡∏ã‡πà‡∏≠‡∏°‡πÅ‡∏ã‡∏° Metadata (Filename, Page)
        4. ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Type Error (Type-Safe Processing)
        """
        if not evidence_map or not isinstance(evidence_map, dict):
            return {}

        cleaned_map = {}
        stats = {"removed": 0, "fixed": 0, "dupes": 0}

        for key, entries in evidence_map.items():
            if not isinstance(entries, list):
                continue
                
            valid_entries = []
            seen_doc_ids = set() # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Deduplication ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô Key (Level) ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô

            for entry in entries:
                # --- üõ°Ô∏è ‡∏ä‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà 1: Type Validation ---
                if not isinstance(entry, dict):
                    if isinstance(entry, str) and len(entry.strip()) > 5:
                        # ‡∏Å‡∏£‡∏ì‡∏µ‡∏´‡∏•‡∏∏‡∏î‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô String (‡πÄ‡∏ä‡πà‡∏ô ID ‡∏´‡∏£‡∏∑‡∏≠ Content) ‡πÉ‡∏´‡πâ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á Dict ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
                        entry = {
                            "doc_id": entry.strip(), 
                            "filename": "Unknown_Reference.pdf", 
                            "relevance_score": 0.1,
                            "page": "N/A"
                        }
                    else:
                        stats["removed"] += 1
                        continue

                # --- üõ°Ô∏è ‡∏ä‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà 2: Garbage Filtering ---
                doc_id = entry.get("doc_id") or entry.get("chunk_uuid")
                if not doc_id:
                    stats["removed"] += 1
                    continue
                
                doc_id_str = str(doc_id).strip()

                # ‡∏Å‡∏£‡∏≠‡∏á‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á ‡∏´‡∏£‡∏∑‡∏≠ Keyword ‡∏Ç‡∏¢‡∏∞
                if doc_id_str.lower() in ["none", "unknown", "n/a", "", "null"]:
                    stats["removed"] += 1
                    continue

                # ‡∏Å‡∏£‡∏≠‡∏á Prefix ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß
                if doc_id_str.startswith(("TEMP-", "HASH-", "REF-")):
                    stats["removed"] += 1
                    continue

                # --- üõ°Ô∏è ‡∏ä‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà 3: Deduplication (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡∏ã‡πâ‡∏≥) ---
                if doc_id_str in seen_doc_ids:
                    stats["dupes"] += 1
                    continue
                seen_doc_ids.add(doc_id_str)

                # --- üõ°Ô∏è ‡∏ä‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà 4: Metadata Repair & Normalization ---
                # 1. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå (Filename)
                filename = str(entry.get("filename") or entry.get("source", "")).strip()
                if not filename or filename.lower() in ["unknown", "none", "n/a", "unknown_file.pdf"]:
                    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ ID ‡∏¢‡πà‡∏≠‡∏°‡∏≤‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏ó‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ User ‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÑ‡∏î‡πâ
                    short_id = doc_id_str[:8]
                    entry["filename"] = f"Reference_{short_id}.pdf"
                    stats["fixed"] += 1
                else:
                    # Clean path ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÅ‡∏Ñ‡πà‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå
                    try:
                        entry["filename"] = os.path.basename(filename)
                    except:
                        entry["filename"] = filename

                # 2. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô (Relevance Score)
                try:
                    score = float(entry.get("relevance_score", 0.0))
                    entry["relevance_score"] = max(0.0, min(1.0, score)) # ‡∏ö‡∏µ‡∏ö‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á 0-1
                except (ValueError, TypeError):
                    entry["relevance_score"] = 0.0

                # 3. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤ (Page)
                if "page" not in entry or entry["page"] is None:
                    entry["page"] = entry.get("page_label") or "N/A"

                valid_entries.append(entry)

            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Key ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û
            if valid_entries:
                # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á (‡∏à‡∏≤‡∏Å‡∏°‡∏≤‡∏Å‡πÑ‡∏õ‡∏ô‡πâ‡∏≠‡∏¢)
                valid_entries.sort(key=lambda x: x.get("relevance_score", 0.0), reverse=True)
                cleaned_map[key] = valid_entries

        self.logger.info(
            f"üßπ [CLEAN-MAP] Stats: Removed={stats['removed']}, Fixed={stats['fixed']}, Dupes={stats['dupes']}"
        )
        return cleaned_map

    def _save_evidence_map(self, map_to_save: Optional[Dict[str, Any]] = None):
        """
        [IRONCLAD FINAL v2026.1.18 ‚Äî Ultra Safe Edition]
        - Load-Merge-Save Pattern (‡πÑ‡∏°‡πà overwrite ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≤‡∏î)
        - Atomic Write + FileLock + Tempfile
        - Backup (.bak) ‡∏Å‡πà‡∏≠‡∏ô save ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á
        - Validate + Clean ID ‡πÄ‡∏Ç‡πâ‡∏°‡∏á‡∏ß‡∏î ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ç‡∏¢‡∏∞
        - Log ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô (‡∏à‡∏≥‡∏ô‡∏ß‡∏ô merge/‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï)
        - Skip ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà‡∏à‡∏£‡∏¥‡∏á ‡πÜ
        """
        try:
            map_file_path = get_evidence_mapping_file_path(
                tenant=self.config.tenant,
                year=self.config.year,
                enabler=self.enabler
            )
        except Exception as e:
            self.logger.critical(f"[EVIDENCE] FATAL: ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Path ‡πÑ‡∏î‡πâ: {e}")
            raise

        lock_path = map_file_path + ".lock"
        tmp_path = None
        backup_path = map_file_path + ".bak"  # Backup ‡∏Å‡πà‡∏≠‡∏ô save

        self.logger.info(f"[EVIDENCE] Preparing atomic save ‚Üí {map_file_path}")

        try:
            os.makedirs(os.path.dirname(map_file_path), exist_ok=True)

            # Backup ‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏¥‡∏°‡∏Å‡πà‡∏≠‡∏ô (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å!)
            if os.path.exists(map_file_path):
                try:
                    shutil.copy2(map_file_path, backup_path)
                    self.logger.debug(f"[EVIDENCE] Backup created: {backup_path}")
                except Exception as be:
                    self.logger.warning(f"[EVIDENCE] Backup failed (non-critical): {be}")

            with FileLock(lock_path, timeout=60):
                # STEP 1: ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤‡∏à‡∏≤‡∏Å Disk (Base) ‚Äî ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡πÄ‡∏™‡∏°‡∏≠
                final_map = self._load_evidence_map(is_for_merge=True) or {}
                self.logger.debug(f"[EVIDENCE] Loaded existing map: {len(final_map)} keys")

                # STEP 2: ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà (Incoming)
                incoming = {}
                if map_to_save is not None:
                    # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á Payload {"evidence_map": ...} ‡πÅ‡∏•‡∏∞ Dict ‡∏ï‡∏£‡∏á ‡πÜ
                    if isinstance(map_to_save, dict) and "evidence_map" in map_to_save:
                        incoming = map_to_save["evidence_map"]
                    else:
                        incoming = map_to_save
                else:
                    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏™‡πà‡∏á‡∏°‡∏≤ ‚Üí ‡πÉ‡∏ä‡πâ‡∏à‡∏≤‡∏Å memory ‡∏Ç‡∏≠‡∏á Engine
                    incoming = getattr(self, 'evidence_map', {}) or {}

                # Skip ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà‡∏à‡∏£‡∏¥‡∏á ‡πÜ
                if not incoming:
                    self.logger.info("[EVIDENCE] No new data incoming. Skipping write.")
                    return

                # STEP 3: Merge + Validate + Clean
                merged_new = 0
                updated_existing = 0

                for key, new_entries in incoming.items():
                    if not isinstance(new_entries, list) or not new_entries:
                        continue

                    # ‡∏î‡∏∂‡∏á entries ‡πÄ‡∏î‡∏¥‡∏° (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ ‚Üí ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà)
                    current = final_map.setdefault(key, [])

                    # Index ‡πÄ‡∏î‡∏¥‡∏°‡∏î‡πâ‡∏ß‡∏¢ Clean ID
                    entry_index = {}
                    for e in current:
                        if not isinstance(e, dict):
                            continue
                        raw_id = e.get("chunk_uuid") or e.get("doc_id") or "N/A"
                        clean_id = str(raw_id).replace("-", "").lower()
                        if clean_id not in ["na", "n/a", "fallback", "none", ""]:
                            entry_index[clean_id] = e

                    # ‡∏ô‡∏≥‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏Ç‡πâ‡∏≤ Merge
                    for new_e in new_entries:
                        if not isinstance(new_e, dict):
                            continue

                        raw_new_id = new_e.get("chunk_uuid") or new_e.get("doc_id") or "N/A"
                        clean_new_id = str(raw_new_id).replace("-", "").lower()

                        # Skip ‡∏Ç‡∏¢‡∏∞
                        if clean_new_id in ["na", "n/a", "fallback", "none", ""]:
                            continue

                        new_score = new_e.get("relevance_score", 0.0)

                        if clean_new_id not in entry_index:
                            entry_index[clean_new_id] = new_e
                            merged_new += 1
                        else:
                            old_e = entry_index[clean_new_id]
                            old_score = old_e.get("relevance_score", 0.0)

                            # ‡∏£‡∏±‡∏Å‡∏©‡∏≤ metadata ‡πÄ‡∏î‡∏¥‡∏°‡∏ñ‡πâ‡∏≤‡∏Ç‡∏≤‡∏î
                            if "page" not in new_e or new_e["page"] in ["N/A", None]:
                                new_e["page"] = old_e.get("page")
                            if "page_label" not in new_e:
                                new_e["page_label"] = old_e.get("page_label")

                            if new_score >= old_score:
                                entry_index[clean_new_id] = new_e
                                updated_existing += 1

                    # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Å‡∏•‡∏±‡∏ö
                    final_map[key] = list(entry_index.values())

                # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£ merge ‡∏à‡∏£‡∏¥‡∏á ‡πÜ ‚Üí skip
                if merged_new == 0 and updated_existing == 0:
                    self.logger.info("[EVIDENCE] No unique new/updated entries. Skipping write.")
                    return

                # STEP 4: Clean + Sort
                final_map = self._clean_temp_entries(final_map)
                for key, entries in final_map.items():
                    entries.sort(key=lambda x: x.get("relevance_score", 0.0), reverse=True)

                # STEP 5: Atomic Write
                with tempfile.NamedTemporaryFile(
                    mode='w', delete=False, encoding="utf-8", dir=os.path.dirname(map_file_path)
                ) as tmp_file:
                    cleaned_data = self._clean_map_for_json(final_map)
                    json.dump(cleaned_data, tmp_file, indent=4, ensure_ascii=False)
                    tmp_path = tmp_file.name

                shutil.move(tmp_path, map_file_path)
                tmp_path = None

                total_keys = len(final_map)
                total_items = sum(len(v) for v in final_map.values())
                self.logger.info(
                    f"‚úÖ [EVIDENCE] SAVED SUCCESSFULLY! "
                    f"Keys: {total_keys} | Items: {total_items} | "
                    f"New: {merged_new} | Updated: {updated_existing}"
                )

        except Exception as e:
            self.logger.critical("[EVIDENCE] FATAL ERROR DURING ATOMIC SAVE")
            self.logger.exception(e)
            raise
        finally:
            # Cleanup
            if os.path.exists(lock_path):
                try:
                    os.unlink(lock_path)
                except:
                    pass
            if tmp_path and os.path.exists(tmp_path):
                try:
                    os.unlink(tmp_path)
                except:
                    pass

    def merge_evidence_mappings(self, results_list: List[Any]) -> Dict[str, List[Dict]]:
        """
        [ULTIMATE STABLE v2026.1.18 ‚Äî Key Mismatch Fix]
        - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Ñ‡∏µ‡∏¢‡πå 'evidence_sources' ‡∏à‡∏≤‡∏Å‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ï‡∏±‡∏ß‡πÉ‡∏´‡∏°‡πà
        - ‡∏ó‡∏≥ Deduplication ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ chunk_uuid/doc_id
        """
        merged_mapping = {}
        
        self.logger.info(f"üß¨ Starting to merge evidence mappings from {len(results_list)} levels...")

        for item in results_list:
            if not item: continue
            
            temp_map = {}
            
            # 1. ‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Mapping ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏ï‡∏≤‡∏°‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡πà‡∏≤‡∏á‡πÜ
            if isinstance(item, tuple) and len(item) == 2:
                temp_map = item[1]
            elif isinstance(item, dict):
                # [FIX] ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡πá‡∏Ñ 'evidence_sources' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å _run_single_assessment ‡πÑ‡∏î‡πâ
                if 'evidence_sources' in item:
                    level_key = f"{item.get('sub_id', 'Unknown')}_L{item.get('level', 0)}"
                    temp_map = {level_key: item['evidence_sources']}
                elif 'temp_map_for_level' in item:
                    level_key = f"{item.get('sub_id', 'Unknown')}_L{item.get('level', 0)}"
                    data = item.get('temp_map_for_level', [])
                    temp_map = {level_key: data} if isinstance(data, list) else {}
                elif 'evidence_mapping' in item:
                    temp_map = item['evidence_mapping']
                else:
                    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏µ‡∏¢‡πå‡∏û‡∏¥‡πÄ‡∏®‡∏© ‡πÉ‡∏´‡πâ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô dict ‡∏Ç‡∏≠‡∏á mapping ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
                    temp_map = item

            if not temp_map or not isinstance(temp_map, dict):
                continue

            # 2. ‡∏ß‡∏ô Loop ‡∏£‡∏ß‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ö‡∏Å‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏Å
            for level_key, evidence_list in temp_map.items():
                actual_list = []
                if isinstance(evidence_list, list):
                    actual_list = evidence_list
                elif isinstance(evidence_list, dict) and 'evidences' in evidence_list:
                    actual_list = evidence_list['evidences']
                else:
                    continue 
                
                if level_key not in merged_mapping:
                    merged_mapping[level_key] = []
                
                # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Set ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ã‡πâ‡∏≥ (Deduplication)
                existing_ids = set()
                for e in merged_mapping[level_key]:
                    eid = str(e.get('chunk_uuid') or e.get('doc_id') or "N/A").replace("-", "").lower()
                    existing_ids.add(eid)
                
                for new_ev in actual_list:
                    if not isinstance(new_ev, dict): continue
                    
                    raw_new_id = new_ev.get('chunk_uuid') or new_ev.get('doc_id') or "N/A"
                    clean_new_id = str(raw_new_id).replace("-", "").lower()

                    if clean_new_id in ["na", "n/a", "fallback", "none", "", "unknown"]:
                        continue

                    if clean_new_id not in existing_ids:
                        merged_mapping[level_key].append(new_ev)
                        existing_ids.add(clean_new_id)
        
        total_items = sum(len(v) for v in merged_mapping.values())
        self.logger.info(f"‚úÖ Merging completed. Levels: {len(merged_mapping)} | Total items: {total_items}")
        
        return merged_mapping

    def _load_evidence_map(self, is_for_merge: bool = False) -> Dict[str, List[Dict[str, Any]]]:
        """
        [REVISED v2026.1.16]
        - ‡πÄ‡∏û‡∏¥‡πà‡∏° cache ‡πÉ‡∏ô memory ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î I/O
        - Clean ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î‡∏°‡∏≤ (‡∏•‡∏ö fallback/na)
        """
        if hasattr(self, '_evidence_cache') and self._evidence_cache is not None:
            return deepcopy(self._evidence_cache)

        try:
            path = get_evidence_mapping_file_path(
                tenant=self.config.tenant,
                year=self.config.year,
                enabler=self.enabler
            )
        except Exception as e:
            self.logger.error(f"[EVIDENCE] FATAL: ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏î‡πâ: {e}")
            return {}

        if not os.path.exists(path):
            if not is_for_merge:
                self.logger.info("[EVIDENCE] No existing evidence map found ‚Äì starting fresh.")
            return {}

        try:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)

            # Clean ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î‡∏°‡∏≤
            for key in list(data.keys()):
                entries = data[key]
                cleaned = []
                for e in entries:
                    raw_id = e.get("chunk_uuid") or e.get("doc_id") or "N/A"
                    clean_id = str(raw_id).replace("-", "").lower()
                    if clean_id not in ["na", "n/a", "fallback", "none", ""]:
                        cleaned.append(e)
                data[key] = cleaned

            # Cache ‡πÉ‡∏ô memory
            self._evidence_cache = deepcopy(data)

            if not is_for_merge:
                total_items = sum(len(v) for v in data.values())
                self.logger.info(f"[EVIDENCE] Loaded evidence map: {len(data)} keys, {total_items} items from {path}")

            return data
        except Exception as e:
            self.logger.error(f"[EVIDENCE] Failed to load evidence map from {path}: {e}")
            return {}

    def _group_statements_by_sub_criteria(
        self,
        flat_statements: List[Dict[str, Any]]
    ) -> Dict[str, Dict[str, Any]]:
        """
        Group flattened rubric statements into Sub-Criteria bundles.

        Expected input item structure (PEA compatible):
        {
            "sub_id": "1.1",
            "sub_criteria_name": "...",
            "weight": 4,
            "levels": [
                { "level": 1, "statement": "..." },
                ...
            ]
        }
        """
        grouped: Dict[str, Dict[str, Any]] = {}

        for item in flat_statements:
            sub_id = item.get("sub_id")

            # üîí Minimal validation only (‡∏≠‡∏¢‡πà‡∏≤ strict ‡πÄ‡∏Å‡∏¥‡∏ô)
            if not sub_id:
                self.logger.warning(f"‚ö†Ô∏è Skip item without sub_id: {item}")
                continue

            levels = item.get("levels")

            if not isinstance(levels, list) or not levels:
                self.logger.warning(
                    f"‚ö†Ô∏è Skip sub_id {sub_id}: invalid or empty levels"
                )
                continue

            sub_id = str(sub_id)

            # ‚úÖ init group
            if sub_id not in grouped:
                grouped[sub_id] = {
                    "sub_id": sub_id,
                    "sub_criteria_name": item.get("sub_criteria_name", ""),
                    "weight": float(item.get("weight", 0.0)),
                    "levels": []
                }

            # ‚úÖ normalize each level
            for lv in levels:
                level_no = lv.get("level")
                statement = lv.get("statement")

                if level_no is None or not statement:
                    self.logger.warning(
                        f"‚ö†Ô∏è Skip invalid level in {sub_id}: {lv}"
                    )
                    continue

                grouped[sub_id]["levels"].append({
                    "level": int(level_no),
                    "statement": statement,
                    "keywords": lv.get("keywords", []),
                    "score_threshold": lv.get("score_threshold"),
                    "raw": lv
                })

        # üîí sort L1 ‚Üí L5 and drop empty subs
        cleaned_grouped = {}
        for sub_id, sub in grouped.items():
            if not sub["levels"]:
                self.logger.warning(
                    f"‚ö†Ô∏è Drop sub_id {sub_id}: no valid levels after normalization"
                )
                continue

            sub["levels"] = sorted(
                sub["levels"], key=lambda x: x["level"]
            )
            cleaned_grouped[sub_id] = sub

        self.logger.info(
            f"üì¶ Grouped rubric into {len(cleaned_grouped)} sub-criteria bundles"
        )

        return cleaned_grouped

    # -------------------- Statement Preparation & Filtering Helpers --------------------
    def _flatten_rubric_to_statements(self) -> List[Dict[str, Any]]:
        """
        Transforms the hierarchical rubric structure loaded in self.rubric
        into a flat list of statements ready for assessment.
        """
        if not self.rubric:
            self.logger.warning("Cannot flatten rubric: self.rubric is empty.")
            return []
            
        data = deepcopy(self.rubric)
        extracted_list = []
        
        if not isinstance(data, dict):
             self.logger.error("Rubric data structure is invalid (expected dict of criteria).")
             return []
             
        # for criteria_id, criteria_data in data.items():
        for criteria_id, criteria_data in data.get('criteria', {}).items():
            # üéØ FIX 1: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Criteria Data
            if not isinstance(criteria_data, dict):
                self.logger.warning(f"Skipping malformed criteria entry: {criteria_id} (not a dict).")
                continue
                
            sub_criteria_map = criteria_data.get('subcriteria', {})
            criteria_name = criteria_data.get('name')
            
            # üéØ FIX 2: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Sub-criteria Map
            if not isinstance(sub_criteria_map, dict):
                 self.logger.warning(f"Skipping criteria {criteria_id}: 'subcriteria' is not a dictionary.")
                 continue

            for sub_id, sub_data in sub_criteria_map.items():
                
                # üéØ FIX 3: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ sub_data ‡πÄ‡∏õ‡πá‡∏ô Dictionary ‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô TypeError)
                if not isinstance(sub_data, dict):
                    self.logger.warning(
                        f"Skipping malformed sub-criteria entry: {criteria_id}.{sub_id} "
                        f"is not a dictionary (found type: {type(sub_data).__name__})."
                    )
                    continue
                
                # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤ Metadata ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
                sub_data['criteria_id'] = criteria_id
                sub_data['criteria_name'] = criteria_name
                sub_data['sub_id'] = sub_id 
                sub_data['sub_criteria_name'] = sub_data.get('name', criteria_name + ' sub')
                if 'weight' not in sub_data:
                    sub_data['weight'] = criteria_data.get('weight', 0)
                extracted_list.append(sub_data)

        # Re-check and re-sort levels
        final_list = []
        for sub_criteria in extracted_list: 
            
            # üéØ FIX 4: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á Level ‡∏à‡∏≤‡∏Å Dict ‡∏ó‡∏µ‡πà‡∏°‡∏µ Key ‡πÄ‡∏õ‡πá‡∏ô String ‡πÄ‡∏õ‡πá‡∏ô List
            if "levels" in sub_criteria and isinstance(sub_criteria["levels"], dict):
                levels_list = []
                for level_str, statement in sub_criteria["levels"].items():
                    try:
                        # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÅ‡∏õ‡∏•‡∏á Level Key ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô Integer
                        level_int = int(level_str)
                        if isinstance(statement, str):
                            levels_list.append({"level": level_int, "statement": statement})
                        else:
                            self.logger.warning(f"Level {level_str} statement in {sub_criteria.get('sub_id')} is not a string.")
                    except ValueError:
                        self.logger.error(f"Invalid level key '{level_str}' found in {sub_criteria.get('sub_id', 'UNKNOWN_ID')}. Skipping.")
                        continue
                        
                sub_criteria["levels"] = levels_list
            
            # ‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏£‡∏∞‡∏î‡∏±‡∏ö
            if "levels" in sub_criteria and isinstance(sub_criteria["levels"], list):
                 sub_criteria["levels"].sort(key=lambda x: x.get("level", 0))
                 final_list.append(sub_criteria)
            else:
                 self.logger.warning(f"Sub-criteria {sub_criteria.get('sub_id', 'UNKNOWN_ID')} missing 'levels' list.")


        return final_list

    
    # -------------------- Evidence Classification Helper (Robust 2026) --------------------
    def _get_mapped_uuids_and_priority_chunks(
        self,
        sub_id: str,
        level: int,
        statement_text: str = "",
        level_constraint: Optional[Any] = None, # üü¢ ‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô Optional ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Error Missing Argument
        vectorstore_manager: Optional['VectorStoreManager'] = None,
        evidence_map: Optional[Dict] = None 
    ) -> Tuple[List[str], List[Dict]]:
        """
        [DYNAMIC CONTINUITY v2026.6.2] - ROBUST SIGNATURE
        ----------------------------------------------
        - Fix: ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÅ‡∏ö‡∏ö‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Error 'missing 1 required positional argument'
        - Logic: ‡∏î‡∏∂‡∏á Baseline ‡∏à‡∏≤‡∏Å Memory/Map ‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á (Inheritance)
        """
        from copy import deepcopy
        priority_chunks = []
        mapped_stable_ids = []

        # 1. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ä‡πâ Evidence Map (‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: 1. Argument ‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏°‡∏≤ -> 2. Class Attribute -> 3. Empty Dict)
        target_map = evidence_map if evidence_map is not None else getattr(self, 'evidence_map', {})

        # 2. üß† [AUTO-HISTORY & INHERITANCE]
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡∏ú‡πà‡∏≤‡∏ô‡πÉ‡∏ô Level ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏≥‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡πÉ‡∏ô Level ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
        for key, evidences in target_map.items():
            if key.startswith(f"{sub_id}.L") and isinstance(evidences, list):
                try:
                    lvl_in_key = int(key.split(".L")[-1])
                    # ‡∏Å‡∏é Inheritance: ‡πÉ‡∏ä‡πâ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏à‡∏≤‡∏Å Level ‡∏ó‡∏µ‡πà <= ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
                    if lvl_in_key <= level:
                        history_items = deepcopy(evidences)
                        for item in history_items:
                            item["is_baseline"] = True
                            # ‡∏ö‡∏π‡∏™‡∏ï‡πå‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡∏≠‡∏á RAG
                            item["rerank_score"] = max(item.get("rerank_score", 0.0), 0.85)
                        priority_chunks.extend(history_items)
                except (ValueError, IndexError):
                    continue

        # 3. üîç [SEMANTIC HINTING] ‡∏Å‡∏£‡∏ì‡∏µ L1 ‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡πÉ‡∏ô Map
        if not priority_chunks and level == 1:
            rule_config = getattr(self, 'contextual_rules_map', {}).get(sub_id, {}).get(str(level), {})
            hints = rule_config.get("plan_keywords", [])[:2]
            if hints and vectorstore_manager:
                self.logger.info(f"üîé L1 Discovery: Searching using hints: {hints}")
                try:
                    discovery_result = vectorstore_manager.quick_search(
                        query=f"{sub_id} {' '.join(hints)}",
                        top_k=5
                    )
                    for chunk in discovery_result:
                        chunk["rerank_score"] = 0.85 # ‡∏ö‡∏π‡∏™‡∏ï‡πå‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠‡∏à‡∏≤‡∏Å Keyword Rule
                        priority_chunks.append(chunk)
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Quick search failed: {e}")

        if not priority_chunks:
            return [], []

        # 4. üíß [ROBUST HYDRATION] ‡πÄ‡∏ï‡∏¥‡∏°‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ Full Text ‡∏à‡∏≤‡∏Å Vector DB
        try:
            priority_chunks = self._robust_hydrate_documents_for_priority_chunks(
                chunks_to_hydrate=priority_chunks,
                vsm=vectorstore_manager
            )
        except Exception as e:
            self.logger.error(f"‚ùå Hydration failed in priority module: {e}")

        # 5. üéØ [ID SYNC] ‡∏™‡∏Å‡∏±‡∏î UUID ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ Filter ‡∏Ç‡∏≠‡∏á Main Retriever
        seen_ids = set()
        for chunk in priority_chunks:
            sid = chunk.get("stable_doc_uuid") or chunk.get("doc_id")
            if sid and isinstance(sid, str):
                if sid not in seen_ids and len(sid) >= 32:
                    mapped_stable_ids.append(sid)
                    seen_ids.add(sid)

        self.logger.info(f"‚úÖ Continuity Ready: {len(priority_chunks)} priority chunks | Sub:{sub_id} L{level}")
        return mapped_stable_ids, priority_chunks

    def _save_level_evidences_and_calculate_strength(
        self,
        level_temp_map: List[Dict[str, Any]],
        sub_id: str,
        level: int,
        llm_result: Dict[str, Any],
        highest_rerank_score: float = 0.0
    ) -> float:
        """ 
        [IRONCLAD REVISE v2026.01.18] - ‡∏£‡∏∞‡∏ö‡∏ö‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏±‡πà‡∏ô (Strength)
        - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏∞‡∏ö‡∏ö Deduplication ‡∏î‡πâ‡∏ß‡∏¢ unique_key (Doc UUID + Chunk UUID)
        - ‡∏£‡∏∞‡∏ö‡∏ö Normalize PDCA Tag ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ (Plan -> P, Detail -> D)
        - ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Strength Score ‡πÇ‡∏î‡∏¢‡πÉ‡∏´‡πâ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å Rerank Score (60%) ‡πÅ‡∏•‡∏∞ PDCA Coverage (40%)
        - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ Retagging ‡∏Å‡∏£‡∏ì‡∏µ Tag ‡πÑ‡∏°‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô (Semantic Fallback)
        """

        map_key = f"{sub_id}.L{level}"
        new_evidence_list: List[Dict[str, Any]] = []
        seen_ids = set()

        self.logger.info(f"üíæ [EVI SAVE] Starting persist for {map_key} | Incoming: {len(level_temp_map)}")

        # üö© Configuration
        STANDARD_TAGS = {"P", "D", "C", "A"}
        PASS_STATUS = "PASS" if llm_result.get("is_passed", False) else "FAIL"

        for chunk in level_temp_map:
            if not chunk: continue

            # 1. ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á Dictionary ‡πÅ‡∏•‡∏∞ LangChain Document)
            meta = chunk.get("metadata", {}) if isinstance(chunk, dict) else getattr(chunk, "metadata", {})
            text = chunk.get("text") or chunk.get("page_content") if isinstance(chunk, dict) else getattr(chunk, "page_content", "")
            
            if not text or not str(text).strip():
                continue

            # 2. Stable ID Generation (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Duplicate Chunks)
            # ‡πÉ‡∏ä‡πâ SHA-256 ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ ID ‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡∏ï‡∏±‡∏ß ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô
            c_uuid = str(chunk.get("chunk_uuid") or meta.get("chunk_uuid") or hashlib.sha256(text.encode()).hexdigest()[:16])
            d_uuid = str(chunk.get("stable_doc_uuid") or meta.get("stable_doc_uuid") or "doc-unknown")
            unique_key = f"{d_uuid}:{c_uuid}"
            
            if unique_key in seen_ids:
                continue
            seen_ids.add(unique_key)

            # 3. ‚ú® PDCA TAG NORMALIZATION & GUARD
            # ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≥‡πÄ‡∏ï‡πá‡∏°‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡πà‡∏≤‡πÄ‡∏™‡∏£‡∏¥‡∏°‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô P, D, C, A ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô
            raw_tag = chunk.get("pdca_tag") or meta.get("pdca_tag") or "Other"
            
            if isinstance(raw_tag, str):
                u_tag = raw_tag.strip().upper()
                if u_tag.startswith("P") or "PLAN" in u_tag: pdca_tag = "P"
                elif u_tag.startswith("D") or "DETAIL" in u_tag or "SUPPORT" in u_tag: pdca_tag = "D"
                elif u_tag.startswith("C") or "CHECK" in u_tag: pdca_tag = "C"
                elif u_tag.startswith("A") or "ACT" in u_tag: pdca_tag = "A"
                else: pdca_tag = "Other"
            else:
                pdca_tag = "Other"

            # Semantic Fallback: ‡∏ñ‡πâ‡∏≤ Tag ‡∏¢‡∏±‡∏á‡πÄ‡∏õ‡πá‡∏ô Other ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ Logic ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
            if pdca_tag == "Other" and hasattr(self, '_get_semantic_tag'):
                pdca_tag = self._get_semantic_tag(text, sub_id, level)

            # 4. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Evidence Entry)
            source_raw = meta.get("source") or meta.get("source_filename") or "Unknown"
            entry = {
                "sub_id": sub_id,
                "level": level,
                "relevance_score": float(chunk.get("rerank_score") or chunk.get("score") or 0.5),
                "doc_id": d_uuid,
                "stable_doc_uuid": d_uuid,
                "chunk_uuid": c_uuid,
                "source": source_raw,
                "source_filename": os.path.basename(str(source_raw)),
                "page": str(meta.get("page_label") or meta.get("page") or "N/A"),
                "pdca_tag": pdca_tag,
                "text_preview": str(text)[:300].replace("\n", " ") + "...",
                "status": PASS_STATUS,
                "timestamp": datetime.now().isoformat(),
            }
            new_evidence_list.append(entry)

        # 5. ‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏Ç‡πá‡∏á‡πÅ‡∏Å‡∏£‡πà‡∏á (Strength)
        if new_evidence_list:
            # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï Memory Map (‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏™‡∏∞‡∏™‡∏°)
            if not hasattr(self, 'evidence_map'): self.evidence_map = {}
            self.evidence_map.setdefault(map_key, []).extend(deepcopy(new_evidence_list))
            
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏•‡∏á‡πÉ‡∏ô Map (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Level ‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ)
            if not hasattr(self, 'assessment_results_map'): self.assessment_results_map = {}
            self.assessment_results_map[map_key] = {
                "is_passed": llm_result.get("is_passed", False),
                "score": llm_result.get("score", 0.0),
                "strength": 0.0 # ‡∏à‡∏∞‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á
            }

            # üìä STRENGTH CALCULATION LOGIC
            # 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏° (Coverage): ‡∏û‡∏ö‡∏Å‡∏µ‡πà‡∏´‡∏°‡∏ß‡∏î‡πÉ‡∏ô P, D, C, A
            found_tags = {ev['pdca_tag'] for ev in new_evidence_list if ev['pdca_tag'] in STANDARD_TAGS}
            coverage_score = len(found_tags) / 4.0  # (0.0 - 1.0)
            
            # 2. ‡∏ú‡∏™‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô: Rerank Max (‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏à‡∏≤‡∏Å Vector) 60% + PDCA Coverage (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô) 40%
            final_strength = round((float(highest_rerank_score) * 0.6) + (coverage_score * 0.4), 2)
            
            # ‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏Ç‡πá‡∏á‡πÅ‡∏Å‡∏£‡πà‡∏á‡∏Å‡∏•‡∏±‡∏ö‡∏•‡∏á‡πÉ‡∏ô Result Map
            self.assessment_results_map[map_key]["strength"] = final_strength

            # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏ó‡∏≤‡∏á Log
            counts = {t: sum(1 for e in new_evidence_list if e['pdca_tag'] == t) for t in (list(STANDARD_TAGS) + ["Other"])}
            self.logger.info(
                f"‚úÖ [SAVED] {map_key}: {len(new_evidence_list)} items | "
                f"P:{counts['P']} D:{counts['D']} C:{counts['C']} A:{counts['A']} | "
                f"Final Strength: {final_strength:.2f}"
            )
            return final_strength
            
        return 0.0
    
    def get_actual_score(self, ev: Any) -> float:
        """
        [v2026.1 - ROBUST SCORING EXTRACTOR]
        ‡∏î‡∏∂‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô ‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö Dict ‡∏´‡∏£‡∏∑‡∏≠ Object
        ‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô 0.0 ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ã‡πà‡∏≠‡∏ô‡∏Ñ‡∏µ‡∏¢‡πå‡πÉ‡∏ô Metadata
        """
        if not ev:
            return 0.0

        # 1. ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ñ‡∏µ‡∏¢‡πå‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏ö‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö (‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç)
        score_keys = ["rerank_score", "score", "relevance_score"]
        
        # 2. ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö Top-level ‡∏Å‡πà‡∏≠‡∏ô
        for key in score_keys:
            # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á dict.get() ‡πÅ‡∏•‡∏∞ getattr() ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Document Object
            val = ev.get(key) if isinstance(ev, dict) else getattr(ev, key, None)
            if val is not None:
                try:
                    return float(val)
                except (ValueError, TypeError):
                    continue

        # 3. Fallback: ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏†‡∏≤‡∏¢‡πÉ‡∏ô Metadata
        meta = ev.get("metadata", {}) if isinstance(ev, dict) else getattr(ev, "metadata", {})
        if isinstance(meta, dict):
            for key in score_keys:
                val = meta.get(key)
                if val is not None:
                    try:
                        return float(val)
                    except (ValueError, TypeError):
                        continue

        return 0.0
    
    def _calculate_evidence_strength_cap(
        self,
        top_evidences: List[Any],
        level: int,
        highest_rerank_score: Optional[float] = None
    ) -> Dict[str, Any]:
        """
        [PROTECTED v2026.1.17 - REVISED]
        - ‡πÉ‡∏ä‡πâ get_actual_score ‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô
        - ‡πÅ‡∏¢‡∏Å Logic ‡∏Å‡∏≤‡∏£‡∏´‡∏≤ '‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•' ‡πÉ‡∏´‡πâ‡∏â‡∏•‡∏≤‡∏î‡∏Ç‡∏∂‡πâ‡∏ô
        - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏∞‡∏ö‡∏ö Logging ‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ Audit
        """
        try:
            # ‚öôÔ∏è Load Configuration
            threshold = getattr(self, "RERANK_THRESHOLD", 0.35)
            cap_value = getattr(self, "MAX_EVI_STR_CAP", 5.0)
            
            # 1. ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏°‡∏≤ (Baseline)
            max_score_found = 0.0
            try:
                if highest_rerank_score is not None:
                    max_score_found = float(highest_rerank_score)
            except (ValueError, TypeError):
                max_score_found = 0.0

            max_score_source = "Adaptive_RAG_Loop"
            
            if not isinstance(top_evidences, list):
                top_evidences = []

            # 2. Iterate ‡∏´‡∏≤‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ
            for idx, doc in enumerate(top_evidences, 1):
                # ‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡∏î‡∏∂‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏ö
                current_score = self.get_actual_score(doc)

                # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏à‡∏≠‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤ ‡πÉ‡∏´‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ß‡πâ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏≤
                if current_score > max_score_found:
                    max_score_found = current_score
                    
                    # --- ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö Robust ---
                    meta = doc.get("metadata", {}) if isinstance(doc, dict) else getattr(doc, "metadata", {})
                    if not isinstance(meta, dict): meta = {}
                    
                    max_score_source = (
                        meta.get("source_filename") or 
                        meta.get("file_name") or 
                        meta.get("source") or 
                        f"Doc_{idx}"
                    )

            # 3. Decision Logic (Gated Check)
            # ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏¢‡∏±‡∏á‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤ Threshold (0.35) -> ‡∏™‡∏±‡πà‡∏á Capped
            is_capped = max_score_found < threshold
            # ‡∏Ñ‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤ Prompt: 5.0 (‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô) ‡∏´‡∏£‡∏∑‡∏≠ 10.0 (‡∏°‡∏µ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏ú‡πà‡∏≤‡∏ô)
            max_evi_str_for_prompt = float(cap_value) if is_capped else 10.0

            # üìä Internal Audit Log
            status_icon = "üö® [CAPPED]" if is_capped else "‚úÖ [FULL-STRENGTH]"
            self.logger.info(
                f"{status_icon} L{level} | Best Score: {max_score_found:.4f} "
                f"from: '{os.path.basename(str(max_score_source))}' | Threshold: {threshold}"
            )

            return {
                "is_capped": bool(is_capped),
                "max_evi_str_for_prompt": float(max_evi_str_for_prompt),
                "top_score": round(float(max_score_found), 4),
                "max_score_source": str(max_score_source),
                "threshold_used": threshold
            }

        except Exception as e:
            self.logger.error(f"‚ùå [CRITICAL-CAP-ERROR] {e}", exc_info=True)
            # ‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡πÑ‡∏ß‡πâ‡∏Å‡πà‡∏≠‡∏ô: ‡∏ñ‡πâ‡∏≤ Error ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á Cap ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ LLM ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏î‡πâ
            return {
                "is_capped": False, 
                "max_evi_str_for_prompt": 10.0, 
                "top_score": 0.0, 
                "max_score_source": "Fallback-Error"
            }
       
    
    def _extract_strategic_gaps(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        [HELPER] ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏´‡∏≤‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á (Gaps) ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á‡∏Å‡∏±‡∏ö‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢
        ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡πÄ‡∏™‡∏ô‡∏≠ Coaching Insight ‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°
        """
        gaps = []
        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á‡∏Ç‡∏≠‡∏á Gap (‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ô‡πâ‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡∏Å‡πà‡∏≠‡∏ô)
        sorted_results = sorted(results, key=lambda x: x.get('score', 0.0))
        
        for res in sorted_results:
            sub_id = res.get('sub_id', 'Unknown')
            current_score = res.get('score', 0.0)
            passed = res.get('is_passed', False)
            
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô ‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤ 0.8 ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô Gap
            if not passed or current_score < 0.8:
                gap_info = {
                    "sub_id": sub_id,
                    "level": res.get('level'),
                    "current_score": current_score,
                    "impact": "High" if current_score < 0.5 else "Medium",
                    "reason": res.get('reason', '‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡∏µ‡πâ'),
                    "suggestion": res.get('coaching_insight', '‡∏Ñ‡∏ß‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏Ç‡∏≤‡∏î‡∏´‡∏≤‡∏¢‡∏ï‡∏≤‡∏° PDCA')
                }
                gaps.append(gap_info)
        
        # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ Gap ‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î 5 ‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å
        return gaps[:5]

    def calculate_audit_confidence(
        self,
        matched_chunks: List[Any],
        sub_id: str = "unknown",
        level: int = 1
    ) -> Dict[str, Any]:
        """
        [ULTIMATE AUDIT CONFIDENCE v2026.3.10 ‚Äì Real-Count Enabled]
        - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏•‡∏Ç (1) ‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á Raw Tags ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏ô‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà (Frequency Counting)
        - ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö PDCA ‡∏ú‡πà‡∏≤‡∏ô 3 ‡∏ä‡∏±‡πâ‡∏ô: Metadata -> Tagging -> Keyword Fallback
        - ‡∏õ‡∏£‡∏±‡∏ö Decision Matrix ‡πÉ‡∏´‡πâ‡∏™‡∏°‡∏î‡∏∏‡∏•‡∏Å‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á
        """
        if not matched_chunks:
            return {
                "level": "NONE", "reason": "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö", "source_count": 0,
                "coverage_ratio": 0.0, "pdca_found": [], "valid_chunks_count": 0,
                "traceability_score": 0.0, "recency_bonus": 0.0
            }

        # 1. Quality Gate: ‡∏Ñ‡∏±‡∏î‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Chunk ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏ô‡πâ‡∏ô‡πÜ (Relevance >= 0.40)
        valid_chunks = [doc for doc in matched_chunks if self.get_actual_score(doc) >= 0.40]
        valid_count = len(valid_chunks)

        if valid_count == 0:
            return {
                "level": "LOW", "reason": "‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û",
                "source_count": 0, "coverage_ratio": 0.0, "pdca_found": [], "valid_chunks_count": 0
            }

        # 2. Independence Check: ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        unique_sources = set()
        for doc in valid_chunks:
            meta = getattr(doc, 'metadata', {}) if hasattr(doc, 'metadata') else doc.get('metadata', {})
            src = next((meta.get(k) for k in ['source_filename', 'filename', 'source'] if meta.get(k)), None)
            if src:
                unique_sources.add(os.path.basename(str(src).strip()))
        
        independence_score = len(unique_sources)

        # 3. PDCA Detection (Revised: Multi-Tag Frequency Collection)
        all_detected_tags = [] # ‡πÄ‡∏Å‡πá‡∏ö List ‡∏Ç‡∏≠‡∏á Tag ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠‡πÉ‡∏ô‡∏ó‡∏∏‡∏Å Chunks (‡∏´‡πâ‡∏≤‡∏°‡∏ó‡∏≥ Unique)
        
        # ‡∏î‡∏∂‡∏á Keyword rules ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Fallback
        cum_rules = {}
        if hasattr(self, "get_cumulative_rules_cached"):
            try:
                loaded = self.get_cumulative_rules_cached(sub_id, level)
                if isinstance(loaded, dict):
                    cum_rules = loaded
                else:
                    self.logger.warning(
                        f"[PDCA] cumulative rules invalid type for {sub_id}-{level}"
                    )
            except Exception as e:
                self.logger.error(
                    f"[PDCA] failed to load cumulative rules for {sub_id}-{level}: {e}"
                )
                
        kw_map = {
            "P": [k.lower() for k in cum_rules.get('plan_keywords', [])],
            "D": [k.lower() for k in cum_rules.get('do_keywords', [])],
            "C": [k.lower() for k in cum_rules.get('check_keywords', [])],
            "A": [k.lower() for k in cum_rules.get('act_keywords', [])]
        }

        for doc in valid_chunks:
            meta = getattr(doc, 'metadata', {}) if hasattr(doc, 'metadata') else doc.get('metadata', {})
            tag = (getattr(doc, 'pdca_tag', None) or meta.get('pdca_tag') or meta.get('tag') or "").strip().upper()
            
            chunk_detected_phases = []
            
            # ‡∏ä‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà 1: ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏≤‡∏Å Tag ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
            if tag in ["P", "D", "C", "A"]:
                chunk_detected_phases.append(tag)
            
            # ‡∏ä‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà 2: Fallback Keyword Detection (‡∏ñ‡πâ‡∏≤ Tag ‡∏ß‡πà‡∏≤‡∏á ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á)
            text = (doc.get('text') or doc.get('page_content') or '').lower()
            for phase, kws in kw_map.items():
                if any(k in text for k in kws):
                    if phase not in chunk_detected_phases: # 1 chunk ‡∏ô‡∏±‡∏ö 1 ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå‡∏ï‡πà‡∏≠ 1 ‡πÄ‡∏ü‡∏™
                        chunk_detected_phases.append(phase)
            
            # ‡∏ô‡∏≥ Tags ‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠‡πÉ‡∏ô Chunk ‡∏ô‡∏µ‡πâ‡πÑ‡∏õ‡∏™‡∏∞‡∏™‡∏°‡∏£‡∏ß‡∏° (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÅ‡∏™‡∏î‡∏á‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô Log)
            all_detected_tags.extend(chunk_detected_phases)

        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Coverage Ratio (‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏°‡∏ß‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠ / 4)
        unique_found_phases = set(all_detected_tags)
        coverage_ratio = len(unique_found_phases) / 4.0

        # 4. Traceability & Recency Check
        traceable_count = 0
        recent_count = 0
        current_year = 2568 
        
        for doc in valid_chunks:
            meta = getattr(doc, 'metadata', {}) if hasattr(doc, 'metadata') else doc.get('metadata', {})
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏Ç‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏≤ (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏ó‡∏±‡πâ‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡∏∞‡∏´‡∏ô‡πâ‡∏≤)
            if any(meta.get(k) for k in ['page', 'page_label']) and any(meta.get(k) for k in ['source', 'filename']):
                traceable_count += 1
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏´‡∏°‡πà (Bonus 2-3 ‡∏õ‡∏µ‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á)
            year_val = str(meta.get('year') or meta.get('doc_year') or "")
            if not year_val: # Fallback search in filename
                year_match = re.search(r'(25[67]\d)', str(meta.get('source', '')))
                if year_match: year_val = year_match.group(1)
            
            if year_val.isdigit() and int(year_val) >= current_year - 2:
                recent_count += 1

        trace_score = traceable_count / valid_count if valid_count > 0 else 0
        recency_score = recent_count / valid_count if valid_count > 0 else 0

        # 5. Decision Matrix: ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏±‡πà‡∏ô (Confidence Level)
        if independence_score >= 8 and coverage_ratio >= 0.75:
            conf_level = "HIGH"
            reason = "‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏£‡∏ö‡∏ß‡∏á‡∏à‡∏£ PDCA ‡∏à‡∏≤‡∏Å‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏£‡∏∞‡∏ö‡∏∏‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô"
        elif independence_score >= 4 and coverage_ratio >= 0.50:
            conf_level = "MEDIUM"
            reason = "‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏°‡∏¥‡∏ï‡∏¥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô"
        else:
            conf_level = "LOW"
            reason = "‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏Ñ‡∏£‡∏ö‡∏ß‡∏á‡∏à‡∏£ PDCA ‡∏´‡∏£‡∏∑‡∏≠‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏û‡∏≠"

        # Penalty: ‡∏•‡∏î‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏´‡∏≤‡∏Å‡∏£‡∏∞‡∏ö‡∏∏‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö
        if trace_score < 0.6 and conf_level != "LOW":
            conf_level = "MEDIUM" if conf_level == "HIGH" else "LOW"
            reason += " (‡∏•‡∏î‡∏£‡∏∞‡∏î‡∏±‡∏ö‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå)"

        return {
            "level": conf_level,
            "reason": reason,
            "source_count": independence_score,
            "coverage_ratio": round(coverage_ratio, 3),
            "traceability_score": round(trace_score, 3),
            "recency_bonus": round(recency_score, 3),
            "valid_chunks_count": valid_count,
            "pdca_found": all_detected_tags  # <--- ‡∏™‡πà‡∏á LIST ‡∏î‡∏¥‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏à‡∏£‡∏¥‡∏á‡πÑ‡∏î‡πâ
        }

    def _get_level_constraint_prompt(self, sub_id: str, level: int, req_phases: list = None, spec_rule: str = None) -> str:
        """
        [ADAPTIVE AUDIT GUIDELINE v2026.1.19 - Concise & Stronger]
        - ‡πÄ‡∏ô‡πâ‡∏ô PDCA ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô + Substance over Form
        - ‡πÉ‡∏ä‡πâ fallback ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ req_phases
        - ‡∏™‡∏±‡πâ‡∏ô‡πÅ‡∏ï‡πà‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LLM
        """
        required_phases = req_phases or self.get_rule_content(sub_id, level, "require_phase") or []
        specific_rule = spec_rule or self.get_rule_content(sub_id, level, "specific_contextual_rule") or ""

        phase_map = {
            "P": "Plan - ‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡πÅ‡∏•‡∏∞‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢",
            "D": "Do - ‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡πÅ‡∏•‡∏∞‡∏Ç‡∏±‡∏ö‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏à‡∏£‡∏¥‡∏á",
            "C": "Check - ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏° ‡∏ß‡∏±‡∏î‡∏ú‡∏•",
            "A": "Act - ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á ‡∏û‡∏±‡∏í‡∏ô‡∏≤ ‡∏¢‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö"
        }
        req_str = ", ".join(phase_map.get(p, p) for p in required_phases) if required_phases else "‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô"

        lines = [
            f"\n### ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô {sub_id} Level {level} ###",
            f"‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏Å: {MATURITY_LEVEL_GOALS.get(level, '‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°')}",
            f"‡∏°‡∏¥‡∏ï‡∏¥ PDCA ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Ñ‡∏£‡∏ö: {req_str}",
            f"‡∏Å‡∏é‡πÄ‡∏â‡∏û‡∏≤‡∏∞: {specific_rule}" if specific_rule else "",
            "\n‡∏Å‡∏é‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç (‡∏ï‡πâ‡∏≠‡∏á‡∏¢‡∏∂‡∏î‡∏ñ‡∏∑‡∏≠):",
            "- Substance over Form: ‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Å‡∏±‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏ó‡∏≥‡∏à‡∏£‡∏¥‡∏á‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡∏Ñ‡∏≥‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á",
            "- Positive First: ‡πÄ‡∏ô‡πâ‡∏ô‡∏´‡∏≤‡∏à‡∏∏‡∏î‡πÅ‡∏Ç‡πá‡∏á‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡πà‡∏≠‡∏ô ‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏∂‡∏á‡∏£‡∏∞‡∏ö‡∏∏‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á",
            "- Coaching Mindset: ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏∏‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏ß‡πà‡∏≤‡∏Ç‡∏≤‡∏î‡∏≠‡∏∞‡πÑ‡∏£ + ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á",
            "- Continuity: ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡πà‡∏≠‡∏¢‡∏≠‡∏î‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤ (‡∏ñ‡πâ‡∏≤‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÅ‡∏ô‡πà‡∏ô ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏∏)"
        ]

        return "\n".join(filter(None, lines))

    def _calculate_weighted_score(
        self, 
        highest_full_level: int, 
        weight: float, 
        level_details: Dict[str, Any] = None
    ) -> float:
        """
        [FIXED & ROBUST] ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ñ‡πà‡∏ß‡∏á‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡∏£‡∏î‡πâ‡∏ß‡∏¢‡∏®‡∏π‡∏ô‡∏¢‡πå‡πÅ‡∏•‡∏∞‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á
        """
        # ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤ MAX_LEVEL ‡∏à‡∏≤‡∏Å config ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ Default ‡πÄ‡∏õ‡πá‡∏ô 5
        max_lv = getattr(self.config, 'max_level', 5) 
        if max_lv <= 0: max_lv = 5 
        
        # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô weight ‡πÄ‡∏õ‡πá‡∏ô None ‡∏´‡∏£‡∏∑‡∏≠ 0
        safe_weight = float(weight) if weight else 4.0

        # 1. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Base Level
        base_level = float(max(0, min(highest_full_level, max_lv)))
        
        # 2. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Partial Score (‡πÇ‡∏´‡∏°‡∏î‡∏™‡∏∞‡∏™‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏à‡∏≤‡∏Å Level ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏≠‡∏¢‡∏π‡πà)
        partial_contribution = 0.0
        # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å global_vars ‡∏´‡∏£‡∏∑‡∏≠ config
        mode = getattr(self, 'scoring_mode', 'PARTIAL_PDCA') 

        if mode == 'PARTIAL_PDCA' and level_details:
            next_lv_idx = int(base_level + 1)
            if next_lv_idx <= max_lv:
                next_level_str = str(next_lv_idx)
                lv_data = level_details.get(next_level_str, {})
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö pdca_breakdown
                pdca = lv_data.get('pdca_breakdown')
                if isinstance(pdca, dict) and pdca:
                    pdca_values = [float(v) for v in pdca.values() if v is not None]
                    if pdca_values:
                        # (P+D+C+A)/4 -> ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 1.0 ‡∏£‡∏∞‡∏î‡∏±‡∏ö
                        partial_contribution = sum(pdca_values) / len(pdca_values)
                        self.logger.debug(f"‚ûï [PARTIAL] L{next_level_str} adds {partial_contribution:.2f}")

        # 3. ‡∏£‡∏ß‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô Maturity (Effective Level)
        effective_level = base_level + partial_contribution
        
        # 4. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Ratio (‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ / ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏ï‡πá‡∏°)
        base_ratio = effective_level / max_lv
        
        # 5. ‡∏™‡∏£‡∏∏‡∏õ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ñ‡πà‡∏ß‡∏á‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å
        scaled_score = base_ratio * safe_weight
        
        # Boost Logic (‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£)
        if mode == 'STEP_LADDER' and base_level >= max_lv - 1:
            scaled_score = min(scaled_score * 1.1, safe_weight)
        
        final_score = round(scaled_score, 4)
        
        self.logger.info(f"üìä [WEIGHT CALC] Mode: {mode} | Eff: {effective_level:.2f}/{max_lv} | Score: {final_score}/{safe_weight}")
        
        return final_score
    
    def _calculate_overall_stats(self, target_sub_id: str):
        """
        [AUDIT-READY v2026.1.24] ‚Äî Bottleneck + Weighted + Force-Max Safe
        - Crash-proof: ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö results ‡∏ß‡πà‡∏≤‡∏á/None ‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ .get() ‡∏ö‡∏ô None
        - Audit-friendly: log ‡∏ä‡∏±‡∏î + reject ‡∏ñ‡πâ‡∏≤ evidence ‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏Å‡∏¥‡∏ô (no dummy accept)
        - Force-max: ‡πÉ‡∏ä‡πâ max ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ force-pass ‡∏´‡∏£‡∏∑‡∏≠‡∏ú‡πà‡∏≤‡∏ô L3+ ‚â•50%
        - Weighted avg ‡∏à‡∏£‡∏¥‡∏á + analytics ‡πÄ‡∏û‡∏¥‡πà‡∏° evidence coverage
        """
        results = self.final_subcriteria_results or []

        if not results:
            self.logger.critical("[AUDIT CRITICAL] No subcriteria results found - Setting L0 with warning")
            self.total_stats = {
                "overall_max_level": 0,
                "overall_min_level": 0,
                "overall_level_label": "L0",
                "overall_avg_score": 0.0,
                "total_weighted_score": 0.0,
                "total_weight": 0.0,
                "force_pass_count": 0,
                "high_level_pass_count": 0,
                "use_max_override": False,
                "audit_note": "No valid subcriteria results - Possible retrieval failure",
                "analytics": {"sub_details": []}
            }
            return

        passed_levels = []
        sub_details = []
        total_weighted_sum = 0.0
        total_weight = 0.0
        force_pass_count = 0
        high_level_pass_count = 0  # ‡∏ô‡∏±‡∏ö sub ‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô L3+
        use_max_override = False

        for r in results:
            sub_id = r.get('sub_id', 'Unknown')
            
            # 1. Flexible Level Details Access
            details_map = r.get('level_details', {})
            if not details_map and '0' in r:
                details_map = r.get('0', {}).get('level_details', {})

            # 2. Step-Ladder Maturity Scan
            current_maturity_lvl = 0
            for l_idx in range(1, 6):
                lv_data = details_map.get(str(l_idx), {})
                is_passed = lv_data.get('is_passed', False)
                is_force = lv_data.get('is_force_pass', False)
                
                if is_passed or is_force:
                    current_maturity_lvl = l_idx
                    if is_force:
                        force_pass_count += 1
                    if current_maturity_lvl >= 3:
                        high_level_pass_count += 1
                else:
                    break

            # 3. Weighted Score
            weight = float(r.get('weight', 4.0))
            total_weight += weight
            
            if hasattr(self, '_calculate_weighted_score'):
                sub_score = self._calculate_weighted_score(
                    highest_full_level=current_maturity_lvl,
                    weight=weight,
                    level_details=details_map
                )
            else:
                sub_score = float(current_maturity_lvl) * (weight / 5.0 if weight > 0 else 0)

            total_weighted_sum += sub_score

            # Update back to result
            r['highest_full_level'] = current_maturity_lvl
            r['weighted_score'] = round(sub_score, 2)
            r['is_passed'] = (current_maturity_lvl >= 1)

            passed_levels.append(current_maturity_lvl)
            
            sub_details.append({
                "sub_id": sub_id,
                "maturity": current_maturity_lvl,
                "score": round(sub_score, 2),
                "weight": weight,
                "is_force_pass": any(lv_data.get('is_force_pass', False) for lv_data in details_map.values()),
                "evidence_count": len(details_map)  # ‡πÄ‡∏û‡∏¥‡πà‡∏° audit info
            })

        # 4. Final Aggregation
        num_subs = len(results)
        avg_score = total_weighted_sum / total_weight if total_weight > 0 else 0.0
        
        overall_min_maturity = min(passed_levels) if passed_levels else 0
        overall_max_maturity = max(passed_levels) if passed_levels else 0

        # Decide label: min default, max ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ force-pass ‡∏´‡∏£‡∏∑‡∏≠‡∏ú‡πà‡∏≤‡∏ô L3+ ‚â•50%
        final_label_level = overall_min_maturity
        use_max_override = force_pass_count > 0 or (high_level_pass_count / num_subs >= 0.5 if num_subs > 0 else False)
        if use_max_override:
            final_label_level = overall_max_maturity
            self.logger.info(f"[STATS OVERRIDE] Using MAX level L{final_label_level} (force-pass: {force_pass_count}, high-level pass: {high_level_pass_count}/{num_subs})")

        # Audit Note: ‡∏ñ‡πâ‡∏≤ evidence ‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏Å‡∏¥‡∏ô ‡πÉ‡∏´‡πâ‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô
        audit_note = "All subcriteria processed" 
        if any(d["evidence_count"] < 3 for d in sub_details):
            audit_note += " - Warning: Some subcriteria have low evidence count (<3) - Audit may require manual review"

        self.total_stats = {
            "overall_max_level": int(overall_max_maturity),
            "overall_min_level": int(overall_min_maturity),
            "overall_level_label": f"L{int(final_label_level)}",
            "overall_avg_score": round(avg_score, 2),
            "total_weighted_score": round(total_weighted_sum, 2),
            "total_weight": round(total_weight, 2),
            "force_pass_count": force_pass_count,
            "high_level_pass_count": high_level_pass_count,
            "use_max_override": use_max_override,
            "audit_note": audit_note,
            
            "total_sub_assessed": num_subs,
            "analytics": {
                "sub_details": sub_details,
                "passed_levels_map": passed_levels,
                "assessed_at": datetime.now().isoformat()
            }
        }

        self.logger.info(
            f"‚úÖ [STATS SUCCESS] Overall: {self.total_stats['overall_level_label']} | "
            f"Avg Score: {self.total_stats['overall_avg_score']} | "
            f"Max/Min: L{overall_max_maturity}/L{overall_min_maturity} | "
            f"Force-Pass: {force_pass_count} | High-Level Pass: {high_level_pass_count}/{num_subs} | "
            f"Audit Note: {audit_note}"
        )

    def _export_results(self, results_data: Any, sub_criteria_id: str, **kwargs) -> str:
        """
        [ULTIMATE EXPORTER v2026.EXPORT.5 - STRATEGIC INTEGRATED]
        ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô, ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô ‡πÅ‡∏•‡∏∞‡πÅ‡∏ú‡∏ô‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå (Tier-3) ‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
        """
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            record_id = kwargs.get("record_id", getattr(self, "current_record_id", f"auto_{timestamp}"))
            tenant = getattr(self.config, 'tenant', 'unknown')
            year = getattr(self.config, 'year', 'unknown')
            enabler = getattr(self, 'enabler', 'unknown').upper()

            # 1. üß© Data Consolidation (‡∏£‡∏ß‡∏°‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•)
            if results_data is None:
                results_data = getattr(self, 'final_subcriteria_results', [])
            
            # ‡∏Å‡∏£‡∏ì‡∏µ‡∏™‡πà‡∏á‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô Dictionary ‡∏ï‡∏±‡∏ß‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡πÉ‡∏´‡πâ‡∏´‡πà‡∏≠‡πÄ‡∏õ‡πá‡∏ô List
            if isinstance(results_data, dict):
                results_data = [results_data]
            
            if not results_data:
                self.logger.warning(f"‚ö†Ô∏è [EXPORT] ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {sub_criteria_id} - ‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å‡∏Å‡∏≤‡∏£ Export")
                return ""

            # 2. üìä Summary Calculation (‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à)
            valid_results = [r for r in results_data if isinstance(r, dict)]
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Maturity Level ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡πÅ‡∏•‡∏∞‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°‡∏ñ‡πà‡∏ß‡∏á‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å
            highest_lvl = max([int(r.get('highest_full_level', 0)) for r in valid_results]) if valid_results else 0
            total_weighted = sum([float(r.get('weighted_score', 0.0)) for r in valid_results])

            # 3. üõ°Ô∏è Robust Evidence Mapping (Audit Trail ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ú‡∏π‡πâ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö)
            # ‡∏î‡∏∂‡∏á Master Map ‡∏ó‡∏µ‡πà‡∏™‡∏∞‡∏™‡∏°‡∏°‡∏≤‡∏à‡∏≤‡∏Å Worker ‡∏ó‡∏∏‡∏Å‡∏ï‡∏±‡∏ß
            master_map = getattr(self, 'evidence_map', {})
            processed_evidence = {}
            
            for lv_key, v_list in master_map.items():
                if not v_list or not isinstance(v_list, list): 
                    continue
                
                try:
                    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ Rerank Score ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÅ‡∏ó‡∏ô‡∏Ç‡∏≠‡∏á Level ‡∏ô‡∏±‡πâ‡∏ô‡πÜ
                    sorted_ev = sorted(
                        [ev for ev in v_list if isinstance(ev, dict)], 
                        key=lambda x: x.get('relevance_score', x.get('rerank_score', 0)), 
                        reverse=True
                    )
                    top_ev = sorted_ev[0] if sorted_ev else None
                    
                    if top_ev:
                        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å document_map ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥
                        doc_id = top_ev.get("doc_id")
                        filename = self.document_map.get(doc_id) if hasattr(self, 'document_map') else None
                        filename = filename or top_ev.get("source") or top_ev.get("file_name") or "Unknown_Source"

                        processed_evidence[str(lv_key)] = {
                            "file": filename,
                            "page": top_ev.get("page", "N/A"),
                            "tag": str(top_ev.get("pdca_tag", "OTHER")).upper(),
                            "confidence": round(float(top_ev.get("relevance_score", top_ev.get("rerank_score", 0))), 4),
                            "content_snippet": str(top_ev.get("content", ""))[:200] + "..." # ‡πÄ‡∏Å‡πá‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
                        }
                except Exception as ev_err:
                    self.logger.debug(f"Skip processing evidence level {lv_key}: {ev_err}")

            # 4. üìù Construct Final Payload (‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏∞‡∏î‡∏±‡∏ö‡πÇ‡∏õ‡∏£‡∏î‡∏±‡∏Å‡∏ä‡∏±‡∏ô)
            payload = {
                "metadata": {
                    "record_id": record_id,
                    "tenant": tenant,
                    "year": year,
                    "enabler": enabler,
                    "engine_version": "SEAM-ENGINE-v2026.3.26",
                    "exported_at": datetime.now().isoformat(),
                    "scoring_mode": "Step-Ladder Maturity"
                },
                "result_summary": {
                    "maturity_level": f"L{highest_lvl}",
                    "is_passed": highest_lvl >= 1,
                    "total_weighted_score": round(total_weighted, 4),
                    "evidence_used_count": len(processed_evidence),
                    "evaluated_sub_count": len(valid_results),
                    "status": "COMPLETED"
                },
                "sub_criteria_details": valid_results,  # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏£‡∏≤‡∏¢ Level ‡πÅ‡∏•‡∏∞ Tier-2 Action Plans
                "evidence_audit_trail": processed_evidence, # ‡∏£‡πà‡∏≠‡∏á‡∏£‡∏≠‡∏¢‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏¢‡πâ‡∏≠‡∏ô‡∏Å‡∏•‡∏±‡∏ö
                "strategic_synthesis": getattr(self, 'master_roadmap_data', {
                    "status": "PENDING",
                    "overall_strategy": "‡πÅ‡∏ú‡∏ô‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ñ‡∏π‡∏Å‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠"
                }) # üéØ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Tier-3 Strategic Roadmap
            }

            # 5. üíæ Smart Path & Persistence (‡∏£‡∏∞‡∏ö‡∏ö‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞)
            filename = f"REPORT_{enabler}_{sub_criteria_id}_{timestamp}.json"
            
            try:
                # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡πÉ‡∏ô Path ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏ï‡∏≤‡∏°‡∏õ‡∏µ‡πÅ‡∏•‡∏∞‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠
                export_path = get_assessment_export_file_path(
                    tenant=tenant, year=year, enabler=enabler.lower(),
                    suffix=f"{sub_criteria_id}_{timestamp}", ext="json"
                )
            except:
                # Fallback: ‡∏´‡∏≤‡∏Å‡∏£‡∏∞‡∏ö‡∏ö Path ‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ ‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß
                out_dir = os.path.join("exports", str(tenant), str(year), enabler.lower())
                os.makedirs(out_dir, exist_ok=True)
                export_path = os.path.join(out_dir, filename)

            # 6. üñäÔ∏è Write JSON File
            with open(export_path, 'w', encoding='utf-8') as f:
                json.dump(payload, f, indent=2, ensure_ascii=False)

            self.logger.info(f"üíæ [EXPORT SUCCESS] ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏ñ‡∏π‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡πâ‡∏ß‡∏ó‡∏µ‡πà: {export_path}")
            return export_path

        except Exception as e:
            self.logger.error(f"‚ùå [EXPORT FAILED] ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏ß‡∏¥‡∏Å‡∏§‡∏ï: {str(e)}", exc_info=True)
            return ""
        
    def _robust_hydrate_documents_for_priority_chunks(
        self,
        chunks_to_hydrate: List[Dict],
        vsm: Optional['VectorStoreManager'],
        current_sub_id: Optional[str] = None,
        level: Optional[int] = None
    ) -> List[Dict]:
        """
        [ULTIMATE HYDRATION v2026.12]
        - ‡∏î‡∏∂‡∏á Full Text ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Priority Chunks ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ LLM ‡πÄ‡∏´‡πá‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô
        - ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏õ‡πÉ‡∏ä‡πâ LLM-based Tagging ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡∏ó‡∏∏‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö (L1-L5)
        - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏∞‡∏ö‡∏ö Boost Score ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ AI ‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Å‡∏±‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ô‡∏µ‡πâ
        """
        from collections import defaultdict
        
        active_sub_id = current_sub_id or getattr(self, 'sub_id', 'unknown')
        if not chunks_to_hydrate:
            self.logger.debug(f"‚ÑπÔ∏è [HYDRATION] No chunks to hydrate for {active_sub_id} L{level}")
            return []

        # 1. üè∑Ô∏è Helper: ‡∏à‡∏±‡∏î‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏î‡πâ‡∏ß‡∏¢ LLM (‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö Core Assessment)
        def _safe_classify(text: str, filename: str = "") -> str:
            try:
                # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ LLM Tagging ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏õ‡∏£‡∏±‡∏ö‡∏à‡∏π‡∏ô‡πÑ‡∏ß‡πâ (Self-contained within class)
                tag = self._get_semantic_tag(
                    text=text, 
                    sub_id=active_sub_id, 
                    level=level or 1,
                    filename=filename
                )
                return tag if tag in {"P", "D", "C", "A"} else "Other"
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è PDCA classify failed in hydration: {e}")
                return "Other"

        # 2. üìè Helper: ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞ Scoring Boost
        def _standardize_chunk(chunk: Dict, score: float):
            chunk.setdefault("is_baseline", True)
            text = chunk.get("text", "").strip()
            meta = chunk.get("metadata", {})
            
            if text:
                fname = os.path.basename(str(meta.get("source") or meta.get("file_name") or "Unknown"))
                # ‚ú® ‡πÉ‡∏ä‡πâ LLM Tagging ‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ‡πÄ‡∏•‡∏¢
                chunk["pdca_tag"] = _safe_classify(text, filename=fname)
                
                # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö Boost ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÉ‡∏´‡πâ‡∏™‡∏π‡∏á ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ä‡∏ô‡∏∞ Chunk ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö
                chunk["rerank_score"] = max(float(chunk.get("rerank_score", 0.0)), score)
                chunk["score"] = max(float(chunk.get("score", 0.0)), score)
            return chunk

        # 3. üîë ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏° IDs
        stable_ids = {
            sid for c in chunks_to_hydrate
            if (sid := (c.get("stable_doc_uuid") or c.get("doc_id") or c.get("chunk_uuid")))
        }

        if not stable_ids or not vsm:
            boosted = [_standardize_chunk(c.copy(), 0.9) for c in chunks_to_hydrate]
            return self._guarantee_text_key(boosted)

        # 4. üõ∞Ô∏è Fetch Full Documents (Hydration Process)
        stable_id_map = defaultdict(list)
        try:
            retrieved_docs = vsm.get_documents_by_id(
                list(stable_ids), doc_type=self.doc_type, enabler=self.config.enabler
            )
            for doc in retrieved_docs:
                sid = doc.metadata.get("stable_doc_uuid") or doc.metadata.get("doc_id")
                if sid:
                    stable_id_map[sid].append({"text": doc.page_content, "metadata": doc.metadata})
        except Exception as e:
            self.logger.error(f"‚ùå [HYDRATION] VSM Fetch Error: {e}")
            return self._guarantee_text_key([_standardize_chunk(c.copy(), 0.9) for c in chunks_to_hydrate])

        # 5. üíß Hydrate & Dedup
        hydrated_priority_docs = []
        seen_signatures = set()
        SAFE_META_KEYS = {"source", "file_name", "page", "page_label", "page_number"}

        for chunk in chunks_to_hydrate:
            new_chunk = chunk.copy()
            sid = new_chunk.get("stable_doc_uuid") or new_chunk.get("doc_id")

            hydrated = False
            if sid and sid in stable_id_map:
                # ‡∏î‡∏∂‡∏á Full Text ‡∏°‡∏≤‡∏ó‡∏±‡∏ö Snippet ‡∏™‡∏±‡πâ‡∏ô‡πÜ
                best_match = stable_id_map[sid][0]
                new_chunk["text"] = best_match["text"]
                meta = best_match.get("metadata", {})
                new_chunk.update({k: v for k, v in meta.items() if k in SAFE_META_KEYS})
                hydrated = True

            # ‡∏ó‡∏≥ Standardize + Tagging (Score 1.0 ‡∏ñ‡πâ‡∏≤‡∏î‡∏∂‡∏á‡πÄ‡∏ï‡πá‡∏°‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à / 0.85 ‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ‡∏Ç‡∏≠‡∏á‡πÄ‡∏î‡∏¥‡∏°)
            new_chunk = _standardize_chunk(new_chunk, score=1.0 if hydrated else 0.85)

            # Check Signature ‡∏Å‡∏±‡∏ô‡∏ã‡πâ‡∏≥
            sig = (sid, new_chunk.get("chunk_uuid"), new_chunk.get("text", "")[:100])
            if sig not in seen_signatures:
                seen_signatures.add(sig)
                hydrated_priority_docs.append(new_chunk)

        self.logger.info(f"‚úÖ [HYDRATION] Complete: {len(hydrated_priority_docs)} priority chunks ready.")
        return self._guarantee_text_key(hydrated_priority_docs)

    def _guarantee_text_key(
        self,
        chunks: List[Dict],
        total_count: int = 0,
        restored_count: int = 0
    ) -> List[Dict]:
        """
        Guarantee 'text' key exists in every chunk
        """
        final_chunks = []

        for chunk in chunks:
            if "text" not in chunk or not isinstance(chunk["text"], str):
                chunk["text"] = ""
                cid = str(chunk.get("chunk_uuid", "N/A"))
                self.logger.debug(f"Guaranteed 'text' key for chunk (ID: {cid[:8]})")
            final_chunks.append(chunk)

        if total_count > 0:
            baseline_count = sum(1 for c in final_chunks if c.get("is_baseline"))
            self.logger.info(
                f"HYDRATION FINAL: Restored {restored_count}/{total_count} "
                f"(Baseline={baseline_count}, Total final={len(final_chunks)})"
            )

        return final_chunks


    def _normalize_meta(self, c: Dict) -> Tuple[str, str]:
        """
        [REVISED] ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡∏∞‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏£‡∏∞‡∏ö‡∏ö Priority Fallback
        - ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤ 0 ‡∏´‡∏≤‡∏¢ (Index-based)
        - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏à‡∏≤‡∏Å Ingest ‡πÉ‡∏´‡∏°‡πà ‡πÅ‡∏•‡∏∞ Reranker Output
        - ‡πÄ‡∏ô‡πâ‡∏ô page_label ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏ö‡∏ô UI
        """
        if not isinstance(c, dict):
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡πá‡∏ô LangChain Document ‡πÉ‡∏´‡πâ‡∏î‡∏∂‡∏á metadata ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤
            if hasattr(c, "metadata"):
                meta = c.metadata
                # ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á dict ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÇ‡∏Ñ‡πâ‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏•‡πà‡∏≤‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏î‡πâ
                c = {"metadata": meta}
            else:
                return "Unknown Source", "N/A"

        # ‡∏î‡∏∂‡∏á metadata ‡∏°‡∏≤‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÑ‡∏ß‡πâ (‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏Å‡∏£‡∏ì‡∏µ None ‡∏´‡∏£‡∏∑‡∏≠ Dict)
        meta = c.get("metadata") or {}

        # 1. üîç ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå (Source Name Priority)
        source_priority = [
            c.get("source_filename"),
            meta.get("source_filename"),
            c.get("filename"),
            meta.get("source"),
            meta.get("file_name"),
            c.get("id") 
        ]
        
        source = "Unknown"
        for s in source_priority:
            if s and str(s).strip():
                source = str(s).strip()
                break

        # 2. üîç ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤ (Page Label Priority)
        page_keys = ["page_label", "page", "page_number"]
        found_page = None
        for key in page_keys:
            val_root = c.get(key)
            if val_root is not None and str(val_root).strip().lower() != "n/a":
                found_page = val_root
                break
            val_meta = meta.get(key)
            if val_meta is not None and str(val_meta).strip().lower() != "n/a":
                found_page = val_meta
                break

        # 3. ‚ú® Final Cleaning & UI Formatting
        clean_source = os.path.basename(source) if "/" in source or "\\" in source else source
        if found_page is not None:
            page_str = str(found_page).strip()
            clean_page = page_str if page_str.lower() != "n/a" else "N/A"
        else:
            clean_page = "N/A"

        return clean_source, clean_page
    

    def _get_heuristic_pdca_tag(self, text: str, level: int) -> Optional[str]:
        t = text.lower()
        
        # Do-specific ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö L1 (‡πÄ‡∏ô‡πâ‡∏ô‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏ó‡∏≥‡∏à‡∏£‡∏¥‡∏á)
        do_keywords = [
            "‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£", "‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥", "‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°", "‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°", "‡∏≠‡∏ö‡∏£‡∏°", "‡∏à‡∏±‡∏î‡∏ó‡∏≥", "‡∏•‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà", 
            "‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏ú‡∏•", "‡∏†‡∏≤‡∏û‡∏ñ‡πà‡∏≤‡∏¢", "‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£", "‡∏°‡∏∏‡πà‡∏á‡∏°‡∏±‡πà‡∏ô", "‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á", "‡∏™‡∏ô‡∏±‡∏ö‡∏™‡∏ô‡∏∏‡∏ô", 
            "‡∏Ç‡∏±‡∏ö‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô", "‡∏ô‡∏≥‡∏£‡πà‡∏≠‡∏á", "‡∏•‡∏á‡∏°‡∏∑‡∏≠‡∏ó‡∏≥", "‡∏ô‡∏≥‡πÑ‡∏õ‡πÉ‡∏ä‡πâ"
        ]
        if level <= 2 and any(k in t for k in do_keywords):
            return "D"

        # Check keywords (‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å log)
        check_keywords = [
            "‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏•", "‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô", "‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°", "‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î", "kpi", "‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå", "‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•", 
            "‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥", "‡∏™‡∏≥‡∏£‡∏ß‡∏à", "‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö", "‡∏ß‡∏±‡∏î‡∏ú‡∏•"
        ]
        if any(k in t for k in check_keywords):
            return "C"

        # Plan & Act (‡∏Ñ‡∏á‡πÄ‡∏î‡∏¥‡∏° ‡πÅ‡∏ï‡πà‡∏•‡∏î priority)
        if any(k in t for k in ["‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢", "‡πÅ‡∏ú‡∏ô", "‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå", "‡∏°‡∏ï‡∏¥", "‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á", "‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢", "‡πÄ‡∏à‡∏ï‡∏ô‡∏≤‡∏£‡∏°‡∏ì‡πå"]):
            return "P"
        if any(k in t for k in ["‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á", "‡∏û‡∏±‡∏í‡∏ô‡∏≤", "‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç", "‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô", "lesson learned", "‡∏ï‡πà‡∏≠‡∏¢‡∏≠‡∏î", "‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°"]):
            return "A"

        return None
    
    def evaluate_pdca(
        self,
        pdca_blocks: Dict[str, Any],
        sub_id: str,
        level: int,
        audit_confidence: Any
    ) -> Dict[str, Any]:
        """
        [ULTIMATE ROUTING ENGINE v2026.3.26]
        ‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡πÄ‡∏•‡∏∑‡∏≠‡∏Å LLM Agent ‡∏ï‡∏≤‡∏°‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á Maturity (Coaching vs Audit)
        ‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ LLM
        """
        
        # 1. ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏à‡∏≤‡∏Å Rubric ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡πÉ‡∏´‡πâ AI
        criteria_info = self.rubric.get(sub_id, {})
        sub_name = criteria_info.get("name", sub_id)
        statement = criteria_info.get("statement", "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô")
        log_prefix = f"üß† [{sub_id}-L{level}]"

        # 2. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Audit Confidence (Type Guard)
        conf_score = 0.0
        if isinstance(audit_confidence, dict):
            conf_score = float(audit_confidence.get("coverage_ratio", 0.0))
        else:
            conf_score = float(audit_confidence or 0.0)

        # 3. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° PDCA Context (‡πÅ‡∏¢‡∏Å‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ä‡πà‡∏ß‡∏¢ AI ‡∏™‡∏Å‡∏±‡∏î‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô) 
        pdca_summary_list = []
        for tag in ["P", "D", "C", "A"]:
            content = pdca_blocks.get(tag, "")
            if content:
                pdca_summary_list.append(f"--- {tag} PHASE EVIDENCE ---\n{content}")
            else:
                pdca_summary_list.append(f"--- {tag} PHASE EVIDENCE ---\n(‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏ô‡∏´‡∏°‡∏ß‡∏î‡∏ô‡∏µ‡πâ)")
        
        pdca_string_context = "\n\n".join(pdca_summary_list)

        # 4. ‡∏î‡∏∂‡∏á‡∏Å‡∏é‡∏™‡∏∞‡∏™‡∏° (Cumulative Rules) ‡∏ó‡∏µ‡πà‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÑ‡∏ß‡πâ‡πÅ‡∏•‡πâ‡∏ß
        rules = self.get_cumulative_rules_cached(sub_id, level)

        # 5. ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏° Parameters ‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏™‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô Keyword Arguments (**kwargs)
        # ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÑ‡∏°‡πà‡πÉ‡∏™‡πà sub_id, level, sub_name, statement ‡πÉ‡∏ô‡∏ô‡∏µ‡πâ 
        # ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏à‡∏∞‡∏™‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô Positional Arguments ‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
        extra_kwargs = {
            "pdca_context": pdca_string_context, 
            "context": str(pdca_blocks),         # ‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡πÑ‡∏ß‡πâ
            "required_phases": rules.get("required_phases", []),
            "specific_contextual_rule": rules.get("all_instructions", "‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô"),
            "llm_executor": self.llm,
            "enabler_full_name": self.config.enabler,
            "enabler_code": self.enabler,
            "plan_keywords": rules.get("plan_keywords", []),
            "confidence_reason": f"Coverage Score: {conf_score:.2f}",
            "ai_confidence": "HIGH" if conf_score >= 0.7 else "MEDIUM"
        }

        # ---------------------------------------------------------------------
        # 6. ROUTING LOGIC (Strategic Separation)
        # ---------------------------------------------------------------------
        
        
        try:
            # üéØ CASE A: [STRATEGIC LEVEL] Level 3 ‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏õ (Audit Mode)
            if level >= 3:
                self.logger.info(f"{log_prefix} ROUTE ‚Üí Standard Audit Agent (Strict Mode)")
                return self.standard_audit_agent(
                    sub_criteria_name=sub_name, # arg 1
                    level=level,                # arg 2
                    statement_text=statement,   # arg 3
                    sub_id=sub_id,              # arg 4
                    **extra_kwargs
                )

            # üéØ CASE B: [FOUNDATION LEVEL] Level 1-2 (Coaching Mode)
            else:
                self.logger.info(f"{log_prefix} ROUTE ‚Üí Foundation Coaching Agent (Helpful Mode)")
                # ‡πÉ‡∏ô‡πÇ‡∏´‡∏°‡∏î Coaching ‡πÄ‡∏£‡∏≤‡∏≠‡∏≤‡∏à‡∏õ‡∏£‡∏±‡∏ö AI Confidence ‡πÉ‡∏´‡πâ‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô
                extra_kwargs["ai_confidence"] = "MEDIUM" 
                
                return self.foundation_coaching_agent(
                    sub_criteria_name=sub_name, # arg 1
                    level=level,                # arg 2
                    statement_text=statement,   # arg 3
                    sub_id=sub_id,              # arg 4
                    **extra_kwargs
                )
                
        except Exception as e:
            self.logger.error(f"üõë [ROUTING-ERROR] {log_prefix} Failure: {str(e)}")
            return {
                "is_passed": False,
                "score": 0.0,
                "reason": f"Routing System Error: {str(e)}",
                "is_error": True
            }

    def _get_pdca_blocks_from_evidences(
        self,
        evidences: List[Dict[str, Any]],
        baseline_evidences: List[Dict[str, Any]],
        level: int,
        sub_id: str,
        contextual_rules_map: Dict[str, Any],
        record_id: str = None
    ) -> Dict[str, Any]:
        """
        [ULTIMATE HYDRATED v2026.3.26 - AUDIT SAFE]

        Responsibilities:
        - Merge new + baseline evidences
        - Classify into PDCA with multi-layer guards
        - Prevent Zero-Chunks illusion
        - Preserve full audit trace (baseline / forced / relevance)

        Design Principles:
        - L1‚ÄìL3 : Evidence existence > perfection (allow forced)
        - L4‚ÄìL5 : Substance over form (no forced guessing)
        """

        pdca_groups = defaultdict(list)
        seen_texts = set()

        # ------------------------------------------------------------------
        # STEP 1: Merge evidences (Foundation Flex)
        # ------------------------------------------------------------------
        all_candidate_evidences = (evidences or []) + (baseline_evidences or [])
        self.logger.info(
            f"üè∑Ô∏è [PDCA] Candidates: {len(all_candidate_evidences)} "
            f"(new={len(evidences or [])}, baseline={len(baseline_evidences or [])}, L{level})"
        )

        # ------------------------------------------------------------------
        # STEP 2: Iterate & classify
        # ------------------------------------------------------------------
        for idx, chunk in enumerate(all_candidate_evidences, start=1):
            txt = (chunk.get("text") or "").strip()
            if not txt:
                continue

            txt_key = txt.lower()
            if txt_key in seen_texts:
                continue
            seen_texts.add(txt_key)

            # ---- Metadata recovery (Audit Trace) ----
            meta = chunk.get("metadata", {}) or {}
            filename = (
                chunk.get("source_filename")
                or meta.get("source_filename")
                or "Unknown_File"
            )
            page = meta.get("page_label") or meta.get("page") or "N/A"

            is_baseline = chunk.get("source") == "BASELINE" or chunk.get("is_baseline", False)
            baseline_level = chunk.get("baseline_level") or (level - 1 if is_baseline else level)

            prefix = f"[BASELINE-L{baseline_level}] " if is_baseline else ""
            source_display = f"{prefix}{filename} (P.{page})"

            # ---- STEP 3: PDCA classification ----
            final_tag = None
            tag_source = None
            is_forced = False

            # 3.1 Heuristic
            if hasattr(self, "_get_heuristic_pdca_tag"):
                final_tag = self._get_heuristic_pdca_tag(txt, level)
                if final_tag:
                    tag_source = "Heuristic"

            # 3.2 Keyword
            if not final_tag:
                txt_lower = txt.lower()
                p_kws = ['‡πÅ‡∏ú‡∏ô', '‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢', '‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå', '‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢', 'master plan', 'policy', '‡πÅ‡∏ú‡∏ô‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£']
                d_kws = ['‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£', '‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°', '‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥', '‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°', '‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å', 'implement', '‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô']
                c_kws = ['‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°', '‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô', '‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô', '‡∏ú‡∏•', 'audit', 'kpi', '‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î', 'monitor', '‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•']
                a_kws = ['‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á', '‡∏ó‡∏ö‡∏ó‡∏ß‡∏ô', 'lesson learned', '‡∏û‡∏±‡∏í‡∏ô‡∏≤', '‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°', 'improve', 'review', '‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô']

                if any(k in txt_lower for k in p_kws):
                    final_tag, tag_source = "P", "Keyword"
                elif any(k in txt_lower for k in d_kws):
                    final_tag, tag_source = "D", "Keyword"
                elif any(k in txt_lower for k in c_kws):
                    final_tag, tag_source = "C", "Keyword"
                elif any(k in txt_lower for k in a_kws):
                    final_tag, tag_source = "A", "Keyword"

            # 3.3 Semantic (AI)
            if not final_tag:
                try:
                    tag = self._get_semantic_tag(txt, sub_id, level, filename)
                    if tag in {"P", "D", "C", "A"}:
                        final_tag, tag_source = tag, "AI-Semantic"
                    else:
                        final_tag = "Other"
                except Exception:
                    final_tag, tag_source = "Other", "AI-Fail"

            # ---- STEP 4: Upper Guard & Forced Logic ----
            if final_tag == "Other":
                if level >= 4:
                    # üö© L4‚ÄìL5: ‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏î‡∏≤ ‚Üí ‡∏ï‡∏±‡∏î‡∏ó‡∏¥‡πâ‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÇ‡∏õ‡∏£‡πà‡∏á‡πÉ‡∏™
                    self.logger.debug(
                        f"üö´ [PDCA] Excluded insufficient evidence (L{level}): {source_display}"
                    )
                    continue
                else:
                    # L1‚ÄìL3: ‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï Forced ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤ existence
                    is_forced = True
                    if level <= 2:
                        final_tag = "P" if idx % 2 == 0 else "D"
                    else:  # level == 3
                        final_tag = "C" if idx % 3 == 0 else "D"
                    tag_source = f"Forced-L{level}"

            # ---- STEP 5: Collect chunk ----
            pdca_groups[final_tag].append({
                "text": txt,
                "pdca_tag": final_tag,
                "tag_source": tag_source,
                "source_display": source_display,
                "is_baseline": is_baseline,
                "is_forced": is_forced,
                "relevance": chunk.get("final_relevance_score") or 0
            })

        # ------------------------------------------------------------------
        # STEP 6: Build final PDCA blocks (consistent ranking)
        # ------------------------------------------------------------------
        blocks: Dict[str, Any] = {"sources": {}, "actual_counts": {}}

        for tag in ["P", "D", "C", "A"]:
            ranked_chunks = sorted(
                pdca_groups.get(tag, []),
                key=lambda x: (
                    x["is_baseline"],   # new before baseline
                    x["is_forced"],     # real before forced
                    -x["relevance"]     # higher relevance first
                )
            )[:5]

            if ranked_chunks:
                blocks[tag] = "\n\n".join([
                    f"[{c['source_display']} | {c['tag_source']}{' ‚ö†Ô∏èFORCED' if c['is_forced'] else ''}]\n"
                    f"{c['text'][:500]}"
                    for c in ranked_chunks
                ])
            else:
                blocks[tag] = f"[‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÉ‡∏ô‡∏´‡∏°‡∏ß‡∏î {tag}]"

            # ---- Persistence & Audit ----
            blocks["sources"][tag] = [c["source_display"] for c in ranked_chunks]
            blocks["actual_counts"][tag] = len([
                c for c in ranked_chunks if not c["is_forced"]
            ])

        return blocks

    def _prepare_worker_tuple(self, sub_criteria_data: Dict, document_map: Optional[Dict]) -> Tuple:
        """‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤ Process ‡πÉ‡∏´‡∏°‡πà (Pickle-friendly)"""
        return (
            sub_criteria_data,                 # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Rubric ‡∏£‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠
            self.config.enabler,               # Enabler Code
            self.config.target_level,          # ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢
            self.config.mock_mode,             # ‡πÇ‡∏´‡∏°‡∏î Mock
            getattr(self, 'evidence_map_path', None), 
            self.config.model_name,            # ‡∏ä‡∏∑‡πà‡∏≠‡∏£‡∏∏‡πà‡∏ô LLM
            self.config.temperature,           # ‡∏Ñ‡πà‡∏≤ Temp
            self.config.min_retry_score,       # ‡πÄ‡∏Å‡∏ì‡∏ë‡πå Rerank
            self.config.max_retrieval_attempts,
            document_map or self.document_map, # ID Mapping
            None,                              # Action Plan Model (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
            self.config.year,
            self.config.tenant
        )
    
    def _create_failed_result(self, record_id: str, message: str, start_ts: float) -> Dict[str, Any]:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô Response ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏Å‡∏¥‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô Orchestrator"""
        self.logger.error(f"‚ùå Assessment Failed: {message}")
        return {
            "record_id": record_id,
            "status": "FAILED",
            "error": message,
            "run_time_seconds": round(time.time() - start_ts, 2),
            "summary": {},
            "sub_criteria_results": {}
        }
    
    def _normalize_evidence_metadata(self, evidence_list: List[Dict[str, Any]]):
        """
        [REVISED v2026] ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á Metadata ‡πÉ‡∏´‡πâ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÅ‡∏•‡∏∞ Export
        - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Ñ‡∏µ‡∏¢‡πå‡∏Å‡∏£‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢ (Flattened vs Nested)
        - ‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏Ç‡∏≠‡∏á‡∏ä‡∏ô‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Type Safety)
        """
        for ev in evidence_list:
            if not isinstance(ev, dict):
                continue
                
            # 1. ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á Metadata (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á Langchain Doc ‡πÅ‡∏•‡∏∞ Dict)
            meta = ev.get("metadata", {})
            if not isinstance(meta, dict): meta = {}
            
            # 2. ‡∏õ‡∏£‡∏±‡∏ö Source (‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏•‡∏∂‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Å‡πà‡∏≠‡∏ô)
            raw_source = (
                meta.get("source_filename") or 
                meta.get("file_name") or 
                ev.get("source") or 
                meta.get("source") or 
                "Unknown_File"
            )
            ev["source"] = os.path.basename(str(raw_source))
            ev["source_filename"] = ev["source"] # Sync ‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á‡∏Ñ‡∏µ‡∏¢‡πå
            
            # 3. ‡∏õ‡∏£‡∏±‡∏ö Page (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏•‡∏Ç 0 ‡∏´‡∏£‡∏∑‡∏≠ None)
            raw_page = (
                meta.get("page_label") or 
                meta.get("page") or 
                meta.get("page_number") or 
                ev.get("page") or 
                "N/A"
            )
            ev["page"] = str(raw_page)
            
            # 4. ‡∏õ‡∏£‡∏±‡∏ö Relevance Score ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á
            ev["relevance_score"] = self.get_actual_score(ev)
            
            # 5. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ ID ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ Audit Trail
            if not ev.get("stable_doc_uuid"):
                ev["stable_doc_uuid"] = (
                    meta.get("stable_doc_uuid") or 
                    ev.get("doc_id") or 
                    meta.get("doc_id") or 
                    f"id_{uuid.uuid4().hex[:8]}"
                )
            
            # 6. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö PDCA Tag (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô Other)
            if not ev.get("pdca_tag"):
                ev["pdca_tag"] = meta.get("pdca_tag") or "Other"

        return evidence_list
    
    # ------------------------------------------------------------------------------------------
    # [FIXED] üß© Persistence Helper: Update Internal Evidence
    # ------------------------------------------------------------------------------------------
    def _update_internal_evidence_map(self, merged_evidence: Dict[str, Any]):
        """
        ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡πÇ‡∏¢‡∏á‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏à‡∏≤‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà Merge ‡πÅ‡∏•‡πâ‡∏ß
        """
        if not hasattr(self, 'evidence_map'):
            self.evidence_map = {}
            
        self.logger.info("üíæ Syncing merged evidence to internal storage...")
        
        if isinstance(merged_evidence, dict):
            for key, ev_list in merged_evidence.items():
                if not isinstance(ev_list, list): continue
                if key not in self.evidence_map:
                    self.evidence_map[key] = []
                
                # Deduplicate content ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏•‡∏∞‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô
                existing_hashes = {hash(str(e.get('content'))[:100]) for e in self.evidence_map[key]}
                for ev in ev_list:
                    ev_hash = hash(str(ev.get('content'))[:100])
                    if ev_hash not in existing_hashes:
                        self.evidence_map[key].append(ev)
                        existing_hashes.add(ev_hash)
        
        self.logger.info(f"‚úÖ Evidence mapping persistence ready. Total groups: {len(self.evidence_map)}")

    # ------------------------------------------------------------------------------------------
    # [ULTIMATE REVISE v2026.1.23] üß© Merge Worker Results (The "Zero-Score" Antidote)
    # ------------------------------------------------------------------------------------------
    def _merge_worker_results(self, sub_result: Dict[str, Any], temp_map: Dict[str, Any]):
        """
        ‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å Worker ‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏•‡∏±‡∏Å ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Evidence 
        ‡∏ó‡∏µ‡πà‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏´‡∏≤‡∏¢‡πÉ‡∏ô‡πÇ‡∏´‡∏°‡∏î Parallel
        """
        if not sub_result:
            return None

        # 1. üîç Identity & Metadata Setup
        sub_id = str(sub_result.get('sub_id', 'Unknown'))
        # ‡∏î‡∏∂‡∏á Level ‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á single ‡πÅ‡∏•‡∏∞ batch result)
        level_received = int(sub_result.get('level') or sub_result.get('highest_full_level', 0))
            
        # 2. üõ°Ô∏è Evidence Mapping Sync (‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Evidence ‡∏´‡∏≤‡∏¢)
        # temp_map ‡∏°‡∏±‡∏Å‡∏à‡∏∞‡∏™‡πà‡∏á‡∏°‡∏≤‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö { "1": [chunks], "2": [chunks] }
        if temp_map and isinstance(temp_map, dict):
            for level_key, evidence_list in temp_map.items():
                if level_key not in self.evidence_map:
                    self.evidence_map[level_key] = []
                
                # ‡∏™‡∏£‡πâ‡∏≤‡∏á Set ‡∏Ç‡∏≠‡∏á ID ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ã‡πâ‡∏≥
                existing_ids = {
                    str(e.get('stable_doc_uuid') or e.get('doc_id') or e.get('source')) 
                    for e in self.evidence_map[level_key] if isinstance(e, dict)
                }
                
                for ev in evidence_list:
                    if not ev or ev in ["na", "n/a"]: continue
                    
                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô
                    ev_id = str(ev.get('stable_doc_uuid') or ev.get('doc_id') or ev.get('source')) if isinstance(ev, dict) else str(ev)
                    
                    if ev_id not in existing_ids:
                        # Normalize ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô Dict ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô
                        if not isinstance(ev, dict):
                            ev = {"content": str(ev), "source": "Manual Evidence", "page": "N/A"}
                        
                        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏≤‡∏Å document_map (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
                        if 'doc_id' in ev and self.document_map:
                            ev['filename'] = self.document_map.get(ev['doc_id'], ev.get('source', 'Unknown'))
                        
                        self.evidence_map[level_key].append(ev)
                        existing_ids.add(ev_id)

        # 3. üèóÔ∏è Manage Target Container (‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Å‡πâ‡∏≠‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏≠‡∏á‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏±‡πâ‡∏ô‡πÜ)
        if not hasattr(self, 'final_subcriteria_results'):
            self.final_subcriteria_results = []

        target = next((r for r in self.final_subcriteria_results if str(r.get('sub_id')) == sub_id), None)
        if not target:
            target = {
                "sub_id": sub_id,
                "sub_criteria_name": sub_result.get('sub_criteria_name') or f"Criteria {sub_id}",
                "weight": float(sub_result.get('weight', 4.0)),
                "level_details": {},
                "highest_full_level": 0,
                "weighted_score": 0.0,
                "is_passed": False,
                "audit_stop_reason": "Initiating...",
                "pdca_overall": {"P": 0.0, "D": 0.0, "C": 0.0, "A": 0.0}
            }
            self.final_subcriteria_results.append(target)

        # 4. üß© Atomic Update (‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏≤‡∏¢ Level)
        if 'level_details' in sub_result and isinstance(sub_result['level_details'], dict):
            target['level_details'].update(sub_result['level_details'])
        else:
            target['level_details'][str(level_received)] = sub_result

        # 5. ‚öñÔ∏è Step-Ladder Maturity Calculation (‡∏à‡∏∏‡∏î‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Score 0.0)
        current_highest = 0
        stop_reason = ""
        total_p, total_d, total_c, total_a, count_lv = 0, 0, 0, 0, 0
        
        for l in range(1, 6):
            l_str = str(l)
            l_data = target['level_details'].get(l_str)
            
            if l_data and isinstance(l_data, dict):
                # üîç ‡πÄ‡∏ä‡πá‡∏Ñ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡πà‡∏≤‡∏ô‡πÅ‡∏ö‡∏ö‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô (Score >= 0.7 ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤ Pass ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ Chain ‡∏Ç‡∏≤‡∏î)
                score_val = float(l_data.get('score', 0))
                is_lv_passed = (
                    l_data.get('is_passed') is True or 
                    l_data.get('is_safety_pass') is True or
                    score_val >= 0.7
                )
                
                if is_lv_passed:
                    current_highest = l
                    l_data['is_passed'] = True # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏•‡∏±‡∏ö
                    
                    # ‡∏™‡∏∞‡∏™‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô PDCA ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°
                    pdca = l_data.get('pdca_breakdown', {})
                    total_p += float(pdca.get('P', 0))
                    total_d += float(pdca.get('D', 0))
                    total_c += float(pdca.get('C', 0))
                    total_a += float(pdca.get('A', 0))
                    count_lv += 1
                else:
                    stop_reason = f"Stopped at L{l}: {str(l_data.get('reason', 'Insufficient evidence'))[:50]}..."
                    break
            else:
                stop_reason = f"No data for L{l}"
                break

        # 6. üí∞ Final Summary Integration
        target['highest_full_level'] = current_highest
        target['is_passed'] = (current_highest >= 1)
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ñ‡πà‡∏ß‡∏á‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å
        target['weighted_score'] = self._calculate_weighted_score(
            highest_full_level=current_highest,
            weight=target['weight'],
            level_details=target['level_details']
        )
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì PDCA ‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ç‡∏≠‡∏á‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ
        if count_lv > 0:
            target['pdca_overall'] = {
                "P": round(total_p / count_lv, 2),
                "D": round(total_d / count_lv, 2),
                "C": round(total_c / count_lv, 2),
                "A": round(total_a / count_lv, 2)
            }
            
        target['audit_stop_reason'] = stop_reason if current_highest < 5 else "Target level achieved"
        
        self.logger.info(f"‚úÖ [MERGE DONE] Sub {sub_id} -> Level {current_highest} (Score: {target['weighted_score']:.2f})")
        return target

           
    def _get_semantic_tag(self, text: str, sub_id: str, level: int, filename: str = "") -> str:
        """
        [ULTIMATE REVISE v2026.25 ‚Äì Required Phase Aware]
        - Follow required_phase ‡∏à‡∏≤‡∏Å contextual_rules ‡∏Ç‡∏≠‡∏á enabler ‡∏ô‡∏±‡πâ‡∏ô ‡πÜ
        - Prompt ‡∏™‡πà‡∏á require_phase ‡πÉ‡∏´‡πâ LLM ‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡∏ï‡πâ‡∏≠‡∏á prefer phase ‡πÑ‡∏´‡∏ô
        - Fallback ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å phase ‡∏à‡∏≤‡∏Å require_phase (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢ ‚Üí random ‡∏ï‡∏≤‡∏° priority)
        - Heuristic ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏ó‡∏∏‡∏Å phase + JSON Clean ‡∏ó‡∏ô‡∏ó‡∏≤‡∏ô
        """
        tenant = getattr(self.config, 'tenant', 'Unknown').upper()
        enabler = getattr(self.config, 'enabler', 'Unknown').upper()
        text_lower = text.lower().strip()

        # ‡∏î‡∏∂‡∏á required_phase ‡∏à‡∏≤‡∏Å rules (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î!)
        require_phases = self.get_rule_content(sub_id, level, "require_phase") or []
        require_str = ", ".join(require_phases) if require_phases else "P,D,C,A"

        # 1. Enhanced Heuristic (‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏ó‡∏∏‡∏Å phase ‚Äì ‡∏Ç‡∏¢‡∏≤‡∏¢‡∏Ñ‡∏≥‡∏à‡∏≤‡∏Å log + defaults)
        if any(k in text_lower for k in [
            "‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢", "‡πÅ‡∏ú‡∏ô", "‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå", "‡∏°‡∏ï‡∏¥", "‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á", "‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢", "‡πÄ‡∏à‡∏ï‡∏ô‡∏≤‡∏£‡∏°‡∏ì‡πå",
            "‡∏ß‡∏¥‡∏™‡∏±‡∏¢‡∏ó‡∏±‡∏®‡∏ô‡πå", "master plan", "roadmap", "km policy", "‡∏à‡∏±‡∏î‡∏ó‡∏≥‡πÅ‡∏ú‡∏ô"
        ]):
            return "P"

        if any(k in text_lower for k in [
            "‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£", "‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥", "‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°", "‡∏≠‡∏ö‡∏£‡∏°", "‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°", "‡∏à‡∏±‡∏î‡∏ó‡∏≥", "‡∏†‡∏≤‡∏û‡∏ñ‡πà‡∏≤‡∏¢",
            "‡∏•‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà", "‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏ú‡∏•", "‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•", "‡∏Ç‡∏±‡∏ö‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô", "‡∏•‡∏á‡∏ô‡∏≤‡∏°", "‡∏°‡∏ï‡∏¥‡∏ö‡∏≠‡∏£‡πå‡∏î",
            "‡πÄ‡∏´‡πá‡∏ô‡∏ä‡∏≠‡∏ö", "deployment", "‡∏à‡∏±‡∏î‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°", "‡∏ñ‡πà‡∏≤‡∏¢‡∏ó‡∏≠‡∏î", "‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£", "‡∏Ñ‡∏ì‡∏∞‡∏ó‡∏≥‡∏á‡∏≤‡∏ô"
        ]):
            return "D"

        if any(k in text_lower for k in [
            "‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏•", "‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô", "‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°", "‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î", "kpi", "‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå", "‡∏™‡∏≥‡∏£‡∏ß‡∏à",
            "‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥", "‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô", "monitoring", "review", "audit", "benchmarking",
            "feedback", "‡πÅ‡∏ö‡∏ö‡∏™‡∏≥‡∏£‡∏ß‡∏à", "‡∏ß‡∏±‡∏î‡∏ú‡∏•"
        ]):
            return "C"

        if any(k in text_lower for k in [
            "‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á", "‡∏û‡∏±‡∏í‡∏ô‡∏≤", "‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç", "‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô", "lesson learned", "‡∏ï‡πà‡∏≠‡∏¢‡∏≠‡∏î",
            "‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°", "‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô", "‡∏¢‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö", "agile", "‡∏ô‡∏≥‡πÑ‡∏õ‡πÉ‡∏ä‡πâ", "‡∏ó‡∏ö‡∏ó‡∏ß‡∏ô",
            "feedback loop"
        ]):
            return "A"

        # 2. LLM tagging (prompt ‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏° required_phase)
        system_prompt = (
            f"Auditor for {tenant} {enabler}. Classify text to ONE PDCA tag only. "
            "P=Plan (‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢/‡πÅ‡∏ú‡∏ô/‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢), D=Do (‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°/‡∏≠‡∏ö‡∏£‡∏°/‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°/‡∏†‡∏≤‡∏û‡∏ñ‡πà‡∏≤‡∏¢/‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£), "
            "C=Check (‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô/KPI/‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô/‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°), A=Act (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á/‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô/‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°). "
            f"Required phases for this level: {require_str}. Prefer one of these phases if ambiguous. "
            "JSON only: {\"tag\":\"P/D/C/A/Other\",\"reason\":\"‡∏™‡∏±‡πâ‡∏ô ‡πÜ ‡πÑ‡∏ó‡∏¢\"}"
        )

        user_prompt = f"File: {filename}\nText (first 500 chars):\n{text[:500]}\n‚Üí JSON"

        try:
            response = self.llm.invoke(system_prompt + "\n" + user_prompt)
            # Clean ‡∏ó‡∏ô‡∏ó‡∏≤‡∏ô‡∏™‡∏∏‡∏î
            cleaned = response.strip()
            cleaned = re.sub(r'^.*?\{', '{', cleaned, flags=re.DOTALL | re.IGNORECASE)
            cleaned = re.sub(r'\}.*?$', '}', cleaned, flags=re.DOTALL | re.IGNORECASE)
            cleaned = cleaned.replace("```json", "").replace("```", "").replace("\n", " ").strip()
            cleaned = re.sub(r',\s*([}\]])', r'\1', cleaned)
            data = json.loads(cleaned)
            if isinstance(data, list) and data:
                data = data[0]
            tag = str(data.get("tag", "Other")).strip().upper()
            if tag in {"P", "D", "C", "A"}:
                reason = data.get('reason', '')
                self.logger.debug(f"[TAG LLM] {tag} | {reason} | {filename[:30]}")
                return tag
        except Exception as e:
            self.logger.warning(f"[TAG LLM FAIL] {sub_id} L{level} {filename[:30]} ‚Üí {str(e)} | Raw: {response[:100]}...")

        # 3. Final fallback (‡∏ï‡∏≤‡∏° required_phase ‚Äì ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢ phase ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏≤‡∏° priority ‡∏Ç‡∏≠‡∏á level)
        if require_phases:
            # Priority: ‡πÄ‡∏ô‡πâ‡∏ô phase ‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ (‡πÄ‡∏ä‡πà‡∏ô L1 ‚Üí D ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ, L4 ‚Üí C, L5 ‚Üí A)
            for phase in require_phases:
                if phase in ["P", "D", "C", "A"]:
                    self.logger.debug(f"[TAG FALLBACK] L{level} ‚Üí {phase} (from required phases)")
                    return phase
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ require_phase ‚Üí fallback ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏ï‡∏≤‡∏° level
        if level <= 3:
            return "D"
        elif level == 4:
            return "C"
        else:
            return "A"
    
    def _build_pdca_context(self, blocks: Dict[str, str]) -> str:
        """
        [REVISED v2026.2]
        - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏ì‡∏µ content ‡∏ß‡πà‡∏≤‡∏á/‡∏™‡∏±‡πâ‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô
        - ‡πÉ‡∏ä‡πâ XML-like ‡πÅ‡∏ï‡πà‡∏ï‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏≠‡∏Å
        """
        tags = ["Plan", "Do", "Check", "Act", "Other"]
        parts = []

        for t in tags:
            content = blocks.get(t, "").strip()
            if not content or len(content) < 10:
                content = "[‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô]"
            # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô prompt ‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô
            content = content[:800]
            parts.append(f"<{t}>{content}</{t}>")

        return "\n".join(parts)
    

    def _is_previous_level_passed(self, sub_id: str, level: int) -> bool:
        """
        [STRICT REVISE v2026.01.18.1] - ‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏ö‡∏ö‡πÄ‡∏Ç‡πâ‡∏°‡∏á‡∏ß‡∏î
        """
        if level <= 1: 
            return True
            
        prev_level = level - 1
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Key ‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏ö‡∏≠‡∏≤‡∏à‡πÉ‡∏ä‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å
        possible_keys = [f"{sub_id}.L{prev_level}", f"{sub_id}_L{prev_level}"]
        
        for key in possible_keys:
            # ‡πÉ‡∏ä‡πâ get() ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô KeyError ‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
            result = getattr(self, 'assessment_results_map', {}).get(key)
            if result:
                if result.get('is_passed') is True:
                    self.logger.info(f"‚úÖ [LEVEL-GATE] Level {prev_level} passed for {sub_id}")
                    return True
                else:
                    self.logger.warning(f"‚ö†Ô∏è [LEVEL-GATE] Level {prev_level} found but status is FAIL")
                    return False

        # Safe Guard: ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≤‡∏° Level ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å
        self.logger.warning(f"üö´ [LEVEL-GATE] No assessment record for L{prev_level}. Blocking L{level}.")
        return False

    def _normalize_thai_text(self, text: str) -> str:
        """
        [ULTIMATE THAI NORMALIZE v2026.1.31 ‚Äì FULL REVISED]
        - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤ '‡∏™‡∏£‡∏∞‡∏´‡∏≤‡∏¢' (‡πÄ‡∏ä‡πà‡∏ô ‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£ -> ‡∏ú‡∏ö‡∏£‡∏´‡∏≤‡∏£) ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ñ‡∏≤‡∏ß‡∏£
        - High Performance: ‡∏ß‡∏ô‡∏•‡∏π‡∏õ Filter + Lowercase ‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏£‡∏≠‡∏ö‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏à‡∏ö
        - Unicode NFC: ‡∏£‡∏ß‡∏° Combining Characters (‡∏™‡∏£‡∏∞/‡∏ß‡∏£‡∏£‡∏ì‡∏¢‡∏∏‡∏Å‡∏ï‡πå) ‡∏Å‡πà‡∏≠‡∏ô‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
        - Strict Thai Range: ‡∏£‡∏±‡∏Å‡∏©‡∏≤ ‡∏Å-‡∏Æ, ‡∏™‡∏£‡∏∞, ‡∏ß‡∏£‡∏£‡∏ì‡∏¢‡∏∏‡∏Å‡∏ï‡πå ‡πÅ‡∏•‡∏∞‡πÄ‡∏•‡∏Ç‡πÑ‡∏ó‡∏¢ (\u0E00-\u0E7F) ‡∏Ñ‡∏£‡∏ö 100%
        """
        if not text or not isinstance(text, str):
            return ""

        # 1. Unicode NFC normalization 
        # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡πÑ‡∏ó‡∏¢ (‡πÄ‡∏ä‡πà‡∏ô ‡∏™‡∏£‡∏∞‡∏≠‡∏≥ ‡∏´‡∏£‡∏∑‡∏≠ ‡∏™‡∏£‡∏∞‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≠‡∏ô‡∏Å‡∏±‡∏ô)
        text = unicodedata.normalize('NFC', text)

        # 2. Compile Regex Pattern ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏ó‡∏µ‡πà‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï
        # ‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏© (a-zA-Z), ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (\u0E00-\u0E7F), ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç (0-9), ‡πÅ‡∏•‡∏∞‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á (\s)
        allowed_pattern = re.compile(r"[a-zA-Z0-9\u0E00-\u0E7F\s]")

        # 3. Single-pass Filter & Lowercase
        # ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£
        res = []
        for char in text:
            if allowed_pattern.match(char):
                # ‡∏ó‡∏≥ Lowercase ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏© (isascii ‡∏ä‡πà‡∏ß‡∏¢‡πÅ‡∏¢‡∏Å‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏≠‡∏≠‡∏Å‡∏ó‡∏±‡∏ô‡∏ó‡∏µ)
                if char.isascii() and char.isalpha():
                    res.append(char.lower())
                else:
                    res.append(char)
        
        # ‡∏£‡∏ß‡∏° List ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô String
        text = "".join(res)

        # 4. Collapse whitespace (‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ã‡πâ‡∏≥‡πÄ‡∏õ‡πá‡∏ô 1 ‡∏ä‡πà‡∏≠‡∏á) ‡πÅ‡∏•‡∏∞ Trim
        text = re.sub(r"\s+", " ", text).strip()

        return text
    
    def enhance_query_for_statement(
        self,
        statement_text: str,
        sub_id: str,
        statement_id: str,
        level: int,
        focus_hint: str = "",
    ) -> List[str]:
        """
        [STRATEGIC QUERY GEN v2026.2.22 ‚Äì FULL REVISION & OPTIMIZED]
        - Robust: handle empty/invalid input, fallback queries ‡πÄ‡∏™‡∏°‡∏≠
        - Phase-aware: weight queries ‡∏ï‡∏≤‡∏° required_phase + level maturity
        - Negative: ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á ‡πÑ‡∏°‡πà‡∏ö‡∏•‡πá‡∏≠‡∏Å‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç (‡πÄ‡∏ä‡πà‡∏ô ‡∏¢‡∏±‡∏á‡∏´‡∏≤ "‡πÅ‡∏ú‡∏ô‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£" ‡πÑ‡∏î‡πâ)
        - Debug: log keywords + sample queries + total count
        - Post-process: dedup fuzzy + truncate 18-24 ‡∏Ñ‡∏≥ + shuffle + max 8
        - Fallback: ‡∏°‡∏µ query ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 2 ‡πÄ‡∏™‡∏°‡∏≠ (tenant + stmt core)
        - Speed: ‡∏à‡∏≥‡∏Å‡∏±‡∏î queries ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î 8 + truncate ‡∏™‡∏±‡πâ‡∏ô‡∏•‡∏á
        """
        logger = logging.getLogger(__name__)
        log_prefix = f"[QUERY-GEN] {sub_id} L{level}"

        # 0. Safety guard
        if not statement_text or not isinstance(statement_text, str):
            logger.warning(f"{log_prefix} Empty/invalid statement_text ‚Üí fallback basic")
            fallback_q = f"{sub_id} {focus_hint or '‡∏ß‡∏¥‡∏™‡∏±‡∏¢‡∏ó‡∏±‡∏®‡∏ô‡πå ‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢'} ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ"
            return [fallback_q, f"{sub_id} ‡πÅ‡∏ú‡∏ô‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£"]

        # Anchors
        enabler_id = getattr(self.config, 'enabler', 'Unknown').upper()
        tenant_name = getattr(self.config, 'tenant', 'Unknown').upper()
        id_anchor = f"{enabler_id} {sub_id}"

        # 1. Required phases + keywords
        require_phases = self.get_rule_content(sub_id, level, "require_phase") or []
        require_str = ", ".join(require_phases) if require_phases else "P,D"

        raw_kws = self.get_rule_content(sub_id, level, "must_include_keywords") or []
        phase_map = {"P": "plan_keywords", "D": "do_keywords", "C": "check_keywords", "A": "act_keywords"}

        for phase in require_phases:
            kw_key = phase_map.get(phase)
            if kw_key:
                raw_kws.extend(self.get_rule_content(sub_id, level, kw_key) or [])

        # Fallback phases if none
        if not require_phases:
            if level <= 3:
                raw_kws.extend(self.get_rule_content(sub_id, 1, "plan_keywords") or [])
                raw_kws.extend(self.get_rule_content(sub_id, 2, "do_keywords") or [])
            else:
                raw_kws.extend(self.get_rule_content(sub_id, 2, "do_keywords") or [])
                raw_kws.extend(self.get_rule_content(sub_id, 3, "check_keywords") or [])

        clean_kws = sorted(set(str(k).strip() for k in raw_kws if k))
        keywords_str = " ".join(clean_kws[:5])
        short_keywords = " ".join(clean_kws[:3])

        logger.debug(f"{log_prefix} Keywords ({len(clean_kws)}): {keywords_str[:100]}...")

        # Clean stmt
        clean_stmt = statement_text.split("‡πÄ‡∏ä‡πà‡∏ô", 1)[0].strip()
        clean_stmt = re.sub(r'[^\w\s]', '', clean_stmt)[:70]

        queries: List[str] = []

        # 2. Negative + Core Queries
        neg_strict = "-‡πÅ‡∏ú‡∏ô‡πÅ‡∏°‡πà‡∏ö‡∏ó -‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏ä‡∏≤‡∏ï‡∏¥ -MasterPlan -‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏£‡∏∏‡∏õ -‡∏†‡∏≤‡∏Ñ‡∏ú‡∏ô‡∏ß‡∏Å"

        queries.append(f"{id_anchor} {clean_stmt} {keywords_str}")
        queries.append(f"{id_anchor} {clean_stmt}")

        if level <= 3:
            queries.append(f"{tenant_name} ‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏® ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á ‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° {id_anchor} {short_keywords}")
            queries.append(f"{id_anchor} (‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£ OR ‡∏•‡∏á‡∏ô‡∏≤‡∏° OR ‡∏°‡∏∏‡πà‡∏á‡∏°‡∏±‡πà‡∏ô OR ‡∏Ç‡∏±‡∏ö‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô) {neg_strict}")
        else:
            queries.append(f"{tenant_name} ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏• KPI ‡∏ú‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à {id_anchor} {short_keywords}")
            queries.append(f"{id_anchor} (‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏• OR ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô OR ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏° OR ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á) {neg_strict}")

        # 3. Source Bias (P/D)
        if "P" in require_phases or "D" in require_phases:
            queries.append(f"{id_anchor} ‡∏°‡∏ï‡∏¥‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏° ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏° ‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏® ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á ‡∏•‡∏á‡∏ô‡∏≤‡∏° {short_keywords}")

        # 4. Priority 1: query_synonyms
        query_syn = self.get_rule_content(sub_id, level, "query_synonyms") or ""
        if query_syn:
            queries.append(f"{id_anchor} {query_syn} {short_keywords}")

        # 5. Priority 2: specific_contextual_rule
        if not query_syn:
            specific_rule = self.get_rule_content(sub_id, level, "specific_contextual_rule") or ""
            if specific_rule:
                rule_words = [w.strip() for w in specific_rule.split() if len(w.strip()) >= 4]
                rule_synonyms = " ".join(list(dict.fromkeys(rule_words))[:8])
                if rule_synonyms:
                    queries.append(f"{id_anchor} {rule_synonyms} {short_keywords}")

        # 6. Priority 3: Fallback PDCA
        fallback_synonyms = {
            "P": "‡πÅ‡∏ú‡∏ô‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£ ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ ‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢ ‡πÄ‡∏à‡∏ï‡∏ô‡∏≤‡∏£‡∏°‡∏ì‡πå ‡∏°‡∏∏‡πà‡∏á‡∏°‡∏±‡πà‡∏ô ‡∏•‡∏á‡∏ô‡∏≤‡∏°",
            "D": "‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥ ‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£ ‡∏Ç‡∏±‡∏ö‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô ‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏° ‡∏≠‡∏ö‡∏£‡∏° ‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏° ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°",
            "C": "‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏° ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏• KPI ‡∏ß‡∏±‡∏î‡∏ú‡∏• ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•",
            "A": "‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á ‡∏û‡∏±‡∏í‡∏ô‡∏≤ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç ‡∏ï‡πà‡∏≠‡∏¢‡∏≠‡∏î ‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏° ‡∏¢‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö Best Practice"
        }

        for phase in require_phases:
            fallback = fallback_synonyms.get(phase, "")
            if fallback:
                queries.append(f"{id_anchor} {fallback} {short_keywords}")

        # 7. KM Specific
        if level <= 3 and enabler_id == "KM" and "D" in require_phases:
            queries.append(f"{id_anchor} (‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏° OR ‡∏≠‡∏ö‡∏£‡∏° OR ‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏° OR ‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏Å‡∏£ OR ‡∏ñ‡πà‡∏≤‡∏¢‡∏ó‡∏≠‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ) {neg_strict}")

        # 8. Advanced/Focus + tenant fallback
        if level >= 4 or focus_hint:
            adv = "‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏° Best Practice Lesson Learned ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå"
            queries.append(f"{id_anchor} {adv} {focus_hint or ''} {tenant_name}")

        # 9. Fallback core (‡∏ñ‡πâ‡∏≤ queries ‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏Å‡∏¥‡∏ô)
        if len(queries) < 3:
            queries.append(f"{tenant_name} {clean_stmt} {short_keywords}")
            queries.append(f"{id_anchor} ‡πÅ‡∏ú‡∏ô‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£ ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ")

        # 10. Post-process: Dedup + Truncate + Shuffle + Limit
        final_queries = []
        seen = set()
        import random
        for q in queries:
            words = q.split()
            trunc_len = random.randint(18, 24)
            q_trunc = " ".join(words[:trunc_len])
            q_norm = " ".join(words[:16])
            if q_trunc and q_norm not in seen:
                final_queries.append(q_trunc)
                seen.add(q_norm)

        random.shuffle(final_queries)
        final_queries = final_queries[:8]  # Max 8

        logger.info(f"üöÄ [Query Gen v2026.2.22] {sub_id} L{level} | Generated {len(final_queries)} queries (Phases: {require_str}) | Neg: {neg_strict}")
        if final_queries:
            logger.debug(f"{log_prefix} Final queries (top 3): {final_queries[:3]}")

        return final_queries
 
    
    def _get_level_aware_queries(self, criteria_id: str, level_key: str) -> List[str]:
        """
        ‡∏î‡∏∂‡∏á‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏à‡∏≤‡∏Å JSON Rules (query_synonyms) ‡∏°‡∏≤‡∏ú‡∏™‡∏°‡∏Å‡∏±‡∏ö PDCA Keywords
        """
        # 1. ‡∏î‡∏∂‡∏á‡∏Å‡∏é‡∏à‡∏≤‡∏Å JSON ‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ß‡πâ‡πÉ‡∏ô self.contextual_rules_map
        criteria_rules = self.contextual_rules_map.get(criteria_id, {})
        level_rule = criteria_rules.get(level_key, {})
        
        # 2. ‡∏î‡∏∂‡∏á Synonyms ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÑ‡∏ß‡πâ (‡πÄ‡∏ä‡πà‡∏ô "‡∏Ñ‡∏ì‡∏∞‡∏ó‡∏≥‡∏á‡∏≤‡∏ô ‡∏Å‡∏•‡πÑ‡∏Å‡∏Ç‡∏±‡∏ö‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô...")
        synonyms = level_rule.get("query_synonyms", "")
        
        # 3. ‡∏î‡∏∂‡∏á‡∏Å‡∏•‡∏∏‡πà‡∏° Phase ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ô‡πâ‡∏ô (P, D, C, A)
        required_phases = level_rule.get("require_phase", ["P", "D"])
        
        # 4. ‡∏î‡∏∂‡∏á‡∏Ñ‡∏≥‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ Phase ‡∏à‡∏≤‡∏Å _enabler_defaults
        defaults = self.contextual_rules_map.get("_enabler_defaults", {})
        
        generated_queries = []
        
        # Query ‡∏´‡∏•‡∏±‡∏Å: ‡πÄ‡∏ô‡πâ‡∏ô‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå + Synonyms
        main_q = f"{self.enabler} {criteria_id} {synonyms}"
        generated_queries.append(self._normalize_thai_text(main_q))
        
        # Query ‡πÄ‡∏™‡∏£‡∏¥‡∏°: ‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏° Phase ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏ß‡∏≤‡∏î‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö PDCA
        for phase in required_phases:
            phase_key = f"{phase.lower()}_keywords"
            phase_words = " ".join(defaults.get(phase_key, [])[:4]) # ‡πÄ‡∏≠‡∏≤‡∏°‡∏≤‡πÅ‡∏Ñ‡πà 4 ‡∏Ñ‡∏≥‡∏Å‡∏±‡∏ô‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô
            combined_q = f"{self.enabler} {criteria_id} {synonyms} {phase_words}"
            generated_queries.append(self._normalize_thai_text(combined_q))
            
        return list(set(generated_queries)) # ‡∏•‡∏ö‡∏ï‡∏±‡∏ß‡∏ã‡πâ‡∏≥

    def _perform_adaptive_retrieval(
        self,
        sub_id: str,
        level: int,
        stmt: str,
        vectorstore_manager: Any,
    ) -> tuple[List[Dict], float]:
        """
        [HYBRID REVISED v2026.1.23] - Optimized for Mac & Thai Language
        - Hybrid Strategy: ‡πÉ‡∏ä‡πâ JSON Rules ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å ‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ Legacy Enhance ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏™‡∏£‡∏¥‡∏°
        - Dynamic Threshold: ‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏Ñ‡∏±‡∏î‡∏≠‡∏≠‡∏Å‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
        - Smart Early Exit: ‡∏´‡∏¢‡∏∏‡∏î‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏à‡∏≠‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà '‡∏î‡∏µ‡∏û‡∏≠' ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà '‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡πÅ‡∏ö‡∏ö' (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Loop ‡∏ô‡∏≤‡∏ô)
        """
        if not stmt or not isinstance(stmt, str):
            return [], 0.0

        # --- 1. Configuration & Local Tuning ---
        level_key = f"L{level}"
        current_tenant = getattr(self.config, "tenant", "PEA").upper()
        
        # ‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Å‡∏±‡∏ö BGE-M3 ‡∏ö‡∏ô Mac
        EXIT_SCORE_THRESHOLD = CRITICAL_CA_THRESHOLD
        LOCAL_RERANK_FLOOR = RERANK_THRESHOLD      # ‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡∏ó‡∏µ‡πà 0.20 (‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏Å‡∏±‡∏ö .env)
        MAX_LOOP_QUERIES = 6           # ‡∏£‡∏±‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î 6 loops ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏ä‡πâ‡∏≤‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
        
        candidates: List[Dict] = []
        final_max_rerank = 0.0
        used_uuids = set()

        # --- 2. Step 1: Priority Document Mapping ---
        # ‡∏î‡∏∂‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏£‡∏∞‡∏ö‡∏∏‡πÑ‡∏ß‡πâ‡πÉ‡∏ô Mapping (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ) ‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ê‡∏≤‡∏ô‡∏Å‡πà‡∏≠‡∏ô
        try:
            mapped_ids, priority_docs = self._get_mapped_uuids_and_priority_chunks(
                sub_id=sub_id, level=level, statement_text=stmt, vectorstore_manager=vectorstore_manager
            ) or (set(), [])
            for p in priority_docs:
                if p.get("chunk_uuid"): used_uuids.add(p.get("chunk_uuid"))
        except Exception as e:
            self.logger.error(f"‚ùå Priority loading failed: {e}")
            mapped_ids, priority_docs = set(), []

        # --- 3. Step 2: Hybrid Query Generation ---
        # ‡∏£‡∏ß‡∏°‡∏û‡∏•‡∏±‡∏á JSON Rules (‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥) + Enhance Query (‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°)
        all_queries = []
        
        # (A) ‡∏ï‡∏±‡∏ß‡∏´‡∏•‡∏±‡∏Å: ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å JSON
        json_queries = self._get_level_aware_queries(sub_id, level_key)
        all_queries.extend(json_queries)
        
        # (B) ‡∏ï‡∏±‡∏ß‡πÄ‡∏™‡∏£‡∏¥‡∏°: ‡∏ñ‡πâ‡∏≤ JSON ‡∏°‡∏µ‡∏ô‡πâ‡∏≠‡∏¢ ‡πÉ‡∏´‡πâ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å Legacy ‡∏°‡∏≤‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏ß‡∏µ‡πà‡∏¢‡∏á‡πÅ‡∏´
        if len(all_queries) < 3:
            legacy_queries = self.enhance_query_for_statement(stmt, sub_id, f"{sub_id}.L{level}", level)
            for q in legacy_queries:
                if q not in all_queries: all_queries.append(q)

        # ‡∏Å‡∏£‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô
        active_queries = [q for q in all_queries if len(q.strip()) > 5][:MAX_LOOP_QUERIES]
        self.logger.info(f"üöÄ [HYBRID-QUERY] {sub_id} {level_key} | Total: {len(active_queries)} (JSON + Fallback)")

        # --- 4. Step 3: Iterative Retrieval Loop ---
        for i, q in enumerate(active_queries):
            q_norm = self._normalize_thai_text(q)
            
            try:
                res = self.rag_retriever(
                    query=q_norm,
                    doc_type=self.doc_type,
                    sub_id=sub_id,
                    level=level,
                    vectorstore_manager=vectorstore_manager,
                    stable_doc_ids=mapped_ids,
                ) or {}
            except Exception as e:
                self.logger.error(f"‚ùå Retrieval error @ loop {i+1}: {e}")
                continue

            loop_docs = res.get("top_evidences") or []
            if not loop_docs: continue

            # ‡∏ß‡∏±‡∏î‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û Loop ‡∏ô‡∏µ‡πâ
            current_max = max([d.get("score", 0.0) for d in loop_docs])
            final_max_rerank = max(final_max_rerank, current_max)

            # ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Chunk ‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÑ‡∏°‡πà‡∏ô‡πà‡∏≤‡πÄ‡∏Å‡∏•‡∏µ‡∏¢‡∏î (>= 0.15)
            new_found = 0
            for d in loop_docs:
                uid = d.get("chunk_uuid")
                score = d.get("score", 0.0)
                if uid and uid not in used_uuids and score >= 0.15:
                    used_uuids.add(uid)
                    candidates.append(d)
                    new_found += 1

            self.logger.info(
                f"üîç [LOOP {i+1}] Query: {q_norm[:40]}... | New: {new_found} | Max Score: {current_max:.4f}"
            )

            # --- SMART EXIT ---
            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏à‡∏≠‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà '‡∏î‡∏µ‡∏û‡∏≠' ‡πÅ‡∏•‡∏∞‡∏à‡∏≥‡∏ô‡∏ß‡∏ô '‡πÄ‡∏¢‡∏≠‡∏∞‡∏û‡∏≠' ‡πÉ‡∏´‡πâ‡∏´‡∏¢‡∏∏‡∏î‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß
            if current_max >= EXIT_SCORE_THRESHOLD and len(candidates) >= 12:
                self.logger.info(f"üéØ [SMART EXIT] Found high-quality match ({current_max:.4f}).")
                break

        # --- 5. Step 4: Recovery (‡∏Å‡∏£‡∏ì‡∏µ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ï‡πà‡∏≥‡∏°‡∏≤‡∏Å) ---
        if final_max_rerank < LOCAL_RERANK_FLOOR and len(candidates) < 5:
            self.logger.warning(f"‚ö†Ô∏è [LOW-RESULT] Final score {final_max_rerank:.4f} is too low. Trying core recovery...")
            recovery_q = self._normalize_thai_text(f"{sub_id} {current_tenant} {stmt[:40]}")
            res_fb = self.rag_retriever(query=recovery_q, doc_type=self.doc_type, vectorstore_manager=vectorstore_manager)
            for d in (res_fb.get("top_evidences") or []):
                if d.get("chunk_uuid") not in used_uuids:
                    candidates.append(d)

        # --- 6. Step 5: Final Assembly ---
        all_results = priority_docs + candidates
        
        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô Rerank
        all_results.sort(key=lambda x: x.get("score", 0.0), reverse=True)

        # ‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ LLM ‡∏ï‡∏≤‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÉ‡∏ô .env (ANALYSIS_FINAL_K)
        final_limit = int(os.environ.get("ANALYSIS_FINAL_K", "15"))
        final_docs = all_results[:final_limit]

        self.logger.info(
            f"üèÅ [DONE] {sub_id} L{level} | Final Chunks: {len(final_docs)} | Max Rerank Score: {final_max_rerank:.4f}"
        )

        return final_docs, float(final_max_rerank)

    def _log_pdca_status(self, sub_id, name, level, blocks, req_phases, sources_count, score, conf_level, **kwargs):
        """
        [THE AUDITOR DASHBOARD v2026.3.10 - FULL REVISED]
        üß© ‡∏£‡∏∞‡∏ö‡∏ö‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ PDCA ‡πÅ‡∏ö‡∏ö Real-Count Dashboard
        - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç Maturity Gap ‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ó‡∏µ‡πà Save ‡∏à‡∏£‡∏¥‡∏á
        - ‡πÉ‡∏ä‡πâ‡∏£‡∏∞‡∏ö‡∏ö Double-Check (Payload Count + Tagging List)
        """
        try:
            # 1. ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏ô‡∏±‡∏ö‡∏à‡∏≤‡∏Å Payload ‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà‡∏â‡∏µ‡∏î‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤ (Single Source of Truth)
            # ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å pdca_breakdown ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏™‡πà‡∏á‡∏°‡∏≤‡∏à‡∏≤‡∏Å _run_single_assessment
            actual_counts = kwargs.get('pdca_breakdown', {}) 
            raw_tagging = kwargs.get('tagging_result') or []
            is_safety_pass = kwargs.get('is_safety_pass', False)
            
            status_parts = []
            extract_parts = []
            
            # Mapping ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á Key ‡πÉ‡∏ô JSON Response ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡∏¢‡πà‡∏≠ Phase
            mapping = [
                ("Extraction_P", "P"), 
                ("Extraction_D", "D"), 
                ("Extraction_C", "C"), 
                ("Extraction_A", "A")
            ]

            # 2. ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏£‡∏≤‡∏¢ Phase
            for full_key, short in mapping:
                # --- [REVISED COUNTING LOGIC] ---
                # ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: 1. ‡∏î‡∏π‡∏à‡∏≤‡∏Å actual_counts | 2. ‡∏ô‡∏±‡∏ö‡∏à‡∏≤‡∏Å raw_tagging list
                if actual_counts and short in actual_counts:
                    count = actual_counts[short]
                elif isinstance(raw_tagging, list):
                    count = raw_tagging.count(short)
                else:
                    count = 0
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ LLM ‡∏™‡∏Å‡∏±‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ (Extraction) ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                # ‡∏Å‡∏£‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á‡∏´‡∏£‡∏∑‡∏≠ N/A ‡∏≠‡∏≠‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Icon ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥
                content = str(blocks.get(full_key, "")).strip()
                ai_found = bool(content and content.lower() not in [
                    "-", "n/a", "none", "null", "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô", "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏"
                ])
                
                # --- [ICON LOGIC v2026.3.10] ---
                # ‚úÖ: ‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÄ‡∏ä‡∏¥‡∏á‡∏õ‡∏£‡∏∞‡∏à‡∏±‡∏Å‡∏©‡πå (Count > 0)
                # üî∑: ‡∏£‡∏∞‡∏ö‡∏ö Force Pass ‡∏´‡∏£‡∏∑‡∏≠ AI ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏à‡∏≠‡πÄ‡∏≠‡∏á‡πÅ‡∏ï‡πà RAG Tagging ‡πÑ‡∏°‡πà‡∏ï‡∏¥‡∏î
                # ‚ûñ: ‡πÄ‡∏ü‡∏™‡∏ô‡∏µ‡πâ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö (Not in req_phases)
                # ‚ùå: ‡πÄ‡∏ü‡∏™‡∏ó‡∏µ‡πà‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡πÄ‡∏•‡∏¢ (Count=0 ‡πÅ‡∏•‡∏∞ AI ‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠)
                
                if count > 0: 
                    icon = "‚úÖ" 
                elif ai_found or (is_safety_pass and short in req_phases): 
                    icon = "üî∑"
                elif short not in req_phases: 
                    icon = "‚ûñ"
                else: 
                    icon = "‚ùå"
                
                # ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ ‡πÄ‡∏ä‡πà‡∏ô P:‚úÖ(15)
                status_parts.append(f"{short}:{icon}({count})")
                
                # ‡πÄ‡∏Å‡πá‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏Å‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏±‡πâ‡∏ô‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥ Trace Log (2 ‡∏ä‡∏¥‡πâ‡∏ô‡πÅ‡∏£‡∏Å)
                if ai_found and len(extract_parts) < 2:
                    clean_content = content.replace("\n", " ")
                    extract_parts.append(f"[{short}: {clean_content[:60]}...]")

            # 3. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•
            display_score = float(score) if score is not None else 0.0
            
            # 4. [DASHBOARD OUTPUT] ‡∏û‡∏¥‡∏°‡∏û‡πå Log ‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠
            # ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á
            self.logger.info(
                f"üìä [PDCA-STATUS] {sub_id} L{level} | {str(name)[:60]}...\n"
                f"   Maturity Gap: {' '.join(status_parts)}{' üõ°Ô∏è[SAFETY-PASS]' if is_safety_pass else ''}\n"
                f"   Summary: Score={display_score:.2f} | Evidence={sources_count} chunks | Conf={conf_level.upper()}"
            )
            
            # 5. ‡∏û‡∏¥‡∏°‡∏û‡πå Log ‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏£‡πà‡∏≠‡∏á‡∏£‡∏≠‡∏¢‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô (Traceability)
            if extract_parts:
                self.logger.info(f"üîç [EXTRACT-TRACE] {' | '.join(extract_parts)}")

        except Exception as e:
            # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏£‡∏∞‡∏ö‡∏ö‡∏û‡∏±‡∏á‡∏´‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏î Error ‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ Log Dashboard
            self.logger.error(f"‚ùå Critical Error in _log_pdca_status: {str(e)}")

    def _summarize_evidence_list_short(self, evidences: list, max_sentences: int = 3) -> str:
        """
        [REVISED v2026.SUMMARY.4]
        - ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô Method ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ self.logger
        - ‡πÄ‡∏ô‡πâ‡∏ô‡∏î‡∏∂‡∏á Source ‡πÅ‡∏•‡∏∞ Page ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á Audit Traceability
        - ‡∏õ‡∏£‡∏±‡∏ö Formatting ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô Bullet points (LLM ‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢‡∏Å‡∏ß‡πà‡∏≤ Pipe '|')
        """
        if not evidences:
            return "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°"
        
        parts = []
        # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û (‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤) ‡πÅ‡∏•‡∏∞‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ï‡∏≤‡∏° max_sentences
        valid_evidences = [
            ev for ev in evidences 
            if isinstance(ev, dict) and (ev.get("text") or ev.get("content", "")).strip()
        ]
        
        # ‡∏ï‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ (‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏à‡∏£‡∏¥‡∏á)
        target_count = max(1, min(len(valid_evidences), max_sentences))
        
        for ev in valid_evidences[:target_count]:
            # 1. ‡∏™‡∏Å‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏≤ (Source Mapping)
            filename = (ev.get("file_name") or ev.get("source") or 
                        ev.get("source_filename") or "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå")
            page = ev.get("page", "-")
            
            # 2. ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ (Data Cleaning)
            raw_text = ev.get("text") or ev.get("content") or ""
            # ‡∏•‡∏ö‡∏Å‡∏≤‡∏£‡∏Ç‡∏∂‡πâ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏î‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡πà‡∏ß‡∏ô‡∏ï‡πâ‡∏ô
            clean_text = " ".join(raw_text.split()).strip()
            text_preview = clean_text[:150] # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡πÄ‡∏õ‡πá‡∏ô 150 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡∏Ç‡∏∂‡πâ‡∏ô
            
            # 3. ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏£‡πà‡∏≤‡∏á (Formatting)
            if text_preview:
                parts.append(f"‚Ä¢ [{filename}, ‡∏´‡∏ô‡πâ‡∏≤ {page}]: \"{text_preview}...\"")
            else:
                parts.append(f"‚Ä¢ [{filename}, ‡∏´‡∏ô‡πâ‡∏≤ {page}]: (‡∏û‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÑ‡∏î‡πâ)")

        # ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡∏Ç‡∏∂‡πâ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ AI ‡πÅ‡∏¢‡∏Å‡πÅ‡∏¢‡∏∞‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ä‡∏¥‡πâ‡∏ô‡πÑ‡∏î‡πâ‡∏á‡πà‡∏≤‡∏¢
        return "\n".join(parts) 
    
    def relevance_score_fn(self, evidence: Dict[str, Any], sub_id: str, level: int) -> float:
        """
        [REVISED RELEVANCE SCORE v2026.3.1 - Balanced & Robust]
        - 45% Rerank + 35% Keyword + 20% Bonuses
        - PDCA tag bonus ‡∏™‡∏π‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡∏ñ‡πâ‡∏≤‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö required phases
        - Source grading ‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏´‡πâ‡∏™‡∏°‡∏î‡∏∏‡∏• (primary + secondary)
        - Min floor ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö rerank ‡∏™‡∏π‡∏á
        - Logging ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô
        """
        if not evidence:
            return 0.0

        # 1. Rerank (45%)
        rerank_raw = evidence.get('rerank_score') or evidence.get('score') or 0.0
        rerank_score = float(rerank_raw) if isinstance(rerank_raw, (int, float)) else 0.0
        normalized_rerank = min(max(rerank_score, 0.0), 1.0)

        # 2. ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        text = (evidence.get('text') or evidence.get('page_content') or '').lower().strip()
        meta = evidence.get('metadata', {}) if isinstance(evidence.get('metadata'), dict) else {}
        filename = (meta.get('source') or meta.get('source_filename') or '').lower()

        # 3. Cumulative rules
        cum_rules = self.get_cumulative_rules_cached(sub_id, level)

        # 4. Source Grading (‡∏õ‡∏£‡∏±‡∏ö‡∏™‡∏°‡∏î‡∏∏‡∏•)
        source_bonus = 0.0
        primary = ["‡∏°‡∏ï‡∏¥", "‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å", "‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á", "‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®", "‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢", "‡πÅ‡∏ú‡∏ô‡πÅ‡∏°‡πà‡∏ö‡∏ó", "‡∏°‡∏ï‡∏¥‡∏ö‡∏≠‡∏£‡πå‡∏î"]
        secondary = ["assessment report", "‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô", "‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•", "‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏•", "kpi"]
        if any(p in filename for p in primary):
            source_bonus += 0.20
        if any(p in filename for p in secondary):
            source_bonus += 0.10  # ‡πÑ‡∏°‡πà‡∏•‡∏ö ‡πÅ‡∏ï‡πà‡∏•‡∏î‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å

        # 5. Keyword Score (35%)
        target_kws = set()
        if level <= 2:
            target_kws.update(cum_rules.get('plan_keywords', []) + cum_rules.get('do_keywords', []))
        else:
            target_kws.update(cum_rules.get('check_keywords', []) + cum_rules.get('act_keywords', []))

        match_count = sum(1 for kw in target_kws if kw.lower() in text)
        expected = max(1, len(target_kws) * 0.3)  # ‡∏õ‡∏£‡∏±‡∏ö threshold ‡∏•‡∏á‡∏ô‡∏¥‡∏î
        keyword_score = min((match_count / expected) ** 0.6, 1.0)  # ^0.6 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏ï‡πà‡∏≥‡πÄ‡∏Å‡∏¥‡∏ô
        keyword_score = max(keyword_score, 0.20 if match_count >= 1 else 0.0)

        # 6. PDCA Tag Bonus (0.30 ‡∏ñ‡πâ‡∏≤‡∏ï‡∏£‡∏á required)
        pdca_bonus = 0.0
        pdca_tag = evidence.get('pdca_tag') or meta.get('pdca_tag') or ""
        required_phases = cum_rules.get('required_phases', [])
        if pdca_tag and str(pdca_tag).upper() in required_phases:
            pdca_bonus = 0.30
        elif pdca_tag and str(pdca_tag).upper() in {'P', 'D', 'C', 'A'}:
            pdca_bonus = 0.15  # bonus ‡πÄ‡∏•‡πá‡∏Å‡∏ñ‡πâ‡∏≤‡∏ï‡∏£‡∏á PDCA ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô required

        # 7. Neighbor Bonus
        neighbor_bonus = 0.15 if evidence.get('is_neighbor', False) or meta.get('is_neighbor', False) else 0.0

        # 8. Specific Rule Bonus (‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö rule ‡πÄ‡∏â‡∏û‡∏≤‡∏∞)
        specific_rule = cum_rules.get('specific_contextual_rule', '').lower()
        rule_bonus = 0.15 if specific_rule and any(word in text for word in specific_rule.split()[:10]) else 0.0

        # 9. ‡∏£‡∏ß‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô (45% Rerank + 35% Keyword + 20% Bonuses)
        final_score = (
            0.45 * normalized_rerank +
            0.35 * keyword_score +
            source_bonus + pdca_bonus + neighbor_bonus + rule_bonus
        )

        # 10. Min floor ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö rerank ‡∏™‡∏π‡∏á
        if normalized_rerank > 0.80:
            final_score = max(final_score, 0.45)

        final_score = min(max(final_score, 0.0), 1.0)

        # 11. Logging (info ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö high score, debug ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏∏‡∏Å‡∏ï‡∏±‡∏ß)
        if normalized_rerank > 0.75 or final_score > 0.60:
            self.logger.info(
                f"[HIGH-RELEVANCE] {sub_id} L{level} | "
                f"final={final_score:.4f} | rerank={normalized_rerank:.4f} | "
                f"kw={keyword_score:.4f} | pdca_bonus={pdca_bonus:.3f} | "
                f"tag={pdca_tag} | source_bonus={source_bonus:.3f}"
            )

        self.logger.debug(
            f"[{sub_id} L{level}] RelScore: {final_score:.4f} | Rerank: {normalized_rerank:.4f} | "
            f"KW: {keyword_score:.4f} | PDCA: {pdca_bonus:.3f} | Src: {source_bonus:.3f}"
        )

        return final_score

    def _build_multichannel_context_for_level( # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô Private Method
        self, # ‡πÄ‡∏û‡∏¥‡πà‡∏° self
        level: int,
        top_evidences: List[Dict[str, Any]],
        previous_levels_map: Optional[Dict[str, Any]] = None,
        previous_levels_evidence: Optional[List[Dict[str, Any]]] = None,
        max_main_context_tokens: int = 3000, 
        max_summary_sentences: int = 4,
        max_context_length: Optional[int] = None, 
        **kwargs
    ) -> Dict[str, Any]:
        """
        [ULTIMATE OPTIMIZED v2026.8 - REFACTORED AS CLASS METHOD]
        """
        K_MAIN = 5
        # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å Config ‡πÉ‡∏ô Class ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢‡∏ñ‡πâ‡∏≤‡∏°‡∏µ ‡πÄ‡∏ä‡πà‡∏ô self.config.l1_threshold
        MIN_RELEVANCE_FOR_AUX = 0.15 if level == 1 else 0.4 

        # 1. Baseline Summary
        baseline_evidence = previous_levels_evidence or []
        if previous_levels_map:
            for lvl_ev in previous_levels_map.values():
                baseline_evidence.extend(lvl_ev)

        # Normalize list (‡∏ï‡∏≤‡∏° Logic ‡πÄ‡∏î‡∏¥‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡∏ö‡∏±‡πä‡∏Å Slice ‡πÅ‡∏•‡πâ‡∏ß)
        baseline_evidence_list = []
        if isinstance(baseline_evidence, list):
            baseline_evidence_list = baseline_evidence
        elif isinstance(baseline_evidence, dict):
            baseline_evidence_list = list(baseline_evidence.values())
        
        summarizable_baseline = [
            item for item in baseline_evidence_list[:40] 
            if isinstance(item, dict) and (item.get("text") or item.get("content", "")).strip()
        ]

        if not summarizable_baseline:
            baseline_summary = "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ (‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏ó‡∏µ‡πà Level 1)"
        else:
            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ú‡πà‡∏≤‡∏ô self
            baseline_summary = self._summarize_evidence_list_short(
                summarizable_baseline,
                max_sentences=max_summary_sentences
            )

        # 2. Direct + Aux Separation
        direct, aux_candidates = [], []

        for idx, ev in enumerate(top_evidences[:40], 1):
            if not isinstance(ev, dict): continue

            tag = (ev.get("pdca_tag") or ev.get("PDCA") or "Other").upper()
            relevance = ev.get("rerank_score") or ev.get("score", 0.0)
            text_preview = (ev.get('text', '')[:80] + "...") if ev.get('text') else "[No text]"

            # ‡πÉ‡∏ä‡πâ self.logger ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢
            self.logger.debug(f"[TAG-CHECK L{level} #{idx}] Rel: {relevance:.3f} | Tag: {tag} | Preview: {text_preview}")

            if tag in {"P", "PLAN", "D", "DO", "C", "CHECK", "A", "ACT"}:
                direct.append(ev)
            elif relevance >= MIN_RELEVANCE_FOR_AUX:
                aux_candidates.append(ev)

        # 3. Logic ‡∏Å‡∏≤‡∏£‡∏¢‡πâ‡∏≤‡∏¢ Aux ‡πÅ‡∏•‡∏∞ Fallback (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏° ‡πÅ‡∏ï‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô self.logger)
        if len(direct) < K_MAIN:
            need = K_MAIN - len(direct)
            moved = aux_candidates[:need]
            direct.extend(moved)
            aux_candidates = aux_candidates[need:]
            self.logger.info(f"[DIRECT-FILL] Moved {need} aux chunks to direct (total direct: {len(direct)})")

        if level == 1 and len(direct) == 0 and top_evidences:
            need = min(K_MAIN, len(top_evidences))
            forced_chunks = sorted(top_evidences, key=lambda e: e.get("rerank_score", 0) or e.get("score", 0), reverse=True)[:need]
            direct.extend(forced_chunks)
            self.logger.warning(f"[L1-ULTRA-FALLBACK] No PDCA tag at all ‚Üí Forced top {need} chunks to direct")

        # 5. ‡∏™‡∏£‡πâ‡∏≤‡∏á aux_summary (‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ú‡πà‡∏≤‡∏ô self)
        aux_summary = self._summarize_evidence_list_short(aux_candidates, max_sentences=3) if aux_candidates else \
            "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏£‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠"

        # 6. Return ‡∏û‡∏£‡πâ‡∏≠‡∏° Debug Meta
        self.logger.info(f"Context L{level} ‚Üí Direct:{len(direct)} | Aux:{len(aux_candidates)} | Baseline:{len(summarizable_baseline)}")

        return {
            "baseline_summary": baseline_summary,
            "direct_context": "", 
            "aux_summary": aux_summary,
            "debug_meta": {
                "level": level,
                "direct_count": len(direct),
                "aux_count": len(aux_candidates),
                "top_relevance": max((ev.get("rerank_score", 0) for ev in top_evidences), default=0)
            },
        }

    def _run_expert_re_evaluation(
        self,
        sub_id: str,
        level: int,
        statement_text: str,
        context: str,
        first_attempt_reason: str,
        missing_tags: Union[List[str], Set[str]],
        highest_rerank_score: float,
        sub_criteria_name: str,
        llm_evaluator_to_use: Any,
        base_kwargs: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        [JUDICIAL REVIEW MODULE - FULL REVISED v2026.3.28]
        - Inject hint_msg ‡πÄ‡∏Ç‡πâ‡∏≤ pdca_blocks ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á (‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ evaluate_pdca ‡πÑ‡∏°‡πà‡∏£‡∏±‡∏ö context)
        - ‡∏°‡∏µ retry mechanism (max 2 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á) + basic timeout handling
        - Validation ‡πÄ‡∏Ç‡πâ‡∏°‡∏á‡∏ß‡∏î + fallback ‡∏ó‡∏µ‡πà‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
        - Log ‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠ traceability ‡πÅ‡∏•‡∏∞ debug
        - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö missing_tags ‡∏ó‡∏±‡πâ‡∏á list ‡πÅ‡∏•‡∏∞ set
        """
        log_prefix = f"Sub:{sub_id} L{level}"
        self.logger.info(f"‚öñÔ∏è [EXPERT-APPEAL START] {log_prefix} | Max Rerank: {highest_rerank_score:.4f}")

        # 1. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° missing string (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á list ‡πÅ‡∏•‡∏∞ set)
        missing_set = set(missing_tags) if isinstance(missing_tags, (list, set)) else set()
        missing_str = ", ".join(sorted(missing_set)) if missing_set else "‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå PDCA"

        # 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á Expert Instruction (Hint) ‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÅ‡∏•‡∏∞‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö
        hint_msg = f"""
    ### üö® EXPERT JUDICIAL REVIEW - SECOND CHANCE (APPEAL) üö®
    CONTEXT ‡∏à‡∏≤‡∏Å‡∏£‡∏≠‡∏ö‡πÅ‡∏£‡∏Å: ‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏û‡∏£‡∏≤‡∏∞ "{first_attempt_reason[:120]}..."
    ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ: ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å (rerank {highest_rerank_score:.4f}) ‡∏ã‡∏∂‡πà‡∏á‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö {missing_str}

    MANDATE (‡∏Å‡∏é‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö):
    - ‡πÉ‡∏ä‡πâ‡∏´‡∏•‡∏±‡∏Å 'Substance over Form' ‚Üí ‡∏´‡∏≤‡∏Å‡∏°‡∏µ‡∏£‡πà‡∏≠‡∏á‡∏£‡∏≠‡∏¢‡∏Å‡∏≤‡∏£‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏à‡∏£‡∏¥‡∏á ‡πÅ‡∏°‡πâ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡πÅ‡∏ö‡∏ö ‡πÉ‡∏´‡πâ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡∏ú‡πà‡∏≤‡∏ô (is_passed: true)
    - ‡∏≠‡∏¢‡πà‡∏≤‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÄ‡∏Ç‡πâ‡∏°‡∏á‡∏ß‡∏î‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ‡∏Å‡∏±‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ substance ‡∏û‡∏≠‡∏™‡∏°‡∏Ñ‡∏ß‡∏£
    - ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ß‡πà‡∏≤‡∏ú‡πà‡∏≤‡∏ô ‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏∏‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏≠‡∏∞‡πÑ‡∏£‡∏™‡∏ô‡∏±‡∏ö‡∏™‡∏ô‡∏∏‡∏ô

    DO NOT reject just because of missing formal signature if practice is evident.
    """

        # 3. Inject hint ‡πÄ‡∏Ç‡πâ‡∏≤ pdca_blocks (‡∏ß‡∏¥‡∏ò‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà evaluate_pdca ‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô)
        expert_pdca_blocks = base_kwargs.get("pdca_blocks", []).copy()

        if isinstance(expert_pdca_blocks, list):
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏õ‡πá‡∏ô block ‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏ó‡πâ‡∏≤‡∏¢‡∏™‡∏∏‡∏î (priority ‡∏™‡∏π‡∏á)
            expert_pdca_blocks.append({
                "type": "judicial_review_instruction",
                "content": hint_msg,
                "metadata": {
                    "priority": "highest",
                    "source": "appeal_system",
                    "rerank_score": highest_rerank_score
                }
            })
            self.logger.debug(f"[APPEAL-INJECT] Added hint block to pdca_blocks (total blocks: {len(expert_pdca_blocks)})")
        else:
            # ‡∏ñ‡πâ‡∏≤ pdca_blocks ‡πÄ‡∏õ‡πá‡∏ô string ‡∏´‡∏£‡∏∑‡∏≠ dict ‡∏≠‡∏∑‡πà‡∏ô ‚Üí concat
            expert_pdca_blocks = f"{expert_pdca_blocks}\n\n--- APPEAL INSTRUCTION ---\n{hint_msg}"
            self.logger.debug("[APPEAL-INJECT] Concatenated hint to pdca_blocks (string mode)")

        # 4. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° kwargs ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LLM evaluator (‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡∏°‡∏±‡∏ô‡∏£‡∏±‡∏ö‡∏à‡∏£‡∏¥‡∏á)
        expert_kwargs = {
            "pdca_blocks": expert_pdca_blocks,
            "sub_id": sub_id,
            "level": level,
            "audit_confidence": getattr(self, "current_audit_meta", {"level": "HIGH", "score": 1.0})
        }

        self.logger.info(f"[APPEAL-SEND] {log_prefix} | Sending pdca_blocks with appeal hint | Confidence: {expert_kwargs['audit_confidence'].get('level')}")

        # 5. Call LLM ‡∏î‡πâ‡∏ß‡∏¢ retry (max 2 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á)
        re_eval_result = None
        max_attempts = 2
        for attempt in range(1, max_attempts + 1):
            try:
                re_eval_result = llm_evaluator_to_use(**expert_kwargs)
                if re_eval_result is not None:
                    self.logger.debug(f"[APPEAL-SUCCESS] Attempt {attempt}: Received result")
                    break
                self.logger.warning(f"[APPEAL-RETRY] Attempt {attempt}: No result returned")
            except Exception as e:
                self.logger.warning(f"[APPEAL-ERROR] Attempt {attempt}: {str(e)}")
                if attempt == max_attempts:
                    return {
                        "is_passed": False,
                        "score": 0.0,
                        "reason": f"Appeal failed after {max_attempts} attempts: {str(e)}",
                        "appeal_status": "FAILED"
                    }

        # 6. Validation ‡πÄ‡∏Ç‡πâ‡∏°‡∏á‡∏ß‡∏î
        if not isinstance(re_eval_result, dict):
            self.logger.error(f"‚ùå [APPEAL-INVALID] {log_prefix}: Result is not dict ‚Üí {type(re_eval_result)}")
            return {
                "is_passed": False,
                "score": 0.0,
                "reason": "Expert System: Invalid response format",
                "appeal_status": "INVALID"
            }

        # 7. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå + ‡πÄ‡∏û‡∏¥‡πà‡∏° traceability
        is_passed = bool(re_eval_result.get("is_passed", False))

        if is_passed:
            self.logger.info(f"üõ°Ô∏è [OVERRIDE-SUCCESS] {log_prefix} | Appeal Granted")
            re_eval_result["is_safety_pass"] = True
            re_eval_result["appeal_status"] = "GRANTED"
            re_eval_result["reason"] = f"üåü [EXPERT OVERRIDE]: {re_eval_result.get('reason', '‡∏ú‡πà‡∏≤‡∏ô‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏≠‡∏∏‡∏ó‡∏ò‡∏£‡∏ì‡πå')}"
        else:
            self.logger.info(f"‚ùå [APPEAL-DENIED] {log_prefix}")
            re_eval_result["appeal_status"] = "DENIED"
            re_eval_result["reason"] = re_eval_result.get("reason", "‡∏≠‡∏∏‡∏ó‡∏ò‡∏£‡∏ì‡πå‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à - ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå")

        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• traceability ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°
        re_eval_result.update({
            "appeal_rerank_score": highest_rerank_score,
            "appeal_missing_tags": missing_str,
            "appeal_attempt": attempt,
            "appeal_timestamp": datetime.now().isoformat()
        })

        return re_eval_result

    def _apply_diversity_filter(
        self,
        evidences: List[Dict[str, Any]],
        max_per_source: int = 3,
        max_total: int = 40,
    ):
        """
        Diversity & dedup filter for evidence chunks

        Rules:
        - Preserve priority chunks
        - Deduplicate by chunk_uuid
        - Limit chunks per source file
        - Sort by (priority, rerank_score)
        - Stable / deterministic
        """

        if not evidences:
            return []

        # --------------------------------------------------
        # 1Ô∏è‚É£ Deduplicate by chunk_uuid (or content hash)
        # --------------------------------------------------
        unique = {}
        for d in evidences:
            if not isinstance(d, dict):
                continue

            uid = d.get("chunk_uuid")
            if not uid:
                uid = hashlib.sha256(
                    str(d.get("page_content", "")).encode()
                ).hexdigest()

            if uid not in unique:
                unique[uid] = d

        docs = list(unique.values())

        # --------------------------------------------------
        # 2Ô∏è‚É£ Sort: priority first, then score
        # --------------------------------------------------
        docs = sorted(
            docs,
            key=lambda x: (
                bool(x.get("is_priority", False)),
                self.get_actual_score(x),
            ),
            reverse=True,
        )

        # --------------------------------------------------
        # 3Ô∏è‚É£ Enforce per-source diversity
        # --------------------------------------------------
        source_counter = {}
        diversified = []

        for d in docs:
            src = (
                d.get("source")
                or d.get("metadata", {}).get("source")
                or "unknown"
            )

            count = source_counter.get(src, 0)
            if count >= max_per_source:
                continue

            diversified.append(d)
            source_counter[src] = count + 1

            if len(diversified) >= max_total:
                break

        return diversified
    
    def run_assessment(
        self,
        target_sub_id: str = "all",
        export: bool = False,
        vectorstore_manager: Optional['VectorStoreManager'] = None,
        sequential: bool = False,
        document_map: Optional[Dict[str, str]] = None,
        record_id: str = None,
    ) -> Dict[str, Any]:
        """
        [ULTIMATE REVISED v2026.1.23 - FULL EVIDENCE TRACEABILITY] 
        Main entry point ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô SE-AM
        - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: ‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô (Evidence) ‡∏à‡∏≤‡∏Å Worker ‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà Master Map
        - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: ‡∏£‡∏∞‡∏ö‡∏ö‡∏™‡∏∞‡∏™‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô PDCA ‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡∏ó‡∏∏‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö
        """
        start_ts = time.time()
        self.is_sequential = sequential
        self.current_record_id = record_id or self.record_id
        
        # ‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏°‡∏µ document_map ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏õ‡∏•‡∏á ID ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå
        if document_map:
            self.document_map.update(document_map)

        # 1. üìÇ ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô
        flat_statements = self._flatten_rubric_to_statements()
        grouped_sub_criteria = self._group_statements_by_sub_criteria(flat_statements)

        is_all = str(target_sub_id).lower() == "all"
        sub_criteria_list = list(grouped_sub_criteria.values()) if is_all else [grouped_sub_criteria.get(target_sub_id)]
        
        if not all(sub_criteria_list):
            return self._create_failed_result(self.current_record_id, f"Criteria '{target_sub_id}' not found", start_ts)

        total_subs = len(sub_criteria_list)
        self.db_update_task_status(progress=5, message=f"üìä ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°: ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô {total_subs} ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠")

        # 2. üß† ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô (Tier-1 & Tier-2)
        results_list = []
        
        if is_all and not sequential:
            # [MODE A] PARALLEL
            max_workers = int(os.environ.get("MAX_PARALLEL_WORKERS", 4))
            worker_args = [self._prepare_worker_tuple(sub, self.document_map) for sub in sub_criteria_list]
            
            ctx = multiprocessing.get_context("spawn")
            with ctx.Pool(processes=max_workers) as pool:
                for idx, res_tuple in enumerate(pool.imap_unordered(_static_worker_process, worker_args)):
                    # res_tuple: (worker_result_dict, worker_evidence_mem)
                    results_list.append(res_tuple)
                    
                    # üéØ [CRITICAL FIX] Merge ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞ '‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô' ‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà Memory ‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
                    self._merge_worker_results(res_tuple[0], res_tuple[1])
                    
                    # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏∑‡∏ö‡∏´‡∏ô‡πâ‡∏≤
                    sub_id_now = res_tuple[0].get('sub_id', '?')
                    self.db_update_task_status(
                        progress=15 + int(((idx+1)/total_subs) * 65), 
                        message=f"üß† ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠ {sub_id_now} ({idx+1}/{total_subs})"
                    )
        else:
            # [MODE B] SEQUENTIAL
            if not vectorstore_manager: self._initialize_vsm_if_none()
            vsm = vectorstore_manager or self.vectorstore_manager

            for idx, sub_criteria in enumerate(sub_criteria_list):
                sub_id = str(sub_criteria.get("sub_id", "Unknown"))
                self.db_update_task_status(progress=15 + int((idx/total_subs)*65), message=f"üß† ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô {sub_id}")
                
                # ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏à‡∏≤‡∏Å Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
                prev_map = self._collect_previous_level_evidences(sub_id=sub_id, current_level=1)
                initial_baseline = [ev for evs in prev_map.values() for ev in evs]
                
                # ‡∏£‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô
                res, worker_mem = self._run_sub_criteria_assessment_worker(sub_criteria, vsm, initial_baseline)
                results_list.append((res, worker_mem))
                
                # üéØ [CRITICAL FIX] Merge ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏∞‡∏™‡∏° (Cumulative) ‡πÑ‡∏°‡πà‡∏´‡∏≤‡∏¢
                self._merge_worker_results(res, worker_mem)

        # 3. üß© ‡∏£‡∏ß‡∏°‡∏ú‡∏•‡πÅ‡∏•‡∏∞‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏ú‡∏ô‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå (Tier-3)
        self.db_update_task_status(progress=85, message="üß© ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏ú‡∏ô‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô (Evidence Trail)
        total_evidence_found = len(self.evidence_map)
        self.logger.info(f"üìä Total Evidence Found: {total_evidence_found} files")

        # --- üíæ [IRONCLAD SAVE POINT] ---
        try:
            self.logger.info("üíæ [EVIDENCE] Initiating ironclad persistence...")
            self._save_evidence_map() # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå mapping ‡∏•‡∏á disk ‡∏Å‡∏±‡∏ô‡πÄ‡∏´‡∏ô‡∏µ‡∏¢‡∏ß
        except Exception as e:
            self.logger.error(f"‚ö†Ô∏è [EVIDENCE] Auto-save failed: {e}")

        # ‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Master Strategic Roadmap (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏∏‡∏Å‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠)
        master_roadmap_data = None
        if is_all and len(self.final_subcriteria_results) > 0:
            master_roadmap_data = self.synthesize_strategic_roadmap(
                sub_criteria_results=self.final_subcriteria_results,
                enabler_name=self.enabler,
                llm_executor=self.llm
            )

        # 4. üèÅ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÅ‡∏•‡∏∞‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•
        overall_stats = self._calculate_overall_stats(target_sub_id)
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏à‡∏£‡∏¥‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô stats
        overall_stats["evidence_used_count"] = total_evidence_found
        
        final_response = {
            "record_id": self.current_record_id,
            "status": "COMPLETED",
            "enabler": self.enabler,
            "summary": overall_stats,
            "sub_criteria_results": self.final_subcriteria_results,
            "evidence_audit_trail": self.evidence_map, # üéØ ‡∏™‡πà‡∏á Mapping ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏≠‡∏≠‡∏Å‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢
            "strategic_roadmap": master_roadmap_data,
            "run_time_seconds": round(time.time() - start_ts, 2)
        }

        self.master_roadmap_data = master_roadmap_data 
        if export:
            # ‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á Roadmap) ‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô Export
            final_response["export_path"] = self._export_results(
                results_data=final_response, # ‡∏™‡πà‡∏á‡∏Å‡πâ‡∏≠‡∏ô‡πÉ‡∏´‡∏ç‡πà‡πÑ‡∏õ‡πÄ‡∏•‡∏¢
                sub_criteria_id=target_sub_id
            )

        self.db_update_task_status(progress=100, message="‚úÖ ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå", status="COMPLETED")
        return final_response

    # ------------------------------------------------------------------
    # üèõÔ∏è [TIER-3 METHOD] synthesize_strategic_roadmap - FINAL PRODUCTION
    # ------------------------------------------------------------------
    def synthesize_strategic_roadmap(
        self,
        sub_criteria_results: List[Dict[str, Any]],
        enabler_name: str,
        llm_executor: Any
    ) -> Dict[str, Any]:
        """
        [TIER-3 STRATEGIC ORCHESTRATOR - v2026.3.26]
        ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏£‡∏≤‡∏¢‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏°‡∏≤‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ú‡∏ô‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏° (Master Roadmap)
        - ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤ JSON Malformed ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£ Quote ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ã‡πâ‡∏≠‡∏ô
        - ‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏° Action Plans ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏ö‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏î‡πâ
        """
        self.logger.info(f"üåê [TIER-3] Starting Master Strategic Roadmap Synthesis for {enabler_name}")
        
        if not sub_criteria_results:
            self.logger.warning("‚ö†Ô∏è No sub-criteria results available for synthesis")
            return {"status": "INCOMPLETE", "overall_strategy": "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏ú‡∏ô"}

        # 1. üìÇ Data Collection (Gap Aggregation)
        aggregated_insights = []
        for res in sub_criteria_results:
            sub_id = res.get("sub_id", "Unknown")
            sub_name = res.get("sub_criteria_name", "N/A")
            highest_lv = res.get("highest_full_level", 0)
            level_details = res.get("level_details", {})
            
            gap_recs = []
            # ‡πÄ‡∏Å‡πá‡∏ö Insight ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà "‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô" ‡∏´‡∏£‡∏∑‡∏≠ "‡∏ú‡πà‡∏≤‡∏ô‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏≤‡∏ö‡πÄ‡∏™‡πâ‡∏ô (Score < 0.7)"
            for lvl_idx in range(1, 6):
                lv_str = str(lvl_idx)
                detail = level_details.get(lv_str, {})
                score = float(detail.get("score", 0))
                is_passed = detail.get("is_passed", False)
                
                if not is_passed or score < 0.7:
                    # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î Quote ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ JSON ‡∏û‡∏±‡∏á‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï
                    insight = str(detail.get("coaching_insight") or "").replace('"', "'").strip()
                    if insight and insight not in gap_recs and len(insight) > 5:
                        gap_recs.append(f"[L{lv_str}] {insight}")

            summary_text = " | ".join(gap_recs[:3]) if gap_recs else "‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á (‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á)"
            aggregated_insights.append(f"üìå [{sub_id}] {sub_name} (Highest: L{highest_lv}): {summary_text}")

        # 2. üß† LLM Orchestration
        formatted_insights_text = "\n".join(aggregated_insights)
        
        # üí° ‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤ Prompt ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏∏‡∏ü‡∏≠‡∏£‡πå‡πÅ‡∏°‡∏ï JSON ‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
        final_prompt = MASTER_ROADMAP_PROMPT.format(
            sub_id="OVERALL",
            sub_criteria_name=enabler_name, 
            enabler=enabler_name, 
            aggregated_insights=formatted_insights_text
        )

        try:
            # ‡πÉ‡∏ä‡πâ Temperature 0.2 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏´‡∏ï‡∏∏‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏•
            response = llm_executor.generate(
                system=SYSTEM_MASTER_ROADMAP_PROMPT, 
                prompts=[final_prompt], 
                temperature=0.2
            )
            
            raw_text = getattr(response, 'content', str(response)).strip()
            
            # 3. üßπ Robust JSON Extraction
            # ‡πÉ‡∏ä‡πâ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏≠‡∏±‡∏õ‡πÄ‡∏Å‡∏£‡∏î‡∏Å‡∏±‡∏ô‡πÑ‡∏õ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ô‡∏µ‡πâ
            strategic_plan = _robust_extract_json(raw_text)
            
            # 4. üõ°Ô∏è Result Normalization (Ensure standard keys for Exporter)
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏´‡∏≤ Roadmap ‡πÉ‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡πÜ Key ‡∏ó‡∏µ‡πà AI ‡∏°‡∏±‡∏Å‡∏à‡∏∞‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏≤
            final_roadmap = (
                strategic_plan.get("roadmap") or 
                strategic_plan.get("strategic_roadmap") or 
                strategic_plan.get("action_plan") or []
            )

            # üö® Fallback: ‡∏ñ‡πâ‡∏≤‡∏™‡∏Å‡∏±‡∏î Roadmap ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ ‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á 1 Step ‡πÉ‡∏´‡∏ç‡πà‡∏à‡∏≤‡∏Å Raw Text
            if not final_roadmap and len(raw_text) > 20:
                final_roadmap = [{
                    "phase": "Strategic Improvement",
                    "target_levels": "Overall",
                    "main_objective": "‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏¢‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°",
                    "key_actions": [raw_text[:200] + "..."],
                    "expected_outcome": "‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡πÉ‡∏ô‡∏£‡∏≠‡∏ö‡∏ñ‡∏±‡∏î‡πÑ‡∏õ"
                }]

            return {
                "status": "SUCCESS",
                "overall_strategy": (strategic_plan.get("overall_strategy") or 
                                    strategic_plan.get("summary") or 
                                    f"‡πÅ‡∏ú‡∏ô‡∏Ç‡∏±‡∏ö‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå {enabler_name}"),
                "roadmap": final_roadmap,
                "metadata": {
                    "generated_at": datetime.now().isoformat(),
                    "input_sub_count": len(sub_criteria_results),
                    "enabler": enabler_name
                }
            }

        except Exception as e:
            self.logger.error(f"üí• Master Roadmap Critical Error: {str(e)}", exc_info=True)
            return {
                "status": "ERROR", 
                "overall_strategy": "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏ú‡∏ô‡πÑ‡∏î‡πâ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•",
                "roadmap": [],
                "reason": str(e)
            }
    
    def create_atomic_action_plan(self, insight: str, level: int) -> List[Dict[str, Any]]:
        """
        [FULL REVISED v2026.1.25 - ROBUST ATOMIC PLAN GENERATION]
        - Prompt ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô + ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á JSON ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î truncation
        - Log raw LLM response ‡πÄ‡∏ï‡πá‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠ debug
        - Regex scavenger ‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö key ‡∏≠‡∏∑‡πà‡∏ô ‡πÜ)
        - Validation + cleanup ‡∏Å‡πà‡∏≠‡∏ô return
        - Fallback ‡∏ó‡∏µ‡πà‡∏™‡∏°‡πÄ‡∏´‡∏ï‡∏∏‡∏™‡∏°‡∏ú‡∏•‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏°
        """
        try:
            # 1. Validation
            if not insight or str(insight).lower().strip() in ["-", "n/a", "none", "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô", ""]:
                self.logger.debug(f"[ATOMIC-SKIP] Insight ‡∏ß‡πà‡∏≤‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö L{level}")
                return []

            # 2. Preparation
            clean_insight = str(insight).replace('"', "'").strip()
            if len(clean_insight) > 800:
                clean_insight = clean_insight[:800] + "... (‡∏ï‡∏±‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢)"

            # Prompt ‡∏ó‡∏µ‡πà‡πÅ‡∏£‡∏á‡∏Ç‡∏∂‡πâ‡∏ô + ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
            human_prompt = ATOMIC_ACTION_PROMPT.format(
                coaching_insight=clean_insight,
                level=level
            )

            # 3. LLM Generation (‡πÄ‡∏û‡∏¥‡πà‡∏° log raw)
            raw_text = _fetch_llm_response(
                system_prompt=(
                    "You are a strict JSON generator. "
                    "Return ONLY a valid JSON array of objects. "
                    "NO extra text, NO explanation, NO markdown. "
                    "Format MUST be: "
                    '[{"action": "‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô", "target_evidence": "‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£/‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á"}]'
                    "\nExample for level 1: "
                    '[{"action": "‡∏à‡∏±‡∏î‡∏ó‡∏≥‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÅ‡∏ï‡πà‡∏á‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡∏ì‡∏∞‡∏ó‡∏≥‡∏á‡∏≤‡∏ô KM", "target_evidence": "‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£ ‡∏â‡∏ö‡∏±‡∏ö‡∏ó‡∏µ‡πà..."}]'
                ),
                user_prompt=human_prompt,
                llm_executor=self.llm
            )

            # Log raw response ‡πÄ‡∏û‡∏∑‡πà‡∏≠ debug (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å!)
            self.logger.debug(f"[ATOMIC-RAW-L{level}] Raw LLM response (length={len(raw_text)}):\n{raw_text[:1000]}...")

            # 4. Hybrid Extraction Logic (‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏î‡πà‡∏≤‡∏ô)
            actions = []

            # ‡∏î‡πà‡∏≤‡∏ô 1: Robust JSON extract
            try:
                actions = _robust_extract_json_list(raw_text)
                if actions and isinstance(actions, list):
                    self.logger.debug(f"[ATOMIC-JSON-SUCCESS] Extracted {len(actions)} actions from JSON")
            except Exception as e:
                self.logger.warning(f"[ATOMIC-JSON-FAIL] {str(e)}")

            # ‡∏î‡πà‡∏≤‡∏ô 2: Enhanced Scavenger Regex (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö key ‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢)
            if not actions:
                # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö "action", "step", "recommendation", "task"
                patterns = [
                    r'"action"\s*:\s*"([^"]+)"',
                    r'"step"\s*:\s*"([^"]+)"',
                    r'"recommendation"\s*:\s*"([^"]+)"',
                    r'"task"\s*:\s*"([^"]+)"',
                    r'"activity"\s*:\s*"([^"]+)"'
                ]
                found_actions = []
                for pat in patterns:
                    found_actions.extend(re.findall(pat, raw_text))

                # ‡∏î‡∏∂‡∏á target_evidence ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
                found_evidences = re.findall(r'"(target_evidence|evidence|reference)"\s*:\s*"([^"]+)"', raw_text)
                found_evidences = [ev[1] for ev in found_evidences]  # ‡πÄ‡∏≠‡∏≤‡πÅ‡∏Ñ‡πà value

                for i, act in enumerate(found_actions):
                    evid = found_evidences[i] if i < len(found_evidences) else "‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö/‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ï‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞"
                    actions.append({"action": act, "target_evidence": evid})

                if actions:
                    self.logger.debug(f"[ATOMIC-SCAVENGER] Found {len(actions)} actions via regex")

            # ‡∏î‡πà‡∏≤‡∏ô 3: Normalization + Validation
            final_actions = []
            if isinstance(actions, list):
                for item in actions:
                    if not isinstance(item, dict):
                        continue

                    # ‡∏î‡∏∂‡∏á action ‡∏à‡∏≤‡∏Å key ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ
                    act_val = (
                        item.get("action") or
                        item.get("step") or
                        item.get("recommendation") or
                        item.get("task") or
                        item.get("activity") or
                        ""
                    ).strip()

                    if not act_val or len(act_val) < 5:
                        continue  # ‡∏Ç‡πâ‡∏≤‡∏°‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå

                    evid = (
                        item.get("target_evidence") or
                        item.get("evidence") or
                        item.get("reference") or
                        "‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏ï‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞"
                    ).strip()

                    final_actions.append({
                        "action": act_val,
                        "target_evidence": evid,
                        "level": int(level)
                    })

            # 4. Emergency Fallback (‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏´‡πâ‡∏™‡∏°‡∏à‡∏£‡∏¥‡∏á‡∏Ç‡∏∂‡πâ‡∏ô)
            if not final_actions:
                self.logger.warning(f"‚ö†Ô∏è [ATOMIC-FALLBACK] Salvaging from insight for L{level}")
                # ‡πÉ‡∏ä‡πâ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡πÅ‡∏£‡∏Å + ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô action ‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
                first_sent = clean_insight.split(".")[0].strip() if "." in clean_insight else clean_insight
                if len(first_sent) > 10:
                    final_actions.append({
                        "action": f"‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏ï‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞‡∏´‡∏•‡∏±‡∏Å: {first_sent}",
                        "target_evidence": "‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô / ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö",
                        "level": int(level)
                    })
                else:
                    final_actions.append({
                        "action": f"‡∏¢‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏ï‡∏≤‡∏°‡∏£‡∏∞‡∏î‡∏±‡∏ö {level}",
                        "target_evidence": "‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÄ‡∏ä‡∏¥‡∏á‡∏õ‡∏£‡∏∞‡∏à‡∏±‡∏Å‡∏©‡πå‡∏à‡∏≤‡∏Å‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£",
                        "level": int(level)
                    })

            # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î 3 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ + log ‡∏™‡∏£‡∏∏‡∏õ
            processed_actions = final_actions[:3]
            if processed_actions:
                self.logger.info(f"‚úÖ [Atomic-Plan] L{level} Success with {len(processed_actions)} actions")
                for i, act in enumerate(processed_actions, 1):
                    self.logger.debug(f"  Action {i}: {act['action'][:80]}... | Evidence: {act['target_evidence'][:60]}...")
            else:
                self.logger.warning(f"[Atomic-Plan] L{level} No valid actions after all stages")

            return processed_actions

        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è [Atomic-Plan] Critical fallback at L{level}: {str(e)}")
            return [{
                "action": f"‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô‡∏ï‡∏≤‡∏°‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏£‡∏∞‡∏î‡∏±‡∏ö {level} (fallback ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î)",
                "target_evidence": "‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö log ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏£‡∏∞‡∏ö‡∏ö",
                "level": int(level)
            }]
    
    # ------------------------------------------------------------------
    # üèõÔ∏è [TIER-3 METHOD] generate_master_roadmap - FULL REVISE v2026.1.23
    # ------------------------------------------------------------------
    def generate_master_roadmap(self, sub_id, sub_criteria_name, enabler, aggregated_insights):
        """
        [TIER-3 STRATEGIC SYNTHESIS - v2026.1.23]
        ‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Roadmap ‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏£‡∏≤‡∏¢ Sub-Criteria ‡πÇ‡∏î‡∏¢‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö Ironclad Fetcher
        - ‡πÉ‡∏ä‡πâ _fetch_llm_response ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏¥‡πà‡∏á‡∏Ç‡∏≠‡∏á JSON ‡πÅ‡∏•‡∏∞‡∏£‡∏∞‡∏ö‡∏ö Retry
        - ‡∏°‡∏µ‡∏£‡∏∞‡∏ö‡∏ö Normalization ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÉ‡∏ô Report ‡πÅ‡∏•‡∏∞ Dashboard
        - ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡∏ö‡∏ö
        """
        
        self.logger.info(f"üîÆ [MASTER-ROADMAP] Starting synthesis for {sub_id} ({sub_criteria_name})")

        # 1. üìÇ Data Condensing: ‡∏ö‡∏µ‡∏ö‡∏≠‡∏±‡∏î Insight ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î Token ‡πÅ‡∏•‡∏∞‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Context Overflow
        if not aggregated_insights:
            self.logger.warning(f"‚ö†Ô∏è No insights for {sub_id} - Using emergency fallback")
            return self._get_emergency_fallback_plan(sub_id, sub_criteria_name, "No insights provided")

        condensed_insights = []
        for item in aggregated_insights:
            status = "‚úÖ PASSED" if item.get('is_passed') or item.get('status') == "PASSED" else "‚ùå FAILED"
            lv = item.get('level', '?')
            # ‡∏î‡∏∂‡∏á‡∏ö‡∏ó‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏™‡πà‡∏ß‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î Token Input
            insight = item.get('insight_summary') or item.get('reason') or '‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î'
            condensed_insights.append(f"Level {lv} [{status}]: {insight[:200]}")

        summary_context = "\n".join(condensed_insights)

        # 2. üìù Prompt Construction
        try:
            formatted_prompt = MASTER_ROADMAP_PROMPT.format(
                sub_id=sub_id,
                sub_criteria_name=sub_criteria_name,
                enabler=enabler,
                aggregated_insights=summary_context
            )
        except Exception as fe:
            self.logger.error(f"‚ùå Prompt formatting error: {fe}")
            formatted_prompt = f"Summarize roadmap for {sub_criteria_name}: {summary_context}"

        # 3. üß† LLM Execution via Ironclad Fetcher
        try:
            # ‡πÉ‡∏ä‡πâ Fetcher ‡∏ï‡∏±‡∏ß‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏Ñ‡∏∏‡∏¢‡∏Å‡∏±‡∏ô ‡∏ã‡∏∂‡πà‡∏á‡∏°‡∏µ‡∏£‡∏∞‡∏ö‡∏ö‡∏•‡πâ‡∏≤‡∏á JSON ‡πÅ‡∏•‡∏∞ Retry ‡πÉ‡∏ô‡∏ï‡∏±‡∏ß
            raw_json_str = _fetch_llm_response(
                system_prompt=SYSTEM_MASTER_ROADMAP_PROMPT,
                user_prompt=formatted_prompt,
                max_retries=3,
                llm_executor=self.llm  # ‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏ï‡∏±‡∏ß‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠ ChatOllama/Ollama Instance
            )

            # 4. üßπ Double-Check Extraction & Normalization
            master_data = _robust_extract_json(raw_json_str)
            
            if not master_data or (not master_data.get("overall_strategy") and not master_data.get("phases")):
                self.logger.warning(f"‚ö†Ô∏è Synthesis result is hollow for {sub_id} - Using fallback")
                return self._get_emergency_fallback_plan(sub_id, sub_criteria_name, "Hollow JSON response")

            # 5. üèóÔ∏è UI-Ready Normalization
            # ‡∏õ‡∏£‡∏±‡∏ö‡∏à‡∏π‡∏ô‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô (Standard Schema)
            final_strategy = master_data.get("overall_strategy") or master_data.get("summary") or "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡πÑ‡∏î‡πâ"
            raw_phases = master_data.get("phases") or master_data.get("roadmap") or master_data.get("atomic_action_plan") or []

            # ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤ Phases ‡πÄ‡∏õ‡πá‡∏ô List of Dict ‡πÄ‡∏™‡∏°‡∏≠
            normalized_phases = []
            if isinstance(raw_phases, list):
                for i, p in enumerate(raw_phases, 1):
                    if isinstance(p, dict):
                        normalized_phases.append(p)
                    else:
                        normalized_phases.append({"step": f"Phase {i}", "action": str(p)})
            elif isinstance(raw_phases, str):
                normalized_phases.append({"step": "General Action", "action": raw_phases})

            self.logger.info(f"‚úÖ [MASTER-ROADMAP] Synthesis Success for {sub_id}")
            
            return {
                "sub_id": sub_id,
                "sub_criteria_name": sub_criteria_name,
                "overall_strategy": final_strategy,
                "phases": normalized_phases,
                "status": "SUCCESS",
                "generated_at": datetime.now().isoformat(),
                "source_insights_count": len(aggregated_insights),
                "maturity_score": master_data.get("score") # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡πÉ‡∏ô Prompt
            }

        except Exception as e:
            self.logger.error(f"üí• Critical error in Master Roadmap {sub_id}: {str(e)}", exc_info=True)
            return self._get_emergency_fallback_plan(sub_id, sub_criteria_name, str(e))
            
    def _get_emergency_fallback_plan(self, sub_id, name, error_msg=""):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡∏™‡∏≥‡∏£‡∏≠‡∏á‡∏Å‡∏£‡∏ì‡∏µ LLM ‡∏û‡∏±‡∏á ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡∏´‡∏¢‡∏∏‡∏î‡∏ó‡∏≥‡∏á‡∏≤‡∏ô"""
        return {
            "overall_strategy": "‡πÅ‡∏ú‡∏ô‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô (Fallback Mode)",
            "phases": [
                {
                    "phase": "Quick Win",
                    "goal": f"‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ç‡πâ‡∏≠‡∏ö‡∏Å‡∏û‡∏£‡πà‡∏≠‡∏á‡πÉ‡∏ô {name}",
                    "actions": [{"action": "‡∏™‡∏≠‡∏ö‡∏ó‡∏≤‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Gap ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô", "priority": "High"}]
                }
            ],
            "status": "FALLBACK",
            "error_context": error_msg[:100]
        }
    
    # ------------------------------------------------------------------------------------------
    # üß† [TIER-1 & TIER-2 WORKER] Sequential Assessment (HYDRATED) - FULL REVISED
    # ------------------------------------------------------------------------------------------
    def _run_sub_criteria_assessment_worker(
        self,
        sub_criteria: Dict[str, Any],
        vectorstore_manager: Optional[Any] = None,
        initial_baseline: Optional[List[Dict[str, Any]]] = None,
    ) -> Tuple[Dict[str, Any], Dict[str, List[Dict[str, Any]]]]:
        """
        [PRODUCTION READY - v2026.3.28]
        ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç Bug: ‡∏™‡πà‡∏á keyword_guide ‡πÉ‡∏´‡πâ Tier-1 Assessment
        ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç Flow: ‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏´‡∏•‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà Tier-2 (Atomic) ‡πÅ‡∏•‡∏∞ Tier-3 (Master) ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
        """
        # 1. ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        sub_id = str(sub_criteria.get("sub_id", "Unknown"))
        sub_name = sub_criteria.get("sub_criteria_name", "No Name")
        sub_weight = float(sub_criteria.get("weight", 0.0))
        
        # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô AttributeError ‡∏ñ‡πâ‡∏≤ self.config ‡πÑ‡∏°‡πà‡∏°‡∏µ target_level
        target_limit = getattr(self.config, "target_level", 5) if hasattr(self, 'config') else 5
        enabler = getattr(self, "enabler", "KM")

        vsm = vectorstore_manager or getattr(self, "vectorstore_manager", None)
        current_highest_level = 0
        level_details = {}
        roadmap_input_bundle = []

        # 2. Evidence Hydration Memory
        baseline_memory = {sub_id: list(initial_baseline or [])}
        levels = sorted(sub_criteria.get("levels", []), key=lambda x: x.get("level", 0))

        self.logger.info(f"üöÄ [START-SUB] {sub_id} | Target Level: {target_limit}")

        for stmt in levels:
            level = int(stmt.get("level", 0))
            if level == 0 or level > target_limit: 
                continue

            # --- üî• STEP 1: Core Assessment (Tier-1) ---
            # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç Bug: ‡πÄ‡∏û‡∏¥‡πà‡∏° keyword_guide ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏ï‡∏≤‡∏° Parameter requirements
            res = self._run_single_assessment(
                sub_id=sub_id, 
                level=level,
                criteria={
                    "name": sub_name, 
                    "statement": stmt.get("statement", ""), 
                    "sub_criteria_name": sub_name
                },
                keyword_guide=stmt.get("keywords", []), # ‚úÖ FIXED: ‡∏™‡πà‡∏á Keywords ‡∏à‡∏≤‡∏Å rubric
                baseline_evidences=baseline_memory.get(sub_id, []),
                vectorstore_manager=vsm,
            )

            is_passed = bool(res.get("is_passed", False))
            
            # üîÑ Evidence Hydration (‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô)
            if is_passed:
                current_highest_level = max(current_highest_level, level)
                new_chunks = res.get("top_chunks_data", [])
                if new_chunks:
                    baseline_memory[sub_id].extend(new_chunks)
                    # ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞ 5 Chunks ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏∏‡∏° Token
                    baseline_memory[sub_id] = baseline_memory[sub_id][-5:]

            # --- üî• STEP 2: Atomic Action Plan (Tier-2 ‡∏£‡∏≤‡∏¢ Level) ---
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á Feedback ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ User ‡πÄ‡∏´‡πá‡∏ô‡πÅ‡∏ú‡∏ô‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏£‡∏≤‡∏¢‡∏£‡∏∞‡∏î‡∏±‡∏ö
            self.logger.info(f"üõ†Ô∏è [ATOMIC] Level {level} for {sub_id}")
            
            atomic_actions = self.create_atomic_action_plan(
                insight=res.get("coaching_insight", ""),
                level=level
            )

            # ‡πÄ‡∏Å‡πá‡∏ö‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏£‡∏≤‡∏¢ Level ‡∏•‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å
            level_details[str(level)] = {
                "level": level, 
                "is_passed": is_passed, 
                "score": float(res.get("score", 0.0)),
                "reason": res.get("reason", ""),
                "coaching_insight": res.get("coaching_insight", ""),
                "atomic_action_plan": atomic_actions, 
                "pdca_breakdown": res.get("pdca_breakdown", {}),
                "audit_confidence": res.get("audit_confidence", {})
            }

            # üì¶ ‡∏™‡∏∞‡∏™‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠‡πÉ‡∏´‡πâ Master Roadmap (Tier-3)
            roadmap_input_bundle.append({
                "level": level,
                "status": "PASSED" if is_passed else "FAILED",
                "insight_summary": res.get("coaching_insight", "")[:200]
            })

        # --- üî• STEP 3: Strategic Master Roadmap (Tier-3 ‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°) ---
        # ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏° Insights ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏°‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á Phase ‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÉ‡∏ô‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß
        self.logger.info(f"üîÆ [MASTER] Synthesis for {sub_id}")
        
        master_roadmap = self.generate_master_roadmap(
            sub_id=sub_id,
            sub_criteria_name=sub_name,
            enabler=enabler,
            aggregated_insights=roadmap_input_bundle
        )

        # 4. Final Output Assembly
        # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ä‡∏∏‡∏î‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏™‡πà‡∏á‡πÑ‡∏õ‡∏ó‡∏µ‡πà Transformer ‡πÅ‡∏•‡∏∞ UI React
        return {
            "sub_id": sub_id, 
            "sub_criteria_name": sub_name, 
            "highest_full_level": current_highest_level,
            "weighted_score": round(current_highest_level * sub_weight, 2),
            "is_passed": current_highest_level >= 1, # <--- ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ï‡∏±‡∏ß‡∏ô‡∏µ‡πâ
            "level_details": level_details, 
            "master_roadmap": master_roadmap 
        }, baseline_memory

    # ------------------------------------------------------------------------------------------
    # üß† [TIER-1 CORE] _run_single_assessment (GOVERNANCE-LOCKED) - REVISED v2026.1.22
    # ------------------------------------------------------------------------------------------
    def _run_single_assessment(
        self,
        sub_id: str,
        level: int,
        criteria: Dict[str, Any],
        keyword_guide: List[str],
        baseline_evidences: List[Dict[str, Any]],
        vectorstore_manager: Optional[Any] = None,
    ) -> Dict[str, Any]:
        """
        [ULTIMATE VERSION - FULL REVISED v2026.1.25]
        - Robust retrieval + evidence fusion
        - Multi-channel context + LLM evaluation
        - Smart rescue in post-process
        - Judicial Review (appeal) with safety net force-pass
        - Enhanced logging & traceability
        """
        log_prefix = f"Sub:{sub_id} L{level}"
        self.logger.info(f"üîç [START-ASSESSMENT] {log_prefix} | {criteria.get('name', '')[:50]}...")

        # ------------------------------------------------------------------
        # STEP 1-2: Adaptive Retrieval & Evidence Fusion
        # ------------------------------------------------------------------
        retrieved_chunks, max_rerank = self._perform_adaptive_retrieval(
            sub_id=sub_id,
            level=level,
            stmt=criteria.get("statement", ""),
            vectorstore_manager=vectorstore_manager,
        )

        # Diversity Filter
        retrieved_chunks = self._apply_diversity_filter(retrieved_chunks, level)

        # Log preview ‡∏Ç‡∏≠‡∏á chunks ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ (‡∏ä‡πà‡∏ß‡∏¢ debug)
        chunk_count = len(retrieved_chunks)
        top_preview = retrieved_chunks[0].get('text', '')[:80] + "..." if retrieved_chunks else "No chunks"
        self.logger.debug(f"[RETRIEVAL] {log_prefix} | Chunks: {chunk_count} | Max Rerank: {max_rerank:.4f} | Top: {top_preview}")

        # Evidence Fusion (hydration)
        evidences = []
        evidences.extend(baseline_evidences or [])
        evidences.extend(retrieved_chunks or [])

        # ------------------------------------------------------------------
        # STEP 3-5: Metadata & Audit Preparation
        # ------------------------------------------------------------------
        pdca_blocks = self._get_pdca_blocks_from_evidences(
            evidences=evidences,
            baseline_evidences=baseline_evidences,
            level=level,
            sub_id=sub_id,
            contextual_rules_map=self.contextual_rules_map
        )

        audit_confidence = self.calculate_audit_confidence(
            matched_chunks=retrieved_chunks,
            sub_id=sub_id,
            level=level,
        )
        self.current_audit_meta = audit_confidence

        # ------------------------------------------------------------------
        # STEP 6-8: Multi-channel LLM Execution
        # ------------------------------------------------------------------
        llm_context = self._build_multichannel_context_for_level(
            level=level,
            top_evidences=retrieved_chunks,
            previous_levels_evidence=baseline_evidences
        )

        # Standard Evaluation (LLM ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å)
        llm_raw = self.evaluate_pdca(
            pdca_blocks=pdca_blocks,
            sub_id=sub_id,
            level=level,
            audit_confidence=audit_confidence
        )
        if not isinstance(llm_raw, dict):
            self.logger.warning(f"[LLM-RAW] {log_prefix} | Invalid LLM output ‚Üí fallback empty dict")
            llm_raw = {}

        # ------------------------------------------------------------------
        # STEP 9: Smart Rescue & Normalization
        # ------------------------------------------------------------------
        current_rules = self.contextual_rules_map.get(sub_id, {}).get(f"L{level}", {})
        
        result = self.post_process_llm_result(
            llm_output=llm_raw,
            level=level,
            sub_id=sub_id,
            contextual_config=current_rules,
            top_evidences=retrieved_chunks
        )

        # ------------------------------------------------------------------
        # STEP 10: Expert Re-evaluation (Judicial Review) + SAFETY NET
        # ------------------------------------------------------------------
        is_safety_pass = False
        if not result.get("is_passed") and max_rerank >= 0.70:  # üü¢ [ADJUSTED] ‡∏•‡∏î threshold ‡πÄ‡∏û‡∏∑‡πà‡∏≠ trigger ‡∏á‡πà‡∏≤‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô
            self.logger.info(f"‚öñÔ∏è [TRIGGER-APPEAL] {log_prefix} | Rerank {max_rerank:.4f} ‚â• 0.70 ‚Üí Starting Judicial Review")

            base_kwargs = {
                "pdca_blocks": pdca_blocks,
                "contextual_config": current_rules,
                "top_evidences": retrieved_chunks
            }

            appeal_result = self._run_expert_re_evaluation(
                sub_id=sub_id,
                level=level,
                statement_text=criteria.get("statement", ""),
                context=str(llm_context.get("full_context", "")),
                first_attempt_reason=result.get("reason", "‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÑ‡∏°‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô"),
                missing_tags=result.get("missing_phases", []),
                highest_rerank_score=max_rerank,
                sub_criteria_name=criteria.get("name", sub_id),
                llm_evaluator_to_use=self.evaluate_pdca,
                base_kwargs=base_kwargs
            )

            # üö® [SAFETY NET] Force pass ‡∏ñ‡πâ‡∏≤ appeal granted
            if appeal_result and appeal_result.get("appeal_status") == "GRANTED":
                self.logger.info(f"‚öñÔ∏è [APPEAL-FORCE-PASS] {log_prefix} | Judicial Review granted ‚Üí Force score ‚â• 1.2")
                
                appeal_result["score"] = max(appeal_result.get("score", 0.0), 1.2)
                appeal_result["is_passed"] = True
                appeal_result["is_safety_pass"] = True
                appeal_result["is_force_pass"] = True
                appeal_result["reason"] = f"{appeal_result.get('reason', '')} [‡∏ú‡πà‡∏≤‡∏ô‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏≠‡∏∏‡∏ó‡∏ò‡∏£‡∏ì‡πå‡πÇ‡∏î‡∏¢ Judicial Review]"

                # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï coaching insight ‡πÉ‡∏´‡πâ‡∏™‡∏∞‡∏ó‡πâ‡∏≠‡∏ô appeal
                if "coaching_insight" in appeal_result:
                    appeal_result["coaching_insight"] += " (‡∏ú‡πà‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤ substance over form)"

            # ‡∏ñ‡πâ‡∏≤ appeal ‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏ú‡πà‡∏≤‡∏ô ‚Üí update result ‡∏î‡πâ‡∏ß‡∏¢ post-process ‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á
            if appeal_result and appeal_result.get("is_passed"):
                final_appeal = self.post_process_llm_result(
                    llm_output=appeal_result,
                    level=level,
                    sub_id=sub_id,
                    contextual_config=current_rules,
                    top_evidences=retrieved_chunks
                )
                result.update(final_appeal)
                is_safety_pass = True
                self.logger.info(f"‚úÖ [APPEAL-SUCCESS] {log_prefix} passed via Judicial Review | Final score: {result.get('score', 0.0):.2f}")

        # ------------------------------------------------------------------
        # STEP 11: Final Insight Refinement
        # ------------------------------------------------------------------
        final_insight = (
            result.get("coaching_insight") or
            result.get("reason") or
            "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏ö‡∏ö"
        ).strip()

        if result.get("is_passed"):
            final_insight = f"[STRENGTH] {final_insight}"
        else:
            final_insight = f"[GAP] {final_insight}"

        # ------------------------------------------------------------------
        # STEP 12: Logging & Final Assembly
        # ------------------------------------------------------------------
        if hasattr(self, "_log_pdca_status"):
            self._log_pdca_status(
                sub_id=sub_id,
                name=criteria.get("name", "Unknown"),
                level=level,
                blocks=llm_raw,
                req_phases=result.get("required_phases", []),
                sources_count=len(retrieved_chunks),
                score=result.get("score", 0.0),
                conf_level=audit_confidence.get("level", "LOW"),
                pdca_breakdown=result.get("pdca_breakdown", {}),
                tagging_result=audit_confidence.get("pdca_found", []),
                is_safety_pass=is_safety_pass
            )

        # Final return with enhanced debug_meta
        return {
            "is_passed": bool(result.get("is_passed", False)),
            "score": float(result.get("score", 0.0)),
            "reason": result.get("reason", ""),
            "coaching_insight": final_insight,
            "pdca_breakdown": result.get("pdca_breakdown", {}),
            "audit_confidence": audit_confidence,
            "top_chunks_data": retrieved_chunks,
            "is_safety_pass": is_safety_pass,
            "debug_meta": {
                "max_rerank": max_rerank,
                "evidence_count": len(evidences),
                "judicial_review": is_safety_pass,
                "appeal_triggered": max_rerank >= 0.70,
                "retrieval_chunks": chunk_count,
                "top_chunk_preview": top_preview
            }
        }