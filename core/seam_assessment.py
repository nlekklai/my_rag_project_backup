# core/seam_assessment.py

import sys
import json
import logging
import time
from datetime import datetime, date
import os
from typing import List, Dict, Any, Optional, Union, Tuple, Set, Final, Literal
from collections import defaultdict, OrderedDict
from dataclasses import dataclass, field
import multiprocessing # NEW: Import for parallel execution
from functools import partial
import pathlib, uuid
from langchain_core.documents import Document as LcDocument
from core.retry_policy import RetryPolicy, RetryResult
from copy import deepcopy
import tempfile
import shutil
from .json_extractor import _robust_extract_json
from filelock import FileLock  # ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á: pip install filelock
import re
import hashlib
import copy
from database import init_db
from database import db_update_task_status as update_db
from pydantic import BaseModel
import random  # Added for shuffle
import psutil
import time

# -------------------- PATH SETUP & IMPORTS --------------------
try:
    PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
    if PROJECT_ROOT not in sys.path:
        sys.path.insert(0, PROJECT_ROOT)

    # 1. Import Constants ‡∏à‡∏≤‡∏Å global_vars
    from config.global_vars import (
        MAX_LEVEL,
        EVIDENCE_DOC_TYPES,
        RERANK_THRESHOLD,
        MAX_EVI_STR_CAP,
        DEFAULT_LLM_MODEL_NAME,
        LLM_TEMPERATURE,
        MIN_RETRY_SCORE,
        MAX_PARALLEL_WORKERS,
        PDCA_PRIORITY_ORDER,
        TARGET_DEVICE,
        PDCA_PHASE_MAP,
        INITIAL_TOP_K,
        FINAL_K_RERANKED,
        MAX_CHUNKS_PER_FILE,
        MAX_CHUNKS_PER_BLOCK,
        MATURITY_LEVEL_GOALS
    )
    
    # 2. Import Logic Functions
    from core.llm_data_utils import ( 
        create_structured_action_plan, evaluate_with_llm,
        retrieve_context_with_filter, retrieve_context_for_low_levels,
        evaluate_with_llm_low_level, LOW_LEVEL_K, 
        set_mock_control_mode as set_llm_data_mock_mode,
        create_context_summary_llm,
        retrieve_context_by_doc_ids,
        _fetch_llm_response,
        _get_emergency_fallback_plan,
        _check_and_handle_empty_context
    )
    from core.vectorstore import VectorStoreManager, load_all_vectorstores, get_global_reranker 
    from core.action_plan_schema import ActionPlanActions, ActionPlanResult

    # 3. üéØ Import Path Utilities
    from utils.path_utils import (
        get_mapping_file_path, 
        get_evidence_mapping_file_path, 
        get_contextual_rules_file_path,
        get_doc_type_collection_key,
        get_assessment_export_file_path,
        get_export_dir,
        get_rubric_file_path,
        _n
    )

    import assessments.seam_mocking as seam_mocking 
    
except ImportError as e:
    # -------------------- Modernized Fallback Code --------------------
    print(f"‚ö†Ô∏è WARNING: Import failed, using dynamic fallback for Mac. Error: {e}", file=sys.stderr)
    
    # Fallback Constants
    EXPORTS_DIR = "exports"
    MAX_LEVEL = 5
    INITIAL_LEVEL = 1
    QA_FINAL_K = 3
    RUBRIC_FILENAME_PATTERN = "{tenant}_{enabler}_rubric.json"
    DEFAULT_ENABLER = "KM"
    EVIDENCE_DOC_TYPES = "evidence"
    INITIAL_TOP_K = 10

    # üìå Placeholder functions for path_utils (‡∏ä‡∏µ‡πâ‡πÄ‡∏Ç‡πâ‡∏≤ data_store ‡∏ï‡∏£‡∏á‡πÜ)
    def _n(s): return str(s).lower().strip()

    def get_mapping_file_path(doc_type, tenant, year=None, enabler=None):
        t = _n(tenant)
        if _n(doc_type) == "evidence":
            return f"data_store/{t}/mapping/{year}/{t}_{year}_{_n(enabler)}_doc_id_mapping.json"
        return f"data_store/{t}/mapping/{t}_{_n(doc_type)}_doc_id_mapping.json"

    def get_evidence_mapping_file_path(tenant, year, enabler):
        t = _n(tenant)
        return f"data_store/{t}/mapping/{year}/{t}_{year}_{_n(enabler)}_evidence_mapping.json"

    def get_contextual_rules_file_path(tenant, enabler):
        t = _n(tenant)
        return f"data_store/{t}/config/{t}_{_n(enabler)}_contextual_rules.json"

    def get_rubric_file_path(tenant, enabler):
        t = _n(tenant)
        return f"data_store/{t}/config/{t}_{_n(enabler)}_rubric.json"

    # Mock Logic Functions
    def create_structured_action_plan(*args, **kwargs): return []
    def evaluate_with_llm(*args, **kwargs): return {"score": 0, "reason": "Import Error Fallback", "is_passed": False}
    def retrieve_context_with_filter(*args, **kwargs): return {"top_evidences": [], "aggregated_context": ""}
    def retrieve_context_for_low_levels(*args, **kwargs): return {"top_evidences": [], "aggregated_context": ""}
    def evaluate_with_llm_low_level(*args, **kwargs): return {"score": 0, "is_passed": False}
    def set_llm_data_mock_mode(mode): pass
    
    class VectorStoreManager: pass
    def load_all_vectorstores(*args, **kwargs): return None
    
    class ActionPlanActions:
        @staticmethod
        def generate(*args, **kwargs): return []
    
    class ActionPlanResult:
        def __init__(self): self.success = False

    PDCA_PHASE_MAP = {
        1: "Plan (‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢)",
        2: "Do (‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡πÅ‡∏ú‡∏ô‡πÑ‡∏õ‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡πÅ‡∏•‡∏∞‡∏Ç‡∏±‡∏ö‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô)",
        3: "Check (‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•)",
        4: "Act (‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°)",
        5: "Sustainability (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏±‡πà‡∏á‡∏¢‡∏∑‡∏ô‡πÅ‡∏•‡∏∞‡∏ï‡πâ‡∏ô‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏î‡∏µ)"
    }

    MATURITY_LEVEL_GOALS = {
        1: "‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô ‡∏°‡∏µ‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢ ‡∏´‡∏£‡∏∑‡∏≠‡∏°‡∏µ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô",
        2: "‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡∏ö‡∏ö ‡∏°‡∏µ‡∏Ñ‡∏ì‡∏∞‡∏ó‡∏≥‡∏á‡∏≤‡∏ô",
        3: "‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á ‡πÅ‡∏•‡∏∞‡πÄ‡∏´‡πá‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô",
        4: "‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ä‡∏¥‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥ ‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°",
        5: "‡πÄ‡∏ô‡πâ‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏±‡πà‡∏á‡∏¢‡∏∑‡∏ô‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏ô‡πÅ‡∏ö‡∏ö (Role Model)"
    }

    def _get_emergency_fallback_plan(sub_id, name, level, *args, **kwargs):
        return {"summary": f"Fallback plan for {sub_id}", "steps": []}
    
    class seam_mocking:
        @staticmethod
        def set_mock_control_mode(mode): pass
    
    def create_context_summary_llm(*args, **kwargs): 
        return {"summary": "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡∏∏‡∏õ‡πÑ‡∏î‡πâ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏ö‡∏ö‡πÇ‡∏´‡∏•‡∏î Module ‡∏û‡∏±‡∏á", "coaching": "‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£ Import"}
    
    def _fetch_llm_response(*args, **kwargs): 
        return "{}"

    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÉ‡∏ô _run_single_assessment ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ
    MAX_EVI_STR_CAP = 10.0
    RERANK_THRESHOLD = 0.35

    if "FATAL ERROR" in str(e):
        pass 
# ----------------------------------------------------------------------

logger = logging.getLogger(__name__)
if not logger.handlers:
    logging.basicConfig(level=logging.DEBUG, format="%(asctime)s - %(levelname)s - %(message)s")


def _static_worker_process(worker_input_tuple: Tuple) -> Any:
    """
    [ULTIMATE WORKER v2026.3] Isolated Execution for Parallel Assessment
    ---------------------------------------------------------------------
    - ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ Process (Zero Memory Leak)
    - ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏ä‡πâ Context ‡∏à‡∏≤‡∏Å‡πÅ‡∏°‡πà‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥
    - ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏£‡∏∞‡∏ö‡∏ö‡∏û‡∏±‡∏á‡∏î‡πâ‡∏ß‡∏¢ Fallback Dictionary
    """

    # 1. üìÇ PATH SETUP
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏≠‡∏á‡πÄ‡∏´‡πá‡∏ô‡πÇ‡∏°‡∏î‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Import AssessmentConfig ‡πÅ‡∏•‡∏∞ Engine ‡πÑ‡∏î‡πâ
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
    if project_root not in sys.path:
        sys.path.append(project_root)
        
    worker_logger = logging.getLogger(f"Worker_{os.getpid()}")

    # 2. üì¶ ROBUST UNPACKING
    try:
        (
            sub_criteria_data, enabler, target_level, mock_mode, 
            evidence_map_path, model_name, temperature,
            min_retry_score, max_retrieval_attempts, document_map, 
            action_plan_model, year, tenant
        ) = worker_input_tuple
        
        sub_id = sub_criteria_data.get('sub_id', 'UNKNOWN')
        worker_logger.info(f"‚öôÔ∏è PID:{os.getpid()} | Starting: {sub_id} ({tenant}/{year})")
        
    except Exception as e:
        return {"error": f"Worker unpacking failed: {str(e)}", "status": "critical_failure"}
        
    # 3. üèóÔ∏è RECONSTRUCT ISOLATED ENGINE
    try:
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Config ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Worker ‡∏ï‡∏±‡∏ß‡∏ô‡∏µ‡πâ
        worker_config = AssessmentConfig(
            enabler=enabler,
            tenant=tenant,
            year=int(year) if year else None,     
            target_level=target_level,
            mock_mode=mock_mode,
            model_name=model_name, 
            temperature=temperature,
            min_retry_score=min_retry_score,            
            max_retrieval_attempts=max_retrieval_attempts 
        )

        # ‡∏Ñ‡∏∑‡∏ô‡∏ä‡∏µ‡∏û Engine (‡∏à‡∏∞‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà __init__ ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏á Patch ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡πÑ‡∏õ)
        worker_instance = SEAMPDCAEngine(
            config=worker_config, 
            evidence_map_path=evidence_map_path, 
            llm_instance=None,              
            vectorstore_manager=None,       
            logger_instance=worker_logger,
            document_map=document_map,      
            ActionPlanActions=action_plan_model
        )
    except Exception as e:
        worker_logger.error(f"‚ùå Worker initialization failed for {sub_id}: {e}")
        return {"sub_id": sub_id, "error": f"Init Error: {str(e)}", "status": "failed"}

    # 4. ‚ö° EXECUTE & TIME TRACKING
    try:
        start_time = time.time()
        
        # ‡∏£‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏£‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠ (Core Logic)
        result = worker_instance._run_sub_criteria_assessment_worker(sub_criteria_data)
        
        elapsed = time.time() - start_time
        worker_logger.info(f"‚úÖ PID:{os.getpid()} | Finished: {sub_id} in {elapsed:.2f}s")
        
        return result
        
    except Exception as e:
        worker_logger.error(f"‚ùå Execution error for {sub_id}: {str(e)}")
        return {
            "sub_id": sub_id,
            "error": str(e),
            "status": "failed",
            "execution_time": 0
        }
    
# =================================================================
# Configuration Class
# =================================================================
@dataclass
class AssessmentConfig:
    """Configuration for the SEAM PDCA Assessment Run."""
    
    # ------------------ 1. Assessment Context ------------------
    enabler: str = None
    tenant: str = None
    year: int = None  # üëà ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å DEFAULT_YEAR ‡πÄ‡∏õ‡πá‡∏ô None
    target_level: int = MAX_LEVEL
    mock_mode: str = "none" # 'none', 'random', 'control'
    force_sequential: bool = field(default=False) # Flag ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏£‡∏±‡∏ô‡πÅ‡∏ö‡∏ö Sequential

    # ------------------ 2. LLM Configuration (Configurable) ------------------
    # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Default ‡∏à‡∏≤‡∏Å global_vars.py
    model_name: str = DEFAULT_LLM_MODEL_NAME 
    temperature: float = LLM_TEMPERATURE

    # ------------------ 3. Adaptive RAG Retrieval Configuration ------------------
    # üü¢ NEW: ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô Rerank ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Adaptive Loop (MIN_RETRY_SCORE)
    # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Default 0.65 ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏û‡∏ö‡πÉ‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î Logic
    min_retry_score: float = MIN_RETRY_SCORE
    # üü¢ NEW: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≠‡∏ö‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á Adaptive RAG Loop (MAX_RETRIEVAL_ATTEMPTS)
    # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Default 3
    max_retrieval_attempts: int = 3
    
    # ------------------ 4. Export Configuration ------------------
    export_output: bool = field(default=False) # Flag ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏¥‡∏î/‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£ Export ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    export_path: str = "" # Path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå Export (‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å)


# =================================================================
# SEAM Assessment Engine (PDCA Focused) - Full Revise v2026
# =================================================================
class SEAMPDCAEngine:
    
    def __init__(
        self, 
        config: AssessmentConfig,
        llm_instance: Any = None, 
        logger_instance: logging.Logger = None,
        rag_retriever_instance: Any = None,
        doc_type: str = None, 
        vectorstore_manager: Optional['VectorStoreManager'] = None,
        evidence_map_path: Optional[str] = None,
        document_map: Optional[Dict[str, str]] = None,
        is_parallel_all_mode: bool = False,
        sub_id: str = 'all',
        **kwargs  
    ):
        """
        [ULTIMATE REVISE v2026.3] SEAM Assessment Engine Constructor
        ‡πÄ‡∏ô‡πâ‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏ô‡∏ó‡∏≤‡∏ô (Resilience), ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢ (Sanity Check) ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        """
        # -------------------------------------------------------
        # 1. Logger Setup (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö 1 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£ Trace Error)
        # -------------------------------------------------------
        self.doc_type = doc_type or getattr(config, 'doc_type', EVIDENCE_DOC_TYPES)
        clean_dt = str(self.doc_type).strip().lower()
        log_year = config.year if clean_dt == EVIDENCE_DOC_TYPES.lower() else "general"

        if logger_instance is not None:
            self.logger = logger_instance
        else:
            self.logger = logging.getLogger(__name__).getChild(
                f"Engine|{config.enabler}|{config.tenant}/{log_year}"
            )

        self.logger.info(f"üöÄ Initializing SEAMPDCAEngine: {config.enabler} ({config.tenant}/{log_year})")

        # -------------------------------------------------------
        # 2. Patch: Sanity Check & Core Configuration
        # -------------------------------------------------------
        self.config = config
        
        # [CRITICAL PATCH] ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏á‡∏≤‡∏ô
        if not self.config.enabler or not self.config.tenant:
            self.logger.critical("‚ùå Mandatory Config Missing: enabler and tenant must be provided!")
            raise ValueError("Enabler and Tenant are required for SEAMPDCAEngine.")

        self.enabler = config.enabler
        self.tenant_id = config.tenant  # üëà ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ: ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ _get_semantic_tag ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ
        self.year = config.year        # üëà ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ: ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô
        self.target_level = config.target_level
        self.sub_id = sub_id
        self.llm = llm_instance
        self.vectorstore_manager = vectorstore_manager
        self.is_parallel_all_mode = is_parallel_all_mode
        self.is_sequential = getattr(config, 'force_sequential', True)
        self.results = {}

        # -------------------------------------------------------
        # 3. Database & System Warm-up
        # -------------------------------------------------------
        try:
            init_db()  # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô 'no such table' ‡∏´‡∏£‡∏∑‡∏≠ Schema mismatch
            self.logger.info("üìÇ Database Schema verified/initialized.")
        except Exception as e:
            self.logger.error(f"‚ö†Ô∏è DB Init Warning: {e} (Check if tables already exist)")

        # -------------------------------------------------------
        # 4. Data Loading (Rubric, Rules & Policies)
        # -------------------------------------------------------
        # ‡πÇ‡∏´‡∏•‡∏î Rubric ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô AttributeError ‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô
        self.rubric = self._load_rubric()
        
        # ‡πÇ‡∏´‡∏•‡∏î‡∏Å‡∏é Contextual Rules ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö PDCA Logic
        self.contextual_rules_map = self._load_contextual_rules_map()
        
        self.retry_policy = RetryPolicy(
            max_attempts=3,
            base_delay=2.0,
            jitter=True,
            exponential_backoff=True,
        )

        # ‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô (Global Constants)
        self.RERANK_THRESHOLD = RERANK_THRESHOLD
        self.MAX_EVI_STR_CAP = MAX_EVI_STR_CAP

        # -------------------------------------------------------
        # 5. Mapping & Evidence Persistence Setup
        # -------------------------------------------------------
        # 5.1 Evidence Mapping (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏∑‡∏ö‡∏Ñ‡πâ‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á)
        self.evidence_map = {}
        if clean_dt == EVIDENCE_DOC_TYPES.lower():
            self.evidence_map_path = evidence_map_path or get_evidence_mapping_file_path(
                tenant=self.config.tenant, 
                year=self.config.year, 
                enabler=self.enabler
            )
            self.evidence_map = self._load_evidence_map()
            self.logger.info(f"üìä Evidence Mapping: Loaded {len(self.evidence_map)} keys.")

        # 5.2 Document Mapping (ID -> Filename ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏≥ Report/Audit)
        loaded_map = document_map or {}
        if not loaded_map:
            is_evi_mode = (clean_dt == EVIDENCE_DOC_TYPES.lower())
            mapping_path = get_mapping_file_path(
                self.doc_type, 
                tenant=self.config.tenant, 
                year=self.config.year if is_evi_mode else None,
                enabler=self.enabler if is_evi_mode else None
            )

            if os.path.exists(mapping_path):
                try:
                    with open(mapping_path, 'r', encoding='utf-8') as f:
                        raw_data = json.load(f)
                    loaded_map = {k: v.get("file_name", k) for k, v in raw_data.items()}
                    self.logger.info(f"üéØ Document Mapping: Loaded {len(loaded_map)} entries.")
                except Exception as e:
                    self.logger.error(f"‚ùå Error parsing mapping file: {e}")

        self.doc_id_to_filename_map = loaded_map
        self.document_map = loaded_map
        self.temp_map_for_save = {}

        # -------------------------------------------------------
        # 6. Lazy Engine Initialization (VSM & LLM)
        # -------------------------------------------------------
        if self.llm is None: self._initialize_llm_if_none()
        if self.vectorstore_manager is None: self._initialize_vsm_if_none()

        # ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ ID Mapping ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô VectorStore (‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô)
        if self.vectorstore_manager:
            try:
                self.vectorstore_manager._load_doc_id_mapping()
            except: pass

        # -------------------------------------------------------
        # 7. Function Registry (Pointers)
        # -------------------------------------------------------
        self.llm_evaluator = evaluate_with_llm
        self.rag_retriever = retrieve_context_with_filter
        self.create_structured_action_plan = create_structured_action_plan
        self.ActionPlanActions = ActionPlanActions
        self.action_plan_model = ActionPlanResult

        # --- [PATCH v2026.1.17] State Management Initialization ---
        self.final_subcriteria_results = []
        self.total_stats = {}
        self.raw_llm_results = []
        self.level_details_map = {} # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ L1-L5 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥ Gap Analysis

        self.logger.info(f"‚úÖ Engine Initialized: Ready for Assessment (Sub-ID: {self.sub_id})")
    

    # =================================================================
    # DB Proxy Methods
    # =================================================================
    def db_update_task_status(self, record_id: str, progress: int, message: str, status: str = "RUNNING"):
        """
        Wrapper ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏ú‡πà‡∏≤‡∏ô Database Module
        - record_id: ID ‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÉ‡∏ô‡∏£‡∏≠‡∏ö‡∏ô‡∏±‡πâ‡∏ô
        - progress: 0-100
        - message: ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞
        """
        if not record_id: return
        try:
            # ‡πÉ‡∏ä‡πâ update_db ‡∏ó‡∏µ‡πà alias ‡∏°‡∏≤‡∏à‡∏≤‡∏Å database.db_update_task_status
            update_db(record_id, progress, message, status=status)
            self.logger.debug(f"[DB-PROGRESS] {record_id}: {progress}% - {message}")
        except Exception as e:
            self.logger.error(f"‚ùå DB Update Error: {e}")

    def _initialize_llm_if_none(self):
        """Initializes LLM instance if self.llm is None."""
        if self.llm is None:
            self.logger.warning("‚ö†Ô∏è Initializing LLM: model=%s, temperature=%s", 
                                self.config.model_name, self.config.temperature)
            try:
                # üü¢ FIX: Import ‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ create_llm_instance
                from models.llm import create_llm_instance 
                self.llm = create_llm_instance( 
                    model_name=self.config.model_name,
                    temperature=self.config.temperature
                )
                self.logger.info("‚úÖ LLM Instance created successfully: %s (Temp: %s)", 
                                 self.config.model_name, self.config.temperature)
            except Exception as e:
                self.logger.error(f"FATAL: Could not initialize LLM: {e}")
                raise

    def _initialize_vsm_if_none(self):
        """
        Initializes VectorStoreManager with Smart Year Selection.
        - If Evidence: Priority 1 = Specific Year, Priority 2 = Root Fallback.
        - If Document: Priority 1 = Root (General), Priority 2 = Specific Year Fallback.
        """
        if self.vectorstore_manager is not None:
            return

        # 1. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ç‡∏≠‡∏á DocType
        clean_dt = str(self.doc_type or getattr(self.config, 'doc_type', EVIDENCE_DOC_TYPES)).strip().lower()
        is_evidence = (clean_dt == EVIDENCE_DOC_TYPES.lower())
        
        self.logger.info(f"üöÄ Loading vectorstore(s) for DocType: '{clean_dt}' (Mode: {'Evidence' if is_evidence else 'General'})")

        try:
            target_enabler = str(self.enabler).lower() if self.enabler else None
            
            # üéØ [SMART SELECTION] ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å
            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô evidence ‡πÉ‡∏´‡πâ‡∏´‡∏≤‡∏ï‡∏≤‡∏°‡∏õ‡∏µ (2568) ‡∏Å‡πà‡∏≠‡∏ô ‡πÅ‡∏ï‡πà‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô document ‡πÉ‡∏´‡πâ‡∏´‡∏≤‡∏ó‡∏µ‡πà Root (None) ‡∏Å‡πà‡∏≠‡∏ô
            primary_year = self.config.year if is_evidence else None
            secondary_year = None if is_evidence else self.config.year

            # 2. First Attempt: ‡πÇ‡∏´‡∏•‡∏î‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
            self.vectorstore_manager = load_all_vectorstores(
                doc_types=[clean_dt], 
                enabler_filter=target_enabler, 
                tenant=self.config.tenant, 
                year=primary_year       
            )
            
            def count_retrievers(vsm):
                if vsm and hasattr(vsm, '_multi_doc_retriever') and vsm._multi_doc_retriever:
                    return len(vsm._multi_doc_retriever._all_retrievers)
                return 0

            len_retrievers = count_retrievers(self.vectorstore_manager)
            
            # 3. Second Attempt (Fallback): ‡∏ñ‡πâ‡∏≤‡∏´‡∏≤‡πÅ‡∏ö‡∏ö‡πÅ‡∏£‡∏Å‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ ‡πÉ‡∏´‡πâ‡∏™‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏´‡∏≤‡∏≠‡∏µ‡∏Å‡πÅ‡∏ö‡∏ö
            if len_retrievers == 0:
                lookup_label = f"year {secondary_year}" if secondary_year else "tenant root"
                self.logger.info(f"‚ö†Ô∏è No collections found in primary path, searching in {lookup_label}...")
                
                self.vectorstore_manager = load_all_vectorstores(
                    doc_types=[clean_dt], 
                    enabler_filter=target_enabler, 
                    tenant=self.config.tenant, 
                    year=secondary_year       
                )
                len_retrievers = count_retrievers(self.vectorstore_manager)

            # 4. Post-Load Process
            if self.vectorstore_manager and len_retrievers > 0:
                self.vectorstore_manager._load_doc_id_mapping() 
                self.logger.info(f"‚úÖ MultiDocRetriever loaded with {len_retrievers} collections.") 
            else:
                # 5. Final Error Handling
                expected_p = f"data_store/{self.config.tenant}/vectorstore/{primary_year or 'root'}"
                self.logger.error(f"‚ùå FATAL: 0 vector store collections loaded. Please check folder: {expected_p}")
                raise ValueError(f"No vector collections found for '{target_enabler}' in {self.config.tenant}")

        except Exception as e:
            self.logger.error(f"‚ùå FATAL: Could not initialize VectorStoreManager: {str(e)}")
            raise

    def relevance_score_fn(self, evidence: Dict[str, Any], sub_id: str, level: int) -> float:
        """
        [REVISED RELEVANCE SCORE v2026.3.1 - Balanced & Robust]
        - 45% Rerank + 35% Keyword + 20% Bonuses
        - PDCA tag bonus ‡∏™‡∏π‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡∏ñ‡πâ‡∏≤‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö required phases
        - Source grading ‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏´‡πâ‡∏™‡∏°‡∏î‡∏∏‡∏• (primary + secondary)
        - Min floor ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö rerank ‡∏™‡∏π‡∏á
        - Logging ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô
        """
        if not evidence:
            return 0.0

        # 1. Rerank (45%)
        rerank_raw = evidence.get('rerank_score') or evidence.get('score') or 0.0
        rerank_score = float(rerank_raw) if isinstance(rerank_raw, (int, float)) else 0.0
        normalized_rerank = min(max(rerank_score, 0.0), 1.0)

        # 2. ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        text = (evidence.get('text') or evidence.get('page_content') or '').lower().strip()
        meta = evidence.get('metadata', {}) if isinstance(evidence.get('metadata'), dict) else {}
        filename = (meta.get('source') or meta.get('source_filename') or '').lower()

        # 3. Cumulative rules
        cum_rules = self.get_cumulative_rules(sub_id, level)

        # 4. Source Grading (‡∏õ‡∏£‡∏±‡∏ö‡∏™‡∏°‡∏î‡∏∏‡∏•)
        source_bonus = 0.0
        primary = ["‡∏°‡∏ï‡∏¥", "‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å", "‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á", "‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®", "‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢", "‡πÅ‡∏ú‡∏ô‡πÅ‡∏°‡πà‡∏ö‡∏ó", "‡∏°‡∏ï‡∏¥‡∏ö‡∏≠‡∏£‡πå‡∏î"]
        secondary = ["assessment report", "‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô", "‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•", "‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏•", "kpi"]
        if any(p in filename for p in primary):
            source_bonus += 0.20
        if any(p in filename for p in secondary):
            source_bonus += 0.10  # ‡πÑ‡∏°‡πà‡∏•‡∏ö ‡πÅ‡∏ï‡πà‡∏•‡∏î‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å

        # 5. Keyword Score (35%)
        target_kws = set()
        if level <= 2:
            target_kws.update(cum_rules.get('plan_keywords', []) + cum_rules.get('do_keywords', []))
        else:
            target_kws.update(cum_rules.get('check_keywords', []) + cum_rules.get('act_keywords', []))

        match_count = sum(1 for kw in target_kws if kw.lower() in text)
        expected = max(1, len(target_kws) * 0.3)  # ‡∏õ‡∏£‡∏±‡∏ö threshold ‡∏•‡∏á‡∏ô‡∏¥‡∏î
        keyword_score = min((match_count / expected) ** 0.6, 1.0)  # ^0.6 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏ï‡πà‡∏≥‡πÄ‡∏Å‡∏¥‡∏ô
        keyword_score = max(keyword_score, 0.20 if match_count >= 1 else 0.0)

        # 6. PDCA Tag Bonus (0.30 ‡∏ñ‡πâ‡∏≤‡∏ï‡∏£‡∏á required)
        pdca_bonus = 0.0
        pdca_tag = evidence.get('pdca_tag') or meta.get('pdca_tag') or ""
        required_phases = cum_rules.get('required_phases', [])
        if pdca_tag and str(pdca_tag).upper() in required_phases:
            pdca_bonus = 0.30
        elif pdca_tag and str(pdca_tag).upper() in {'P', 'D', 'C', 'A'}:
            pdca_bonus = 0.15  # bonus ‡πÄ‡∏•‡πá‡∏Å‡∏ñ‡πâ‡∏≤‡∏ï‡∏£‡∏á PDCA ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô required

        # 7. Neighbor Bonus
        neighbor_bonus = 0.15 if evidence.get('is_neighbor', False) or meta.get('is_neighbor', False) else 0.0

        # 8. Specific Rule Bonus (‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö rule ‡πÄ‡∏â‡∏û‡∏≤‡∏∞)
        specific_rule = cum_rules.get('specific_contextual_rule', '').lower()
        rule_bonus = 0.15 if specific_rule and any(word in text for word in specific_rule.split()[:10]) else 0.0

        # 9. ‡∏£‡∏ß‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô (45% Rerank + 35% Keyword + 20% Bonuses)
        final_score = (
            0.45 * normalized_rerank +
            0.35 * keyword_score +
            source_bonus + pdca_bonus + neighbor_bonus + rule_bonus
        )

        # 10. Min floor ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö rerank ‡∏™‡∏π‡∏á
        if normalized_rerank > 0.80:
            final_score = max(final_score, 0.45)

        final_score = min(max(final_score, 0.0), 1.0)

        # 11. Logging (info ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö high score, debug ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏∏‡∏Å‡∏ï‡∏±‡∏ß)
        if normalized_rerank > 0.75 or final_score > 0.60:
            self.logger.info(
                f"[HIGH-RELEVANCE] {sub_id} L{level} | "
                f"final={final_score:.4f} | rerank={normalized_rerank:.4f} | "
                f"kw={keyword_score:.4f} | pdca_bonus={pdca_bonus:.3f} | "
                f"tag={pdca_tag} | source_bonus={source_bonus:.3f}"
            )

        self.logger.debug(
            f"[{sub_id} L{level}] RelScore: {final_score:.4f} | Rerank: {normalized_rerank:.4f} | "
            f"KW: {keyword_score:.4f} | PDCA: {pdca_bonus:.3f} | Src: {source_bonus:.3f}"
        )

        return final_score

    # -------------------- Contextual Rules Handlers (FIXED) --------------------
    def _load_contextual_rules_map(self) -> Dict[str, Any]:
        """
        [FINAL REVISED v2026.5] ‡πÇ‡∏´‡∏•‡∏î Contextual Rules ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
        - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö multi-tenant ‡πÅ‡∏•‡∏∞ multi-enabler ‡πÄ‡∏ï‡πá‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö
        - ‡∏°‡∏µ fallback ‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
        - ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Maturity (L1-L5) ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏Ç‡πâ‡∏°‡∏á‡∏ß‡∏î
        - Logging ‡∏ó‡∏µ‡πà‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏ä‡πà‡∏ß‡∏¢ debug
        - ‡πÑ‡∏°‡πà raise exception (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ engine ‡∏•‡πâ‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏£‡∏∞‡∏ö‡∏ö)
        """
        # 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á path ‡∏ï‡∏≤‡∏° tenant + enabler
        try:
            filepath = get_contextual_rules_file_path(
                tenant=self.config.tenant,
                enabler=self.enabler
            )
            self.logger.debug(f"üîç Attempting to load contextual rules from: {filepath}")
        except Exception as e:
            self.logger.error(f"‚ùå FATAL: Failed to generate rules file path: {e}")
            return {"_enabler_defaults": {}}

        # 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if not os.path.exists(filepath):
            self.logger.warning(
                f"‚ö†Ô∏è Contextual Rules file not found: {filepath}\n"
                f"   ‚Üí Using only global defaults (if available from fallback)."
            )
            return {"_enabler_defaults": {}}

        # 3. ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞ parse JSON
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
        except json.JSONDecodeError as e:
            self.logger.error(f"‚ùå JSON Decode Error in {filepath}: {e} (line {e.lineno}, col {e.colno})")
            return {"_enabler_defaults": {}}
        except Exception as e:
            self.logger.error(f"‚ùå Unexpected error reading {filepath}: {e}")
            return {"_enabler_defaults": {}}

        # 4. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        if not isinstance(data, dict):
            self.logger.error(f"‚ùå Invalid rules format in {filepath}: Expected dict, got {type(data)}")
            return {"_enabler_defaults": {}}

        # 5. ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô criteria ‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Maturity Structure
        sub_criteria_keys = [k for k in data.keys() if not k.startswith("_")]
        num_criteria = len(sub_criteria_keys)

        if num_criteria == 0:
            self.logger.warning(f"‚ö†Ô∏è No sub-criteria found in {filepath} (only defaults).")
        else:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ Maturity Levels (L1, L2, ...) ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            sample_sub = sub_criteria_keys[0]
            sub_data = data[sample_sub]
            level_keys = [k for k in sub_data.keys() if k.startswith("L") and len(k) >= 2]
            
            if level_keys:
                detected_levels = sorted(level_keys)
                self.logger.info(
                    f"‚úÖ Maturity-Based Rules loaded successfully!\n"
                    f"   File: {filepath}\n"
                    f"   Criteria: {num_criteria} (e.g., {sample_sub})\n"
                    f"   Levels detected: {', '.join(detected_levels)}"
                )
            else:
                self.logger.warning(
                    f"‚ö†Ô∏è Rules for '{sample_sub}' in {filepath} do not contain Maturity Levels (L1-L5).\n"
                    f"   Falling back to flat structure or defaults."
                )

        # 6. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö _enabler_defaults
        if "_enabler_defaults" in data:
            defaults = data["_enabler_defaults"]
            if isinstance(defaults, dict) and any(k.endswith("_keywords") for k in defaults.keys()):
                self.logger.info("‚úÖ Global PDCA Keywords (_enabler_defaults) loaded.")
            else:
                self.logger.warning("‚ö†Ô∏è _enabler_defaults exists but has invalid structure.")
        else:
            self.logger.info("‚ÑπÔ∏è No _enabler_defaults section found. Using empty defaults.")

        # 7. ‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô data ‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
        self.logger.info(f"‚úÖ Contextual Rules fully loaded from {filepath} ({num_criteria} criteria).")
        return data
    
    def get_rule_content(self, sub_id: str, level: int, key_type: str):
        """
        ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏à‡∏≤‡∏Å Contextual Rules ‡πÅ‡∏ö‡∏ö‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ä‡∏±‡πâ‡∏ô
        Priority: Specific Level > Sub-ID Root > Global Defaults > Fallback
        """
        rule = self.contextual_rules_map.get(sub_id, {})
        level_key = f"L{level}"

        # 1. Specific Level
        if key_type in rule.get(level_key, {}):
            return rule[level_key][key_type]

        # 2. Sub-ID Root
        if key_type in rule:
            return rule[key_type]

        # 3. Global Defaults
        defaults = self.contextual_rules_map.get("_enabler_defaults", {})
        if key_type in defaults:
            return defaults[key_type]

        # 4. Fallback ‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó (‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ string check)
        fallbacks = {
            "require_phase": None,
            "must_include_keywords": [],
            "plan_keywords": [],
            "do_keywords": [],
            "check_keywords": [],
            "act_keywords": [],
            # ‡πÄ‡∏û‡∏¥‡πà‡∏° key ‡∏≠‡∏∑‡πà‡∏ô ‡πÜ ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô list ‡πÑ‡∏î‡πâ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà
        }
        return fallbacks.get(key_type, "")  # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å key_type ‚Üí ‡∏Ñ‡∏∑‡∏ô string ‡∏ß‡πà‡∏≤‡∏á    

    def _check_contextual_rule_condition(
        self, 
        condition: Dict[str, Any], 
        sub_id: str, 
        level: int, 
        top_evidences: List[Dict[str, Any]]
    ) -> bool:
        """
        [SIMPLIFIED v2026] ‡πÅ‡∏Ñ‡πà log ‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô continuity + min evidence
        ‡πÑ‡∏°‡πà block ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô (return True ‡πÄ‡∏™‡∏°‡∏≠)
        """
        if level > 1:
            prev_level = level - 1
            is_prev_passed = False
            if hasattr(self, 'level_details_map') and str(prev_level) in self.level_details_map:
                is_prev_passed = self.level_details_map[str(prev_level)].get('is_passed', False)
            
            if not is_prev_passed:
                self.logger.warning(f"‚ö†Ô∏è [GAP DETECTED] L{prev_level} not passed for {sub_id} L{level} - may affect validity")

        min_docs = condition.get('min_evidences', 1)
        if len(top_evidences) < min_docs:
            self.logger.warning(f"‚ö†Ô∏è [LOW EVIDENCE] {sub_id} L{level}: {len(top_evidences)} docs (required: {min_docs})")

        return True  # ‡πÑ‡∏°‡πà block
    
    def get_cumulative_rules(self, sub_id: str, current_level: int) -> Dict[str, Any]:
        """
        [REVISED CUMULATIVE RULES ENGINE v2026.8 - PRIORITY & SMART ACCUMULATION]
        ----------------------------------------------------------------------------
        - ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏Å‡∏é‡∏™‡∏∞‡∏™‡∏°‡∏à‡∏≤‡∏Å L1 ‚Üí current_level ‡πÇ‡∏î‡∏¢‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Å‡∏±‡∏ö level ‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤
        - ‡πÉ‡∏ä‡πâ OrderedDict ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏•‡∏≥‡∏î‡∏±‡∏ö (specific > default)
        - Required phases ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏à‡∏≤‡∏Å level ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î (maturity-driven)
        - ‡πÅ‡∏¢‡∏Å instructions ‡πÄ‡∏õ‡πá‡∏ô dict {level: rule} + string ‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        - Logging ‡πÅ‡∏¢‡∏Å level: info ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö summary, debug ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö detail
        - Fallback ‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ rules ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö sub_id ‡∏ô‡∏±‡πâ‡∏ô

        Args:
            sub_id (str): ‡∏£‡∏´‡∏±‡∏™ sub-criteria ‡πÄ‡∏ä‡πà‡∏ô "1.2"
            current_level (int): ‡∏£‡∏∞‡∏î‡∏±‡∏ö maturity ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô (1-5)

        Returns:
            Dict[str, Any]: {
                "plan_keywords": List[str],
                "do_keywords": List[str],
                "check_keywords": List[str],
                "act_keywords": List[str],
                "required_phases": List[str],          # sorted, unique
                "level_specific_instructions": Dict[int, str],
                "all_instructions": str,               # ‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö prompt
                "source_summary": str                  # ‡∏™‡∏£‡∏∏‡∏õ‡∏ß‡πà‡∏≤‡∏°‡∏≤‡∏à‡∏≤‡∏Å level ‡πÑ‡∏´‡∏ô‡∏ö‡πâ‡∏≤‡∏á
            }
        """
        # 1. ‡∏î‡∏∂‡∏á defaults ‡πÄ‡∏õ‡πá‡∏ô‡∏ê‡∏≤‡∏ô (fallback ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ rules ‡πÄ‡∏â‡∏û‡∏≤‡∏∞)
        defaults = self.contextual_rules_map.get('_enabler_defaults', {})
        
        # ‡πÉ‡∏ä‡πâ OrderedDict ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏•‡∏≥‡∏î‡∏±‡∏ö: default ‚Üí L1 ‚Üí L2 ‚Üí ... ‚Üí current_level
        cum_keywords = {
            "plan": OrderedDict((k, None) for k in defaults.get('plan_keywords', [])),
            "do":   OrderedDict((k, None) for k in defaults.get('do_keywords', [])),
            "check": OrderedDict((k, None) for k in defaults.get('check_keywords', [])),
            "act":  OrderedDict((k, None) for k in defaults.get('act_keywords', []))
        }

        required_phases: Set[str] = set()
        level_specific_instructions: Dict[int, str] = {}
        source_levels: List[int] = []

        # 2. ‡∏î‡∏∂‡∏á rules ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡∏≠‡∏á sub_id
        sub_rules = self.contextual_rules_map.get(sub_id, {})
        if not sub_rules:
            logger.warning(f"[RULE_CUMULATIVE] No specific rules for {sub_id} ‚Üí using defaults only")
        
        # 3. ‡∏™‡∏∞‡∏™‡∏°‡∏à‡∏≤‡∏Å L1 ‡∏ñ‡∏∂‡∏á current_level (‡πÉ‡∏´‡πâ level ‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤‡∏ó‡∏±‡∏ö‡∏•‡∏≥‡∏î‡∏±‡∏ö)
        for lv in range(1, current_level + 1):
            lv_key = f"L{lv}"
            level_rule = sub_rules.get(lv_key, {})
            
            if not level_rule:
                continue  # ‡∏Ç‡πâ‡∏≤‡∏° level ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ rules
            
            source_levels.append(lv)
            
            # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï keywords (level ‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤‡∏ó‡∏±‡∏ö default ‡πÅ‡∏•‡∏∞ level ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤)
            for phase, key_name in [("plan", "plan_keywords"), ("do", "do_keywords"),
                                   ("check", "check_keywords"), ("act", "act_keywords")]:
                new_kws = level_rule.get(key_name, [])
                for kw in new_kws:
                    cum_keywords[phase][kw] = None  # OrderedDict ‡∏à‡∏∞‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏•‡∏≥‡∏î‡∏±‡∏ö
            
            # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï required phases (union ‡∏ó‡∏∏‡∏Å level)
            if 'require_phase' in level_rule:
                required_phases.update(level_rule['require_phase'])
            
            # ‡πÄ‡∏Å‡πá‡∏ö specific rule ‡∏ï‡∏≤‡∏° level
            specific = level_rule.get('specific_contextual_rule')
            if specific:
                level_specific_instructions[lv] = specific.strip()

        # 4. ‡πÅ‡∏õ‡∏•‡∏á OrderedDict ‚Üí list (‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏•‡∏≥‡∏î‡∏±‡∏ö)
        result_keywords = {phase: list(cum_keywords[phase].keys()) for phase in cum_keywords}

        # 5. Required phases: ‡πÉ‡∏ä‡πâ‡∏à‡∏≤‡∏Å level ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å (‡∏ñ‡πâ‡∏≤ L5 ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ P,D,C,A ‚Üí ‡πÉ‡∏ä‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)
        # ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡∏Ñ‡∏á union ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
        final_required = sorted(list(required_phases)) if required_phases else []

        # 6. ‡∏£‡∏ß‡∏° instructions ‡πÄ‡∏õ‡πá‡∏ô string (‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ LLM ‡πÇ‡∏ü‡∏Å‡∏±‡∏™‡∏ñ‡∏π‡∏Å‡∏à‡∏∏‡∏î)
        instructions_lines = ["‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏£‡∏∞‡∏î‡∏±‡∏ö Maturity:"]
        for lv in sorted(level_specific_instructions.keys()):
            prefix = "üéØ " if lv == current_level else "‚úÖ " # ‡πÄ‡∏ô‡πâ‡∏ô‡πÄ‡∏•‡πÄ‡∏ß‡∏•‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
            instructions_lines.append(f"{prefix}‡∏£‡∏∞‡∏î‡∏±‡∏ö L{lv}: {level_specific_instructions[lv]}")
        all_instructions = "\n".join(instructions_lines)

        # 7. ‡∏™‡∏£‡∏∏‡∏õ source ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö debug
        source_str = f"from levels {source_levels}" if source_levels else "defaults only"

        # 8. Logging (info ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö production monitoring, debug ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö trace)
        logger.info(
            f"[RULE_CUMULATIVE] {sub_id} L{current_level} | "
            f"Keywords (dedup): P={len(result_keywords['plan'])} | "
            f"D={len(result_keywords['do'])} | C={len(result_keywords['check'])} | "
            f"A={len(result_keywords['act'])} | Required={final_required} | "
            f"Instructions={len(level_specific_instructions)} | Source={source_str}"
        )

        if logger.isEnabledFor(logging.DEBUG):
            logger.debug(
                f"[RULE_DETAIL] {sub_id} L{current_level} | "
                f"Plan keywords sample: {result_keywords['plan'][:5]}... | "
                f"Required phases accumulated from levels: {sorted(source_levels)}"
            )

        # 9. Return structure ‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢
        return {
            "plan_keywords": result_keywords["plan"],
            "do_keywords": result_keywords["do"],
            "check_keywords": result_keywords["check"],
            "act_keywords": result_keywords["act"],
            "required_phases": final_required,
            "level_specific_instructions": level_specific_instructions,  # Dict[int, str]
            "all_instructions": all_instructions,                       # string ‡∏£‡∏ß‡∏°
            "source_summary": source_str,
            "accumulated_levels": sorted(source_levels)
        }

    def post_process_llm_result(
        self,
        llm_output: Any,
        level: int,
        sub_id: str = None,
        contextual_config: Dict = {},
        top_evidences: List[Dict[str, Any]] = []
    ) -> Dict[str, Any]:
        """
        [POST-PROCESS v2026.1.19 ‚Äî Production Stable]
        - JSON Repair ‡∏ó‡∏ô‡∏ó‡∏≤‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î (markdown, trailing comma, calculation text, encoding)
        - Rescue Logic ‡πÅ‡∏Ç‡πá‡∏á‡πÅ‡∏£‡∏á (keyword + rerank conflict)
        - PDCA Normalization ‡∏™‡∏°‡∏î‡∏∏‡∏• (required phases ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å)
        - Safety Net ‡∏â‡∏•‡∏≤‡∏î (force 1.2 + ‡∏â‡∏µ‡∏î PDCA breakdown)
        - Coaching + Missing phases ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
        """
        log_prefix = f"{sub_id or 'Unknown'} L{level}"

        # 1. JSON Repair & Unpacking (Robust v2)
        if isinstance(llm_output, tuple):
            llm_output = llm_output[0] if len(llm_output) > 0 else {}
        if isinstance(llm_output, str):
            try:
                # ‡∏•‡πâ‡∏≤‡∏á markdown, calculation, trailing comma
                cleaned = re.sub(r'```json\s*|\s*```', '', llm_output)
                cleaned = re.sub(r'(\d+\.?\d*)\s*[\+\-]\s*(\d+\.?\d*)\s*=\s*(\d+\.?\d*)', r'\3', cleaned)
                cleaned = cleaned.strip().replace(",\n}", "\n}").replace(",}", "}")
                cleaned = re.sub(r',\s*([}\]])', r'\1', cleaned)  # ‡∏•‡∏ö trailing comma
                cleaned = cleaned.encode('utf-8', 'ignore').decode('utf-8')  # ‡∏•‡∏ö‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏û‡∏¥‡πÄ‡∏®‡∏©
                llm_output = json.loads(cleaned)
            except Exception as e:
                self.logger.error(f"‚ùå [JSON REPAIR FAILED] {log_prefix}: {str(e)}")
                return {"is_passed": False, "score": 0.0, "reason": "JSON Parsing Error"}

        if not isinstance(llm_output, dict):
            self.logger.error(f"‚ùå [INVALID OUTPUT] {log_prefix}: Not a dict")
            return {"is_passed": False, "score": 0.0, "reason": "Invalid LLM Output Format"}

        # 2. Required Phases (‡∏à‡∏≤‡∏Å config ‡∏´‡∏£‡∏∑‡∏≠ default)
        rubric = contextual_config.get(sub_id, {}).get(str(level), {})
        if level <= 3:
            default_phases = ["P", "D"]
        elif level == 4:
            default_phases = ["P", "D", "C"]
        else:
            default_phases = ["P", "D", "C", "A"]
        required_phases = rubric.get("require_phase", default_phases)

        # 3. PDCA Score Extraction + Smart Rescue
        pdca_results = {"P": 0.0, "D": 0.0, "C": 0.0, "A": 0.0}
        reason_raw = str(llm_output.get('reason', '')).lower()
        extraction_raw = {p: str(llm_output.get(f"Extraction_{p}", "")).lower() for p in ["P", "D", "C", "A"]}

        for phase in ["P", "D", "C", "A"]:
            val = float(llm_output.get(f"{phase}_Plan_Score") or 
                        llm_output.get(f"score_{phase.lower()}") or 
                        llm_output.get(f"{phase}_Score") or 0.0)
            score = min(val, 2.0)

            # Rescue 1: Keyword in reason/extraction
            phase_keywords = rubric.get(f"{phase.lower()}_keywords", [])
            if score < 1.0 and any(kw.lower() in (reason_raw + extraction_raw[phase]) for kw in phase_keywords):
                score = max(score, 1.5)
                self.logger.info(f"üõ°Ô∏è [RESCUE: KEYWORD] {log_prefix} {phase} boosted to {score}")

            pdca_results[phase] = score

        # 4. Adaptive Normalization
        raw_total_required = sum(pdca_results[p] for p in required_phases)
        max_possible_required = len(required_phases) * 2.0
        normalized_score = (raw_total_required / max_possible_required) * 2.0 if max_possible_required > 0 else 0.0
        normalized_score = round(normalized_score, 2)

        # 5. Rerank Safety Net (‡πÅ‡∏Å‡πâ L0 Gap)
        max_rerank = max(ev.get('relevance_score', 0.0) for ev in top_evidences) if top_evidences else 0.0
        is_conflict = (normalized_score < 1.2) and (max_rerank > 0.85)

        if is_conflict:
            normalized_score = 1.2  # Force Pass Threshold
            llm_output["needs_human_review"] = True
            self.logger.info(f"üõ°Ô∏è [RERANK-SAFETY] Force Passed (1.2) | Rerank: {max_rerank:.2f} | Original: {normalized_score}")

            # ‡∏â‡∏µ‡∏î‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏ü‡∏™‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô (‡∏™‡∏°‡∏î‡∏∏‡∏• PDCA breakdown)
            min_per_phase = 1.2 / len(required_phases)
            for phase in required_phases:
                if pdca_results[phase] < min_per_phase:
                    pdca_results[phase] = round(min_per_phase + 0.1, 2)
            llm_output["raw_pdca_sum"] = round(sum(pdca_results.values()), 2)

        # 6. Threshold & Decision
        is_passed = normalized_score >= 1.2

        # 7. Missing Phases + Enhanced Coaching
        missing_phases = [p for p in required_phases if pdca_results[p] < 1.0]
        coaching = llm_output.get("coaching_insight", "").strip()
        if missing_phases:
            missing_str = ", ".join(missing_phases)
            coaching = f"‚ö†Ô∏è ‡∏Ç‡∏≤‡∏î‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô: {missing_str}. {coaching}"
            if is_conflict:
                coaching += " (‡∏ú‡πà‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ rerank ‡∏™‡∏π‡∏á ‡πÅ‡∏ï‡πà‡∏Ñ‡∏ß‡∏£‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏ô‡πÄ‡∏ü‡∏™‡∏ó‡∏µ‡πà‡∏Ç‡∏≤‡∏î)"

        # 8. Final Packaging
        llm_output.update({
            "score": normalized_score,
            "pdca_breakdown": pdca_results,
            "is_passed": is_passed,
            "required_phases": required_phases,
            "coaching_insight": coaching,
            "status": "PASSED" if is_passed else "FAILED",
            "missing_phases": missing_phases
        })

        return llm_output

    def _is_previous_level_passed(self, sub_id: str, level: int) -> bool:
        """
        [STRICT REVISE v2026.01.18.1] - ‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏ö‡∏ö‡πÄ‡∏Ç‡πâ‡∏°‡∏á‡∏ß‡∏î
        - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Fallback ‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≤‡∏° Level ‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå
        - ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠‡πÄ‡∏â‡∏û‡∏≤‡∏∞ assessment_results_map ‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
        """
        if level <= 1: 
            return True
            
        prev_level = level - 1
        possible_keys = [f"{sub_id}.L{prev_level}", f"{sub_id}_L{prev_level}"]
        
        # 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á (‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠‡πÑ‡∏î‡πâ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î)
        for key in possible_keys:
            result = self.assessment_results_map.get(key)
            if result:
                # ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô bool ‡πÅ‡∏•‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô True ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
                if result.get('is_passed') is True:
                    self.logger.info(f"‚úÖ [LEVEL-GATE] Level {prev_level} passed for {sub_id}")
                    return True
                else:
                    self.logger.warning(f"‚ö†Ô∏è [LEVEL-GATE] Level {prev_level} found but status is FAIL")
                    return False

        # 2. üõ°Ô∏è Safe Guard: ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô Map ‡πÄ‡∏•‡∏¢ (‡πÄ‡∏ä‡πà‡∏ô ‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡πÄ‡∏û‡∏¥‡πà‡∏á‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πâ‡∏≤‡∏°‡∏°‡∏≤) 
        # ‡πÉ‡∏´‡πâ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤ "‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô" ‡πÑ‡∏ß‡πâ‡∏Å‡πà‡∏≠‡∏ô ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏ï‡∏≤‡∏°‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô SE-AM
        self.logger.warning(f"üö´ [LEVEL-GATE] No assessment record for L{prev_level}. Blocking L{level}.")
        return False

    def _expand_context_with_neighbor_pages(self, top_evidences: List[Any], collection_name: str) -> List[Any]:
        """
        [REVISE v6] Optimized Context Expansion
        - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏∞‡∏ö‡∏ö Duplicate Prevention ‡∏Å‡πà‡∏≠‡∏ô Query DB
        - ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á PDCA Tagging ‡∏ï‡∏≤‡∏° Offset (Before=Support, After=Detail)
        - ‡∏à‡∏≥‡∏Å‡∏±‡∏î Max Expansion ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Token Overload
        """
        if not self.vectorstore_manager or not top_evidences:
            return top_evidences

        expanded_evidences = list(top_evidences)
        seen_keys = set()
        added_pages = 0
        MAX_PAGES_PER_SUB = 10 # üö© ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ö‡∏ß‡∏°‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
        
        strategic_triggers = ["‡∏ß‡∏¥‡∏™‡∏±‡∏¢‡∏ó‡∏±‡∏®‡∏ô‡πå", "‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢", "‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á", "‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏Å", "‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå", "‡∏™‡∏≤‡∏£‡∏à‡∏≤‡∏Å", "‡∏Ñ‡∏≥‡∏ô‡∏≥"]
        check_triggers = ["‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏∂‡∏á‡∏û‡∏≠‡πÉ‡∏à", "‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô", "‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•", "‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î", "‡∏ú‡∏•‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô", "kpi", "score", "‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô"]

        for doc in top_evidences:
            if added_pages >= MAX_PAGES_PER_SUB: break

            meta = doc.metadata if hasattr(doc, 'metadata') else doc.get('metadata', {})
            text = (doc.get('text') or doc.get('page_content') or "").lower()
            
            filename = meta.get("source") or meta.get("source_filename") or "Unknown File"
            doc_uuid = meta.get("stable_doc_uuid") or meta.get("doc_id")
            if not doc_uuid: continue

            try:
                current_page_str = str(meta.get("page_label", meta.get("page", "1")))
                current_page = int("".join(filter(str.isdigit, current_page_str)))
            except: continue

            # üéØ Offset Strategy: ‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏° Auditor
            offsets = []
            if any(k in text for k in strategic_triggers): offsets.extend([-1, 1, 2])
            if any(k in text for k in check_triggers): offsets.extend([-1, 1, 2, 3])
            
            for offset in sorted(list(set(offsets))):
                target_page = current_page + offset
                if target_page < 1 or target_page == current_page: continue
                
                cache_key = f"{doc_uuid}_{target_page}"
                if cache_key in seen_keys: continue
                seen_keys.add(cache_key) # üö© ‡∏•‡πá‡∏≠‡∏Å Key ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏†‡∏≤‡∏£‡∏∞ DB

                neighbor_chunks = self.vectorstore_manager.get_chunks_by_page(
                    collection_name=collection_name,
                    stable_doc_uuid=doc_uuid,
                    page_label=str(target_page)
                )

                if neighbor_chunks:
                    self.logger.info(f"‚ûï Neighbor Fetch: Page {target_page} in {filename}")
                    
                    for nc in neighbor_chunks:
                        fixed_metadata = (nc.metadata.copy() if hasattr(nc, 'metadata') else {}).copy()
                        fixed_metadata.update({
                            "stable_doc_uuid": doc_uuid,
                            "page_label": str(target_page),
                            "source": filename,
                            "is_supplemental": True
                        })

                        # üè∑Ô∏è Smart Tagging: ‡∏´‡∏ô‡πâ‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏°‡∏±‡∏Å‡πÄ‡∏õ‡πá‡∏ô Context, ‡∏´‡∏ô‡πâ‡∏≤‡∏´‡∏•‡∏±‡∏á‡∏°‡∏±‡∏Å‡πÄ‡∏õ‡πá‡∏ô Detail
                        assigned_tag = "Support" if offset < 0 else "Detail"
                        if any(k in nc.page_content.lower() for k in check_triggers):
                            assigned_tag = "Act/Check"

                        expanded_evidences.append({
                            "text": f"[Supplemental Context - Page {target_page}]:\n{nc.page_content}",
                            "page_content": nc.page_content,
                            "metadata": fixed_metadata,
                            "pdca_tag": assigned_tag,
                            "is_supplemental": True,
                            "rerank_score": doc.get('rerank_score', 0.0) if isinstance(doc, dict) else 0.0
                        })
                    added_pages += 1

        return expanded_evidences
        

    def _resolve_evidence_filenames(self, evidence_entries: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        [REVISED v2026.1.18] - ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏∑‡∏ö‡∏Ñ‡πâ‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡πâ‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Ç‡∏∂‡πâ‡∏ô
        - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Metadata ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á (Direct Metadata Check)
        - ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á Display Source ‡πÉ‡∏´‡πâ‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°‡πÅ‡∏•‡∏∞‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô
        """
        resolved_entries = []
        for entry in evidence_entries:
            resolved_entry = deepcopy(entry)
            doc_id = resolved_entry.get("doc_id", "")
            # ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å metadata ‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏™‡∏≥‡∏£‡∏≠‡∏á (Fallback)
            meta = resolved_entry.get("metadata", {}) if isinstance(resolved_entry.get("metadata"), dict) else {}
            meta_filename = meta.get("source") or meta.get("source_filename")
            
            content_raw = resolved_entry.get('content') or resolved_entry.get('text', '')
            level_origin = resolved_entry.get('level', 'N/A')
            page_label = resolved_entry.get("page_label") or resolved_entry.get("page") or "N/A"

            # 1. AI Generated Reference (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô AI ‡∏°‡πÇ‡∏ô‡∏£‡∏´‡∏±‡∏™‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£)
            if str(doc_id).startswith("UNKNOWN-") or not doc_id:
                if not content_raw:
                    continue # ‡∏Ç‡πâ‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏¢‡∏∞
                resolved_entry["filename"] = "AI-GENERATED-REF"
                resolved_entry["display_source"] = f"Reference (‡∏´‡∏ô‡πâ‡∏≤ {page_label})"
            
            # 2. ‡πÄ‡∏Ñ‡∏™‡∏õ‡∏Å‡∏ï‡∏¥: ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô Map ‡∏´‡∏£‡∏∑‡∏≠ Metadata
            elif doc_id in self.doc_id_to_filename_map:
                mapped_name = self.doc_id_to_filename_map[doc_id]
                resolved_entry["filename"] = mapped_name
                resolved_entry["display_source"] = f"{mapped_name} (‡∏´‡∏ô‡πâ‡∏≤ {page_label})"
            
            elif meta_filename:
                # üö© [NEW] ‡∏ñ‡πâ‡∏≤‡πÉ‡∏ô Map ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÅ‡∏ï‡πà‡πÉ‡∏ô Metadata ‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏•‡∏¢
                resolved_entry["filename"] = meta_filename
                resolved_entry["display_source"] = f"{meta_filename} (‡∏´‡∏ô‡πâ‡∏≤ {page_label})"

            # 3. Fallback: ‡∏Å‡∏£‡∏ì‡∏µ‡∏£‡∏´‡∏±‡∏™‡∏ú‡πà‡∏≤‡∏ô‡πÅ‡∏ï‡πà‡∏´‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡∏à‡∏£‡∏¥‡∏á‡πÜ
            else:
                short_id = str(doc_id)[:8]
                resolved_entry["filename"] = f"DOC-{short_id}"
                resolved_entry["display_source"] = f"‡∏£‡∏´‡∏±‡∏™‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ {short_id} (‡∏´‡∏ô‡πâ‡∏≤ {page_label})"

            resolved_entries.append(resolved_entry)
        return resolved_entries
    

    # ----------------------------------------------------------------------
    # üéØ FINAL FIX 2.3: Manual Map Reload Function (inside SEAMPDCAEngine)
    # ----------------------------------------------------------------------
    def _collect_previous_level_evidences(self, sub_id: str, current_level: int) -> Dict[str, List[Dict]]:
        """
        [REVISED v2026.1.18] - Robust Context Hydration
        - ‡∏î‡∏∂‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏ï‡πá‡∏°‡∏à‡∏≤‡∏Å VectorStore ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡πá‡∏ô Baseline ‡πÉ‡∏´‡πâ‡πÄ‡∏•‡πÄ‡∏ß‡∏•‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
        - ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏≤‡∏£ Match UUID ‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏ó‡∏∏‡∏Å Format (Strip dashes)
        """
        if getattr(self, 'is_parallel_all_mode', False):
            return {}

        collected = {}
        for key, ev_list in self.evidence_map.items():
            # ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏•‡πÄ‡∏ß‡∏•‡∏ó‡∏µ‡πà‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡πÉ‡∏ô Sub-Criteria ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
            if key.startswith(f"{sub_id}.L"):
                try:
                    level_num = int(key.split(".L")[-1])
                    if level_num < current_level:
                        collected[key] = ev_list
                except: continue

        if not collected: return {}

        # 1. ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏° Unique IDs (‡∏Ñ‡∏±‡∏î‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡∏¢‡∏∞)
        stable_ids = set()
        for ev_list in collected.values():
            for ev in ev_list:
                sid = ev.get("stable_doc_uuid") or ev.get("doc_id")
                if sid and str(sid).lower() not in ["n/a", "none", ""]:
                    stable_ids.add(str(sid))

        if not stable_ids: return collected

        # 2. Bulk Hydration (Query ‡∏ó‡∏µ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û)
        vsm = self.vectorstore_manager
        chunk_map = {}
        try:
            full_chunks = vsm.get_documents_by_id(list(stable_ids), self.doc_type, self.enabler)
            for chunk in full_chunks:
                m = chunk.metadata
                # ‡πÄ‡∏Å‡πá‡∏ö Map ‡∏ó‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö‡∏°‡∏µ‡∏Ç‡∏µ‡∏î‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡∏µ‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏ß‡∏£‡πå
                keys = [str(m.get(k)) for k in ["stable_doc_uuid", "doc_id", "chunk_uuid"] if m.get(k)]
                for k in keys:
                    chunk_map[k] = {"text": chunk.page_content, "metadata": m}
                    chunk_map[k.replace("-", "")] = {"text": chunk.page_content, "metadata": m}
        except Exception as e:
            self.logger.error(f"‚ùå Hydration VSM Error: {e}")
            return collected

        # 3. Restoration Loop
        restored_count = 0
        for key, ev_list in collected.items():
            for ev in ev_list:
                sid = str(ev.get("stable_doc_uuid") or ev.get("doc_id") or "")
                data = chunk_map.get(sid) or chunk_map.get(sid.replace("-", ""))

                if data:
                    ev.update({
                        "text": data["text"],
                        "metadata": data["metadata"],
                        "is_baseline": True
                    })
                    restored_count += 1
                else:
                    ev["is_baseline"] = False
                
        self.logger.info(f"‚úÖ Hydrated {restored_count} baseline chunks for {sub_id} L{current_level}")
        return collected

    def _get_contextual_rules_prompt(self, sub_id: str, level: int) -> str:
        """
        Retrieves the specific Contextual Rule prompt for a given Sub-Criteria and Level,
        ‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á‡∏Å‡∏≤‡∏£ Inject ‡∏Å‡∏é L5 ‡∏û‡∏¥‡πÄ‡∏®‡∏© ‡∏´‡∏≤‡∏Å Level == 5
        """
        sub_id_rules = self.contextual_rules_map.get(sub_id)
        rule_text = ""
        
        # 1. ‡∏î‡∏∂‡∏á‡∏Å‡∏é‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏¢‡πà‡∏≠‡∏¢ (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
        if sub_id_rules:
            level_key = f"L{level}"
            specific_rule = sub_id_rules.get(level_key)
            if specific_rule:
                rule_text += f"\n--- ‡∏Å‡∏é‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏¢‡πà‡∏≠‡∏¢ ({sub_id} L{level}) ---\n‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Ñ‡∏ß‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ: {specific_rule}\n"
        
        # 2. **INJECT L5 SPECIAL RULE (Safe Injection)**
        # ‡πÉ‡∏™‡πà‡∏Å‡∏é Bonus 2.0 ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô L5 ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏ö‡∏Å‡∏ß‡∏ô L3/L4
        if level == 5:
            l5_bonus_rule = """
            \n--- L5 SPECIAL RULE (Innovation & Sustainability) ---
            * **L5 PASS Condition (‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö):** ‡∏´‡∏≤‡∏Å Level ‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠ **L5** ‡∏ó‡πà‡∏≤‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏° PDCA (P+D+C+A) ‡∏ï‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô L3/L4 ‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏ô‡∏û‡∏ö‡∏Å‡πà‡∏≠‡∏ô (‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î 8.0)
            * **‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç Bonus 2.0:** ‡∏´‡∏≤‡∏Å‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏° PDCA ‡πÑ‡∏î‡πâ **‚â• 7.0** **‡πÅ‡∏•‡∏∞** ‡∏ó‡πà‡∏≤‡∏ô‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠ **‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 1 ‡∏ä‡∏¥‡πâ‡∏ô** ‡πÉ‡∏ô Context ‡∏ó‡∏µ‡πà‡πÅ‡∏™‡∏î‡∏á‡∏ñ‡∏∂‡∏á:
                * (a) ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏• KM / ‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏° (Innovation Award)
                * (b) ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡∏ó‡∏µ‡πà‡∏ß‡∏±‡∏î‡πÑ‡∏î‡πâ (ROI, Productivity, Cost Saving)
                * (c) ‡∏Å‡∏≤‡∏£‡πÄ‡∏ú‡∏¢‡πÅ‡∏û‡∏£‡πà/‡∏Å‡∏≤‡∏£‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å (External Recognition/Publication)
            * **‡πÉ‡∏´‡πâ‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡πÉ‡∏´‡πâ Bonus Score 2.0 ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ **Score ‚â• 9.0** ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡πâ‡∏á **is_passed=true** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏∞‡∏ó‡πâ‡∏≠‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏•‡∏¥‡∏® (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£ Reset ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô)
            """
            rule_text += l5_bonus_rule
            
        return rule_text

    def _load_rubric(self) -> Dict[str, Any]:
        """ Loads the SEAM rubric JSON file using path_utils. """
        
        # üéØ FIX: ‡πÉ‡∏ä‡πâ get_rubric_file_path ‡∏à‡∏≤‡∏Å path_utils ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Path ‡πÄ‡∏≠‡∏á
        filepath = None # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô UnboundLocalError
        
        try:
            # 1. ‡∏£‡∏±‡∏ö Path ‡∏à‡∏≤‡∏Å path_utils ‡∏ã‡∏∂‡πà‡∏á‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏ä‡∏µ‡πâ‡πÑ‡∏õ‡∏ó‡∏µ‡πà 'config/' ‡πÅ‡∏•‡πâ‡∏ß
            filepath = get_rubric_file_path(
                tenant=self.config.tenant,
                enabler=self.enabler
            )
        except Exception as e:
            # ‡∏î‡∏±‡∏Å‡∏à‡∏±‡∏ö Exception ‡∏´‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏î‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô path_utils
            self.logger.error(f"‚ùå FATAL: Error calling get_rubric_file_path: {e}")
            return {} 

        # 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if filepath is None or not os.path.exists(filepath):
            self.logger.error(f"‚ö†Ô∏è Rubric file not found at expected path: {filepath}")
            return {}

        # 3. ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå JSON
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
            self.logger.info(f"‚úÖ Rubric loaded successfully from: {filepath}")
            return data
        except json.JSONDecodeError:
            self.logger.error(f"‚ùå Error decoding Rubric JSON. File might be corrupted: {filepath}")
            return {}
        except Exception as e:
            self.logger.error(f"‚ùå Error loading Rubric file from {filepath}: {e}")
            return {}
    
    # -------------------- Helper Function for Map Processing --------------------
    def _get_level_order_value(self, level_str: str) -> int:
        """Converts Level string ('L1', 'L5') to an integer for comparison."""
        try:
            return int(level_str.upper().replace('L', ''))
        except:
            return 0

    def _clean_temp_entries(self, evidence_map: Dict[str, List[Any]]) -> Dict[str, List[Dict]]:
        """
        ‡∏Å‡∏£‡∏≠‡∏á TEMP-, HASH-, ‡πÅ‡∏•‡∏∞ Unknown ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å evidence map ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô 'str' object has no attribute 'get' ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ã‡πà‡∏≠‡∏°‡πÅ‡∏ã‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        """
        if not evidence_map or not isinstance(evidence_map, dict):
            return {}

        cleaned_map = {}
        total_removed = 0
        total_unknown_fixed = 0
        total_invalid_type = 0

        for key, entries in evidence_map.items():
            if not isinstance(entries, list):
                continue
                
            valid_entries = []
            for entry in entries:
                # üõ°Ô∏è Defense: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
                if not isinstance(entry, dict):
                    if isinstance(entry, str) and entry.strip():
                        # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏Å‡∏π‡πâ‡∏Ñ‡∏∑‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô string
                        entry = {"doc_id": entry, "filename": "Unknown", "relevance_score": 0.0}
                    else:
                        total_invalid_type += 1
                        continue

                doc_id = entry.get("doc_id")
                if doc_id is None:
                    total_removed += 1
                    continue
                
                doc_id_str = str(doc_id)

                # 1. ‡∏Å‡∏£‡∏≠‡∏á TEMP- ‡πÅ‡∏•‡∏∞ HASH-
                if doc_id_str.startswith("TEMP-") or doc_id_str.startswith("HASH-"):
                    total_removed += 1
                    continue

                # 2. ‡∏Å‡∏£‡∏≠‡∏á Unknown
                if not doc_id_str or doc_id_str.lower() == "unknown":
                    total_removed += 1
                    continue

                # 3. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Filename
                filename = str(entry.get("filename", "")).strip()
                if not filename or filename.lower() in ["unknown", "none", "unknown_file.pdf", "n/a"]:
                    short_id = doc_id_str[:8]
                    entry["filename"] = f"‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á_{short_id}.pdf"
                    total_unknown_fixed += 1
                else:
                    try:
                        entry["filename"] = os.path.basename(filename)
                    except:
                        entry["filename"] = filename

                valid_entries.append(entry)

            if valid_entries:
                cleaned_map[key] = valid_entries

        return cleaned_map

    
    def _clean_map_for_json(self, data: Union[Dict, List, Set, Any]) -> Union[Dict, List, Any]:
        """Recursively converts objects that cannot be serialized (like sets) into lists."""
        if isinstance(data, dict):
            return {k: self._clean_map_for_json(v) for k, v in data.items()}
        elif isinstance(data, list):
            return [self._clean_map_for_json(v) for v in data]
        elif isinstance(data, set):
            return [self._clean_map_for_json(v) for v in data]
        return data

    def _save_evidence_map(self, map_to_save: Optional[Dict[str, Any]] = None):
        """
        [IRONCLAD FINAL v2026.1.18 ‚Äî Ultra Safe Edition]
        - Load-Merge-Save Pattern (‡πÑ‡∏°‡πà overwrite ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≤‡∏î)
        - Atomic Write + FileLock + Tempfile
        - Backup (.bak) ‡∏Å‡πà‡∏≠‡∏ô save ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á
        - Validate + Clean ID ‡πÄ‡∏Ç‡πâ‡∏°‡∏á‡∏ß‡∏î ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ç‡∏¢‡∏∞
        - Log ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô (‡∏à‡∏≥‡∏ô‡∏ß‡∏ô merge/‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï)
        - Skip ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà‡∏à‡∏£‡∏¥‡∏á ‡πÜ
        """
        try:
            map_file_path = get_evidence_mapping_file_path(
                tenant=self.config.tenant,
                year=self.config.year,
                enabler=self.enabler
            )
        except Exception as e:
            self.logger.critical(f"[EVIDENCE] FATAL: ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Path ‡πÑ‡∏î‡πâ: {e}")
            raise

        lock_path = map_file_path + ".lock"
        tmp_path = None
        backup_path = map_file_path + ".bak"  # Backup ‡∏Å‡πà‡∏≠‡∏ô save

        self.logger.info(f"[EVIDENCE] Preparing atomic save ‚Üí {map_file_path}")

        try:
            os.makedirs(os.path.dirname(map_file_path), exist_ok=True)

            # Backup ‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏¥‡∏°‡∏Å‡πà‡∏≠‡∏ô (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å!)
            if os.path.exists(map_file_path):
                try:
                    shutil.copy2(map_file_path, backup_path)
                    self.logger.debug(f"[EVIDENCE] Backup created: {backup_path}")
                except Exception as be:
                    self.logger.warning(f"[EVIDENCE] Backup failed (non-critical): {be}")

            with FileLock(lock_path, timeout=60):
                # STEP 1: ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤‡∏à‡∏≤‡∏Å Disk (Base) ‚Äî ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡πÄ‡∏™‡∏°‡∏≠
                final_map = self._load_evidence_map(is_for_merge=True) or {}
                self.logger.debug(f"[EVIDENCE] Loaded existing map: {len(final_map)} keys")

                # STEP 2: ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà (Incoming)
                incoming = {}
                if map_to_save is not None:
                    # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á Payload {"evidence_map": ...} ‡πÅ‡∏•‡∏∞ Dict ‡∏ï‡∏£‡∏á ‡πÜ
                    if isinstance(map_to_save, dict) and "evidence_map" in map_to_save:
                        incoming = map_to_save["evidence_map"]
                    else:
                        incoming = map_to_save
                else:
                    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏™‡πà‡∏á‡∏°‡∏≤ ‚Üí ‡πÉ‡∏ä‡πâ‡∏à‡∏≤‡∏Å memory ‡∏Ç‡∏≠‡∏á Engine
                    incoming = getattr(self, 'evidence_map', {}) or {}

                # Skip ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà‡∏à‡∏£‡∏¥‡∏á ‡πÜ
                if not incoming:
                    self.logger.info("[EVIDENCE] No new data incoming. Skipping write.")
                    return

                # STEP 3: Merge + Validate + Clean
                merged_new = 0
                updated_existing = 0

                for key, new_entries in incoming.items():
                    if not isinstance(new_entries, list) or not new_entries:
                        continue

                    # ‡∏î‡∏∂‡∏á entries ‡πÄ‡∏î‡∏¥‡∏° (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ ‚Üí ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà)
                    current = final_map.setdefault(key, [])

                    # Index ‡πÄ‡∏î‡∏¥‡∏°‡∏î‡πâ‡∏ß‡∏¢ Clean ID
                    entry_index = {}
                    for e in current:
                        if not isinstance(e, dict):
                            continue
                        raw_id = e.get("chunk_uuid") or e.get("doc_id") or "N/A"
                        clean_id = str(raw_id).replace("-", "").lower()
                        if clean_id not in ["na", "n/a", "fallback", "none", ""]:
                            entry_index[clean_id] = e

                    # ‡∏ô‡∏≥‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏Ç‡πâ‡∏≤ Merge
                    for new_e in new_entries:
                        if not isinstance(new_e, dict):
                            continue

                        raw_new_id = new_e.get("chunk_uuid") or new_e.get("doc_id") or "N/A"
                        clean_new_id = str(raw_new_id).replace("-", "").lower()

                        # Skip ‡∏Ç‡∏¢‡∏∞
                        if clean_new_id in ["na", "n/a", "fallback", "none", ""]:
                            continue

                        new_score = new_e.get("relevance_score", 0.0)

                        if clean_new_id not in entry_index:
                            entry_index[clean_new_id] = new_e
                            merged_new += 1
                        else:
                            old_e = entry_index[clean_new_id]
                            old_score = old_e.get("relevance_score", 0.0)

                            # ‡∏£‡∏±‡∏Å‡∏©‡∏≤ metadata ‡πÄ‡∏î‡∏¥‡∏°‡∏ñ‡πâ‡∏≤‡∏Ç‡∏≤‡∏î
                            if "page" not in new_e or new_e["page"] in ["N/A", None]:
                                new_e["page"] = old_e.get("page")
                            if "page_label" not in new_e:
                                new_e["page_label"] = old_e.get("page_label")

                            if new_score >= old_score:
                                entry_index[clean_new_id] = new_e
                                updated_existing += 1

                    # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Å‡∏•‡∏±‡∏ö
                    final_map[key] = list(entry_index.values())

                # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£ merge ‡∏à‡∏£‡∏¥‡∏á ‡πÜ ‚Üí skip
                if merged_new == 0 and updated_existing == 0:
                    self.logger.info("[EVIDENCE] No unique new/updated entries. Skipping write.")
                    return

                # STEP 4: Clean + Sort
                final_map = self._clean_temp_entries(final_map)
                for key, entries in final_map.items():
                    entries.sort(key=lambda x: x.get("relevance_score", 0.0), reverse=True)

                # STEP 5: Atomic Write
                with tempfile.NamedTemporaryFile(
                    mode='w', delete=False, encoding="utf-8", dir=os.path.dirname(map_file_path)
                ) as tmp_file:
                    cleaned_data = self._clean_map_for_json(final_map)
                    json.dump(cleaned_data, tmp_file, indent=4, ensure_ascii=False)
                    tmp_path = tmp_file.name

                shutil.move(tmp_path, map_file_path)
                tmp_path = None

                total_keys = len(final_map)
                total_items = sum(len(v) for v in final_map.values())
                self.logger.info(
                    f"‚úÖ [EVIDENCE] SAVED SUCCESSFULLY! "
                    f"Keys: {total_keys} | Items: {total_items} | "
                    f"New: {merged_new} | Updated: {updated_existing}"
                )

        except Exception as e:
            self.logger.critical("[EVIDENCE] FATAL ERROR DURING ATOMIC SAVE")
            self.logger.exception(e)
            raise
        finally:
            # Cleanup
            if os.path.exists(lock_path):
                try:
                    os.unlink(lock_path)
                except:
                    pass
            if tmp_path and os.path.exists(tmp_path):
                try:
                    os.unlink(tmp_path)
                except:
                    pass

    def merge_evidence_mappings(self, results_list: List[Any]) -> Dict[str, List[Dict]]:
        """
        [ULTIMATE STABLE v2026.1.18 ‚Äî Key Mismatch Fix]
        - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Ñ‡∏µ‡∏¢‡πå 'evidence_sources' ‡∏à‡∏≤‡∏Å‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ï‡∏±‡∏ß‡πÉ‡∏´‡∏°‡πà
        - ‡∏ó‡∏≥ Deduplication ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ chunk_uuid/doc_id
        """
        merged_mapping = {}
        
        self.logger.info(f"üß¨ Starting to merge evidence mappings from {len(results_list)} levels...")

        for item in results_list:
            if not item: continue
            
            temp_map = {}
            
            # 1. ‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Mapping ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏ï‡∏≤‡∏°‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡πà‡∏≤‡∏á‡πÜ
            if isinstance(item, tuple) and len(item) == 2:
                temp_map = item[1]
            elif isinstance(item, dict):
                # [FIX] ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡πá‡∏Ñ 'evidence_sources' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å _run_single_assessment ‡πÑ‡∏î‡πâ
                if 'evidence_sources' in item:
                    level_key = f"{item.get('sub_id', 'Unknown')}_L{item.get('level', 0)}"
                    temp_map = {level_key: item['evidence_sources']}
                elif 'temp_map_for_level' in item:
                    level_key = f"{item.get('sub_id', 'Unknown')}_L{item.get('level', 0)}"
                    data = item.get('temp_map_for_level', [])
                    temp_map = {level_key: data} if isinstance(data, list) else {}
                elif 'evidence_mapping' in item:
                    temp_map = item['evidence_mapping']
                else:
                    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏µ‡∏¢‡πå‡∏û‡∏¥‡πÄ‡∏®‡∏© ‡πÉ‡∏´‡πâ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô dict ‡∏Ç‡∏≠‡∏á mapping ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
                    temp_map = item

            if not temp_map or not isinstance(temp_map, dict):
                continue

            # 2. ‡∏ß‡∏ô Loop ‡∏£‡∏ß‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ö‡∏Å‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏Å
            for level_key, evidence_list in temp_map.items():
                actual_list = []
                if isinstance(evidence_list, list):
                    actual_list = evidence_list
                elif isinstance(evidence_list, dict) and 'evidences' in evidence_list:
                    actual_list = evidence_list['evidences']
                else:
                    continue 
                
                if level_key not in merged_mapping:
                    merged_mapping[level_key] = []
                
                # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Set ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ã‡πâ‡∏≥ (Deduplication)
                existing_ids = set()
                for e in merged_mapping[level_key]:
                    eid = str(e.get('chunk_uuid') or e.get('doc_id') or "N/A").replace("-", "").lower()
                    existing_ids.add(eid)
                
                for new_ev in actual_list:
                    if not isinstance(new_ev, dict): continue
                    
                    raw_new_id = new_ev.get('chunk_uuid') or new_ev.get('doc_id') or "N/A"
                    clean_new_id = str(raw_new_id).replace("-", "").lower()

                    if clean_new_id in ["na", "n/a", "fallback", "none", "", "unknown"]:
                        continue

                    if clean_new_id not in existing_ids:
                        merged_mapping[level_key].append(new_ev)
                        existing_ids.add(clean_new_id)
        
        total_items = sum(len(v) for v in merged_mapping.values())
        self.logger.info(f"‚úÖ Merging completed. Levels: {len(merged_mapping)} | Total items: {total_items}")
        
        return merged_mapping

    def _load_evidence_map(self, is_for_merge: bool = False) -> Dict[str, List[Dict[str, Any]]]:
        """
        [REVISED v2026.1.16]
        - ‡πÄ‡∏û‡∏¥‡πà‡∏° cache ‡πÉ‡∏ô memory ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î I/O
        - Clean ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î‡∏°‡∏≤ (‡∏•‡∏ö fallback/na)
        """
        if hasattr(self, '_evidence_cache') and self._evidence_cache is not None:
            return deepcopy(self._evidence_cache)

        try:
            path = get_evidence_mapping_file_path(
                tenant=self.config.tenant,
                year=self.config.year,
                enabler=self.enabler
            )
        except Exception as e:
            self.logger.error(f"[EVIDENCE] FATAL: ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏î‡πâ: {e}")
            return {}

        if not os.path.exists(path):
            if not is_for_merge:
                self.logger.info("[EVIDENCE] No existing evidence map found ‚Äì starting fresh.")
            return {}

        try:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)

            # Clean ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î‡∏°‡∏≤
            for key in list(data.keys()):
                entries = data[key]
                cleaned = []
                for e in entries:
                    raw_id = e.get("chunk_uuid") or e.get("doc_id") or "N/A"
                    clean_id = str(raw_id).replace("-", "").lower()
                    if clean_id not in ["na", "n/a", "fallback", "none", ""]:
                        cleaned.append(e)
                data[key] = cleaned

            # Cache ‡πÉ‡∏ô memory
            self._evidence_cache = deepcopy(data)

            if not is_for_merge:
                total_items = sum(len(v) for v in data.values())
                self.logger.info(f"[EVIDENCE] Loaded evidence map: {len(data)} keys, {total_items} items from {path}")

            return data
        except Exception as e:
            self.logger.error(f"[EVIDENCE] Failed to load evidence map from {path}: {e}")
            return {}


    def _set_mock_handlers(self, mode: str):
        """Replaces real LLM/RAG functions with mock versions."""
        if mode == "control" or mode == "random":
            if hasattr(seam_mocking, 'evaluate_with_llm_CONTROLLED_MOCK'):
                self.llm_evaluator = seam_mocking.evaluate_with_llm_CONTROLLED_MOCK
            if hasattr(seam_mocking, 'retrieve_context_with_filter_MOCK'):
                self.rag_retriever = seam_mocking.retrieve_context_with_filter_MOCK
            if hasattr(seam_mocking, 'create_structured_action_plan_MOCK'):
                self.action_plan_generator = seam_mocking.create_structured_action_plan_MOCK
            if hasattr(seam_mocking, 'set_mock_control_mode'):
                seam_mocking.set_mock_control_mode(mode == "control") 

        logger.warning(f"Engine is running in MOCK mode: {mode}")


    # -------------------- Statement Preparation & Filtering Helpers --------------------
    def _flatten_rubric_to_statements(self) -> List[Dict[str, Any]]:
        """
        Transforms the hierarchical rubric structure loaded in self.rubric
        into a flat list of statements ready for assessment.
        """
        if not self.rubric:
            self.logger.warning("Cannot flatten rubric: self.rubric is empty.")
            return []
            
        data = deepcopy(self.rubric)
        extracted_list = []
        
        if not isinstance(data, dict):
             self.logger.error("Rubric data structure is invalid (expected dict of criteria).")
             return []
             
        # for criteria_id, criteria_data in data.items():
        for criteria_id, criteria_data in data.get('criteria', {}).items():
            # üéØ FIX 1: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Criteria Data
            if not isinstance(criteria_data, dict):
                self.logger.warning(f"Skipping malformed criteria entry: {criteria_id} (not a dict).")
                continue
                
            sub_criteria_map = criteria_data.get('subcriteria', {})
            criteria_name = criteria_data.get('name')
            
            # üéØ FIX 2: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Sub-criteria Map
            if not isinstance(sub_criteria_map, dict):
                 self.logger.warning(f"Skipping criteria {criteria_id}: 'subcriteria' is not a dictionary.")
                 continue

            for sub_id, sub_data in sub_criteria_map.items():
                
                # üéØ FIX 3: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ sub_data ‡πÄ‡∏õ‡πá‡∏ô Dictionary ‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô TypeError)
                if not isinstance(sub_data, dict):
                    self.logger.warning(
                        f"Skipping malformed sub-criteria entry: {criteria_id}.{sub_id} "
                        f"is not a dictionary (found type: {type(sub_data).__name__})."
                    )
                    continue
                
                # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤ Metadata ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
                sub_data['criteria_id'] = criteria_id
                sub_data['criteria_name'] = criteria_name
                sub_data['sub_id'] = sub_id 
                sub_data['sub_criteria_name'] = sub_data.get('name', criteria_name + ' sub')
                if 'weight' not in sub_data:
                    sub_data['weight'] = criteria_data.get('weight', 0)
                extracted_list.append(sub_data)

        # Re-check and re-sort levels
        final_list = []
        for sub_criteria in extracted_list: 
            
            # üéØ FIX 4: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á Level ‡∏à‡∏≤‡∏Å Dict ‡∏ó‡∏µ‡πà‡∏°‡∏µ Key ‡πÄ‡∏õ‡πá‡∏ô String ‡πÄ‡∏õ‡πá‡∏ô List
            if "levels" in sub_criteria and isinstance(sub_criteria["levels"], dict):
                levels_list = []
                for level_str, statement in sub_criteria["levels"].items():
                    try:
                        # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÅ‡∏õ‡∏•‡∏á Level Key ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô Integer
                        level_int = int(level_str)
                        if isinstance(statement, str):
                            levels_list.append({"level": level_int, "statement": statement})
                        else:
                            self.logger.warning(f"Level {level_str} statement in {sub_criteria.get('sub_id')} is not a string.")
                    except ValueError:
                        self.logger.error(f"Invalid level key '{level_str}' found in {sub_criteria.get('sub_id', 'UNKNOWN_ID')}. Skipping.")
                        continue
                        
                sub_criteria["levels"] = levels_list
            
            # ‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏£‡∏∞‡∏î‡∏±‡∏ö
            if "levels" in sub_criteria and isinstance(sub_criteria["levels"], list):
                 sub_criteria["levels"].sort(key=lambda x: x.get("level", 0))
                 final_list.append(sub_criteria)
            else:
                 self.logger.warning(f"Sub-criteria {sub_criteria.get('sub_id', 'UNKNOWN_ID')} missing 'levels' list.")


        return final_list

    
    # -------------------- Evidence Classification Helper (Robust 2026) --------------------
    def _get_mapped_uuids_and_priority_chunks(
        self,
        sub_id: str,
        level: int,
        statement_text: str = "",
        level_constraint: Optional[Any] = None, # üü¢ ‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô Optional ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Error Missing Argument
        vectorstore_manager: Optional['VectorStoreManager'] = None,
        evidence_map: Optional[Dict] = None 
    ) -> Tuple[List[str], List[Dict]]:
        """
        [DYNAMIC CONTINUITY v2026.6.2] - ROBUST SIGNATURE
        ----------------------------------------------
        - Fix: ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÅ‡∏ö‡∏ö‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Error 'missing 1 required positional argument'
        - Logic: ‡∏î‡∏∂‡∏á Baseline ‡∏à‡∏≤‡∏Å Memory/Map ‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á (Inheritance)
        """
        from copy import deepcopy
        priority_chunks = []
        mapped_stable_ids = []

        # 1. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ä‡πâ Evidence Map (‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: 1. Argument ‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏°‡∏≤ -> 2. Class Attribute -> 3. Empty Dict)
        target_map = evidence_map if evidence_map is not None else getattr(self, 'evidence_map', {})

        # 2. üß† [AUTO-HISTORY & INHERITANCE]
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡∏ú‡πà‡∏≤‡∏ô‡πÉ‡∏ô Level ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏≥‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡πÉ‡∏ô Level ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
        for key, evidences in target_map.items():
            if key.startswith(f"{sub_id}.L") and isinstance(evidences, list):
                try:
                    lvl_in_key = int(key.split(".L")[-1])
                    # ‡∏Å‡∏é Inheritance: ‡πÉ‡∏ä‡πâ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏à‡∏≤‡∏Å Level ‡∏ó‡∏µ‡πà <= ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
                    if lvl_in_key <= level:
                        history_items = deepcopy(evidences)
                        for item in history_items:
                            item["is_baseline"] = True
                            # ‡∏ö‡∏π‡∏™‡∏ï‡πå‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡∏≠‡∏á RAG
                            item["rerank_score"] = max(item.get("rerank_score", 0.0), 0.85)
                        priority_chunks.extend(history_items)
                except (ValueError, IndexError):
                    continue

        # 3. üîç [SEMANTIC HINTING] ‡∏Å‡∏£‡∏ì‡∏µ L1 ‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡πÉ‡∏ô Map
        if not priority_chunks and level == 1:
            rule_config = getattr(self, 'contextual_rules_map', {}).get(sub_id, {}).get(str(level), {})
            hints = rule_config.get("plan_keywords", [])[:2]
            if hints and vectorstore_manager:
                self.logger.info(f"üîé L1 Discovery: Searching using hints: {hints}")
                try:
                    discovery_result = vectorstore_manager.quick_search(
                        query=f"{sub_id} {' '.join(hints)}",
                        top_k=5
                    )
                    for chunk in discovery_result:
                        chunk["rerank_score"] = 0.85 # ‡∏ö‡∏π‡∏™‡∏ï‡πå‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠‡∏à‡∏≤‡∏Å Keyword Rule
                        priority_chunks.append(chunk)
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Quick search failed: {e}")

        if not priority_chunks:
            return [], []

        # 4. üíß [ROBUST HYDRATION] ‡πÄ‡∏ï‡∏¥‡∏°‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ Full Text ‡∏à‡∏≤‡∏Å Vector DB
        try:
            priority_chunks = self._robust_hydrate_documents_for_priority_chunks(
                chunks_to_hydrate=priority_chunks,
                vsm=vectorstore_manager
            )
        except Exception as e:
            self.logger.error(f"‚ùå Hydration failed in priority module: {e}")

        # 5. üéØ [ID SYNC] ‡∏™‡∏Å‡∏±‡∏î UUID ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ Filter ‡∏Ç‡∏≠‡∏á Main Retriever
        seen_ids = set()
        for chunk in priority_chunks:
            sid = chunk.get("stable_doc_uuid") or chunk.get("doc_id")
            if sid and isinstance(sid, str):
                if sid not in seen_ids and len(sid) >= 32:
                    mapped_stable_ids.append(sid)
                    seen_ids.add(sid)

        self.logger.info(f"‚úÖ Continuity Ready: {len(priority_chunks)} priority chunks | Sub:{sub_id} L{level}")
        return mapped_stable_ids, priority_chunks

    def _save_level_evidences_and_calculate_strength(
        self,
        level_temp_map: List[Dict[str, Any]],
        sub_id: str,
        level: int,
        llm_result: Dict[str, Any],
        highest_rerank_score: float = 0.0
    ) -> float:
        """ 
        [IRONCLAD REVISE v2026.01.18] - ‡∏£‡∏∞‡∏ö‡∏ö‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏±‡πà‡∏ô (Strength)
        - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏∞‡∏ö‡∏ö Deduplication ‡∏î‡πâ‡∏ß‡∏¢ unique_key (Doc UUID + Chunk UUID)
        - ‡∏£‡∏∞‡∏ö‡∏ö Normalize PDCA Tag ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ (Plan -> P, Detail -> D)
        - ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Strength Score ‡πÇ‡∏î‡∏¢‡πÉ‡∏´‡πâ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å Rerank Score (60%) ‡πÅ‡∏•‡∏∞ PDCA Coverage (40%)
        - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ Retagging ‡∏Å‡∏£‡∏ì‡∏µ Tag ‡πÑ‡∏°‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô (Semantic Fallback)
        """
        import hashlib
        import os
        from datetime import datetime
        from copy import deepcopy

        map_key = f"{sub_id}.L{level}"
        new_evidence_list: List[Dict[str, Any]] = []
        seen_ids = set()

        self.logger.info(f"üíæ [EVI SAVE] Starting persist for {map_key} | Incoming: {len(level_temp_map)}")

        # üö© Configuration
        STANDARD_TAGS = {"P", "D", "C", "A"}
        PASS_STATUS = "PASS" if llm_result.get("is_passed", False) else "FAIL"

        for chunk in level_temp_map:
            if not chunk: continue

            # 1. ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á Dictionary ‡πÅ‡∏•‡∏∞ LangChain Document)
            meta = chunk.get("metadata", {}) if isinstance(chunk, dict) else getattr(chunk, "metadata", {})
            text = chunk.get("text") or chunk.get("page_content") if isinstance(chunk, dict) else getattr(chunk, "page_content", "")
            
            if not text or not str(text).strip():
                continue

            # 2. Stable ID Generation (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Duplicate Chunks)
            # ‡πÉ‡∏ä‡πâ SHA-256 ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ ID ‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡∏ï‡∏±‡∏ß ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô
            c_uuid = str(chunk.get("chunk_uuid") or meta.get("chunk_uuid") or hashlib.sha256(text.encode()).hexdigest()[:16])
            d_uuid = str(chunk.get("stable_doc_uuid") or meta.get("stable_doc_uuid") or "doc-unknown")
            unique_key = f"{d_uuid}:{c_uuid}"
            
            if unique_key in seen_ids:
                continue
            seen_ids.add(unique_key)

            # 3. ‚ú® PDCA TAG NORMALIZATION & GUARD
            # ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≥‡πÄ‡∏ï‡πá‡∏°‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡πà‡∏≤‡πÄ‡∏™‡∏£‡∏¥‡∏°‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô P, D, C, A ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô
            raw_tag = chunk.get("pdca_tag") or meta.get("pdca_tag") or "Other"
            
            if isinstance(raw_tag, str):
                u_tag = raw_tag.strip().upper()
                if u_tag.startswith("P") or "PLAN" in u_tag: pdca_tag = "P"
                elif u_tag.startswith("D") or "DETAIL" in u_tag or "SUPPORT" in u_tag: pdca_tag = "D"
                elif u_tag.startswith("C") or "CHECK" in u_tag: pdca_tag = "C"
                elif u_tag.startswith("A") or "ACT" in u_tag: pdca_tag = "A"
                else: pdca_tag = "Other"
            else:
                pdca_tag = "Other"

            # Semantic Fallback: ‡∏ñ‡πâ‡∏≤ Tag ‡∏¢‡∏±‡∏á‡πÄ‡∏õ‡πá‡∏ô Other ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ Logic ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
            if pdca_tag == "Other" and hasattr(self, '_get_semantic_tag'):
                pdca_tag = self._get_semantic_tag(text, sub_id, level)

            # 4. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Evidence Entry)
            source_raw = meta.get("source") or meta.get("source_filename") or "Unknown"
            entry = {
                "sub_id": sub_id,
                "level": level,
                "relevance_score": float(chunk.get("rerank_score") or chunk.get("score") or 0.5),
                "doc_id": d_uuid,
                "stable_doc_uuid": d_uuid,
                "chunk_uuid": c_uuid,
                "source": source_raw,
                "source_filename": os.path.basename(str(source_raw)),
                "page": str(meta.get("page_label") or meta.get("page") or "N/A"),
                "pdca_tag": pdca_tag,
                "text_preview": str(text)[:300].replace("\n", " ") + "...",
                "status": PASS_STATUS,
                "timestamp": datetime.now().isoformat(),
            }
            new_evidence_list.append(entry)

        # 5. ‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏Ç‡πá‡∏á‡πÅ‡∏Å‡∏£‡πà‡∏á (Strength)
        if new_evidence_list:
            # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï Memory Map (‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏™‡∏∞‡∏™‡∏°)
            if not hasattr(self, 'evidence_map'): self.evidence_map = {}
            self.evidence_map.setdefault(map_key, []).extend(deepcopy(new_evidence_list))
            
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏•‡∏á‡πÉ‡∏ô Map (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Level ‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ)
            if not hasattr(self, 'assessment_results_map'): self.assessment_results_map = {}
            self.assessment_results_map[map_key] = {
                "is_passed": llm_result.get("is_passed", False),
                "score": llm_result.get("score", 0.0),
                "strength": 0.0 # ‡∏à‡∏∞‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á
            }

            # üìä STRENGTH CALCULATION LOGIC
            # 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏° (Coverage): ‡∏û‡∏ö‡∏Å‡∏µ‡πà‡∏´‡∏°‡∏ß‡∏î‡πÉ‡∏ô P, D, C, A
            found_tags = {ev['pdca_tag'] for ev in new_evidence_list if ev['pdca_tag'] in STANDARD_TAGS}
            coverage_score = len(found_tags) / 4.0  # (0.0 - 1.0)
            
            # 2. ‡∏ú‡∏™‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô: Rerank Max (‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏à‡∏≤‡∏Å Vector) 60% + PDCA Coverage (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô) 40%
            final_strength = round((float(highest_rerank_score) * 0.6) + (coverage_score * 0.4), 2)
            
            # ‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏Ç‡πá‡∏á‡πÅ‡∏Å‡∏£‡πà‡∏á‡∏Å‡∏•‡∏±‡∏ö‡∏•‡∏á‡πÉ‡∏ô Result Map
            self.assessment_results_map[map_key]["strength"] = final_strength

            # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏ó‡∏≤‡∏á Log
            counts = {t: sum(1 for e in new_evidence_list if e['pdca_tag'] == t) for t in (list(STANDARD_TAGS) + ["Other"])}
            self.logger.info(
                f"‚úÖ [SAVED] {map_key}: {len(new_evidence_list)} items | "
                f"P:{counts['P']} D:{counts['D']} C:{counts['C']} A:{counts['A']} | "
                f"Final Strength: {final_strength:.2f}"
            )
            return final_strength
            
        return 0.0
    
    def get_actual_score(self, ev: Any) -> float:
        """
        [v2026.1 - ROBUST SCORING EXTRACTOR]
        ‡∏î‡∏∂‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô ‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö Dict ‡∏´‡∏£‡∏∑‡∏≠ Object
        ‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô 0.0 ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ã‡πà‡∏≠‡∏ô‡∏Ñ‡∏µ‡∏¢‡πå‡πÉ‡∏ô Metadata
        """
        if not ev:
            return 0.0

        # 1. ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ñ‡∏µ‡∏¢‡πå‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏ö‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö (‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç)
        score_keys = ["rerank_score", "score", "relevance_score"]
        
        # 2. ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö Top-level ‡∏Å‡πà‡∏≠‡∏ô
        for key in score_keys:
            # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á dict.get() ‡πÅ‡∏•‡∏∞ getattr() ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Document Object
            val = ev.get(key) if isinstance(ev, dict) else getattr(ev, key, None)
            if val is not None:
                try:
                    return float(val)
                except (ValueError, TypeError):
                    continue

        # 3. Fallback: ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏†‡∏≤‡∏¢‡πÉ‡∏ô Metadata
        meta = ev.get("metadata", {}) if isinstance(ev, dict) else getattr(ev, "metadata", {})
        if isinstance(meta, dict):
            for key in score_keys:
                val = meta.get(key)
                if val is not None:
                    try:
                        return float(val)
                    except (ValueError, TypeError):
                        continue

        return 0.0
    
    def _calculate_evidence_strength_cap(
        self,
        top_evidences: List[Any],
        level: int,
        highest_rerank_score: Optional[float] = None
    ) -> Dict[str, Any]:
        """
        [PROTECTED v2026.1.17 - REVISED]
        - ‡πÉ‡∏ä‡πâ get_actual_score ‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô
        - ‡πÅ‡∏¢‡∏Å Logic ‡∏Å‡∏≤‡∏£‡∏´‡∏≤ '‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•' ‡πÉ‡∏´‡πâ‡∏â‡∏•‡∏≤‡∏î‡∏Ç‡∏∂‡πâ‡∏ô
        - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏∞‡∏ö‡∏ö Logging ‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ Audit
        """
        try:
            # ‚öôÔ∏è Load Configuration
            threshold = getattr(self, "RERANK_THRESHOLD", 0.35)
            cap_value = getattr(self, "MAX_EVI_STR_CAP", 5.0)
            
            # 1. ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏°‡∏≤ (Baseline)
            max_score_found = 0.0
            try:
                if highest_rerank_score is not None:
                    max_score_found = float(highest_rerank_score)
            except (ValueError, TypeError):
                max_score_found = 0.0

            max_score_source = "Adaptive_RAG_Loop"
            
            if not isinstance(top_evidences, list):
                top_evidences = []

            # 2. Iterate ‡∏´‡∏≤‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ
            for idx, doc in enumerate(top_evidences, 1):
                # ‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡∏î‡∏∂‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏ö
                current_score = self.get_actual_score(doc)

                # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏à‡∏≠‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤ ‡πÉ‡∏´‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ß‡πâ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏≤
                if current_score > max_score_found:
                    max_score_found = current_score
                    
                    # --- ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö Robust ---
                    meta = doc.get("metadata", {}) if isinstance(doc, dict) else getattr(doc, "metadata", {})
                    if not isinstance(meta, dict): meta = {}
                    
                    max_score_source = (
                        meta.get("source_filename") or 
                        meta.get("file_name") or 
                        meta.get("source") or 
                        f"Doc_{idx}"
                    )

            # 3. Decision Logic (Gated Check)
            # ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏¢‡∏±‡∏á‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤ Threshold (0.35) -> ‡∏™‡∏±‡πà‡∏á Capped
            is_capped = max_score_found < threshold
            # ‡∏Ñ‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤ Prompt: 5.0 (‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô) ‡∏´‡∏£‡∏∑‡∏≠ 10.0 (‡∏°‡∏µ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏ú‡πà‡∏≤‡∏ô)
            max_evi_str_for_prompt = float(cap_value) if is_capped else 10.0

            # üìä Internal Audit Log
            status_icon = "üö® [CAPPED]" if is_capped else "‚úÖ [FULL-STRENGTH]"
            self.logger.info(
                f"{status_icon} L{level} | Best Score: {max_score_found:.4f} "
                f"from: '{os.path.basename(str(max_score_source))}' | Threshold: {threshold}"
            )

            return {
                "is_capped": bool(is_capped),
                "max_evi_str_for_prompt": float(max_evi_str_for_prompt),
                "top_score": round(float(max_score_found), 4),
                "max_score_source": str(max_score_source),
                "threshold_used": threshold
            }

        except Exception as e:
            self.logger.error(f"‚ùå [CRITICAL-CAP-ERROR] {e}", exc_info=True)
            # ‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡πÑ‡∏ß‡πâ‡∏Å‡πà‡∏≠‡∏ô: ‡∏ñ‡πâ‡∏≤ Error ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á Cap ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ LLM ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏î‡πâ
            return {
                "is_capped": False, 
                "max_evi_str_for_prompt": 10.0, 
                "top_score": 0.0, 
                "max_score_source": "Fallback-Error"
            }
       
    
    def _extract_strategic_gaps(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        [HELPER] ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏´‡∏≤‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á (Gaps) ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á‡∏Å‡∏±‡∏ö‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢
        ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡πÄ‡∏™‡∏ô‡∏≠ Coaching Insight ‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°
        """
        gaps = []
        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á‡∏Ç‡∏≠‡∏á Gap (‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ô‡πâ‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡∏Å‡πà‡∏≠‡∏ô)
        sorted_results = sorted(results, key=lambda x: x.get('score', 0.0))
        
        for res in sorted_results:
            sub_id = res.get('sub_id', 'Unknown')
            current_score = res.get('score', 0.0)
            passed = res.get('is_passed', False)
            
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô ‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤ 0.8 ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô Gap
            if not passed or current_score < 0.8:
                gap_info = {
                    "sub_id": sub_id,
                    "level": res.get('level'),
                    "current_score": current_score,
                    "impact": "High" if current_score < 0.5 else "Medium",
                    "reason": res.get('reason', '‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡∏µ‡πâ'),
                    "suggestion": res.get('coaching_insight', '‡∏Ñ‡∏ß‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏Ç‡∏≤‡∏î‡∏´‡∏≤‡∏¢‡∏ï‡∏≤‡∏° PDCA')
                }
                gaps.append(gap_info)
        
        # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ Gap ‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î 5 ‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å
        return gaps[:5]

    def calculate_audit_confidence(
        self,
        matched_chunks: List[Any],
        sub_id: str = "unknown",  # ‡πÄ‡∏û‡∏¥‡πà‡∏° argument ‡∏ô‡∏µ‡πâ (optional)
        level: int = 1           # ‡πÄ‡∏û‡∏¥‡πà‡∏° level ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÉ‡∏ô fallback
    ) -> Dict[str, Any]:
        """
        [ULTIMATE AUDIT CONFIDENCE v2026.3.4 ‚Äì Final Production Stable]
        - ‡πÅ‡∏Å‡πâ NameError ‡πÇ‡∏î‡∏¢‡∏£‡∏±‡∏ö sub_id ‡πÄ‡∏õ‡πá‡∏ô argument (fallback "unknown")
        - PDCA Detection ‡πÅ‡∏Ç‡πá‡∏á‡πÅ‡∏£‡∏á‡∏™‡∏∏‡∏î (tag + metadata + fallback text + keywords ‡∏à‡∏≤‡∏Å rules)
        - Decision Matrix ‡πÄ‡∏Ç‡πâ‡∏°‡∏á‡∏ß‡∏î‡∏Ç‡∏∂‡πâ‡∏ô (MEDIUM ‡∏ï‡πâ‡∏≠‡∏á coverage ‚â• 0.5 + independence ‚â• 5)
        - Recency Bonus fallback ‡∏à‡∏≤‡∏Å filename
        - Guard ‡∏Ñ‡∏£‡∏ö‡∏ó‡∏∏‡∏Å‡∏à‡∏∏‡∏î + Log debug ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
        """
        if not matched_chunks:
            return {
                "level": "NONE",
                "reason": "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö",
                "source_count": 0,
                "coverage_ratio": 0.0,
                "traceability_score": 0.0,
                "recency_bonus": 0.0,
                "valid_chunks_count": 0,
                "pdca_found": []
            }

        # 0. Quality Gate
        valid_chunks = [doc for doc in matched_chunks if self.get_actual_score(doc) >= 0.40]
        valid_count = len(valid_chunks)

        if valid_count == 0:
            return {
                "level": "LOW",
                "reason": "‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏°‡∏µ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô relevance ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤ 0.40",
                "source_count": 0,
                "coverage_ratio": 0.0,
                "traceability_score": 0.0,
                "recency_bonus": 0.0,
                "valid_chunks_count": 0,
                "pdca_found": []
            }

        # 1. Independence (unique sources - robust)
        unique_sources = set()
        for doc in valid_chunks:
            meta = getattr(doc, 'metadata', {}) if hasattr(doc, 'metadata') else doc.get('metadata', {})
            src_keys = ['source_filename', 'filename', 'file_name', 'source', 'file_path']
            src = next((meta.get(k) for k in src_keys if meta.get(k)), None)
            if src:
                unique_sources.add(os.path.basename(str(src).strip()))

        independence_score = len(unique_sources)

        # 2. PDCA Coverage (enhanced multi-layer detection)
        pdca_map = {"P": False, "D": False, "C": False, "A": False}
        
        # ‡∏î‡∏∂‡∏á keywords ‡∏à‡∏≤‡∏Å rules ‡πÄ‡∏û‡∏∑‡πà‡∏≠ fallback (‡πÉ‡∏ä‡πâ sub_id ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ö‡∏°‡∏≤)
        cum_rules = self.get_cumulative_rules(sub_id, level) if hasattr(self, 'get_cumulative_rules') else {}
        do_kws = [k.lower() for k in cum_rules.get('do_keywords', [])]
        check_kws = [k.lower() for k in cum_rules.get('check_keywords', [])]
        plan_kws = [k.lower() for k in cum_rules.get('plan_keywords', [])]
        act_kws = [k.lower() for k in cum_rules.get('act_keywords', [])]

        for doc in valid_chunks:
            meta = getattr(doc, 'metadata', {}) if hasattr(doc, 'metadata') else doc.get('metadata', {})
            tag = (
                getattr(doc, 'pdca_tag', None) or
                meta.get('pdca_tag') or
                meta.get('tag') or
                ""  # fallback ‡∏ß‡πà‡∏≤‡∏á
            )
            tag = str(tag).strip().upper()

            if tag in pdca_map:
                pdca_map[tag] = True
            else:
                # Fallback 1: Text keyword detection
                text = (doc.get('text') or doc.get('page_content') or '').lower()
                if any(k in text for k in plan_kws):
                    pdca_map["P"] = True
                if any(k in text for k in do_kws):
                    pdca_map["D"] = True
                if any(k in text for k in check_kws):
                    pdca_map["C"] = True
                if any(k in text for k in act_kws):
                    pdca_map["A"] = True

        found_tags = [k for k, v in pdca_map.items() if v]
        coverage_ratio = len(found_tags) / 4.0

        # Debug PDCA detection
        self.logger.info(f"[PDCA DETECTION DEBUG] {sub_id} L{level} | "
                         f"Detected tags: {found_tags} | Coverage: {coverage_ratio:.2f} | "
                         f"Total chunks checked: {valid_count} | "
                         f"Plan kws sample: {plan_kws[:5] if plan_kws else 'N/A'}...")

        # 3. Traceability
        traceable_count = 0
        for doc in valid_chunks:
            meta = getattr(doc, 'metadata', {}) if hasattr(doc, 'metadata') else doc.get('metadata', {})
            page_keys = ['page_label', 'page', 'page_number', 'page_idx']
            has_page = any(meta.get(k) is not None for k in page_keys)
            has_file = any(meta.get(k) for k in ['source_filename', 'filename', 'file_name', 'source'])
            if has_page and has_file:
                traceable_count += 1

        traceability_score = traceable_count / valid_count if valid_count > 0 else 0.0

        # 4. Recency Bonus (enhanced fallback from filename)
        recency_bonus = 0.0
        current_year = 2568
        recent_count = 0
        for doc in valid_chunks:
            meta = getattr(doc, 'metadata', {}) if hasattr(doc, 'metadata') else doc.get('metadata', {})
            year_str = meta.get('year') or meta.get('doc_year')
            if not year_str:
                # Fallback ‡∏à‡∏≤‡∏Å filename (‡πÄ‡∏ä‡πà‡∏ô ‡∏õ‡∏µ 2567 ‡πÉ‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå)
                filename = str(meta.get('source_filename') or meta.get('source') or "")
                year_match = re.search(r'(25[67]\d)', filename)
                if year_match:
                    year_str = year_match.group(1)
            if year_str and str(year_str).isdigit():
                doc_year = int(year_str)
                if doc_year >= current_year - 2:
                    recent_count += 1
        if valid_count > 0:
            recency_bonus = round(recent_count / valid_count, 3)

        # 5. Decision Matrix (‡πÄ‡∏Ç‡πâ‡∏°‡∏á‡∏ß‡∏î‡∏Ç‡∏∂‡πâ‡∏ô + ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡πÉ‡∏´‡∏°‡πà)
        if independence_score <= 1 or coverage_ratio <= 0.25:
            level = "LOW"
            reason = "‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏±‡πà‡∏ô‡∏ï‡πà‡∏≥‡∏°‡∏≤‡∏Å: ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡∏≤‡∏î‡∏°‡∏¥‡∏ï‡∏¥ PDCA ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 3 ‡∏î‡πâ‡∏≤‡∏ô"
        elif independence_score <= 4 or coverage_ratio < 0.50:
            level = "LOW"
            reason = "‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏±‡πà‡∏ô‡∏ï‡πà‡∏≥: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏° PDCA ‡πÑ‡∏°‡πà‡∏ñ‡∏∂‡∏á 50%"
        elif independence_score <= 7 or coverage_ratio < 0.75:
            level = "MEDIUM"
            reason = "‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏±‡πà‡∏ô‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏°‡∏µ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå (‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ 2+ ‡πÄ‡∏ü‡∏™ PDCA)"
        else:
            level = "HIGH"
            reason = "‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏±‡πà‡∏ô‡∏™‡∏π‡∏á: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏£‡∏ö‡∏ß‡∏á‡∏à‡∏£ PDCA ‡∏à‡∏≤‡∏Å‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢"

        # Penalty System
        if traceability_score < 0.60:
            if level == "HIGH":
                level = "MEDIUM"
                reason += " (‡∏Å‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô)"
            elif level == "MEDIUM":
                level = "LOW"
                reason += " (‡∏Å‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ï‡πà‡∏≥‡∏°‡∏≤‡∏Å)"

        return {
            "level": level,
            "reason": reason,
            "source_count": independence_score,
            "coverage_ratio": round(coverage_ratio, 3),
            "traceability_score": round(traceability_score, 3),
            "recency_bonus": round(recency_bonus, 3),
            "valid_chunks_count": valid_count,
            "pdca_found": found_tags
        }

    def _get_level_constraint_prompt(self, sub_id: str, level: int, req_phases: list = None, spec_rule: str = None) -> str:
        """
        [ADAPTIVE AUDIT GUIDELINE v2026.1.19 - Concise & Stronger]
        - ‡πÄ‡∏ô‡πâ‡∏ô PDCA ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô + Substance over Form
        - ‡πÉ‡∏ä‡πâ fallback ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ req_phases
        - ‡∏™‡∏±‡πâ‡∏ô‡πÅ‡∏ï‡πà‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LLM
        """
        required_phases = req_phases or self.get_rule_content(sub_id, level, "require_phase") or []
        specific_rule = spec_rule or self.get_rule_content(sub_id, level, "specific_contextual_rule") or ""

        phase_map = {
            "P": "Plan - ‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡πÅ‡∏•‡∏∞‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢",
            "D": "Do - ‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡πÅ‡∏•‡∏∞‡∏Ç‡∏±‡∏ö‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏à‡∏£‡∏¥‡∏á",
            "C": "Check - ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏° ‡∏ß‡∏±‡∏î‡∏ú‡∏•",
            "A": "Act - ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á ‡∏û‡∏±‡∏í‡∏ô‡∏≤ ‡∏¢‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö"
        }
        req_str = ", ".join(phase_map.get(p, p) for p in required_phases) if required_phases else "‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô"

        lines = [
            f"\n### ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô {sub_id} Level {level} ###",
            f"‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏Å: {MATURITY_LEVEL_GOALS.get(level, '‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°')}",
            f"‡∏°‡∏¥‡∏ï‡∏¥ PDCA ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Ñ‡∏£‡∏ö: {req_str}",
            f"‡∏Å‡∏é‡πÄ‡∏â‡∏û‡∏≤‡∏∞: {specific_rule}" if specific_rule else "",
            "\n‡∏Å‡∏é‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç (‡∏ï‡πâ‡∏≠‡∏á‡∏¢‡∏∂‡∏î‡∏ñ‡∏∑‡∏≠):",
            "- Substance over Form: ‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Å‡∏±‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏ó‡∏≥‡∏à‡∏£‡∏¥‡∏á‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡∏Ñ‡∏≥‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á",
            "- Positive First: ‡πÄ‡∏ô‡πâ‡∏ô‡∏´‡∏≤‡∏à‡∏∏‡∏î‡πÅ‡∏Ç‡πá‡∏á‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡πà‡∏≠‡∏ô ‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏∂‡∏á‡∏£‡∏∞‡∏ö‡∏∏‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á",
            "- Coaching Mindset: ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏∏‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏ß‡πà‡∏≤‡∏Ç‡∏≤‡∏î‡∏≠‡∏∞‡πÑ‡∏£ + ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á",
            "- Continuity: ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡πà‡∏≠‡∏¢‡∏≠‡∏î‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤ (‡∏ñ‡πâ‡∏≤‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÅ‡∏ô‡πà‡∏ô ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏∏)"
        ]

        return "\n".join(filter(None, lines))

    def _calculate_weighted_score(self, highest_full_level: int, weight: float) -> float:
        """
        [SAFE WEIGHTED SCORE v2026.1.18]
        """
        MAX_LEVEL = getattr(self, 'MAX_LEVEL', 5)  # ‡πÉ‡∏ä‡πâ attribute ‡∏Å‡πà‡∏≠‡∏ô fallback
        try:
            from config.global_vars import MAX_LEVEL as GLOBAL_MAX
            MAX_LEVEL = GLOBAL_MAX
        except:
            self.logger.debug("Using fallback MAX_LEVEL=5")

        level = max(0, min(float(highest_full_level), float(MAX_LEVEL)))
        if MAX_LEVEL <= 0:
            return 0.0

        return round((level / MAX_LEVEL) * float(weight), 4)

    def _export_results(self, results_data: Any, sub_criteria_id: str, **kwargs) -> str:
        """
        [ULTIMATE EXPORTER v2026.EXPORT.3 - FIXED & FULL REPORT]
        - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç AttributeError: 'list' object has no attribute 'get'
        - ‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå PDCA ‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö (L1-L5) ‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà Report ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
        - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô (Sanity Check) ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÑ‡∏ü‡∏•‡πå
        """
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            record_id = kwargs.get("record_id", getattr(self, "current_record_id", f"auto_{timestamp}"))
            tenant = getattr(self.config, 'tenant', 'unknown')
            year = getattr(self.config, 'year', 'unknown')
            enabler = getattr(self, 'enabler', 'unknown').upper()

            # 1. ‡∏à‡∏±‡∏î‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Input (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á List ‡πÅ‡∏•‡∏∞ Dict)
            if results_data is None:
                results_data = self.final_subcriteria_results if hasattr(self, 'final_subcriteria_results') else []
            
            if isinstance(results_data, dict):
                results_data = [results_data]
            
            if not results_data:
                self.logger.warning(f"‚ö†Ô∏è [EXPORT] No results found for {sub_criteria_id}. Generating empty report.")

            # 2. ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏£‡∏∏‡∏õ (Summary Calculation)
            valid_results = [r for r in results_data if isinstance(r, dict)]
            highest_lvl = 0
            total_weighted = 0.0

            for res in valid_results:
                # ‡∏î‡∏∂‡∏á‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á (‡πÄ‡∏ä‡πá‡∏Ñ is_passed)
                lvl = int(res.get('level', 0)) if res.get('is_passed') else 0
                if lvl > highest_lvl:
                    highest_lvl = lvl
                
                # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ñ‡πà‡∏ß‡∏á‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
                weight = float(res.get('weight', 4.0))
                res['weighted_score'] = (lvl / 5.0) * weight
                total_weighted += res['weighted_score']

            # 3. ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ö‡∏±‡πä‡∏Å Evidence Mapping (FIXED: List handling)
            master_map = self._load_evidence_map() or {}
            processed_evidence = {}
            
            for k, v in master_map.items():
                if not v: continue
                
                # üõ°Ô∏è FIX: ‡∏ñ‡πâ‡∏≤ v ‡πÄ‡∏õ‡πá‡∏ô list ‡πÉ‡∏´‡πâ‡πÄ‡∏≠‡∏≤‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å‡∏°‡∏≤‡πÇ‡∏ä‡∏ß‡πå ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô dict ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢
                item = v[0] if isinstance(v, list) and len(v) > 0 else v
                
                if isinstance(item, dict):
                    processed_evidence[k] = {
                        "file": item.get("filename") or item.get("file_name") or "unknown",
                        "page": item.get("page", "-"),
                        "pdca": (item.get("pdca_tag") or item.get("phase") or "Other").upper(),
                        "score": item.get("rerank_score", item.get("score", 0))
                    }

            # 4. ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏£‡πà‡∏≤‡∏á Payload (Full Report Structure)
            payload = {
                "record_id": record_id,
                "assessment_info": {
                    "tenant": tenant,
                    "year": year,
                    "enabler": enabler,
                    "sub_id": sub_criteria_id,
                    "engine_version": "SEAM-PDCA-v2026.1.20",
                    "exported_at": datetime.now().isoformat()
                },
                "summary": {
                    "maturity_level": f"L{highest_lvl}",
                    "is_passed": highest_lvl >= 1,
                    "total_weighted_score": round(total_weighted, 2),
                    "evidence_count": len(processed_evidence)
                },
                "detailed_results": valid_results,
                "evidence_mapping": processed_evidence,
                "action_plan": getattr(self, 'last_action_plan', {}) # ‡πÄ‡∏Å‡πá‡∏ö Roadmap ‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡πÄ‡∏•‡∏¢
            }

            # 5. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå (Smart Path Selection)
            try:
                export_path = get_assessment_export_file_path(
                    tenant=tenant, year=year, enabler=enabler.lower(),
                    suffix=f"{record_id}_{sub_criteria_id}_{timestamp}", ext="json"
                )
            except:
                # Fallback ‡∏Å‡∏£‡∏ì‡∏µ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á Path ‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤
                base_dir = f"exports/{tenant}/{year}"
                os.makedirs(base_dir, exist_ok=True)
                export_path = f"{base_dir}/{enabler}_{sub_criteria_id}_{timestamp}.json"

            with open(export_path, 'w', encoding='utf-8') as f:
                json.dump(payload, f, indent=2, ensure_ascii=False)

            self.logger.info(f"‚úÖ [FULL EXPORT SUCCESS] Path: {export_path}")
            return export_path

        except Exception as e:
            self.logger.critical(f"‚ùå [EXPORT CRASH] Error: {str(e)}", exc_info=True)
            return ""
        
    def _calculate_overall_stats(self, target_sub_id: str):
        """
        [REVISED v2026.PDCA.FINAL]
        - ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏• Maturity Level ‡πÇ‡∏î‡∏¢‡πÄ‡∏à‡∏≤‡∏∞ Path Nested '0' ‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
        - ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô L0 ‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£ Scan ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏£‡∏≤‡∏¢‡πÄ‡∏•‡πÄ‡∏ß‡∏•‡∏ã‡πâ‡∏≥ (Rescue Scan)
        """
        from datetime import datetime
        results = self.final_subcriteria_results
        if not results: return

        passed_levels = []
        for r in results:
            # üö© [FIX]: ‡πÄ‡∏à‡∏≤‡∏∞ Path ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Ç‡πâ‡∏≤‡∏´‡∏≤ Level 1-5 ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö‡πÉ‡∏ô Key '0'
            level_zero = r.get('level_details', {}).get('0', {})
            details_map = level_zero.get('level_details', {})
            
            # Rescue Scan: ‡∏ï‡∏£‡∏ß‡∏à‡∏´‡∏≤‡πÄ‡∏•‡πÄ‡∏ß‡∏•‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
            lvl = 0
            for l_idx in range(1, 6):
                lv_data = details_map.get(str(l_idx))
                if lv_data and lv_data.get('is_passed') is True:
                    lvl = l_idx
                else:
                    break # ‡∏Å‡∏é‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á SE-AM
            
            # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏ú‡∏•‡∏£‡∏∞‡∏î‡∏±‡∏ö‡πÅ‡∏•‡∏∞‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ñ‡πà‡∏ß‡∏á‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å
            r['highest_full_level'] = lvl 
            weight = float(r.get('weight', 4.0))
            if lvl > 0:
                new_score = self._calculate_weighted_score(lvl, weight)
                r['weighted_score'] = new_score
                r['is_passed'] = True
            else:
                r['weighted_score'] = 0.0
                r['is_passed'] = False

            passed_levels.append(lvl)

        # ‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°
        avg_level = sum(passed_levels) / len(results) if results else 0
        total_score = sum(float(r.get('weighted_score', 0.0)) for r in results)
        total_weight = sum(float(r.get('weight', 0.0)) for r in results)

        self.total_stats = {
            "overall_avg_score": round(total_score / total_weight, 4) if total_weight > 0 else 0,
            "overall_level_label": f"L{int(avg_level)}",
            "total_weighted_score": round(total_score, 2),
            "analytics": {
                "passed_levels_map": passed_levels,
                "strategic_gaps": self._extract_strategic_gaps(results)
            },
            "assessed_at": datetime.now().isoformat(),
            "highest_pass_level": int(avg_level)
        }
        
    def _robust_hydrate_documents_for_priority_chunks(
        self,
        chunks_to_hydrate: List[Dict],
        vsm: Optional['VectorStoreManager'],
        current_sub_id: Optional[str] = None,
        level: Optional[int] = None
    ) -> List[Dict]:
        """
        [ULTIMATE HYDRATION v2026.12]
        - ‡∏î‡∏∂‡∏á Full Text ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Priority Chunks ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ LLM ‡πÄ‡∏´‡πá‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô
        - ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏õ‡πÉ‡∏ä‡πâ LLM-based Tagging ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡∏ó‡∏∏‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö (L1-L5)
        - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏∞‡∏ö‡∏ö Boost Score ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ AI ‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Å‡∏±‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ô‡∏µ‡πâ
        """
        from collections import defaultdict
        
        active_sub_id = current_sub_id or getattr(self, 'sub_id', 'unknown')
        if not chunks_to_hydrate:
            self.logger.debug(f"‚ÑπÔ∏è [HYDRATION] No chunks to hydrate for {active_sub_id} L{level}")
            return []

        # 1. üè∑Ô∏è Helper: ‡∏à‡∏±‡∏î‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏î‡πâ‡∏ß‡∏¢ LLM (‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö Core Assessment)
        def _safe_classify(text: str, filename: str = "") -> str:
            try:
                # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ LLM Tagging ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏õ‡∏£‡∏±‡∏ö‡∏à‡∏π‡∏ô‡πÑ‡∏ß‡πâ (Self-contained within class)
                tag = self._get_semantic_tag(
                    text=text, 
                    sub_id=active_sub_id, 
                    level=level or 1,
                    filename=filename
                )
                return tag if tag in {"P", "D", "C", "A"} else "Other"
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è PDCA classify failed in hydration: {e}")
                return "Other"

        # 2. üìè Helper: ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞ Scoring Boost
        def _standardize_chunk(chunk: Dict, score: float):
            chunk.setdefault("is_baseline", True)
            text = chunk.get("text", "").strip()
            meta = chunk.get("metadata", {})
            
            if text:
                fname = os.path.basename(str(meta.get("source") or meta.get("file_name") or "Unknown"))
                # ‚ú® ‡πÉ‡∏ä‡πâ LLM Tagging ‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ‡πÄ‡∏•‡∏¢
                chunk["pdca_tag"] = _safe_classify(text, filename=fname)
                
                # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö Boost ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÉ‡∏´‡πâ‡∏™‡∏π‡∏á ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ä‡∏ô‡∏∞ Chunk ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö
                chunk["rerank_score"] = max(float(chunk.get("rerank_score", 0.0)), score)
                chunk["score"] = max(float(chunk.get("score", 0.0)), score)
            return chunk

        # 3. üîë ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏° IDs
        stable_ids = {
            sid for c in chunks_to_hydrate
            if (sid := (c.get("stable_doc_uuid") or c.get("doc_id") or c.get("chunk_uuid")))
        }

        if not stable_ids or not vsm:
            boosted = [_standardize_chunk(c.copy(), 0.9) for c in chunks_to_hydrate]
            return self._guarantee_text_key(boosted)

        # 4. üõ∞Ô∏è Fetch Full Documents (Hydration Process)
        stable_id_map = defaultdict(list)
        try:
            retrieved_docs = vsm.get_documents_by_id(
                list(stable_ids), doc_type=self.doc_type, enabler=self.config.enabler
            )
            for doc in retrieved_docs:
                sid = doc.metadata.get("stable_doc_uuid") or doc.metadata.get("doc_id")
                if sid:
                    stable_id_map[sid].append({"text": doc.page_content, "metadata": doc.metadata})
        except Exception as e:
            self.logger.error(f"‚ùå [HYDRATION] VSM Fetch Error: {e}")
            return self._guarantee_text_key([_standardize_chunk(c.copy(), 0.9) for c in chunks_to_hydrate])

        # 5. üíß Hydrate & Dedup
        hydrated_priority_docs = []
        seen_signatures = set()
        SAFE_META_KEYS = {"source", "file_name", "page", "page_label", "page_number"}

        for chunk in chunks_to_hydrate:
            new_chunk = chunk.copy()
            sid = new_chunk.get("stable_doc_uuid") or new_chunk.get("doc_id")

            hydrated = False
            if sid and sid in stable_id_map:
                # ‡∏î‡∏∂‡∏á Full Text ‡∏°‡∏≤‡∏ó‡∏±‡∏ö Snippet ‡∏™‡∏±‡πâ‡∏ô‡πÜ
                best_match = stable_id_map[sid][0]
                new_chunk["text"] = best_match["text"]
                meta = best_match.get("metadata", {})
                new_chunk.update({k: v for k, v in meta.items() if k in SAFE_META_KEYS})
                hydrated = True

            # ‡∏ó‡∏≥ Standardize + Tagging (Score 1.0 ‡∏ñ‡πâ‡∏≤‡∏î‡∏∂‡∏á‡πÄ‡∏ï‡πá‡∏°‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à / 0.85 ‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ‡∏Ç‡∏≠‡∏á‡πÄ‡∏î‡∏¥‡∏°)
            new_chunk = _standardize_chunk(new_chunk, score=1.0 if hydrated else 0.85)

            # Check Signature ‡∏Å‡∏±‡∏ô‡∏ã‡πâ‡∏≥
            sig = (sid, new_chunk.get("chunk_uuid"), new_chunk.get("text", "")[:100])
            if sig not in seen_signatures:
                seen_signatures.add(sig)
                hydrated_priority_docs.append(new_chunk)

        self.logger.info(f"‚úÖ [HYDRATION] Complete: {len(hydrated_priority_docs)} priority chunks ready.")
        return self._guarantee_text_key(hydrated_priority_docs)

    def _guarantee_text_key(
        self,
        chunks: List[Dict],
        total_count: int = 0,
        restored_count: int = 0
    ) -> List[Dict]:
        """
        Guarantee 'text' key exists in every chunk
        """
        final_chunks = []

        for chunk in chunks:
            if "text" not in chunk or not isinstance(chunk["text"], str):
                chunk["text"] = ""
                cid = str(chunk.get("chunk_uuid", "N/A"))
                self.logger.debug(f"Guaranteed 'text' key for chunk (ID: {cid[:8]})")
            final_chunks.append(chunk)

        if total_count > 0:
            baseline_count = sum(1 for c in final_chunks if c.get("is_baseline"))
            self.logger.info(
                f"HYDRATION FINAL: Restored {restored_count}/{total_count} "
                f"(Baseline={baseline_count}, Total final={len(final_chunks)})"
            )

        return final_chunks


    def _normalize_meta(self, c: Dict) -> Tuple[str, str]:
        """
        [REVISED] ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡∏∞‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏£‡∏∞‡∏ö‡∏ö Priority Fallback
        - ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤ 0 ‡∏´‡∏≤‡∏¢ (Index-based)
        - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏à‡∏≤‡∏Å Ingest ‡πÉ‡∏´‡∏°‡πà ‡πÅ‡∏•‡∏∞ Reranker Output
        - ‡πÄ‡∏ô‡πâ‡∏ô page_label ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏ö‡∏ô UI
        """
        if not isinstance(c, dict):
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡πá‡∏ô LangChain Document ‡πÉ‡∏´‡πâ‡∏î‡∏∂‡∏á metadata ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤
            if hasattr(c, "metadata"):
                meta = c.metadata
                # ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á dict ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÇ‡∏Ñ‡πâ‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏•‡πà‡∏≤‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏î‡πâ
                c = {"metadata": meta}
            else:
                return "Unknown Source", "N/A"

        # ‡∏î‡∏∂‡∏á metadata ‡∏°‡∏≤‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÑ‡∏ß‡πâ (‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏Å‡∏£‡∏ì‡∏µ None ‡∏´‡∏£‡∏∑‡∏≠ Dict)
        meta = c.get("metadata") or {}

        # 1. üîç ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå (Source Name Priority)
        source_priority = [
            c.get("source_filename"),
            meta.get("source_filename"),
            c.get("filename"),
            meta.get("source"),
            meta.get("file_name"),
            c.get("id") 
        ]
        
        source = "Unknown"
        for s in source_priority:
            if s and str(s).strip():
                source = str(s).strip()
                break

        # 2. üîç ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤ (Page Label Priority)
        page_keys = ["page_label", "page", "page_number"]
        found_page = None
        for key in page_keys:
            val_root = c.get(key)
            if val_root is not None and str(val_root).strip().lower() != "n/a":
                found_page = val_root
                break
            val_meta = meta.get(key)
            if val_meta is not None and str(val_meta).strip().lower() != "n/a":
                found_page = val_meta
                break

        # 3. ‚ú® Final Cleaning & UI Formatting
        clean_source = os.path.basename(source) if "/" in source or "\\" in source else source
        if found_page is not None:
            page_str = str(found_page).strip()
            clean_page = page_str if page_str.lower() != "n/a" else "N/A"
        else:
            clean_page = "N/A"

        return clean_source, clean_page
    

    def _get_heuristic_pdca_tag(self, text: str, level: int) -> Optional[str]:
        t = text.lower()
        
        # Do-specific ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö L1 (‡πÄ‡∏ô‡πâ‡∏ô‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏ó‡∏≥‡∏à‡∏£‡∏¥‡∏á)
        do_keywords = [
            "‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£", "‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥", "‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°", "‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°", "‡∏≠‡∏ö‡∏£‡∏°", "‡∏à‡∏±‡∏î‡∏ó‡∏≥", "‡∏•‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà", 
            "‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏ú‡∏•", "‡∏†‡∏≤‡∏û‡∏ñ‡πà‡∏≤‡∏¢", "‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£", "‡∏°‡∏∏‡πà‡∏á‡∏°‡∏±‡πà‡∏ô", "‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á", "‡∏™‡∏ô‡∏±‡∏ö‡∏™‡∏ô‡∏∏‡∏ô", 
            "‡∏Ç‡∏±‡∏ö‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô", "‡∏ô‡∏≥‡∏£‡πà‡∏≠‡∏á", "‡∏•‡∏á‡∏°‡∏∑‡∏≠‡∏ó‡∏≥", "‡∏ô‡∏≥‡πÑ‡∏õ‡πÉ‡∏ä‡πâ"
        ]
        if level <= 2 and any(k in t for k in do_keywords):
            return "D"

        # Check keywords (‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å log)
        check_keywords = [
            "‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏•", "‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô", "‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°", "‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î", "kpi", "‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå", "‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•", 
            "‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥", "‡∏™‡∏≥‡∏£‡∏ß‡∏à", "‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö", "‡∏ß‡∏±‡∏î‡∏ú‡∏•"
        ]
        if any(k in t for k in check_keywords):
            return "C"

        # Plan & Act (‡∏Ñ‡∏á‡πÄ‡∏î‡∏¥‡∏° ‡πÅ‡∏ï‡πà‡∏•‡∏î priority)
        if any(k in t for k in ["‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢", "‡πÅ‡∏ú‡∏ô", "‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå", "‡∏°‡∏ï‡∏¥", "‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á", "‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢", "‡πÄ‡∏à‡∏ï‡∏ô‡∏≤‡∏£‡∏°‡∏ì‡πå"]):
            return "P"
        if any(k in t for k in ["‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á", "‡∏û‡∏±‡∏í‡∏ô‡∏≤", "‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç", "‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô", "lesson learned", "‡∏ï‡πà‡∏≠‡∏¢‡∏≠‡∏î", "‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°"]):
            return "A"

        return None
    
    def _get_pdca_blocks_from_evidences(
        self, 
        evidences: List[Dict[str, Any]], 
        baseline_evidences: Any, 
        level: int, 
        sub_id: str, 
        contextual_rules_map: Dict[str, Any], 
        record_id: str = None
    ) -> Dict[str, str]:
        """
        [ULTIMATE REVISE v2026.1.18 - Fixed PDCA Coverage Bias]
        - Heuristic tag ‡πÅ‡∏Ç‡πá‡∏á‡πÅ‡∏£‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤‡∏Å (‡∏ä‡πà‡∏ß‡∏¢‡∏î‡∏∂‡∏á D/C/A ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö L1)
        - Fallback tag ‡∏ï‡∏≤‡∏° level (L1 ‚Üí 50% P/D ‡∏ñ‡πâ‡∏≤ Other)
        - ‡πÉ‡∏™‡πà source + page ‡πÉ‡∏ô block ‡∏ó‡∏∏‡∏Å chunk
        - ‡∏à‡∏≥‡∏Å‡∏±‡∏î 5 chunks ‡∏ï‡πà‡∏≠ tag (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô context ‡∏•‡πâ‡∏ô)
        """
        pdca_groups = defaultdict(list)
        seen_texts = set()

        total_chunks = len(evidences or [])
        self.logger.info(f"üè∑Ô∏è [TAGGING START] Processing {total_chunks} chunks for {sub_id} L{level}")

        for idx, chunk in enumerate(evidences or [], start=1):
            txt = chunk.get("text", "").strip()
            if not txt or txt in seen_texts:
                continue
            seen_texts.add(txt)

            # --- Metadata Recovery (robust) ---
            meta = chunk.get("metadata", {})
            filename = (
                chunk.get("source_filename") or 
                meta.get("source_filename") or 
                meta.get("source") or 
                meta.get("file_name") or 
                "Unknown_File"
            )
            page = meta.get("page_label") or meta.get("page") or meta.get("page_number") or "N/A"
            display_name = f"{filename} (P.{page})"

            # --- 1. Enhanced Heuristic Tag (Priority) ---
            heuristic_tag = self._get_heuristic_pdca_tag(txt, level)
            final_tag = heuristic_tag

            if final_tag:
                self.logger.info(f"üè∑Ô∏è  [{idx}/{total_chunks}] {final_tag} | (Heuristic) | {display_name}")
            else:
                # --- 2. AI Tag (‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠ heuristic ‡πÑ‡∏°‡πà‡∏ä‡∏±‡∏î) ---
                try:
                    tag = self._get_semantic_tag(txt, sub_id, level, filename)
                    final_tag = tag if tag in {"P", "D", "C", "A"} else "Other"
                    self.logger.info(f"üè∑Ô∏è  [{idx}/{total_chunks}] {final_tag} | (AI) | {display_name}")
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è AI Tag failed {display_name}: {e}")
                    final_tag = "Other"

            # --- 3. Level-specific Fallback (‡πÅ‡∏Å‡πâ bias L1 ‡∏°‡∏µ‡πÅ‡∏ï‡πà P) ---
            if final_tag == "Other":
                if level <= 2:
                    # ‡∏™‡∏∏‡πà‡∏° P ‡∏´‡∏£‡∏∑‡∏≠ D 50/50 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö L1-L2
                    final_tag = "P" if idx % 2 == 0 else "D"
                elif level == 3:
                    final_tag = "C"  # L3 ‡πÄ‡∏ô‡πâ‡∏ô Check
                else:
                    final_tag = "A"  # L4+ ‡πÄ‡∏ô‡πâ‡∏ô Act

            chunk["pdca_tag"] = final_tag
            chunk["source_display"] = display_name

            pdca_groups[final_tag].append(chunk)

        # Summary Log
        tag_counts = {t: len(pdca_groups[t]) for t in ["P", "D", "C", "A", "Other"]}
        self.logger.info(
            f"üìä [TAGGING RESULT] {sub_id} L{level} -> "
            f"P:{tag_counts['P']} | D:{tag_counts['D']} | C:{tag_counts['C']} | A:{tag_counts['A']} | Other:{tag_counts['Other']}"
        )

        # 4. Build Blocks (‡∏à‡∏≥‡∏Å‡∏±‡∏î 5 chunks/tag)
        blocks = {}
        for tag in ["P", "D", "C", "A", "Other"]:
            chunks = pdca_groups[tag][:5]  # ‡∏à‡∏≥‡∏Å‡∏±‡∏î 5
            if chunks:
                parts = []
                for c in chunks:
                    source = c.get("source_display", "Unknown")
                    parts.append(f"[{source}]\n{c.get('text', '').strip()[:400]}...")
                blocks[tag] = "\n\n".join(parts)
            else:
                blocks[tag] = "[‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÉ‡∏ô‡∏´‡∏°‡∏ß‡∏î‡∏ô‡∏µ‡πâ]"

        # ‡πÄ‡∏û‡∏¥‡πà‡∏° sources ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö traceability
        blocks["sources"] = {
            tag: [c.get("source_display") for c in pdca_groups[tag][:5]]
            for tag in ["Plan", "Do", "Check", "Act"]
        }

        return blocks

    def _generate_action_plan_safe(
        self, 
        sub_id: str, 
        name: str, 
        enabler: str, 
        results: List[Dict]
    ) -> Any:
        """
        [ULTIMATE STRATEGIC REVISE v2026.1.18 - Production Ready]
        - Strength Awareness: ‡∏î‡∏∂‡∏á summary_thai ‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏∏‡∏î‡πÅ‡∏Ç‡πá‡∏á
        - Missing Phases: ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏à‡∏≤‡∏Å pdca_breakdown ‡∏à‡∏£‡∏¥‡∏á
        - Recommendation Type: ‡πÅ‡∏¢‡∏Å Remediation / Refinement / Excellence
        - Emergency Fallback: ‡∏°‡∏µ PDCA ‡∏™‡∏±‡πâ‡∏ô ‡πÜ ‡∏û‡∏£‡πâ‡∏≠‡∏° coaching
        """
        try:
            self.logger.info(f"üöÄ [ACTION PLAN] Generating for {sub_id} - {name}")

            to_recommend = []
            has_major_gap = False

            sorted_results = sorted(results, key=lambda x: x.get('level', 0))

            for r in sorted_results:
                level = r.get('level', 0)
                is_passed = r.get('is_passed', False)
                score = float(r.get('score', 0.0))
                pdca_raw = r.get('pdca_breakdown', {})

                # Missing phases (‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤ 0.5)
                missing = [p for p in ['P', 'D', 'C', 'A'] if float(pdca_raw.get(p, 0.0)) < 0.5]

                coaching = r.get('coaching_insight', '').strip()
                strength = r.get('summary_thai', r.get('reason', '')).strip() if is_passed and score >= 0.8 else ""

                payload = {
                    "level": level,
                    "is_passed": is_passed,
                    "score": score,
                    "missing_phases": missing,
                    "coaching_insight": coaching,
                    "strength_context": strength,
                    "recommendation_type": "FAILED_REMEDIATION" if not is_passed else
                                          "QUALITY_REFINEMENT" if missing or score < 1.0 else
                                          "EXCELLENCE_MAINTENANCE"
                }

                if not is_passed or missing:
                    has_major_gap = True
                to_recommend.append(payload)

            # ‡∏Å‡∏£‡∏ì‡∏µ‡∏ú‡πà‡∏≤‡∏ô‡∏´‡∏°‡∏î ‚Üí Excellence
            if not has_major_gap:
                self.logger.info(f"üåü {sub_id} EXCELLENT - No major gaps")
                return {
                    "status": "EXCELLENT",
                    "message": "‡∏ö‡∏£‡∏£‡∏•‡∏∏‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå",
                    "coaching_summary": "‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÄ‡∏õ‡πá‡∏ô Best Practice ‡∏ï‡πà‡∏≠‡πÑ‡∏õ"
                }

            # ‡∏™‡πà‡∏á‡πÑ‡∏õ‡∏™‡∏£‡πâ‡∏≤‡∏á roadmap
            action_plan_args = {
                "recommendation_statements": to_recommend,
                "sub_id": sub_id,
                "sub_criteria_name": name,
                "enabler": enabler,
                "target_level": getattr(self.config, 'target_level', 5),
                "llm_executor": self.llm,
                "logger": self.logger
            }

            self.logger.info(f"[ACTION PLAN] Invoking engine with {len(to_recommend)} items")
            return create_structured_action_plan(**action_plan_args)

        except Exception as e:
            self.logger.error(f"‚ùå Action Plan Failed: {str(e)}")
            return _get_emergency_fallback_plan(
                sub_id, name, 
                getattr(self.config, 'target_level', 5), 
                has_major_gap, False, enabler
            )

    def _create_text_block_from_chunks(self, tag, chunks):
        """ ‡∏ï‡∏±‡∏ß‡∏ä‡πà‡∏ß‡∏¢‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å chunk """
        if not chunks: return ""
        parts = [f"### [{tag} Evidence]\n{c.get('text','')}" for c in chunks[:5]]
        return "\n\n".join(parts)
    
    def _prepare_worker_tuple(self, sub_data: Dict, document_map: Optional[Dict]) -> Tuple:
        return (
            sub_data,                          # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏ì‡∏ë‡πå (1.1, 1.2...)
            self.config.enabler,               # KM / IT / ...
            self.config.target_level,          # ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ L5
            self.config.mock_mode,             # none/random
            self.evidence_map_path,            # ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Å‡πá‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô
            self.config.model_name,            # llama3.1:70b ‡∏´‡∏£‡∏∑‡∏≠ 8b
            self.config.temperature,           # 0.0
            getattr(self.config, 'min_retry_score', 0.65),
            getattr(self.config, 'max_retrieval_attempts', 3),
            document_map or self.document_map, # ‡πÅ‡∏ú‡∏ô‡∏ú‡∏±‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
            getattr(self, 'ActionPlanActions', None),
            self.config.year,                  # 2567
            self.config.tenant                 # pea
        )
    

    # ----------------------------------------------------------------------
    # üöÄ CORE WORKER: Assessment Execution (REVISED v2026.1.18 - FULL RECOVERY)
    # ----------------------------------------------------------------------
    def _run_sub_criteria_assessment_worker(
        self,
        sub_criteria: Dict[str, Any],
        vectorstore_manager: Optional['VectorStoreManager'] = None
    ) -> Tuple[Dict[str, Any], Dict[str, List[Dict[str, Any]]]]:
        """
        [FIXED & FINAL PRODUCTION v2026.1.18]
        - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤ L0: ‡πÉ‡∏ä‡πâ is_passed ‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£ Rescue/Rerank ‡∏°‡∏≤‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Sequential Logic
        - ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏™‡∏∞‡∏û‡∏≤‡∏ô‡∏Ç‡∏≤‡∏î: ‡∏´‡∏≤‡∏Å L1-L3 ‡∏ú‡πà‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Safety Net ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡∏ô‡∏±‡∏ö‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡πÉ‡∏´‡πâ‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå SE-AM
        - ‡∏à‡∏±‡∏î‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Nested '0' ‡πÉ‡∏´‡πâ‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°
        """
        MAX_RETRY_ATTEMPTS = 2
        sub_id = sub_criteria['sub_id']
        sub_criteria_name = sub_criteria['sub_criteria_name']
        sub_weight = sub_criteria.get('weight', 0)
        
        current_enabler = getattr(self.config, 'enabler', 'Unknown')
        vsm = vectorstore_manager or getattr(self, 'vectorstore_manager', None)
        
        # --- [TRACKING STATES] ---
        current_sequential_pass_level = 0 
        found_primary_gap = False  
        
        raw_results_for_sub_seq: List[Dict[str, Any]] = []
        level_details_map = {} 
        current_worker_evidence_map = {} 
        start_ts = time.time() 

        all_rules_for_sub = getattr(self, 'contextual_rules_map', {}).get(sub_id, {})
        levels_to_assess = sorted(sub_criteria.get('levels', []), key=lambda x: x.get('level', 0))

        # -----------------------------------------------------------
        # EVALUATION LOOP
        # -----------------------------------------------------------
        for statement_data in levels_to_assess:
            level = statement_data.get('level')
            if level is None or level > getattr(self.config, 'target_level', 5):
                continue
            
            level_result = {}
            for attempt_num in range(1, MAX_RETRY_ATTEMPTS + 1):
                try:
                    raw_res = self._run_single_assessment(
                        sub_criteria=sub_criteria,
                        statement_data=statement_data,
                        vectorstore_manager=vsm,
                        attempt=attempt_num,
                        record_id=self.current_record_id,
                        evidence_map=self.evidence_map,
                        **all_rules_for_sub.get(str(level), {})
                    )

                    if isinstance(raw_res, tuple):
                        level_result = raw_res[0] if len(raw_res) > 0 else {}
                    else:
                        level_result = raw_res if isinstance(raw_res, dict) else {}

                    if level_result and "is_passed" in level_result:
                        break
                except Exception as e:
                    self.logger.error(f"‚ùå [L{level}] Error: {str(e)}")
                    level_result = {"level": level, "is_passed": False, "score": 0.0}

            # --- [SEQUENTIAL GAP LOGIC - FIXED] ---
            # üö© ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡πÉ‡∏ä‡πâ is_passed ‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô Post-process ‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß (‡∏£‡∏ß‡∏° Rerank Safety)
            is_passed_final = level_result.get('is_passed', False)

            if is_passed_final and not found_primary_gap:
                current_sequential_pass_level = level
                level_result.update({"display_status": "PASSED", "gap_type": "NONE", "is_passed": True})
            elif not is_passed_final and not found_primary_gap:
                found_primary_gap = True
                level_result.update({"display_status": "FAILED", "gap_type": "PRIMARY_GAP", "is_passed": False})
            elif is_passed_final and found_primary_gap:
                level_result.update({"display_status": "PASSED (CAPPED)", "gap_type": "SEQUENTIAL_GAP", "is_passed": False})
            else:
                level_result.update({"display_status": "FAILED (GAP)", "gap_type": "COMPOUND_GAP", "is_passed": False})

            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏≤‡∏¢‡πÄ‡∏•‡πÄ‡∏ß‡∏•
            level_details_map[str(level)] = {
                "level": level,
                "is_passed": level_result.get('is_passed', False),
                "score": float(level_result.get('score', 0.0)),
                "pdca_breakdown": level_result.get('pdca_breakdown', {}),
                "reason": level_result.get('reason', ""),
                "display_status": level_result.get("display_status"),
                "gap_type": level_result.get("gap_type")
            }
            raw_results_for_sub_seq.append(level_result)

        # -----------------------------------------------------------
        # FINAL SYNTHESIS
        # -----------------------------------------------------------
        weighted_score = round(self._calculate_weighted_score(current_sequential_pass_level, sub_weight), 2)
        action_plan_result = self._generate_action_plan_safe(sub_id, sub_criteria_name, current_enabler, raw_results_for_sub_seq)

        return {
            "sub_id": sub_id,
            "sub_criteria_name": sub_criteria_name,
            "highest_full_level": current_sequential_pass_level,
            "weighted_score": weighted_score,
            "weight": sub_weight,
            "is_passed": current_sequential_pass_level > 0,
            "level_details": {
                "0": { # ‡∏´‡πà‡∏≠‡∏î‡πâ‡∏ß‡∏¢ "0" ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Aggregator
                    "sub_id": sub_id,
                    "highest_pass_level": current_sequential_pass_level,
                    "weighted_score": weighted_score,
                    "level_details": level_details_map,
                    "action_plan": action_plan_result
                }
            }
        }, current_worker_evidence_map
    
    def run_assessment(
        self,
        target_sub_id: str = "all",
        export: bool = False,
        vectorstore_manager: Optional['VectorStoreManager'] = None,
        sequential: bool = False,
        document_map: Optional[Dict[str, str]] = None,
        record_id: str = None,
    ) -> Dict[str, Any]:
        """
        [PRODUCTION FINAL v2026.1.17 ‚Äî Ultimate Stability Fix]
        - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô L0 ‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏≤‡∏£ Merge Results ‡πÅ‡∏•‡∏∞ Evidence
        - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö Nested ‡∏à‡∏≤‡∏Å Worker ‡∏ó‡∏±‡πâ‡∏á Parallel ‡πÅ‡∏•‡∏∞ Sequential
        - ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏≤‡∏£ Deduplicate ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Total Items ‡∏™‡∏∞‡∏ó‡πâ‡∏≠‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏£‡∏¥‡∏á
        """
        start_ts = time.time()
        self.is_sequential = sequential
        self.current_record_id = record_id 

        # 1. üéØ ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô
        all_statements = self._flatten_rubric_to_statements()
        is_all = str(target_sub_id).lower() == "all"
        sub_criteria_list = all_statements if is_all else [
            s for s in all_statements if str(s.get('sub_id')).lower() == str(target_sub_id).lower()
        ]

        if not sub_criteria_list:
            return self._create_failed_result(record_id, f"Criteria '{target_sub_id}' not found", start_ts)

        self.logger.info(f"üéØ Assessment Start | Target: {target_sub_id} | Record ID: {record_id} | Sub-items: {len(sub_criteria_list)}")

        # 2. üîÑ Resumption: ‡πÇ‡∏´‡∏•‡∏î Evidence Map ‡πÄ‡∏î‡∏¥‡∏°‡∏à‡∏≤‡∏Å Disk
        existing_data = self._load_evidence_map()
        if isinstance(existing_data, dict):
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Key ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£ Save ‡∏ï‡πà‡∏≤‡∏á‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô
            self.evidence_map = existing_data.get("evidence_map", existing_data)
            self.logger.info(f"üîÑ Resumed Evidence Map: {len(self.evidence_map)} keys from disk")
        else:
            self.evidence_map = {}
            self.logger.info("üÜï Starting with fresh Evidence Map")

        # 3. ‚öôÔ∏è ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Execution Mode
        max_workers = int(os.environ.get('MAX_PARALLEL_WORKERS', 4))
        run_parallel = is_all and not sequential
        
        results_list = []
        execution_start = time.time()

        # 4. üöÄ Execution Phase
        if run_parallel:
            self.logger.info(f"üöÄ Running Parallel Assessment (Workers: {max_workers})")
            worker_args = [self._prepare_worker_tuple(s, document_map) for s in sub_criteria_list]
            try:
                # ‡πÉ‡∏ä‡πâ 'spawn' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏Ç‡∏≠‡∏á‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö RAG
                ctx = multiprocessing.get_context('spawn')
                with ctx.Pool(processes=max_workers) as pool:
                    results_list = pool.map(_static_worker_process, worker_args)
            except Exception as e:
                self.logger.critical(f"‚ùå Parallel execution failed: {e}")
                raise
        else:
            self.logger.info(f"üßµ Running Sequential Assessment: {target_sub_id}")
            vsm = vectorstore_manager or self._init_local_vsm()
            for sub_criteria in sub_criteria_list:
                res = self._run_sub_criteria_assessment_worker(sub_criteria, vsm)
                results_list.append(res)

        execution_time = time.time() - execution_start
        self.logger.info(f"[EXECUTION] Completed | Time: {execution_time:.2f}s | Results: {len(results_list)}")

        # 5. üß© Integration Phase (Merge Results + Evidence Mapping)
        integration_start = time.time()
        self.logger.info(f"üß© Integrating {len(results_list)} results...")

        if not results_list:
            self.logger.warning("[INTEGRATION] No results to merge.")
        else:
            # --- 5.1 Merge Scores & Details ---
            for res in results_list:
                # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á (data, map) ‡∏à‡∏≤‡∏Å Parallel ‡πÅ‡∏•‡∏∞ dict ‡∏à‡∏≤‡∏Å Sequential
                worker_data = res[0] if isinstance(res, tuple) else res
                worker_map = res[1] if isinstance(res, tuple) else res.get('temp_map_for_level', {})
                
                # ‡∏´‡∏≤‡∏Å temp_map_for_level ‡∏™‡πà‡∏á‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô List ‡∏ï‡∏£‡∏á‡πÜ ‡πÉ‡∏´‡πâ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô Dict Key
                if isinstance(worker_map, list):
                    key = f"{worker_data.get('sub_id')}_L{worker_data.get('level')}"
                    worker_map = {key: worker_map}
                
                self._merge_worker_results(worker_data, worker_map)

            # --- 5.2 Merge Evidence Mapping (Fix Total Items: 0) ---
            merged_evidence = self.merge_evidence_mappings(results_list)
            merge_stats = {"new_levels": 0, "added_items": 0, "updated_levels": 0}
            
            if merged_evidence:
                for key, items in merged_evidence.items():
                    if key not in self.evidence_map:
                        self.evidence_map[key] = items
                        merge_stats["new_levels"] += 1
                        merge_stats["added_items"] += len(items)
                    else:
                        # Deduplicate ID ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ö‡∏ß‡∏°
                        existing_ids = {
                            str(e.get('chunk_uuid') or e.get('doc_id') or "N/A").replace("-", "").lower() 
                            for e in self.evidence_map[key]
                        }
                        
                        new_to_add = []
                        for item in items:
                            clean_id = str(item.get('chunk_uuid') or item.get('doc_id') or "N/A").replace("-", "").lower()
                            if clean_id not in existing_ids and clean_id not in ["na", "n/a", ""]:
                                new_to_add.append(item)
                                existing_ids.add(clean_id)
                        
                        self.evidence_map[key].extend(new_to_add)
                        merge_stats["added_items"] += len(new_to_add)
                        merge_stats["updated_levels"] += 1
                
                self.logger.info(f"üß¨ [MERGE-EVIDENCE] New: {merge_stats['new_levels']} | Updated: {merge_stats['updated_levels']} | Total Items: {sum(len(v) for v in self.evidence_map.values())}")
            else:
                self.logger.warning("[MERGE-EVIDENCE] No valid evidence found in results_list")

        integration_time = time.time() - integration_start

        # 6. üíæ Persistence
        persistence_start = time.time()
        try:
            save_payload = {
                "record_id": record_id,
                "evidence_map": self.evidence_map,
                "timestamp": datetime.now().isoformat()
            }
            self._save_evidence_map(save_payload)
            self.logger.info("‚úÖ Final merged evidence map persisted to disk")
        except Exception as e:
            self.logger.error(f"‚ùå Save Failed: {e}")

        # 7. üìä Final Summary
        self._calculate_overall_stats(target_sub_id)
        
        final_response = {
            "record_id": record_id,
            "summary": self.total_stats,
            "sub_criteria_results": self.final_subcriteria_results,
            "run_time_seconds": round(time.time() - start_ts, 2),
            "evidence_mapping_summary": {
                "total_levels": len(self.evidence_map),
                "total_items": sum(len(v) for v in self.evidence_map.values())
            }
        }

        if export:
            final_response["export_path"] = self._export_results(
                self.final_subcriteria_results, 
                target_sub_id, 
                record_id=record_id  # üëà ‡πÉ‡∏™‡πà‡∏ä‡∏∑‡πà‡∏≠ parameter ‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏°‡∏±‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô **kwargs
            )

        return final_response
    
    def _normalize_evidence_metadata(self, evidence_list: List[Dict[str, Any]]):
        """
        [REVISED v2026] ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á Metadata ‡πÉ‡∏´‡πâ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÅ‡∏•‡∏∞ Export
        - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Ñ‡∏µ‡∏¢‡πå‡∏Å‡∏£‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢ (Flattened vs Nested)
        - ‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏Ç‡∏≠‡∏á‡∏ä‡∏ô‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Type Safety)
        """
        for ev in evidence_list:
            if not isinstance(ev, dict):
                continue
                
            # 1. ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á Metadata (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á Langchain Doc ‡πÅ‡∏•‡∏∞ Dict)
            meta = ev.get("metadata", {})
            if not isinstance(meta, dict): meta = {}
            
            # 2. ‡∏õ‡∏£‡∏±‡∏ö Source (‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏•‡∏∂‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Å‡πà‡∏≠‡∏ô)
            raw_source = (
                meta.get("source_filename") or 
                meta.get("file_name") or 
                ev.get("source") or 
                meta.get("source") or 
                "Unknown_File"
            )
            ev["source"] = os.path.basename(str(raw_source))
            ev["source_filename"] = ev["source"] # Sync ‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á‡∏Ñ‡∏µ‡∏¢‡πå
            
            # 3. ‡∏õ‡∏£‡∏±‡∏ö Page (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏•‡∏Ç 0 ‡∏´‡∏£‡∏∑‡∏≠ None)
            raw_page = (
                meta.get("page_label") or 
                meta.get("page") or 
                meta.get("page_number") or 
                ev.get("page") or 
                "N/A"
            )
            ev["page"] = str(raw_page)
            
            # 4. ‡∏õ‡∏£‡∏±‡∏ö Relevance Score ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á
            ev["relevance_score"] = self.get_actual_score(ev)
            
            # 5. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ ID ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ Audit Trail
            if not ev.get("stable_doc_uuid"):
                ev["stable_doc_uuid"] = (
                    meta.get("stable_doc_uuid") or 
                    ev.get("doc_id") or 
                    meta.get("doc_id") or 
                    f"id_{uuid.uuid4().hex[:8]}"
                )
            
            # 6. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö PDCA Tag (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô Other)
            if not ev.get("pdca_tag"):
                ev["pdca_tag"] = meta.get("pdca_tag") or "Other"

        return evidence_list
    
    def _merge_worker_results(self, sub_result: Dict[str, Any], temp_map: Dict[str, List[Dict]]):
        """
        [ULTIMATE REVISE v2026.PDCA.FINAL - THE CHAIN RESTORER]
        - FIXED: ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô L0 ‡∏ó‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á‡∏ú‡πà‡∏≤‡∏ô (‡∏î‡πâ‡∏ß‡∏¢‡∏£‡∏∞‡∏ö‡∏ö Smart Chain Validation)
        - IMPROVED: ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ Merge ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Parallel Workers ‡∏ó‡∏µ‡πà‡∏°‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô
        - ADDED: ‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ 'is_passed' ‡∏ó‡∏µ‡πà‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Safety Net ‡πÅ‡∏•‡∏∞ Manual Boost)
        """
        if not sub_result:
            return

        sub_id = str(sub_result.get('sub_id', 'Unknown'))
        # ‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏£‡∏∞‡∏î‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô int
        try:
            level = int(sub_result.get('level', 0))
        except:
            level = 0
            
        # 1. üõ°Ô∏è Evidence Mapping Integration (Deduplicated)
        if temp_map and isinstance(temp_map, dict):
            for level_key, evidence_list in temp_map.items():
                if not isinstance(evidence_list, list): continue
                
                if level_key not in self.evidence_map:
                    self.evidence_map[level_key] = []
                
                # ‡∏™‡∏£‡πâ‡∏≤‡∏á set ‡∏Ç‡∏≠‡∏á ID ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡πá‡∏Ñ
                existing_ids = {
                    str(e.get('chunk_uuid') or e.get('doc_id') or id(e)) 
                    for e in self.evidence_map[level_key]
                }
                
                for ev in evidence_list:
                    ev_id = str(ev.get('chunk_uuid') or ev.get('doc_id') or id(ev))
                    if ev_id not in existing_ids and ev_id not in ["na", "n/a", ""]:
                        self.evidence_map[level_key].append(ev)
                        existing_ids.add(ev_id)

        # 2. üîç ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á Container ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Sub-Criteria ‡∏ô‡∏µ‡πâ
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ List ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á
        if not hasattr(self, 'final_subcriteria_results'):
            self.final_subcriteria_results = []

        target = next((r for r in self.final_subcriteria_results if str(r.get('sub_id')) == sub_id), None)
        
        if not target:
            target = {
                "sub_id": sub_id,
                "sub_criteria_name": sub_result.get('sub_criteria_name') or sub_id,
                "weight": float(sub_result.get('weight', 4.0)),
                "level_details": {},  # ‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÅ‡∏¢‡∏Å‡πÅ‡∏ï‡πà‡∏•‡∏∞ Level { "1": {...}, "2": {...} }
                "highest_full_level": 0,
                "weighted_score": 0.0,
                "is_passed": False,
                "audit_stop_reason": "Starting integration..."
            }
            self.final_subcriteria_results.append(target)

        # 3. üß© ‡∏ú‡∏™‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô level_details
        # ‡πÉ‡∏ä‡πâ String Key ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ï‡∏≠‡∏ô JSON Export
        target['level_details'][str(level)] = sub_result
        
        # 4. ‚öñÔ∏è ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Sequential Maturity (‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á)
        # ‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ä‡πá‡∏Ñ‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà L1 ‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏õ ‡∏ñ‡πâ‡∏≤ L ‡πÑ‡∏´‡∏ô‡∏û‡∏±‡∏á Chain ‡∏à‡∏∞‡∏´‡∏¢‡∏∏‡∏î‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
        current_highest = 0
        stop_reason = ""
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö L1 ‡∏ñ‡∏∂‡∏á L5
        for l in range(1, 6):
            l_str = str(l)
            l_data = target['level_details'].get(l_str)
            
            if l_data:
                # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ú‡πà‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏à‡∏≤‡∏Å AI ‡∏õ‡∏Å‡∏ï‡∏¥ ‡∏´‡∏£‡∏∑‡∏≠ Safety Net (is_passed)
                is_passed = l_data.get('is_passed', False)
                
                if is_passed:
                    current_highest = l
                else:
                    # ‡∏ñ‡πâ‡∏≤‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÅ‡∏•‡πâ‡∏ß‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô
                    stop_reason = f"Stopped at L{l} because: {l_data.get('reason', '‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå')[:60]}..."
                    break
            else:
                # ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡∏µ‡πâ (‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏¢‡∏±‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÑ‡∏°‡πà‡∏ñ‡∏∂‡∏á‡πÉ‡∏ô‡πÇ‡∏´‡∏°‡∏î Parallel ‡∏´‡∏£‡∏∑‡∏≠‡∏ñ‡∏π‡∏Å Block)
                if level > l:
                    # ‡∏ñ‡πâ‡∏≤‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡πÄ‡∏£‡∏≤‡∏Å‡∏≥‡∏•‡∏±‡∏á Merge L ‡∏ó‡∏µ‡πà‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤ ‡πÅ‡∏ï‡πà L ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• 
                    # ‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤ L ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß (Critical Error) ‡∏´‡∏£‡∏∑‡∏≠‡∏ñ‡∏π‡∏Å‡∏Ç‡πâ‡∏≤‡∏°
                    stop_reason = f"Chain broken: L{l} data is missing"
                else:
                    stop_reason = f"Assessment pending for L{l} and above"
                break 
        
        # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ñ‡πà‡∏≤‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
        target['highest_full_level'] = current_highest
        target['audit_stop_reason'] = stop_reason
        target['is_passed'] = (current_highest >= 1)
        
        # 5. üí∞ ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Weighted Score (Maturity / 5.0 * Weight)
        target['weighted_score'] = round((current_highest / 5.0) * target['weight'], 2)
        
        # üìä Logging ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£ Trace
        log_status = "CONTINUE" if current_highest == level else "CEASED/PENDING"
        self.logger.info(
            f"‚úÖ [MERGE] {sub_id} L{level} -> Current Maturity: L{current_highest} | "
            f"Score: {target['weighted_score']} | Status: {log_status}"
        )
        if stop_reason and level >= current_highest:
             self.logger.debug(f"‚ÑπÔ∏è [REASON] {stop_reason}")

        return target
    
    def _run_expert_re_evaluation(
        self,
        sub_id: str,
        level: int,
        statement_text: str,
        context: str,
        first_attempt_reason: str,
        missing_tags: Union[List[str], Set[str]],
        highest_rerank_score: float,
        sub_criteria_name: str,
        llm_evaluator_to_use: Any,
        base_kwargs: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        [REVISED v2026.1.18 - JUDICIAL AUDITOR EDITION]
        - ‡∏¢‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏≠‡∏∏‡∏ó‡∏ò‡∏£‡∏ì‡πå‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏´‡∏•‡∏±‡∏Å 'Evidence-Based Reasoning'
        - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÇ‡∏õ‡∏£‡πà‡∏á‡πÉ‡∏™‡∏Ç‡∏≠‡∏á‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡∏∏‡∏ó‡∏ò‡∏£‡∏ì‡πå (Explainable AI)
        - ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ 'Golden Sentence' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏Å‡∏≤‡∏£ Override ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô
        """
        self.logger.info(f"‚öñÔ∏è [EXPERT-APPEAL] Processing {sub_id} L{level} | Evidence Strength: {highest_rerank_score:.4f}")

        # 1. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å‡∏£‡∏≠‡∏ö‡πÅ‡∏£‡∏Å (Gap Identification)
        missing_str = ", ".join(sorted(set(missing_tags))) if missing_tags else "‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå PDCA"
        
        # 2. ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á Hint Message ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ä‡∏¥‡∏á‡∏£‡∏∏‡∏Å (Proactive Instruction)
        # ‡∏™‡∏±‡πà‡∏á‡πÉ‡∏´‡πâ AI ‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà '‡πÄ‡∏Å‡∏∑‡∏≠‡∏ö‡∏ú‡πà‡∏≤‡∏ô' ‡πÅ‡∏ï‡πà‡∏ñ‡∏π‡∏Å‡∏°‡∏≠‡∏á‡∏Ç‡πâ‡∏≤‡∏°
        hint_msg = f"""
        ### üö® EXPERT AUDIT INSTRUCTION: DO NOT MISS KEY SUBSTANCE üö®
        
        [CONTEXT]: ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏£‡∏≠‡∏ö‡πÅ‡∏£‡∏Å‡πÉ‡∏´‡πâ "‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô" ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å: "{first_attempt_reason[:150]}..."
        [OPPORTUNITY]: ‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏™‡∏π‡∏á ({highest_rerank_score:.4f}) 
        ‡∏ã‡∏∂‡πà‡∏á‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏™‡∏∞‡∏ó‡πâ‡∏≠‡∏ô‡∏ñ‡∏∂‡∏á: {missing_str}
        
        [TASK]: ‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏∞ '‡∏ú‡∏π‡πâ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏≠‡∏≤‡∏ß‡∏∏‡πÇ‡∏™' ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∏‡∏ì‡∏°‡∏≠‡∏á‡∏Ç‡πâ‡∏≤‡∏°‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á Keyword ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡πÄ‡∏õ‡πä‡∏∞ (Form) 
        ‡πÅ‡∏•‡∏∞‡∏°‡∏∏‡πà‡∏á‡πÄ‡∏ô‡πâ‡∏ô‡∏ó‡∏µ‡πà '‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏ó‡∏≥‡∏à‡∏£‡∏¥‡∏á' (Substance) ‡∏´‡∏≤‡∏Å‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÅ‡∏°‡πâ‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏à‡∏∏‡∏î‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏ó‡∏µ‡πà‡∏™‡∏∞‡∏ó‡πâ‡∏≠‡∏ô‡∏ñ‡∏∂‡∏á‡πÄ‡∏Å‡∏ì‡∏ë‡πå 
        "{base_kwargs.get('specific_contextual_rule', '‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô')}" ‡πÉ‡∏´‡πâ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡∏ß‡πà‡∏≤ "‡∏ú‡πà‡∏≤‡∏ô"
        
        [REQUIRED]: ‡∏´‡∏≤‡∏Å‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏´‡πâ '‡∏ú‡πà‡∏≤‡∏ô' ‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏∏‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å Context ‡∏°‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 1 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ
        """

        expert_kwargs = base_kwargs.copy()
        expert_kwargs["context"] = f"{context}\n\n{hint_msg}"
        expert_kwargs["sub_criteria_name"] = f"{sub_criteria_name} (Expert Re-assessment)"
        
        # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Confidence ‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏´‡πâ AI ‡πÉ‡∏ä‡πâ‡∏™‡∏°‡∏≤‡∏ò‡∏¥‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
        expert_kwargs["ai_confidence"] = "MAX" 

        try:
            # 3. ‡∏£‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏£‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏á (The Appeal)
            re_eval_result = llm_evaluator_to_use(**expert_kwargs)
            
            re_eval_result["is_expert_evaluated"] = True
            re_eval_result["appeal_status"] = "PENDING"

            if re_eval_result.get("is_passed", False):
                # ‚úÖ ‡∏Å‡∏£‡∏ì‡∏µ‡∏≠‡∏∏‡∏ó‡∏ò‡∏£‡∏ì‡πå‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (Override)
                self.logger.info(f"üõ°Ô∏è [OVERRIDE-SUCCESS] {sub_id} L{level} | Appeal Granted by Expert Reasoning")
                re_eval_result["appeal_status"] = "GRANTED"
                
                # ‡∏ï‡∏Å‡πÅ‡∏ï‡πà‡∏á‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡πÉ‡∏´‡πâ Auditor ‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå‡∏≠‡πà‡∏≤‡∏ô‡πÅ‡∏•‡πâ‡∏ß‡∏õ‡∏£‡∏∞‡∏ó‡∏±‡∏ö‡πÉ‡∏à
                orig_reason = re_eval_result.get('reason', '')
                re_eval_result["reason"] = f"üåü [EXPERT OVERRIDE]: {orig_reason}"
            else:
                # ‚ùå ‡∏Å‡∏£‡∏ì‡∏µ‡∏≠‡∏∏‡∏ó‡∏ò‡∏£‡∏ì‡πå‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (Confirm Failure)
                self.logger.warning(f"‚öñÔ∏è [APPEAL-DENIED] {sub_id} L{level} | Evidence insufficient even under expert review.")
                re_eval_result["appeal_status"] = "DENIED"
                re_eval_result["reason"] = f"Confirmed Fail: {re_eval_result.get('reason', '')}"

            return re_eval_result

        except Exception as e:
            self.logger.error(f"üõë [EXPERT-ERROR] {sub_id} Appeal Process Crashed: {str(e)}")
            return {
                "is_passed": False, 
                "score": 0.0, 
                "reason": f"System error during Expert Appeal: {str(e)}",
                "is_expert_evaluated": True,
                "appeal_status": "ERROR"
            }
        
    def _apply_diversity_filter(self, evidences: List[Dict], level: int) -> List[Dict]:
        """
        [REVISED v2026] ‡∏Å‡∏£‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        - ‡πÉ‡∏ä‡πâ get_actual_score ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥
        - ‡∏õ‡∏£‡∏±‡∏ö Limit ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏°‡∏¥‡∏ï‡∏¥ PDCA (‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏°‡∏¥‡∏ï‡∏¥ D ‡πÅ‡∏•‡∏∞ C)
        """
        if not evidences:
            return []

        # 1. ‡∏à‡∏±‡∏î‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏à‡∏£‡∏¥‡∏á‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏ö
        sorted_evidences = sorted(
            evidences,
            key=lambda x: self.get_actual_score(x),
            reverse=True
        )

        # 2. ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Level ‡∏ï‡πà‡∏≥ (1-2) ‡πÄ‡∏ô‡πâ‡∏ô‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô (P/D)
        if level <= 2:
            return sorted_evidences[:20] 

        # 3. ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Level ‡∏™‡∏π‡∏á (3+) ‡πÉ‡∏ä‡πâ‡∏£‡∏∞‡∏ö‡∏ö Diversity Guard
        diverse_results = []
        file_counts = defaultdict(int)
        per_file_limit = 5  # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 5 chunks ‡∏ï‡πà‡∏≠ 1 ‡πÑ‡∏ü‡∏•‡πå
        max_total = 30      # ‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 30 chunks ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ Prompt ‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ

        for ev in sorted_evidences:
            # ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô Key ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ô‡∏±‡∏ö
            meta = ev.get('metadata', {}) if isinstance(ev, dict) else getattr(ev, 'metadata', {})
            source = meta.get('source_filename') or meta.get('file_name') or ev.get('source') or 'Unknown'
            source_key = os.path.basename(str(source))
            
            file_counts[source_key] += 1

            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ‡∏¢‡∏±‡∏á‡∏™‡πà‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡πÇ‡∏Ñ‡∏ß‡∏ï‡∏≤ ‡πÉ‡∏´‡πâ‡∏£‡∏±‡∏ö‡πÄ‡∏û‡∏¥‡πà‡∏°
            if file_counts[source_key] <= per_file_limit:
                diverse_results.append(ev)

            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏î‡πâ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏ß‡∏°‡∏ï‡∏≤‡∏°‡πÄ‡∏õ‡πâ‡∏≤‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏´‡πâ‡∏´‡∏¢‡∏∏‡∏î
            if len(diverse_results) >= max_total:
                break

        return diverse_results

    def enhance_query_for_statement(
        self,
        statement_text: str,
        sub_id: str,
        statement_id: str,
        level: int,
        focus_hint: str = "",
    ) -> List[str]:
        """
        [REVISED STRATEGIC v2026.2.12 ‚Äì Balanced & Required Phase Optimized]
        - Follow required_phase ‡∏à‡∏≤‡∏Å contextual_rules ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏Ñ‡∏£‡πà‡∏á‡∏Ñ‡∏£‡∏±‡∏î (keywords + fallback + bias)
        - Priority: query_synonyms (json) > specific_contextual_rule (‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥) > fallback PDCA
        - Keywords ‡∏à‡∏≤‡∏Å _enabler_defaults + required_phase (‡∏™‡∏°‡∏î‡∏∏‡∏•‡∏ó‡∏∏‡∏Å phase)
        - General phase-focused query ‡∏ï‡∏≤‡∏° required_phase (‡πÑ‡∏°‡πà hard-code ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ 1.2)
        - Negative ‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏° level (L1‚ÄìL3 ‡∏•‡∏ö Plan ‡πÄ‡∏¢‡∏≠‡∏∞, L4+ ‡∏•‡∏ö‡∏ô‡πâ‡∏≠‡∏¢‡∏•‡∏á)
        - log ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô + post-process ‡πÄ‡∏î‡∏¥‡∏°
        """
        import random
        from typing import List

        logger = logging.getLogger(__name__)

        # 1. Anchors
        enabler_id = getattr(self.config, 'enabler', 'Unknown').upper()
        tenant_name = getattr(self.config, 'tenant', 'Unknown').upper()
        id_anchor = f"{enabler_id} {sub_id}"

        # ‡∏î‡∏∂‡∏á required_phase (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‚Äì ‡πÉ‡∏ä‡πâ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á)
        require_phases = self.get_rule_content(sub_id, level, "require_phase") or []
        require_str = ", ".join(require_phases) if require_phases else "P,D"

        # 2. Keywords ‡∏à‡∏≤‡∏Å _enabler_defaults + required_phase (‡∏™‡∏°‡∏î‡∏∏‡∏•‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏à‡∏£‡∏¥‡∏á)
        raw_kws = []
        must_list = self.get_rule_content(sub_id, level, "must_include_keywords")
        if isinstance(must_list, list):
            raw_kws.extend(must_list)

        phase_keywords_map = {
            "P": "plan_keywords",
            "D": "do_keywords",
            "C": "check_keywords",
            "A": "act_keywords"
        }

        # ‡∏î‡∏∂‡∏á keywords ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ phase ‡∏ó‡∏µ‡πà required
        for phase in require_phases:
            kw_key = phase_keywords_map.get(phase)
            if kw_key:
                raw_kws.extend(self.get_rule_content(sub_id, level, kw_key) or [])

        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ require_phase ‚Üí fallback ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏ï‡∏≤‡∏° level
        if not require_phases:
            if level <= 3:
                raw_kws.extend(self.get_rule_content(sub_id, 1, "plan_keywords") or [])
                raw_kws.extend(self.get_rule_content(sub_id, 2, "do_keywords") or [])
            elif level == 4:
                raw_kws.extend(self.get_rule_content(sub_id, 2, "do_keywords") or [])
                raw_kws.extend(self.get_rule_content(sub_id, 3, "check_keywords") or [])
            else:
                raw_kws.extend(self.get_rule_content(sub_id, 2, "do_keywords") or [])
                raw_kws.extend(self.get_rule_content(sub_id, 3, "check_keywords") or [])
                raw_kws.extend(self.get_rule_content(sub_id, 4, "act_keywords") or [])

        clean_kws = sorted(set(str(k).strip() for k in raw_kws if k))
        keywords_str = " ".join(clean_kws[:5])
        short_keywords = " ".join(clean_kws[:3])

        clean_stmt = statement_text.split("‡πÄ‡∏ä‡πà‡∏ô", 1)[0].strip()
        clean_stmt = re.sub(r'[^\w\s]', '', clean_stmt)[:70]

        queries: List[str] = []

        # 3. Queries ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô (‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏° level)
        queries.append(f"{id_anchor} {clean_stmt} {keywords_str}")
        queries.append(f"{id_anchor} {clean_stmt}")

        if level <= 3:
            queries.append(f"{tenant_name} ‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏® ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á ‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö {id_anchor} {short_keywords}")
        else:
            queries.append(f"{tenant_name} ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏• KPI ‡∏†‡∏≤‡∏Ñ‡∏ú‡∏ô‡∏ß‡∏Å {id_anchor} {short_keywords}")

        # Negative ‡∏ï‡∏≤‡∏° level (L1‚ÄìL3 ‡∏•‡∏ö Plan ‡πÄ‡∏¢‡∏≠‡∏∞, L4+ ‡∏•‡∏ö‡∏ô‡πâ‡∏≠‡∏¢‡∏•‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ C/A ‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤)
        if level <= 3:
            queries.append(f"{id_anchor} (‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£ OR ‡∏°‡∏∏‡πà‡∏á‡∏°‡∏±‡πà‡∏ô OR ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á OR ‡∏Ç‡∏±‡∏ö‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô) -‡πÅ‡∏ú‡∏ô -‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢ -‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå")
        else:
            queries.append(f"{id_anchor} (‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏• OR ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô OR ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏° OR ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á OR ‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°) -‡πÅ‡∏ú‡∏ô -‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢")

        # 4. Priority 1: query_synonyms ‡∏à‡∏≤‡∏Å json
        query_syn = self.get_rule_content(sub_id, level, "query_synonyms") or ""
        if query_syn:
            queries.append(f"{id_anchor} {query_syn} {short_keywords}")
            logger.info(f"[QUERY_SYNONYMS] Used {len(query_syn.split())} words from json for {sub_id} L{level}: {query_syn[:80]}...")

        # 5. Priority 2: Rule-based synonyms ‡∏à‡∏≤‡∏Å specific_contextual_rule
        specific_rule = ""  # default
        if not query_syn:
            specific_rule = self.get_rule_content(sub_id, level, "specific_contextual_rule") or ""
            if specific_rule:
                rule_words = [w.strip() for w in specific_rule.split() if len(w.strip()) >= 4]
                unique_rule_words = list(dict.fromkeys(rule_words))[:8]
                rule_synonyms = " ".join(unique_rule_words)
                if rule_synonyms:
                    queries.append(f"{id_anchor} {rule_synonyms} {short_keywords}")
                    logger.info(f"[RULE SYNONYMS] Used {len(unique_rule_words)} words from specific_contextual_rule for {sub_id} L{level}")

        # 6. Priority 3: Fallback PDCA synonyms (‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏° required_phase)
        fallback_synonyms = {
            "P": "‡πÅ‡∏ú‡∏ô ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ ‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢ ‡πÄ‡∏à‡∏ï‡∏ô‡∏≤‡∏£‡∏°‡∏ì‡πå ‡∏°‡∏∏‡πà‡∏á‡∏°‡∏±‡πà‡∏ô ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á",
            "D": "‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥ ‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£ ‡∏Ç‡∏±‡∏ö‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô ‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏° ‡∏≠‡∏ö‡∏£‡∏° ‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°",
            "C": "‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏° ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏• KPI ‡∏ß‡∏±‡∏î‡∏ú‡∏• ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•",
            "A": "‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á ‡∏û‡∏±‡∏í‡∏ô‡∏≤ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç ‡∏ï‡πà‡∏≠‡∏¢‡∏≠‡∏î ‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏° ‡∏¢‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö Best Practice"
        }

        for phase in require_phases:
            fallback = fallback_synonyms.get(phase, "")
            if fallback:
                queries.append(f"{id_anchor} {fallback} {short_keywords}")
                logger.info(f"[FALLBACK {phase}] Used for {sub_id} L{level}")

        # 7. General phase-focused ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö L1‚ÄìL3 (‡πÄ‡∏â‡∏û‡∏≤‡∏∞ KM + ‡∏ï‡∏≤‡∏° required_phase)
        if level <= 3 and enabler_id == "KM" and "D" in require_phases:
            queries.append(f"{id_anchor} (‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏° OR ‡∏≠‡∏ö‡∏£‡∏° OR ‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏° OR ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á OR ‡∏°‡∏∏‡πà‡∏á‡∏°‡∏±‡πà‡∏ô OR ‡∏™‡∏ô‡∏±‡∏ö‡∏™‡∏ô‡∏∏‡∏ô OR ‡∏Ç‡∏±‡∏ö‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô) -‡πÅ‡∏ú‡∏ô -‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢")

        # 8. Advanced/Focus hint (L4+)
        if level >= 4 or focus_hint:
            adv = "‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏° Best Practice Lesson Learned"
            queries.append(f"{id_anchor} {adv} {focus_hint or ''}")

        # Post-process
        final_queries = []
        seen = set()
        for q in queries:
            words = q.split()
            trunc_len = random.randint(22, 28)
            q_trunc = " ".join(words[:trunc_len])
            q_norm = " ".join(words[:18])
            if q_trunc and q_norm not in seen:
                final_queries.append(q_trunc)
                seen.add(q_norm)

        random.shuffle(final_queries)

        logger.info(f"üöÄ [Query Gen] {sub_id} L{level} | Generated {len(final_queries)} diverse queries "
                    f"(required phases: {require_str}) "
                    f"(used query_synonyms: {bool(query_syn)}) "
                    f"(used rule synonyms: {bool(specific_rule and not query_syn)})")
        return final_queries[:7]
    
    def _get_semantic_tag(self, text: str, sub_id: str, level: int, filename: str = "") -> str:
        """
        [ULTIMATE REVISE v2026.25 ‚Äì Required Phase Aware]
        - Follow required_phase ‡∏à‡∏≤‡∏Å contextual_rules ‡∏Ç‡∏≠‡∏á enabler ‡∏ô‡∏±‡πâ‡∏ô ‡πÜ
        - Prompt ‡∏™‡πà‡∏á require_phase ‡πÉ‡∏´‡πâ LLM ‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡∏ï‡πâ‡∏≠‡∏á prefer phase ‡πÑ‡∏´‡∏ô
        - Fallback ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å phase ‡∏à‡∏≤‡∏Å require_phase (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢ ‚Üí random ‡∏ï‡∏≤‡∏° priority)
        - Heuristic ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏ó‡∏∏‡∏Å phase + JSON Clean ‡∏ó‡∏ô‡∏ó‡∏≤‡∏ô
        """
        tenant = getattr(self.config, 'tenant', 'Unknown').upper()
        enabler = getattr(self.config, 'enabler', 'Unknown').upper()
        text_lower = text.lower().strip()

        # ‡∏î‡∏∂‡∏á required_phase ‡∏à‡∏≤‡∏Å rules (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î!)
        require_phases = self.get_rule_content(sub_id, level, "require_phase") or []
        require_str = ", ".join(require_phases) if require_phases else "P,D,C,A"

        # 1. Enhanced Heuristic (‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏ó‡∏∏‡∏Å phase ‚Äì ‡∏Ç‡∏¢‡∏≤‡∏¢‡∏Ñ‡∏≥‡∏à‡∏≤‡∏Å log + defaults)
        if any(k in text_lower for k in [
            "‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢", "‡πÅ‡∏ú‡∏ô", "‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå", "‡∏°‡∏ï‡∏¥", "‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á", "‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢", "‡πÄ‡∏à‡∏ï‡∏ô‡∏≤‡∏£‡∏°‡∏ì‡πå",
            "‡∏ß‡∏¥‡∏™‡∏±‡∏¢‡∏ó‡∏±‡∏®‡∏ô‡πå", "master plan", "roadmap", "km policy", "‡∏à‡∏±‡∏î‡∏ó‡∏≥‡πÅ‡∏ú‡∏ô"
        ]):
            return "P"

        if any(k in text_lower for k in [
            "‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£", "‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥", "‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°", "‡∏≠‡∏ö‡∏£‡∏°", "‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°", "‡∏à‡∏±‡∏î‡∏ó‡∏≥", "‡∏†‡∏≤‡∏û‡∏ñ‡πà‡∏≤‡∏¢",
            "‡∏•‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà", "‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏ú‡∏•", "‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•", "‡∏Ç‡∏±‡∏ö‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô", "‡∏•‡∏á‡∏ô‡∏≤‡∏°", "‡∏°‡∏ï‡∏¥‡∏ö‡∏≠‡∏£‡πå‡∏î",
            "‡πÄ‡∏´‡πá‡∏ô‡∏ä‡∏≠‡∏ö", "deployment", "‡∏à‡∏±‡∏î‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°", "‡∏ñ‡πà‡∏≤‡∏¢‡∏ó‡∏≠‡∏î", "‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£", "‡∏Ñ‡∏ì‡∏∞‡∏ó‡∏≥‡∏á‡∏≤‡∏ô"
        ]):
            return "D"

        if any(k in text_lower for k in [
            "‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏•", "‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô", "‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°", "‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î", "kpi", "‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå", "‡∏™‡∏≥‡∏£‡∏ß‡∏à",
            "‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥", "‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô", "monitoring", "review", "audit", "benchmarking",
            "feedback", "‡πÅ‡∏ö‡∏ö‡∏™‡∏≥‡∏£‡∏ß‡∏à", "‡∏ß‡∏±‡∏î‡∏ú‡∏•"
        ]):
            return "C"

        if any(k in text_lower for k in [
            "‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á", "‡∏û‡∏±‡∏í‡∏ô‡∏≤", "‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç", "‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô", "lesson learned", "‡∏ï‡πà‡∏≠‡∏¢‡∏≠‡∏î",
            "‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°", "‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô", "‡∏¢‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö", "agile", "‡∏ô‡∏≥‡πÑ‡∏õ‡πÉ‡∏ä‡πâ", "‡∏ó‡∏ö‡∏ó‡∏ß‡∏ô",
            "feedback loop"
        ]):
            return "A"

        # 2. LLM tagging (prompt ‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏° required_phase)
        system_prompt = (
            f"Auditor for {tenant} {enabler}. Classify text to ONE PDCA tag only. "
            "P=Plan (‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢/‡πÅ‡∏ú‡∏ô/‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢), D=Do (‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°/‡∏≠‡∏ö‡∏£‡∏°/‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°/‡∏†‡∏≤‡∏û‡∏ñ‡πà‡∏≤‡∏¢/‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£), "
            "C=Check (‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô/KPI/‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô/‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°), A=Act (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á/‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô/‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°). "
            f"Required phases for this level: {require_str}. Prefer one of these phases if ambiguous. "
            "JSON only: {\"tag\":\"P/D/C/A/Other\",\"reason\":\"‡∏™‡∏±‡πâ‡∏ô ‡πÜ ‡πÑ‡∏ó‡∏¢\"}"
        )

        user_prompt = f"File: {filename}\nText (first 500 chars):\n{text[:500]}\n‚Üí JSON"

        try:
            response = self.llm.invoke(system_prompt + "\n" + user_prompt)
            # Clean ‡∏ó‡∏ô‡∏ó‡∏≤‡∏ô‡∏™‡∏∏‡∏î
            cleaned = response.strip()
            cleaned = re.sub(r'^.*?\{', '{', cleaned, flags=re.DOTALL | re.IGNORECASE)
            cleaned = re.sub(r'\}.*?$', '}', cleaned, flags=re.DOTALL | re.IGNORECASE)
            cleaned = cleaned.replace("```json", "").replace("```", "").replace("\n", " ").strip()
            cleaned = re.sub(r',\s*([}\]])', r'\1', cleaned)
            data = json.loads(cleaned)
            if isinstance(data, list) and data:
                data = data[0]
            tag = str(data.get("tag", "Other")).strip().upper()
            if tag in {"P", "D", "C", "A"}:
                reason = data.get('reason', '')
                self.logger.debug(f"[TAG LLM] {tag} | {reason} | {filename[:30]}")
                return tag
        except Exception as e:
            self.logger.warning(f"[TAG LLM FAIL] {sub_id} L{level} {filename[:30]} ‚Üí {str(e)} | Raw: {response[:100]}...")

        # 3. Final fallback (‡∏ï‡∏≤‡∏° required_phase ‚Äì ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢ phase ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏≤‡∏° priority ‡∏Ç‡∏≠‡∏á level)
        if require_phases:
            # Priority: ‡πÄ‡∏ô‡πâ‡∏ô phase ‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ (‡πÄ‡∏ä‡πà‡∏ô L1 ‚Üí D ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ, L4 ‚Üí C, L5 ‚Üí A)
            for phase in require_phases:
                if phase in ["P", "D", "C", "A"]:
                    self.logger.debug(f"[TAG FALLBACK] L{level} ‚Üí {phase} (from required phases)")
                    return phase
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ require_phase ‚Üí fallback ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏ï‡∏≤‡∏° level
        if level <= 3:
            return "D"
        elif level == 4:
            return "C"
        else:
            return "A"
    
    def _build_pdca_context(self, blocks: Dict[str, str]) -> str:
        """
        [REVISED v2026.2]
        - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏ì‡∏µ content ‡∏ß‡πà‡∏≤‡∏á/‡∏™‡∏±‡πâ‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô
        - ‡πÉ‡∏ä‡πâ XML-like ‡πÅ‡∏ï‡πà‡∏ï‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏≠‡∏Å
        """
        tags = ["Plan", "Do", "Check", "Act", "Other"]
        parts = []

        for t in tags:
            content = blocks.get(t, "").strip()
            if not content or len(content) < 10:
                content = "[‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô]"
            # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô prompt ‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô
            content = content[:800]
            parts.append(f"<{t}>{content}</{t}>")

        return "\n".join(parts)

    def _log_pdca_status(self, sub_id, name, level, blocks, req_phases, sources_count, score, conf_level, **kwargs):
        """
        [THE AUDITOR DASHBOARD v2026.1.18]
        - Sync ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á Tagging (RAG) ‡πÅ‡∏•‡∏∞ Extraction (LLM)
        """
        try:
            display_name = (str(name)[:50] + "...") if len(str(name)) > 50 else str(name)
            tagging_result = kwargs.get('tagging_result') or {}
            is_safety_pass = kwargs.get('is_safety_pass', False)

            status_parts = []
            extract_parts = []
            
            # Mapping ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö LLM Result Key
            mapping = [("Extraction_P", "P"), ("Extraction_D", "D"), ("Extraction_C", "C"), ("Extraction_A", "A")]

            for full_key, short in mapping:
                evidence_count = tagging_result.get(short, 0)
                content = str(blocks.get(full_key, "")).strip()
                
                # AI ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå‡∏ß‡πà‡∏≤‡∏á
                ai_found = bool(content and content not in ["-", "N/A", "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"])
                
                # ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô Icon: ‡∏ú‡πà‡∏≤‡∏ô‡∏ñ‡πâ‡∏≤ AI ‡πÄ‡∏à‡∏≠ OR RAG ‡πÄ‡∏à‡∏≠ OR Force Pass
                if ai_found or evidence_count > 0 or (is_safety_pass and short in req_phases):
                    icon = "‚úÖ"
                else:
                    icon = "‚ùå"
                
                status_parts.append(f"{short}:{icon}({evidence_count})")
                
                if ai_found:
                    snippet = content[:45].replace('\n', ' ') + "..."
                    extract_parts.append(f"[{short}: {snippet}]")

            safety_tag = " üõ°Ô∏è[FORCE-PASS]" if is_safety_pass else ""
            
            # Log 1: ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ PDCA
            self.logger.info(
                f"üìä [PDCA-STATUS] {sub_id} L{level} | {display_name} | "
                f"Req:{','.join(req_phases)} | Res: {' '.join(status_parts)}{safety_tag} | "
                f"Docs:{sources_count} | Score:{score:.2f} | Conf:{conf_level}"
            )
            
            # Log 2: ‡∏Ñ‡∏≥‡∏™‡∏Å‡∏±‡∏î‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô (Extraction Trace)
            if extract_parts:
                self.logger.info(f"üîç [EXTRACT-TRACE] {sub_id} L{level} | {' | '.join(extract_parts[:2])}")

        except Exception as e:
            self.logger.error(f"‚ùå Error in _log_pdca_status: {str(e)}")   

    def _summarize_evidence_list_short(self, evidences: list, max_sentences: int = 3) -> str:
        """
        [REVISED v2026.SUMMARY.4]
        - ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô Method ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ self.logger
        - ‡πÄ‡∏ô‡πâ‡∏ô‡∏î‡∏∂‡∏á Source ‡πÅ‡∏•‡∏∞ Page ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á Audit Traceability
        - ‡∏õ‡∏£‡∏±‡∏ö Formatting ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô Bullet points (LLM ‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢‡∏Å‡∏ß‡πà‡∏≤ Pipe '|')
        """
        if not evidences:
            return "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°"
        
        parts = []
        # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û (‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤) ‡πÅ‡∏•‡∏∞‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ï‡∏≤‡∏° max_sentences
        valid_evidences = [
            ev for ev in evidences 
            if isinstance(ev, dict) and (ev.get("text") or ev.get("content", "")).strip()
        ]
        
        # ‡∏ï‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ (‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏à‡∏£‡∏¥‡∏á)
        target_count = max(1, min(len(valid_evidences), max_sentences))
        
        for ev in valid_evidences[:target_count]:
            # 1. ‡∏™‡∏Å‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏≤ (Source Mapping)
            filename = (ev.get("file_name") or ev.get("source") or 
                        ev.get("source_filename") or "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå")
            page = ev.get("page", "-")
            
            # 2. ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ (Data Cleaning)
            raw_text = ev.get("text") or ev.get("content") or ""
            # ‡∏•‡∏ö‡∏Å‡∏≤‡∏£‡∏Ç‡∏∂‡πâ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏î‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡πà‡∏ß‡∏ô‡∏ï‡πâ‡∏ô
            clean_text = " ".join(raw_text.split()).strip()
            text_preview = clean_text[:150] # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡πÄ‡∏õ‡πá‡∏ô 150 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡∏Ç‡∏∂‡πâ‡∏ô
            
            # 3. ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏£‡πà‡∏≤‡∏á (Formatting)
            if text_preview:
                parts.append(f"‚Ä¢ [{filename}, ‡∏´‡∏ô‡πâ‡∏≤ {page}]: \"{text_preview}...\"")
            else:
                parts.append(f"‚Ä¢ [{filename}, ‡∏´‡∏ô‡πâ‡∏≤ {page}]: (‡∏û‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÑ‡∏î‡πâ)")

        # ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡∏Ç‡∏∂‡πâ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ AI ‡πÅ‡∏¢‡∏Å‡πÅ‡∏¢‡∏∞‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ä‡∏¥‡πâ‡∏ô‡πÑ‡∏î‡πâ‡∏á‡πà‡∏≤‡∏¢
        return "\n".join(parts) 

    def _build_multichannel_context_for_level( # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô Private Method
        self, # ‡πÄ‡∏û‡∏¥‡πà‡∏° self
        level: int,
        top_evidences: List[Dict[str, Any]],
        previous_levels_map: Optional[Dict[str, Any]] = None,
        previous_levels_evidence: Optional[List[Dict[str, Any]]] = None,
        max_main_context_tokens: int = 3000, 
        max_summary_sentences: int = 4,
        max_context_length: Optional[int] = None, 
        **kwargs
    ) -> Dict[str, Any]:
        """
        [ULTIMATE OPTIMIZED v2026.8 - REFACTORED AS CLASS METHOD]
        """
        K_MAIN = 5
        # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å Config ‡πÉ‡∏ô Class ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢‡∏ñ‡πâ‡∏≤‡∏°‡∏µ ‡πÄ‡∏ä‡πà‡∏ô self.config.l1_threshold
        MIN_RELEVANCE_FOR_AUX = 0.15 if level == 1 else 0.4 

        # 1. Baseline Summary
        baseline_evidence = previous_levels_evidence or []
        if previous_levels_map:
            for lvl_ev in previous_levels_map.values():
                baseline_evidence.extend(lvl_ev)

        # Normalize list (‡∏ï‡∏≤‡∏° Logic ‡πÄ‡∏î‡∏¥‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡∏ö‡∏±‡πä‡∏Å Slice ‡πÅ‡∏•‡πâ‡∏ß)
        baseline_evidence_list = []
        if isinstance(baseline_evidence, list):
            baseline_evidence_list = baseline_evidence
        elif isinstance(baseline_evidence, dict):
            baseline_evidence_list = list(baseline_evidence.values())
        
        summarizable_baseline = [
            item for item in baseline_evidence_list[:40] 
            if isinstance(item, dict) and (item.get("text") or item.get("content", "")).strip()
        ]

        if not summarizable_baseline:
            baseline_summary = "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ (‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏ó‡∏µ‡πà Level 1)"
        else:
            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ú‡πà‡∏≤‡∏ô self
            baseline_summary = self._summarize_evidence_list_short(
                summarizable_baseline,
                max_sentences=max_summary_sentences
            )

        # 2. Direct + Aux Separation
        direct, aux_candidates = [], []

        for idx, ev in enumerate(top_evidences[:40], 1):
            if not isinstance(ev, dict): continue

            tag = (ev.get("pdca_tag") or ev.get("PDCA") or "Other").upper()
            relevance = ev.get("rerank_score") or ev.get("score", 0.0)
            text_preview = (ev.get('text', '')[:80] + "...") if ev.get('text') else "[No text]"

            # ‡πÉ‡∏ä‡πâ self.logger ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢
            self.logger.debug(f"[TAG-CHECK L{level} #{idx}] Rel: {relevance:.3f} | Tag: {tag} | Preview: {text_preview}")

            if tag in {"P", "PLAN", "D", "DO", "C", "CHECK", "A", "ACT"}:
                direct.append(ev)
            elif relevance >= MIN_RELEVANCE_FOR_AUX:
                aux_candidates.append(ev)

        # 3. Logic ‡∏Å‡∏≤‡∏£‡∏¢‡πâ‡∏≤‡∏¢ Aux ‡πÅ‡∏•‡∏∞ Fallback (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏° ‡πÅ‡∏ï‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô self.logger)
        if len(direct) < K_MAIN:
            need = K_MAIN - len(direct)
            moved = aux_candidates[:need]
            direct.extend(moved)
            aux_candidates = aux_candidates[need:]
            self.logger.info(f"[DIRECT-FILL] Moved {need} aux chunks to direct (total direct: {len(direct)})")

        if level == 1 and len(direct) == 0 and top_evidences:
            need = min(K_MAIN, len(top_evidences))
            forced_chunks = sorted(top_evidences, key=lambda e: e.get("rerank_score", 0) or e.get("score", 0), reverse=True)[:need]
            direct.extend(forced_chunks)
            self.logger.warning(f"[L1-ULTRA-FALLBACK] No PDCA tag at all ‚Üí Forced top {need} chunks to direct")

        # 5. ‡∏™‡∏£‡πâ‡∏≤‡∏á aux_summary (‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ú‡πà‡∏≤‡∏ô self)
        aux_summary = self._summarize_evidence_list_short(aux_candidates, max_sentences=3) if aux_candidates else \
            "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏£‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠"

        # 6. Return ‡∏û‡∏£‡πâ‡∏≠‡∏° Debug Meta
        self.logger.info(f"Context L{level} ‚Üí Direct:{len(direct)} | Aux:{len(aux_candidates)} | Baseline:{len(summarizable_baseline)}")

        return {
            "baseline_summary": baseline_summary,
            "direct_context": "", 
            "aux_summary": aux_summary,
            "debug_meta": {
                "level": level,
                "direct_count": len(direct),
                "aux_count": len(aux_candidates),
                "top_relevance": max((ev.get("rerank_score", 0) for ev in top_evidences), default=0)
            },
        }

    def _perform_adaptive_retrieval(self, sub_id: str, level: int, stmt: str, vectorstore_manager: Any):
        """
        [ULTIMATE RETRIEVAL v2026.1.22 ‚Äì FULL & STABLE]
        - Early Exit: New >= 5 + Max >= 0.88 + >= 2 queries
        - Fallback: Multi-stage (Phase-based & Minimal) ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà
        - Smart Priority Floor: 0.70 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Priority Chunks
        """
        import hashlib
        
        # 0. Configuration & Parameters
        MAX_TOTAL_CHUNKS = 45
        MIN_QUALITY_FOR_EXIT = 0.88
        MIN_NEW_FOR_EXIT = 5
        MIN_QUERIES_FOR_EXIT = 2
        FORCE_EXTRA_LOOP_THRESHOLD = 3
        
        # ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠ Tenant ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô NameError 'tenant_name'
        current_tenant = getattr(self.config, 'tenant', '‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£')
        
        # 1. Priority Docs (‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏ö‡∏ö Mapping/History)
        mapped_ids, priority_docs = self._get_mapped_uuids_and_priority_chunks(
            sub_id, level, stmt, vectorstore_manager
        ) or (set(), [])

        candidates = []
        final_max_rerank = 0.0
        all_scores = []
        used_queries = 0
        forced_continue = False
        new_counts = []
        priority_uuids = {p.get('chunk_uuid') for p in priority_docs if p and p.get('chunk_uuid')}

        if priority_docs:
            p_scores = [self.get_actual_score(p) for p in priority_docs if p]
            if p_scores:
                final_max_rerank = max(p_scores)
                all_scores.extend(p_scores)
            self.logger.info(f"üìå [PRIORITY] {sub_id} L{level} | {len(priority_docs)} chunks | Max: {final_max_rerank:.4f}")
        else:
            self.logger.warning(f"[PRIORITY] No priority chunks found for {sub_id}")

        # 2. Adaptive Search Loop
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Queries ‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏à‡∏≤‡∏Å Statement
        queries = self.enhance_query_for_statement(stmt, sub_id, f"{sub_id}.L{level}", level)
        queries = queries[:5]  # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î 5 queries ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£

        for i, q in enumerate(queries):
            used_queries += 1
            res = self.rag_retriever(
                query=q, doc_type=self.doc_type, sub_id=sub_id, level=level,
                vectorstore_manager=vectorstore_manager, stable_doc_ids=mapped_ids
            ) or {"top_evidences": []}

            loop_docs = res.get("top_evidences", [])
            if not loop_docs:
                self.logger.debug(f"[LOOP {i+1}] No docs returned for query: {q[:50]}...")
                new_counts.append(0)
                continue

            loop_scores = [self.get_actual_score(d) for d in loop_docs if d]
            if not loop_scores:
                new_counts.append(0)
                continue

            current_max = max(loop_scores)
            final_max_rerank = max(final_max_rerank, current_max)
            all_scores.extend(loop_scores)

            # Deduplication: ‡∏ï‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ö Priority
            new_docs = [d for d in loop_docs if d.get('chunk_uuid') not in priority_uuids]
            
            # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï UUIDs ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏ã‡πâ‡∏≥‡πÉ‡∏ô Loop ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
            for d in new_docs:
                if d.get('chunk_uuid'):
                    priority_uuids.add(d.get('chunk_uuid'))

            candidates.extend(new_docs)
            new_counts.append(len(new_docs))

            log_msg = f"üîç [LOOP {i+1}] Query: {q[:50]}... | New: {len(new_docs)} | Max: {current_max:.4f}"
            self.logger.info(log_msg)

            # Force continue Logic
            if i == 0 and len(new_docs) == 0:
                forced_continue = True
                self.logger.info("[FORCE-CONTINUE] Loop 1 New=0 ‚Üí ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏£‡∏±‡∏ô loop 2 ‡∏ï‡πà‡∏≠")

            # Enhanced Early Exit Check
            total_current = len(priority_docs) + len(candidates)
            if (final_max_rerank >= MIN_QUALITY_FOR_EXIT and
                total_current >= 12 and
                len(new_docs) >= MIN_NEW_FOR_EXIT and
                used_queries >= MIN_QUERIES_FOR_EXIT and
                not forced_continue):
                self.logger.info(f"üéØ [SMART EXIT] Loop {i+1} ‡∏ö‡∏£‡∏£‡∏•‡∏∏‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì")
                break

            # Force extra loop ‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏¢‡∏±‡∏á‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
            if i == 1 and sum(new_counts[:2]) < FORCE_EXTRA_LOOP_THRESHOLD:
                forced_continue = True
                self.logger.info(f"[FORCE EXTRA] New docs ({sum(new_counts[:2])}) ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏Å‡∏ì‡∏ë‡πå ‚Üí ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö loop 3")
            else:
                forced_continue = False

        # 3. Fallback Mechanism (‡∏Å‡∏£‡∏ì‡∏µ‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢)
        if used_queries >= 3 and sum(new_counts) == 0:
            self.logger.warning("[FALLBACK] ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô 0 ‡πÉ‡∏ô 3 ‡∏•‡∏π‡∏õ‡∏´‡∏•‡∏±‡∏Å ‚Üí ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÅ‡∏ú‡∏ô‡∏™‡∏≥‡∏£‡∏≠‡∏á")
            
            # Fallback 1: Phase-based (P, D, C, A)
            require_phases = self.get_rule_content(sub_id, level, "require_phase") or ["P", "D"]
            fallback_q = f"{sub_id} {' OR '.join(require_phases)} {current_tenant}"
            
            res = self.rag_retriever(
                query=fallback_q, doc_type=self.doc_type, sub_id=sub_id, level=level,
                vectorstore_manager=vectorstore_manager, stable_doc_ids=mapped_ids
            ) or {"top_evidences": []}
            
            fb_docs = res.get("top_evidences", [])
            fb_new_count = 0
            if fb_docs:
                fb_new = [d for d in fb_docs if d.get('chunk_uuid') not in priority_uuids]
                candidates.extend(fb_new)
                fb_new_count = len(fb_new)
                self.logger.info(f"[FALLBACK-PHASE] ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÑ‡∏î‡πâ {fb_new_count} chunks")

            # Fallback 2: Minimal query (‡∏Å‡∏ß‡∏≤‡∏î‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î)
            if fb_new_count == 0:
                minimal_q = f"{sub_id} {current_tenant}"
                res = self.rag_retriever(
                    query=minimal_q, doc_type=self.doc_type, sub_id=sub_id, level=level,
                    vectorstore_manager=vectorstore_manager, stable_doc_ids=mapped_ids
                ) or {"top_evidences": []}
                min_docs = res.get("top_evidences", [])
                if min_docs:
                    min_new = [d for d in min_docs if d.get('chunk_uuid') not in priority_uuids]
                    candidates.extend(min_new)
                    self.logger.info(f"[FALLBACK-MINIMAL] ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÑ‡∏î‡πâ {len(min_new)} chunks")

        # 4. Deduplication & Final Hard Cap
        unique_docs = {}
        for doc in (priority_docs + candidates):
            if not doc: continue
            uid = doc.get('chunk_uuid')
            if not uid:
                content = doc.get('page_content') or doc.get('text') or str(doc)
                uid = hashlib.sha256(content.encode('utf-8')).hexdigest()
            
            if uid not in unique_docs:
                unique_docs[uid] = doc

        final_docs = list(unique_docs.values())

        # ‡∏ï‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ñ‡πâ‡∏≤‡πÄ‡∏Å‡∏¥‡∏ô Hard Cap (‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î)
        if len(final_docs) > MAX_TOTAL_CHUNKS:
            final_docs = sorted(final_docs, key=lambda x: self.get_actual_score(x), reverse=True)[:MAX_TOTAL_CHUNKS]
            self.logger.warning(f"[HARD CAP] ‡∏ï‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Chunks ‡∏à‡∏≤‡∏Å {len(unique_docs)} ‡πÄ‡∏´‡∏•‡∏∑‡∏≠ {MAX_TOTAL_CHUNKS}")

        self._normalize_evidence_metadata(final_docs)

        # 5. Safety Net Floor & Priority Tagging
        for p in priority_docs:
            if isinstance(p, dict):
                p['is_priority'] = True
                # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ó‡∏µ‡πà 0.70 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ú‡πà‡∏≤‡∏ô Filter ‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
                p['rerank_score'] = max(self.get_actual_score(p), 0.70)

        # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£ Retrieval ‡πÄ‡∏Ç‡πâ‡∏≤ Log
        self.logger.info(f"üìä [RETRIEVAL SUMMARY] {sub_id} L{level} | Final Chunks: {len(final_docs)} | Max Rerank: {final_max_rerank:.4f} | Loops: {used_queries}")

        return final_docs, final_max_rerank
    
    # ------------------------------------------------------------------------------------------
    # [CRITICAL: SYSTEM CORE ARCHITECTURE v2026.1.19 - FULL TRACEABILITY]
    # üö© ‡∏Ñ‡∏≥‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô: ‡∏´‡πâ‡∏≤‡∏°‡∏ï‡∏±‡∏î‡∏ï‡πà‡∏≠‡∏´‡∏£‡∏∑‡∏≠‡∏•‡∏î‡∏ó‡∏≠‡∏ô‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô 10 ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≤‡∏î ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô Audit Traceability
    # 1. Dependency Gate: (_is_previous_level_passed) ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç Level Skip
    # 2. Baseline Hydration: (_collect_previous_level_evidences) ‡∏î‡∏∂‡∏á‡∏°‡∏£‡∏î‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤
    # 3. Adaptive Retrieval: (_perform_adaptive_retrieval) Multi-loop RAG ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏∂‡∏Å
    # 4. Quality Gate & Diversity: (_apply_diversity_filter) ‡∏Å‡∏£‡∏≠‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô‡∏Ç‡∏≠‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
    # 5. Neighbor Expansion: (_expand_context_with_neighbor_pages) ‡πÄ‡∏ï‡∏¥‡∏°‡πÄ‡∏ï‡πá‡∏°‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡∏Ç‡πâ‡∏≤‡∏á‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á
    # 6. Multichannel Context: (_build_multichannel_context_for_level) ‡πÅ‡∏¢‡∏Å‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà P,D,C,A & Baseline
    # 7. PDCA Blocks Construction: (_get_pdca_blocks_from_evidences & _build_pdca_context)
    # 8. Dual-Round Evaluation: (evaluate_with_llm -> _build_audit_result_object) ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô & ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏£‡πà‡∏≤‡∏á
    # 9. Expert Safety Net: (High Rerank Boost) ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ç‡∏±‡∏î‡πÅ‡∏¢‡πâ‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á Score ‡πÅ‡∏•‡∏∞ Evidence
    # 10. Persistence & Traceability: (_save_level_evidences & _log_pdca_status) ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÅ‡∏•‡∏∞‡πÅ‡∏™‡∏î‡∏á Log
    # ------------------------------------------------------------------------------------------

    # ------------------------------------------------------------------------------------------
    # [CRITICAL: SYSTEM CORE ARCHITECTURE v2026.1.19 - FULL TRACEABILITY]
    # üö© ‡∏Ñ‡∏≥‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô: ‡∏´‡πâ‡∏≤‡∏°‡∏ï‡∏±‡∏î‡∏ï‡πà‡∏≠‡∏´‡∏£‡∏∑‡∏≠‡∏•‡∏î‡∏ó‡∏≠‡∏ô‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô 10 ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≤‡∏î ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô Audit Traceability
    # ------------------------------------------------------------------------------------------

    def _run_single_assessment(
        self,
        sub_criteria: Dict[str, Any],
        statement_data: Dict[str, Any],
        vectorstore_manager: Optional[Any],
        **kwargs
    ) -> Dict[str, Any]:
        """
        [ULTIMATE MASTER REVISE v2026.1.20]
        - FIXED: Baseline slice KeyError by flatten in build_multichannel...
        - IMPROVED: Prioritize top-ranked chunks for LLM context (fix AI blindness)
        - ADDED: Debug log for sent context preview
        """
        import time
        import json

        start_time = time.time()
        sub_id = str(sub_criteria.get('sub_id', 'Unknown'))
        level = int(statement_data.get('level', 1))
        name = str(sub_criteria.get('name', sub_criteria.get('sub_criteria_name', 'No Title')))
        stmt = str(statement_data.get('statement', f"‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏£‡∏∞‡∏î‡∏±‡∏ö {level}"))
        
        diverse_docs = []
        res = {"is_passed": False, "score": 0.0, "reason": "‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô"}
        raw_res = {} 

        try:
            # --- STEP 1-2: GATE & BASELINE HYDRATION ---
            if not self._is_previous_level_passed(sub_id, level):
                return {"sub_id": sub_id, "level": level, "score": 0.0, "is_passed": False, "status": "SKIPPED"}

            previous_evidences = self._collect_previous_level_evidences(sub_id, level)

            # --- STEP 3-5: ADAPTIVE RETRIEVAL & EXPANSION ---
            all_evidences, raw_max_score = self._perform_adaptive_retrieval(sub_id, level, stmt, vectorstore_manager) or ([], 0.0)
            diverse_docs = self._apply_diversity_filter(all_evidences, level) or []
            
            if hasattr(self, '_expand_context_with_neighbor_pages') and vectorstore_manager and raw_max_score > 0.35:
                diverse_docs = self._expand_context_with_neighbor_pages(diverse_docs, f"evidence_{self.enabler.lower()}")

            # --- STEP 6: MULTICHANNEL CONTEXT BUILDING ---
            multichannel_data = self._build_multichannel_context_for_level( # ‡πÄ‡∏û‡∏¥‡πà‡∏° self. ‡πÅ‡∏•‡∏∞ _
                level=level,
                top_evidences=diverse_docs,
                previous_levels_evidence=previous_evidences
            )
            
            baseline_summary = str(multichannel_data.get("baseline_summary", ""))
            current_tagging = multichannel_data.get("debug_meta", {}).get("tagging_result")
            
            if not current_tagging:
                current_tagging = {p: len([d for d in diverse_docs if str(d.get('pdca_tag') or d.get('phase', '')).upper() == p]) for p in ["P", "D", "C", "A"]}

            # --- STEP 7: PDCA BLOCKS & RULE PREPARATION ---
            rules_map = getattr(self, 'contextual_rules_map', {})
            current_rules = rules_map.get(sub_id, {}).get(str(level), {})
            defaults = rules_map.get("_enabler_defaults", {})

            req_phases = self.get_rule_content(sub_id, level, "require_phase") or (['P', 'D'] if level <= 2 else ['P', 'D', 'C'])
            
            raw_plan_kws = current_rules.get("plan_keywords") or defaults.get("plan_keywords", ["‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢", "‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô"])
            plan_kws_str = ", ".join(raw_plan_kws) if isinstance(raw_plan_kws, list) else str(raw_plan_kws)

            blocks = self._get_pdca_blocks_from_evidences(diverse_docs, previous_evidences, level, sub_id, rules_map)
            full_context_text = self._build_pdca_context(blocks) or "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô"

            # === IMPROVEMENT: Prioritize top-ranked chunks for LLM context ===
            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á diverse_docs ‡∏ï‡∏≤‡∏° rerank_score ‡∏à‡∏≤‡∏Å‡∏™‡∏π‡∏á‡πÑ‡∏õ‡∏ï‡πà‡∏≥
            if diverse_docs:
                sorted_diverse_docs = sorted(
                    diverse_docs,
                    key=lambda d: float(d.get('rerank_score', 0) or d.get('score', 0)),
                    reverse=True
                )
            else:
                sorted_diverse_docs = []

            # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å top chunks (‡∏õ‡∏£‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏° level)
            max_chunks = 40 if level <= 2 else 25
            top_chunks = sorted_diverse_docs[:max_chunks]

            # ‡∏™‡∏£‡πâ‡∏≤‡∏á context ‡∏à‡∏≤‡∏Å top chunks (‡πÅ‡∏ó‡∏ô full_context_text ‡πÄ‡∏î‡∏¥‡∏°)
            context_parts = []
            for idx, chunk in enumerate(top_chunks, 1):
                score = chunk.get('rerank_score', chunk.get('score', 'N/A'))
                source = chunk.get('source', chunk.get('file_name', '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏'))
                page = chunk.get('page', '-')
                text = chunk.get('text', '').strip()
                if text:
                    context_parts.append(
                        f"[‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö {idx} | Score: {score} | {source} ‡∏´‡∏ô‡πâ‡∏≤ {page}]\n"
                        f"{text}\n"
                        f"{'-'*80}\n"
                    )

            prioritized_context = "".join(context_parts) or "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á"

            if baseline_summary:
                prioritized_context += f"\n\n=== ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ (Baseline) ===\n{baseline_summary}\n"

            # ‡∏ï‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏ñ‡πâ‡∏≤‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô (‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏ï‡∏±‡∏î‡∏Å‡∏•‡∏≤‡∏á chunk)
            max_context_len = 15000 if level <= 2 else 10000
            if len(prioritized_context) > max_context_len:
                prioritized_context = prioritized_context[:max_context_len].rsplit('\n', 1)[0] + "\n... (‡∏ï‡∏±‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°)"

            # Debug: Preview context ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏™‡πà‡∏á‡∏à‡∏£‡∏¥‡∏á
            self.logger.info(f"[CONTEXT PRIORITY L{level}] Length: {len(prioritized_context)} | First 300 chars: {prioritized_context[:300]}...")

            # --- STEP 8: EVALUATION ---
            eval_fn = evaluate_with_llm_low_level if level <= 2 else evaluate_with_llm
            audit_conf = self.calculate_audit_confidence(diverse_docs, sub_id=sub_id, level=level)

            llm_params = {
                "context": prioritized_context,  # ‚Üê ‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡πÉ‡∏´‡∏°‡πà‡∏ô‡∏µ‡πâ‡πÅ‡∏ó‡∏ô full_context_text
                "sub_criteria_name": name,
                "level": level,
                "statement_text": stmt,
                "sub_id": sub_id,
                "llm_executor": self.llm,
                "required_phases": req_phases,
                "baseline_summary": baseline_summary,
                "plan_keywords": plan_kws_str,
                "ai_confidence": str(audit_conf.get('level', "MEDIUM")),
                "specific_contextual_rule": str(current_rules.get("rule_text", current_rules.get("specific_contextual_rule", "‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô")))
            }

            try:
                res = eval_fn(**llm_params)
                if res is None:
                    raise ValueError("LLM Evaluation returned None")
                
                raw_res = res.copy() 
                # self.logger.critical(f"LLM RAW RESPONSE (DEBUG): {json.dumps(raw_res, ensure_ascii=False)}")
                
            except Exception as eval_err:
                self.logger.error(f"üõë Evaluation Error at {sub_id} L{level}: {str(eval_err)}")
                res = {
                    "is_passed": False, "score": 0.0, 
                    "reason": f"System Evaluation Error: {str(eval_err)}",
                    "summary_thai": "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÑ‡∏î‡πâ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏ö‡∏ö‡∏Ç‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏á",
                    "P_Plan_Score": 0.0, "D_Do_Score": 0.0
                }

            # --- STEP 9: EXPERT SAFETY NET ---
            is_force_pass = False
            if not res.get("is_passed", False) and raw_max_score >= 0.85:
                self.logger.info(f"üõ°Ô∏è [SAFETY-NET] {sub_id} L{level} | High Rerank {raw_max_score:.2f} -> Boosted.")
                res.update({
                    "is_passed": True, 
                    "score": max(res.get("score", 0.0), 1.2),
                    "reason": str(res.get("reason", "")) + " (Safety Net Pass: ‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å)"
                })
                is_force_pass = True

            # --- STEP 10: PERSISTENCE & LOGGING ---
            final_strength = self._save_level_evidences_and_calculate_strength(diverse_docs, sub_id, level, res, raw_max_score)
            evidence_sources = self._resolve_evidence_filenames(diverse_docs)

            self._log_pdca_status(
                sub_id=sub_id, name=name, level=level, 
                blocks=raw_res, 
                req_phases=req_phases, 
                sources_count=len(evidence_sources),
                score=float(res.get('score', 0.0)), 
                conf_level=str(audit_conf.get('level', 'N/A')),
                tagging_result=current_tagging,
                is_safety_pass=is_force_pass
            )

            return {
                "sub_id": sub_id, "level": level, 
                "score": float(res.get('score', 0.0)),
                "is_passed": bool(res.get('is_passed', False)), 
                "reason": str(res.get('reason', "")),
                "summary_thai": str(res.get('summary_thai', "")), 
                "coaching_insight": str(res.get('coaching_insight', "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°")),
                "P_Plan_Score": float(res.get("P_Plan_Score", 0.0)), 
                "D_Do_Score": float(res.get("D_Do_Score", 0.0)),
                "evidence_sources": evidence_sources,
                "evidence_strength": final_strength, 
                "duration": round(time.time() - start_time, 2)
            }

        except Exception as e:
            self.logger.critical(f"üõë CRITICAL FAILURE {sub_id} L{level}: {str(e)}", exc_info=True)
            return {
                "sub_id": sub_id, "level": level, "score": 0.0, "is_passed": False, 
                "reason": f"Critical Error: {str(e)}", "summary_thai": "‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏£‡πâ‡∏≤‡∏¢‡πÅ‡∏£‡∏á‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö"
            }