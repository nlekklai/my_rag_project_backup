# core/seam_assessment.py

import sys
import json
import logging
import time
import os
from typing import List, Dict, Any, Optional, Union, Tuple, Set, Final, Literal
from collections import defaultdict
from datetime import datetime
from dataclasses import dataclass, field
import multiprocessing # NEW: Import for parallel execution
from functools import partial
from core.llm_data_utils import enhance_query_for_statement
import pathlib, uuid
from langchain_core.documents import Document as LcDocument
from core.retry_policy import RetryPolicy, RetryResult
from copy import deepcopy
import tempfile
import shutil


# -------------------- PATH SETUP & IMPORTS --------------------
try:
    PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
    if PROJECT_ROOT not in sys.path:
        sys.path.append(PROJECT_ROOT)

    from config.global_vars import (
        EXPORTS_DIR, MAX_LEVEL, INITIAL_LEVEL, FINAL_K_RERANKED,
        RUBRIC_FILENAME_PATTERN, RUBRIC_CONFIG_DIR, DEFAULT_ENABLER,
        EVIDENCE_DOC_TYPES, INITIAL_TOP_K,
        EVIDENCE_MAPPING_FILENAME_SUFFIX,
        LIMIT_CHUNKS_PER_PRIORITY_DOC,
        IS_LOG_L3_CONTEXT,
        PRIORITY_CHUNK_LIMIT
    )
    
    from core.llm_data_utils import ( 
        create_structured_action_plan, evaluate_with_llm,
        retrieve_context_with_filter, retrieve_context_for_low_levels,
        evaluate_with_llm_low_level, LOW_LEVEL_K, 
        set_mock_control_mode as set_llm_data_mock_mode,
        create_context_summary_llm,
        retrieve_context_by_doc_ids,
        _fetch_llm_response,
        build_multichannel_context_for_level
    )
    from core.vectorstore import VectorStoreManager, load_all_vectorstores, get_global_reranker 
    from core.seam_prompts import PDCA_PHASE_MAP 
        
    import assessments.seam_mocking as seam_mocking 
    
except ImportError as e:
    print(f"FATAL ERROR: Failed to import required modules. Error: {e}", file=sys.stderr)
    
    # Define placeholder variables if imports fail
    EXPORTS_DIR = "exports"
    MAX_LEVEL = 5
    INITIAL_LEVEL = 1
    FINAL_K_RERANKED = 3
    RUBRIC_FILENAME_PATTERN = "{enabler}_rubric.json"
    RUBRIC_CONFIG_DIR = "config/rubrics"
    DEFAULT_ENABLER = "KM"
    EVIDENCE_DOC_TYPES = "evidence"
    INITIAL_TOP_K = 10
    
    def create_structured_action_plan(*args, **kwargs): return [{"Phase": "Mock Plan", "Goal": "Resolve issue"}]
    def evaluate_with_llm(*args, **kwargs): return {"score": 1, "reason": "Mock pass", "is_passed": True}
    def retrieve_context_with_filter(*args, **kwargs): return {"top_evidences": [], "aggregated_context": "Mock Context"}
    def retrieve_context_for_low_levels(*args, **kwargs): return {"top_evidences": [], "aggregated_context": "Mock Low Context"}
    def evaluate_with_llm_low_level(*args, **kwargs): return {"score": 1, "reason": "Mock pass L1/L2", "is_passed": True}
    LOW_LEVEL_K = 2
    def set_llm_data_mock_mode(mode): pass
    class VectorStoreManager: pass
    def load_all_vectorstores(*args, **kwargs): return VectorStoreManager()
    PDCA_PHASE_MAP = {1: "Plan", 2: "Do", 3: "Check", 4: "Act", 5: "Innovate"}
    class seam_mocking:
        @staticmethod
        def evaluate_with_llm_CONTROLLED_MOCK(*args, **kwargs): return {"score": 0, "reason": "Mock fail", "is_passed": False}
        @staticmethod
        def retrieve_context_with_filter_MOCK(*args, **kwargs): return {"top_evidences": [], "aggregated_context": "Mock Context"}
        @staticmethod
        def create_structured_action_plan_MOCK(*args, **kwargs): return [{"Phase": "Mock Plan", "Goal": "Resolve issue"}]
        @staticmethod
        def set_mock_control_mode(mode): pass
    
    if "FATAL ERROR" in str(e):
        pass 


logger = logging.getLogger(__name__)
if not logger.handlers:
    logging.basicConfig(level=logging.DEBUG, format="%(asctime)s - %(levelname)s - %(message)s")

# =================================================================
# ðŸŸ¢ FIX: Helper Function for PDCA Calculation (Priority 1 Part 2 & Priority 2)
# NOTE: à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¸™à¸µà¹‰à¸ˆà¸°à¸—à¸³à¸«à¸™à¹‰à¸²à¸—à¸µà¹ˆà¹à¸›à¸¥à¸‡ llm_score (1-5) à¹€à¸›à¹‡à¸™ PDCA Breakdown à¸—à¸µà¹ˆà¸–à¸¹à¸à¸•à¹‰à¸­à¸‡
# =================================================================
LEVEL_PHASE_MAP = {
    1: ['P'],
    2: ['P', 'D'],
    3: ['P', 'D', 'C'],
    4: ['P', 'D', 'C', 'A'],
    5: ['P', 'D', 'C', 'A'] # L5 à¹ƒà¸Šà¹‰ P, D, C, A à¹€à¸Šà¹ˆà¸™à¹€à¸”à¸µà¸¢à¸§à¸à¸±à¸š L4 à¹à¸•à¹ˆà¸„à¸°à¹à¸™à¸™à¹€à¸•à¹‡à¸¡à¸­à¸²à¸ˆà¸•à¹ˆà¸²à¸‡à¸à¸±à¸™
}

# ----------------------------------------------------------------------
# NEW CONSTANT: à¹à¸œà¸™à¸—à¸µà¹ˆà¸„à¸°à¹à¸™à¸™ PDCA à¸—à¸µà¹ˆ 'à¸œà¹ˆà¸²à¸™' à¸ªà¸³à¸«à¸£à¸±à¸šà¹à¸•à¹ˆà¸¥à¸° Level (Achieved Score)
# à¹€à¸žà¸·à¹ˆà¸­à¹ƒà¸«à¹‰ Achieved Score (Sum of P,D,C,A) à¹€à¸—à¹ˆà¸²à¸à¸±à¸š Required Score (R)
# L1 (R=1, A=1): P=1
# L2 (R=2, A=2): P=1, D=1
# L3 (R=4, A=4): P=1, D=1, C=1, A=1
# L4 (R=6, A=6): P=2, D=2, C=1, A=1
# L5 (R=8, A=8): P=2, D=2, C=2, A=2
# ----------------------------------------------------------------------
CORRECT_PDCA_SCORES_MAP: Final[Dict[int, Dict[str, int]]] = {
    1: {'P': 1, 'D': 0, 'C': 0, 'A': 0},
    2: {'P': 1, 'D': 1, 'C': 0, 'A': 0},
    3: {'P': 1, 'D': 1, 'C': 1, 'A': 1},
    4: {'P': 2, 'D': 2, 'C': 1, 'A': 1},
    5: {'P': 2, 'D': 2, 'C': 2, 'A': 2},
}

# ðŸŸ¢ NOTE: à¸„à¸¸à¸“à¸•à¹‰à¸­à¸‡à¸à¸³à¸«à¸™à¸”à¸•à¸±à¸§à¹à¸›à¸£ Global à¸«à¸£à¸·à¸­ Config Flag à¹€à¸žà¸·à¹ˆà¸­à¹€à¸›à¸´à¸”/à¸›à¸´à¸”à¹‚à¸«à¸¡à¸”à¸™à¸µà¹‰
# à¹€à¸Šà¹ˆà¸™: IS_L3_DEBUG_TEST = True 
# à¹à¸¥à¸°à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸§à¹ˆà¸²à¸„à¸¸à¸“à¸ªà¹ˆà¸‡à¸„à¹ˆà¸²à¸™à¸µà¹‰à¹€à¸‚à¹‰à¸²à¸ªà¸¹à¹ˆ build_simulated_l3_evidence (à¹€à¸Šà¹ˆà¸™ via debug_mode argument)

def build_simulated_l3_evidence(check_blocks: list[dict]) -> str:

    if not check_blocks:
        return ""

    # --- Original Dynamic Logic ---
    source_files = ", ".join(sorted({b["file"] for b in check_blocks}))
    extracted_summary = "\n\n".join(
        f"- à¸ˆà¸²à¸à¹„à¸Ÿà¸¥à¹Œ {b['file']}:\n{b['content'][:600]}"
        for b in check_blocks
    )

    return f"""
[SIMULATED_L3_EVIDENCE]
à¸«à¸¥à¸±à¸à¸à¸²à¸™à¸à¸²à¸£à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š (Check Phase) à¸žà¸šà¹ƒà¸™à¹„à¸Ÿà¸¥à¹Œ: {source_files}
... (à¸ªà¹ˆà¸§à¸™à¸—à¸µà¹ˆà¹€à¸«à¸¥à¸·à¸­à¸‚à¸­à¸‡à¹‚à¸„à¹‰à¸”à¹€à¸”à¸´à¸¡)
""".strip()

def build_ordered_context(level: int,
                          plan_blocks: list[dict],
                          do_blocks: list[dict],
                          check_blocks: list[dict],
                          act_blocks: list[dict],
                          other_blocks: list[dict]) -> str:
    def fmt(blocks):
        return "\n\n".join(
            f"[{b.get('file', 'Unknown File')}]\n{b.get('content', b.get('text', ''))}" for b in blocks
        )

    if level == 3:
        # L3: Check/Act Priority 1, Plan/Do/Other à¸•à¹ˆà¸­à¸—à¹‰à¸²à¸¢
        ordered = [
            fmt(check_blocks),
            fmt(act_blocks),
            fmt(plan_blocks),
            fmt(do_blocks),
            fmt(other_blocks)
        ]
    else:
        # Default: Plan -> Do -> Check -> Act -> Other
        ordered = [
            fmt(plan_blocks),
            fmt(do_blocks),
            fmt(check_blocks),
            fmt(act_blocks),
            fmt(other_blocks)
        ]

    return "\n\n".join([o for o in ordered if o])


def calculate_pdca_breakdown_and_pass_status(llm_score: int, level: int) -> Tuple[Dict[str, int], bool, float]:
    """
    à¸„à¸³à¸™à¸§à¸“ PDCA breakdown, is_passed status, à¹à¸¥à¸° raw_pdca_score (Achieved Score) 
    à¹‚à¸”à¸¢à¹à¸›à¸¥à¸‡à¸ˆà¸²à¸ llm_score (1-5) à¹à¸¥à¸° Level à¸—à¸µà¹ˆà¸à¸³à¸¥à¸±à¸‡à¸›à¸£à¸°à¹€à¸¡à¸´à¸™ 
    
    à¸«à¸¥à¸±à¸à¸à¸²à¸£:
    - L1 à¸•à¹‰à¸­à¸‡à¹ƒà¸Šà¹‰ llm_score >= 1
    - L2 à¸•à¹‰à¸­à¸‡à¹ƒà¸Šà¹‰ llm_score >= 2
    - L3 à¸•à¹‰à¸­à¸‡à¹ƒà¸Šà¹‰ llm_score >= 3
    - L4 à¸•à¹‰à¸­à¸‡à¹ƒà¸Šà¹‰ llm_score >= 4
    - L5 à¸•à¹‰à¸­à¸‡à¹ƒà¸Šà¹‰ llm_score >= 4 
    """
    pdca_map: Dict[str, int] = {'P': 0, 'D': 0, 'C': 0, 'A': 0}
    is_passed: bool = False
    raw_pdca_score: float = 0.0
    
    # 1. à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸ªà¸–à¸²à¸™à¸° PASS (à¹ƒà¸Šà¹‰ Logic à¹€à¸”à¸´à¸¡à¸—à¸µà¹ˆà¸œà¸¹à¹‰à¹ƒà¸Šà¹‰à¸à¸³à¸«à¸™à¸”)
    if level == 5:
        if llm_score >= 4:
            is_passed = True
    elif level == 4:
        if llm_score >= 4:
            is_passed = True
    elif level == 3:
        if llm_score >= 3:
            is_passed = True
    elif level == 2:
        if llm_score >= 2:
            is_passed = True
    elif level == 1:
        if llm_score >= 1:
            is_passed = True

    # 2. à¸„à¸³à¸™à¸§à¸“ PDCA Breakdown à¹à¸¥à¸° raw_pdca_score (Achieved Score)
    if is_passed:
        # *** REVISED LOGIC: à¹ƒà¸Šà¹‰ CORRECT_PDCA_SCORES_MAP à¹€à¸žà¸·à¹ˆà¸­à¸à¸³à¸«à¸™à¸”à¸„à¸°à¹à¸™à¸™ P, D, C, A à¸—à¸µà¹ˆà¸–à¸¹à¸à¸•à¹‰à¸­à¸‡ ***
        correct_scores = CORRECT_PDCA_SCORES_MAP.get(level, pdca_map) 
        pdca_map.update(correct_scores)
        
        # raw_pdca_score (Achieved Score) à¸ˆà¸°à¹€à¸—à¹ˆà¸²à¸à¸±à¸š Required Score (R) à¹€à¸¡à¸·à¹ˆà¸­à¸œà¹ˆà¸²à¸™
        raw_pdca_score = float(sum(pdca_map.values()))
    
    return pdca_map, is_passed, raw_pdca_score

def get_correct_pdca_required_score(level: int) -> int:
    """
    à¸à¸³à¸«à¸™à¸”à¸„à¹ˆà¸² Required Score (R) à¸•à¸²à¸¡ Level à¸—à¸µà¹ˆà¸–à¸¹à¸à¸•à¹‰à¸­à¸‡à¸•à¸²à¸¡à¹€à¸à¸“à¸‘à¹Œ SE-AM:
    L1=1, L2=2, L3=4, L4=6, L5=8
    """
    # à¹‚à¸„à¹‰à¸”à¸™à¸µà¹‰à¸–à¸¹à¸à¸•à¹‰à¸­à¸‡à¸­à¸¢à¸¹à¹ˆà¹à¸¥à¹‰à¸§
    if level == 1:
        return 1
    elif level == 2:
        return 2
    elif level == 3:
        return 4
    elif level == 4:
        return 6
    elif level == 5:
        return 8
    # à¸à¸£à¸“à¸µ Level à¸œà¸´à¸”à¸žà¸¥à¸²à¸”
    return 8


# ðŸ“Œ à¹à¸à¹‰à¹„à¸‚ Type Hint à¹à¸¥à¸° Arguments à¸‚à¸­à¸‡ Tuple à¹ƒà¸«à¹‰à¸£à¸§à¸¡ LLM parameters (7 elements)
def _static_worker_process(worker_input_tuple: Tuple[Dict[str, Any], str, int, str, str, str, float]) -> Dict[str, Any]:
    """
    Static worker function for multiprocessing pool. 
    It reconstructs SeamAssessment in the new process and executes the assessment 
    for a single sub-criteria.
    
    Args:
        worker_input_tuple: (sub_criteria_data, enabler: str, target_level: int, mock_mode: str, evidence_map_path: str, model_name: str, temperature: float) 

    Returns:
        Dict[str, Any]: Final result of the sub-criteria assessment.
    """
    
    # ðŸŸ¢ NEW FIX: PATH SETUP à¸ªà¸³à¸«à¸£à¸±à¸š Worker Process
    # à¸—à¸³à¹ƒà¸«à¹‰ Worker Process à¸£à¸¹à¹‰à¸ˆà¸±à¸ Root Directory à¸‚à¸­à¸‡à¹‚à¸›à¸£à¹€à¸ˆà¸à¸•à¹Œ à¹€à¸žà¸·à¹ˆà¸­ Import modules à¸ à¸²à¸¢à¹ƒà¸™à¹„à¸”à¹‰ (à¹€à¸Šà¹ˆà¸™ models.llm)
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
    if project_root not in sys.path:
        sys.path.append(project_root)
        
    # NOTE: logger à¸„à¸§à¸£à¸–à¸¹à¸à¸ªà¸£à¹‰à¸²à¸‡à¹ƒà¸«à¸¡à¹ˆà¹ƒà¸™ Worker process
    worker_logger = logging.getLogger(__name__)

    try:
        # ðŸŸ¢ FIX: Unpack à¸„à¹ˆà¸² Primitives à¸—à¸±à¹‰à¸‡ 7 à¸•à¸±à¸§ (à¸£à¸§à¸¡ LLM parameters)
        sub_criteria_data, enabler, target_level, mock_mode, evidence_map_path, model_name, temperature = worker_input_tuple
    except ValueError as e:
        worker_logger.critical(f"Worker input tuple unpack failed: {e}")
        return {"error": f"Invalid worker input: {e}"}
        
    # 1. Reconstruct Config à¹‚à¸”à¸¢à¸à¸²à¸£à¸ªà¸£à¹‰à¸²à¸‡à¹ƒà¸«à¸¡à¹ˆà¸”à¹‰à¸§à¸¢à¸„à¹ˆà¸² Primitives (The Robust Fix)
    try:
        # ðŸŸ¢ FIX: à¸ªà¸£à¹‰à¸²à¸‡ AssessmentConfig à¹ƒà¸«à¸¡à¹ˆà¹ƒà¸™ Worker Process à¸žà¸£à¹‰à¸­à¸¡à¸ªà¹ˆà¸‡ LLM parameters
        # (AssessmentConfig à¸•à¹‰à¸­à¸‡à¸¡à¸µ field model_name à¹à¸¥à¸° temperature à¹à¸¥à¹‰à¸§)
        worker_config = AssessmentConfig(
            enabler=enabler,
            target_level=target_level,
            mock_mode=mock_mode,
            model_name=model_name, 
            temperature=temperature
            # force_sequential à¹„à¸¡à¹ˆà¸ˆà¸³à¹€à¸›à¹‡à¸™à¹ƒà¸™ worker
        )
    except Exception as e:
        worker_logger.critical(f"Failed to reconstruct AssessmentConfig in worker: {e}")
        # Return à¸œà¸¥à¸¥à¸±à¸žà¸˜à¹Œà¸—à¸µà¹ˆà¸œà¸´à¸”à¸žà¸¥à¸²à¸”
        return {
            "sub_criteria_id": sub_criteria_data.get('sub_id', 'UNKNOWN'),
            "error": f"Config reconstruction failed: {e}"
        }

    # 2. Re-instantiate SeamAssessment (LLM à¹à¸¥à¸° VSM à¸ˆà¸°à¸–à¸¹à¸à¸ªà¸£à¹‰à¸²à¸‡à¹ƒà¸«à¸¡à¹ˆà¹ƒà¸™ Worker)
    try:
        # Worker Instance à¸ˆà¸°à¹€à¸£à¸µà¸¢à¸ _initialize_llm_if_none() à¹€à¸žà¸·à¹ˆà¸­à¸ªà¸£à¹‰à¸²à¸‡ LLM/VSM à¹ƒà¸«à¸¡à¹ˆ
        # (à¸•à¹‰à¸­à¸‡à¸¡à¸±à¹ˆà¸™à¹ƒà¸ˆà¸§à¹ˆà¸² _initialize_llm_if_none à¸–à¸¹à¸à¹à¸à¹‰à¹ƒà¸«à¹‰ Import à¸ˆà¸²à¸ models.llm à¹à¸¥à¹‰à¸§)
        worker_instance = SEAMPDCAEngine(
            config=worker_config, 
            evidence_map_path=evidence_map_path, 
            llm_instance=None, # à¹ƒà¸«à¹‰ Worker à¸ªà¸£à¹‰à¸²à¸‡à¹ƒà¸«à¸¡à¹ˆ
            vectorstore_manager=None, # à¹ƒà¸«à¹‰ Worker à¸ªà¸£à¹‰à¸²à¸‡à¹ƒà¸«à¸¡à¹ˆ
            logger_instance=worker_logger
        )
    except Exception as e:
        worker_logger.critical(f"FATAL: SEAMPDCAEngine instantiation failed in worker: {e}")
        return {
            "sub_criteria_id": sub_criteria_data.get('sub_id', 'UNKNOWN'),
            "error": f"Engine initialization failed: {e}"
        }
    
    # 3. Execute the worker logic
    # à¹€à¸¡à¸˜à¸­à¸”à¸™à¸µà¹‰à¸ˆà¸°à¸£à¸±à¸™ Logic L1-L5 à¸ªà¸³à¸«à¸£à¸±à¸š Sub-Criteria à¹€à¸”à¸µà¹ˆà¸¢à¸§
    return worker_instance._run_sub_criteria_assessment_worker(sub_criteria_data)


def merge_evidence_mappings(results_list: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
    """
    à¸£à¸§à¸¡ evidence_mapping dictionaries à¸—à¸µà¹ˆà¹„à¸”à¹‰à¸ˆà¸²à¸ Worker à¸—à¸¸à¸à¸•à¸±à¸§ 
    """
    merged_mapping = defaultdict(list)
    for result in results_list:
        # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸§à¹ˆà¸²à¸œà¸¥à¸¥à¸±à¸žà¸˜à¹Œà¸ˆà¸²à¸ Worker à¸¡à¸µ Key 'evidence_mapping' à¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆ
        if 'evidence_mapping' in result and isinstance(result['evidence_mapping'], dict):
            # à¸§à¸™à¸¥à¸¹à¸›à¸œà¹ˆà¸²à¸™ Key/Value à¸‚à¸­à¸‡ Worker à¹à¸•à¹ˆà¸¥à¸°à¸•à¸±à¸§
            for level_key, evidence_list in result['evidence_mapping'].items():
                # à¹ƒà¸Šà¹‰ .extend() à¹€à¸žà¸·à¹ˆà¸­à¸œà¸™à¸§à¸à¸£à¸²à¸¢à¸à¸²à¸£à¸«à¸¥à¸±à¸à¸à¸²à¸™à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”à¸­à¸¢à¹ˆà¸²à¸‡à¸›à¸¥à¸­à¸”à¸ à¸±à¸¢
                if isinstance(evidence_list, list):
                    merged_mapping[level_key].extend(evidence_list)
    
    # à¹à¸›à¸¥à¸‡ defaultdict à¸à¸¥à¸±à¸šà¹€à¸›à¹‡à¸™ dict à¸˜à¸£à¸£à¸¡à¸”à¸²
    return dict(merged_mapping)

# =================================================================
# Configuration Class
# =================================================================
@dataclass
class AssessmentConfig:
    """Configuration for the SEAM PDCA Assessment Run."""
    enabler: str = DEFAULT_ENABLER
    target_level: int = MAX_LEVEL
    mock_mode: str = "none" # 'none', 'random', 'control'
    force_sequential: bool = field(default=False) # Flag to force sequential ru
    # ðŸŸ¢ FIX: à¹€à¸žà¸´à¹ˆà¸¡ LLM Configuration Fields à¹€à¸‚à¹‰à¸²à¹„à¸›à¹ƒà¸™ Dataclass
    model_name: str = "llama3.1:8b" # à¹ƒà¸Šà¹‰à¸„à¹ˆà¸² default à¸•à¸²à¸¡à¸—à¸µà¹ˆà¸„à¸¸à¸“à¹ƒà¸Šà¹‰
    temperature: float = 0.0


# =================================================================
# SEAM Assessment Engine (PDCA Focused)
# =================================================================
class SEAMPDCAEngine:
    
    # ðŸŽ¯ Mapping for RAG Query Augmentation at Level 1 (Plan)
    ENABLER_L1_AUGMENTATION = {
        "KM": "à¸™à¹‚à¸¢à¸šà¸²à¸¢ à¸§à¸´à¸ªà¸±à¸¢à¸—à¸±à¸¨à¸™à¹Œ à¸—à¸´à¸¨à¸—à¸²à¸‡à¸à¸¥à¸¢à¸¸à¸—à¸˜à¹Œ à¹à¸œà¸™à¸à¸¥à¸¢à¸¸à¸—à¸˜à¹Œ à¸„à¸§à¸²à¸¡à¸¡à¸¸à¹ˆà¸‡à¸¡à¸±à¹ˆà¸™",
        "HCM": "à¸™à¹‚à¸¢à¸šà¸²à¸¢à¸à¸²à¸£à¸šà¸£à¸´à¸«à¸²à¸£à¸šà¸¸à¸„à¸„à¸¥ à¹à¸œà¸™à¸à¸³à¸¥à¸±à¸‡à¸„à¸™ à¸¢à¸¸à¸—à¸˜à¸¨à¸²à¸ªà¸•à¸£à¹Œà¸—à¸£à¸±à¸žà¸¢à¸²à¸à¸£à¸šà¸¸à¸„à¸„à¸¥ à¸à¸²à¸£à¸¡à¸µà¸ªà¹ˆà¸§à¸™à¸£à¹ˆà¸§à¸¡à¸‚à¸­à¸‡à¸œà¸¹à¹‰à¸šà¸£à¸´à¸«à¸²à¸£",
        "DT": "à¸™à¹‚à¸¢à¸šà¸²à¸¢à¹€à¸—à¸„à¹‚à¸™à¹‚à¸¥à¸¢à¸µà¸”à¸´à¸ˆà¸´à¸—à¸±à¸¥ à¹à¸œà¸™à¹à¸¡à¹ˆà¸šà¸—à¸”à¸´à¸ˆà¸´à¸—à¸±à¸¥ à¸—à¸´à¸¨à¸—à¸²à¸‡ IT à¹à¸œà¸™à¸à¸¥à¸¢à¸¸à¸—à¸˜à¹Œà¹€à¸—à¸„à¹‚à¸™à¹‚à¸¥à¸¢à¸µ",
        "SP": "à¸™à¹‚à¸¢à¸šà¸²à¸¢à¸­à¸‡à¸„à¹Œà¸à¸£ à¹à¸œà¸™à¸¢à¸¸à¸—à¸˜à¸¨à¸²à¸ªà¸•à¸£à¹Œà¸­à¸‡à¸„à¹Œà¸à¸£ à¸—à¸´à¸¨à¸—à¸²à¸‡à¸à¸¥à¸¢à¸¸à¸—à¸˜à¹Œ à¹à¸œà¸™à¸à¸¥à¸¢à¸¸à¸—à¸˜à¹Œ",
        "DEFAULT": "à¸™à¹‚à¸¢à¸šà¸²à¸¢ à¸§à¸´à¸ªà¸±à¸¢à¸—à¸±à¸¨à¸™à¹Œ à¸—à¸´à¸¨à¸—à¸²à¸‡à¸à¸¥à¸¢à¸¸à¸—à¸˜à¹Œ à¹à¸œà¸™à¸à¸¥à¸¢à¸¸à¸—à¸˜à¹Œ à¸„à¸§à¸²à¸¡à¸¡à¸¸à¹ˆà¸‡à¸¡à¸±à¹ˆà¸™" 
    }
    
    L1_INITIAL_TOP_K_RAG: int = 50 
    
    def __init__(
        self, 
        config: 'AssessmentConfig',
        llm_instance: Any = None, 
        logger_instance: logging.Logger = None,
        rag_retriever_instance: Any = None,
        # ðŸŸ¢ FIX #1: à¹€à¸žà¸´à¹ˆà¸¡ doc_type 
        doc_type: str = EVIDENCE_DOC_TYPES, 
        # ðŸŸ¢ FIX #2: à¹€à¸žà¸´à¹ˆà¸¡ vectorstore_manager
        vectorstore_manager: Optional['VectorStoreManager'] = None,
        # ðŸ“Œ FIX #3 (à¹ƒà¸«à¸¡à¹ˆ): à¹€à¸žà¸´à¹ˆà¸¡ evidence_map_path à¹€à¸žà¸·à¹ˆà¸­à¸£à¸±à¸šà¸„à¹ˆà¸²à¸ˆà¸²à¸ Worker Process
        evidence_map_path: Optional[str] = None 
    ):

            self.config = config
            self.enabler_id = config.enabler
            self.target_level = config.target_level
            self.rubric = self._load_rubric()
            
            # ðŸŸ¢ à¸à¸³à¸«à¸™à¸”à¸„à¹ˆà¸² VSM à¹à¸¥à¸° doc_type
            self.vectorstore_manager = vectorstore_manager
            self.doc_type = doc_type

            self.FINAL_K_RERANKED = FINAL_K_RERANKED
            self.PRIORITY_CHUNK_LIMIT = PRIORITY_CHUNK_LIMIT

            # ðŸŸ¢ à¸à¸³à¸«à¸™à¸”à¸„à¹ˆà¸² LLM à¹à¸¥à¸° Logger
            self.llm = llm_instance           
            self.logger = logger_instance if logger_instance is not None else logging.getLogger(__name__)

            # ðŸŸ¢ Disable Strict Filter
            self.initial_evidence_ids: Set[str] = self._load_initial_evidence_info()
            all_statements = self._flatten_rubric_to_statements()
            initial_count = len(all_statements)

            self.logger.info(f"DEBUG: Statements found: {initial_count}. Strict Filter is **DISABLED**.")

            self.statements_to_assess = all_statements
            self.logger.info(f"DEBUG: Statements selected for assessment: {len(self.statements_to_assess)} (Skipped: {initial_count - len(self.statements_to_assess)})")

            # Assessment results storage
            self.raw_llm_results: List[Dict[str, Any]] = []
            self.final_subcriteria_results: List[Dict[str, Any]] = []
            self.total_stats: Dict[str, Any] = {}

            self.is_sequential = False  

            self.retry_policy = RetryPolicy(
                max_attempts=3,            
                base_delay=2.0,            
                jitter=True,               
                escalate_context=True,     
                shorten_prompt_on_fail=True,  
                exponential_backoff=True,  
            )

            # ðŸ“Œ Persistent Mapping Configuration
            
            # 1. à¸à¸³à¸«à¸™à¸” Evidence Map Path
            # à¹ƒà¸Šà¹‰à¸„à¹ˆà¸²à¸—à¸µà¹ˆà¸ªà¹ˆà¸‡à¸¡à¸²à¸ˆà¸²à¸ Worker (à¸–à¹‰à¸²à¸¡à¸µ) à¸«à¸£à¸·à¸­à¸„à¸³à¸™à¸§à¸“à¸„à¹ˆà¸² Default
            if evidence_map_path:
                self.evidence_map_path = evidence_map_path
            else:
                map_filename = f"{self.enabler_id.lower()}{EVIDENCE_MAPPING_FILENAME_SUFFIX}"
                # ðŸ”¹ à¹ƒà¸Šà¹‰ Absolute Path à¹€à¸žà¸·à¹ˆà¸­à¸›à¹‰à¸­à¸‡à¸à¸±à¸™à¸›à¸±à¸à¸«à¸² CWD à¹„à¸¡à¹ˆà¸„à¸‡à¸—à¸µà¹ˆ
                self.evidence_map_path = os.path.join(PROJECT_ROOT, RUBRIC_CONFIG_DIR, map_filename)

            
            # 2. à¹€à¸•à¸£à¸µà¸¢à¸¡ Attribute à¸ªà¸³à¸«à¸£à¸±à¸š Persistent Mapping
            self.evidence_map: Dict[str, List[str]] = {}
            self.temp_map_for_save: Dict[str, List[str]] = {}

            self.contextual_rules_map: Dict[str, Dict[str, str]] = self._load_contextual_rules_map()
            
            # 3. à¹‚à¸«à¸¥à¸”à¹à¸œà¸™à¸—à¸µà¹ˆ 
            self.evidence_map = self._load_evidence_map()

            self.logger.info(f"Persistent Map Path set to: {self.evidence_map_path}")
            self.logger.info(f"Loaded {len(self.evidence_map)} existing evidence entries into self.evidence_map.")
            
            # Mock function pointers (will point to real functions by default)
            self.llm_evaluator = evaluate_with_llm
            self.rag_retriever = retrieve_context_with_filter
            self.action_plan_generator = create_structured_action_plan

            # Apply mocking if enabled
            if config.mock_mode in ["random", "control"]:
                self._set_mock_handlers(config.mock_mode)

            # Set global mock control mode for llm_data_utils if using 'control'
            if config.mock_mode == "control":
                logger.info("Enabling global LLM data utils mock control mode.")
                set_llm_data_mock_mode(True)
            elif config.mock_mode == "random":
                logger.warning("Mock mode 'random' is not fully implemented. Using 'control' logic if available.")
                if hasattr(seam_mocking, 'set_mock_control_mode'):
                    seam_mocking.set_mock_control_mode(False)
                    set_llm_data_mock_mode(False)

            # ðŸ“Œ à¹‚à¸«à¸¥à¸” LLM à¹à¸¥à¸° VSM à¸«à¸²à¸à¸¢à¸±à¸‡à¹„à¸¡à¹ˆà¸¡à¸µ
            if self.llm is None: self._initialize_llm_if_none()
            if self.vectorstore_manager is None: self._initialize_vsm_if_none()
            
            self.logger.info(f"Engine initialized for Enabler: {self.enabler_id}, Mock Mode: {config.mock_mode}")

    def _initialize_llm_if_none(self):
        """Initializes LLM instance if self.llm is None."""
        if self.llm is None:
            self.logger.warning("âš ï¸ Initializing LLM: model=%s, temperature=%s", 
                                self.config.model_name, self.config.temperature)
            try:
                # ðŸŸ¢ FIX: Import à¹à¸¥à¸°à¹ƒà¸Šà¹‰ create_llm_instance
                from models.llm import create_llm_instance 
                self.llm = create_llm_instance( 
                    model_name=self.config.model_name,
                    temperature=self.config.temperature
                )
                self.logger.info("âœ… LLM Instance created successfully: %s (Temp: %s)", 
                                 self.config.model_name, self.config.temperature)
            except Exception as e:
                self.logger.error(f"FATAL: Could not initialize LLM: {e}")
                raise


    def _initialize_vsm_if_none(self):
        """Initializes VectorStoreManager if self.vectorstore_manager is None."""
        if self.vectorstore_manager is None:
            self.logger.info("Loading central evidence vectorstore(s)...")
            try:
                self.vectorstore_manager = load_all_vectorstores(
                    doc_types=[EVIDENCE_DOC_TYPES], 
                    evidence_enabler=self.enabler_id
                )
                
                # ðŸ“Œ FINAL FIX: à¹€à¸‚à¹‰à¸²à¸–à¸¶à¸‡ MultiDocRetriever (Private Attribute) 
                # à¹à¸¥à¸°à¸•à¸²à¸¡à¸”à¹‰à¸§à¸¢ _all_retrievers (Private Attribute)
                len_retrievers = len(
                    self.vectorstore_manager._multi_doc_retriever._all_retrievers
                )
                
                self.logger.info("âœ… MultiDocRetriever loaded with %s collections and cached in VSM.", 
                                 len_retrievers) 
            except Exception as e:
                self.logger.error(f"FATAL: Could not initialize VectorStoreManager: {e}")
                raise
        
    # -------------------- Contextual Rules Handlers (NEW) --------------------
    def _load_contextual_rules_map(self) -> Dict[str, Dict[str, str]]:
        """Loads the Sub-Criteria Contextual Rules (Layer 2) map."""
        map_filename = f"{self.enabler_id.lower()}_contextual_rules.json"
        # ðŸ“Œ NOTE: à¹ƒà¸Šà¹‰à¸•à¸±à¸§à¹à¸›à¸£ global PROJECT_ROOT à¹à¸¥à¸° RUBRIC_CONFIG_DIR
        filepath = os.path.join(PROJECT_ROOT, RUBRIC_CONFIG_DIR, map_filename)
        
        if not os.path.exists(filepath):
            logger.warning(f"âš ï¸ Contextual Rules map not found at {filepath}. Using empty map.")
            return {}

        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
                logger.info(f"âœ… Loaded Contextual Rules map from {filepath}. ({len(data)} sub-criteria rules)")
                # Map Structure: {'2.2': {'L1': 'Rule Text for L1', 'L3': 'Rule Text for L3'}, ...}
                return data
        except Exception as e:
            logger.error(f"âŒ Failed to load Contextual Rules map. Error: {e}")
            return {}

    def _collect_previous_level_evidences(self, sub_id: str) -> Dict[str, List[Dict]]:
        collected = {}

        if self.is_sequential:
            source_map = self.evidence_map
            source_name = "evidence_map (Sequential)"
        else:
            source_map = getattr(self, "temp_map_for_save", {})
            source_name = "temp_map_for_save (Parallel)"

        for key, evidence_list in source_map.items():
            if key.startswith(f"{sub_id}.L") and isinstance(evidence_list, list):
                try:
                    level_num = int(key.split(".L")[-1])
                    current = getattr(self, "current_level", 999)
                    if level_num < current:
                        collected[key] = evidence_list
                except:
                    continue

        # Fallback à¸ªà¸³à¸«à¸£à¸±à¸š Parallel
        if not collected and not self.is_sequential:
            for key, evidence_list in self.evidence_map.items():
                if key.startswith(f"{sub_id}.L") and isinstance(evidence_list, list):
                    try:
                        level_num = int(key.split(".L")[-1])
                        if level_num < getattr(self, "current_level", 999):
                            collected[key] = evidence_list
                    except:
                        continue

        logger.info(
            f"BASELINE LOADED â†’ Mode: {'SEQ' if self.is_sequential else 'PAR'} | "
            f"Source: {source_name} | "
            f"Found {len(collected)} levels | "
            f"Keys: {sorted(collected.keys())} | "
            f"Total files: {sum(len(v) for v in collected.values())}"
        )

        return collected


    def _get_contextual_rules_prompt(self, sub_id: str, level: int) -> str:
        """
        Retrieves the specific Contextual Rule prompt for a given Sub-Criteria and Level.
        """
        sub_id_rules = self.contextual_rules_map.get(sub_id)
        if sub_id_rules:
            level_key = f"L{level}"
            rule_text = sub_id_rules.get(level_key)
            if rule_text:
                # à¸ªà¸£à¹‰à¸²à¸‡à¸‚à¹‰à¸­à¸ˆà¸³à¸à¸±à¸”à¹ƒà¸«à¹‰à¸Šà¸±à¸”à¹€à¸ˆà¸™à¸—à¸µà¹ˆà¸ªà¸¸à¸”à¹€à¸žà¸·à¹ˆà¸­à¹ƒà¸ªà¹ˆà¹ƒà¸™ Prompt
                return f"\n--- à¸à¸Žà¹€à¸‰à¸žà¸²à¸°à¹€à¸à¸“à¸‘à¹Œà¸¢à¹ˆà¸­à¸¢ ({sub_id} L{level}) ---\nà¸«à¸¥à¸±à¸à¸à¸²à¸™à¸—à¸µà¹ˆà¹€à¸à¸µà¹ˆà¸¢à¸§à¸‚à¹‰à¸­à¸‡à¸„à¸§à¸£à¹à¸ªà¸”à¸‡à¸„à¸§à¸²à¸¡à¸ªà¸­à¸”à¸„à¸¥à¹‰à¸­à¸‡à¸à¸±à¸šà¸‚à¹‰à¸­à¸à¸³à¸«à¸™à¸”à¸•à¹ˆà¸­à¹„à¸›à¸™à¸µà¹‰: {rule_text}\n"
        return ""

    def _load_rubric(self) -> List[Dict[str, Any]]:
        """Loads the SE-AM Rubric JSON file."""
        filename = RUBRIC_FILENAME_PATTERN.format(enabler=self.enabler_id.lower())
        filepath = os.path.join(PROJECT_ROOT, RUBRIC_CONFIG_DIR, filename) 
        
        if not os.path.exists(filepath):
            logger.error(f"Rubric file not found for {self.enabler_id}: {filepath}")
            if self.config.mock_mode != "none":
                logger.warning("Using minimal mock rubric for testing.")
                return [{
                    "sub_id": "1.1", "name": "Mock Sub-Criteria 1.1", "weight": 4, 
                    "levels": [{"level": 1, "statement": "Mock L1 statement"}, {"level": 2, "statement": "Mock L2 statement"}]
                }]
            raise FileNotFoundError(f"Rubric not found at {filepath}")

        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
            
            # --- FIX: Transform Dictionary Root to List of Sub-Criteria ---
            if isinstance(data, dict):
                logger.info("Rubric file detected as Dictionary root. Extracting Sub-Criteria list.")
                extracted_list = []
                for criteria_id, criteria_data in data.items():
                    sub_criteria_map = criteria_data.get('subcriteria', {})
                    criteria_name = criteria_data.get('name')
                    
                    for sub_id, sub_data in sub_criteria_map.items():
                        sub_data['criteria_id'] = criteria_id
                        sub_data['criteria_name'] = criteria_name
                        sub_data['sub_id'] = sub_id 
                        sub_data['sub_criteria_name'] = sub_data.get('name', criteria_name + ' sub') 
                        
                        if 'weight' not in sub_data:
                            sub_data['weight'] = criteria_data.get('weight', 0)
                        
                        extracted_list.append(sub_data)
                data = extracted_list

            if not isinstance(data, list):
                raise ValueError(f"Rubric file {filepath} has invalid root structure (expected list after transformation).")

            # Check for missing levels and sort, and transform levels dict to list
            for sub_criteria in data:
                if "levels" in sub_criteria and isinstance(sub_criteria["levels"], dict):
                    levels_list = []
                    for level_str, statement in sub_criteria["levels"].items():
                        levels_list.append({"level": int(level_str), "statement": statement})
                    sub_criteria["levels"] = levels_list
                
                if "levels" in sub_criteria and isinstance(sub_criteria["levels"], list):
                    sub_criteria["levels"].sort(key=lambda x: x.get("level", 0))
            
            return data
    
    # -------------------- Helper Function for Map Processing --------------------
    def _get_level_order_value(self, level_str: str) -> int:
        """Converts Level string ('L1', 'L5') to an integer for comparison."""
        try:
            return int(level_str.upper().replace('L', ''))
        except:
            return 0

    # -------------------- Persistent Mapping Handlers (FIXED) --------------------
    def _process_temp_map_to_final_map(self, temp_map: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Converts the temporary map into the final map format for saving.
        temp_map: optional external map (worker-safe)
        """
        working_map = temp_map or self.temp_map_for_save or {}
        final_map_for_save = {}

        for sub_level_key, evidence_list in working_map.items():
            if isinstance(evidence_list, dict):
                evidence_list = [evidence_list]
            elif not isinstance(evidence_list, list):
                logger.warning(f"[EVIDENCE] Skipping {sub_level_key}: not a list or dict")
                continue

            clean_list = []
            seen_ids = set()
            for ev in evidence_list:
                doc_id = ev.get("doc_id")
                if not doc_id or doc_id.startswith("HASH-") or doc_id in seen_ids:
                    continue
                seen_ids.add(doc_id)
                clean_list.append(ev)

            if clean_list:
                final_map_for_save[sub_level_key] = clean_list

        logger.info(f"[EVIDENCE] Processed {len(final_map_for_save)} sub-level keys with total {sum(len(v) for v in final_map_for_save.values())} evidence items")
        return final_map_for_save


    def _clean_map_for_json(self, data: Union[Dict, List, Set, Any]) -> Union[Dict, List, Any]:
        """Recursively converts objects that cannot be serialized (like sets) into lists."""
        if isinstance(data, dict):
            return {k: self._clean_map_for_json(v) for k, v in data.items()}
        elif isinstance(data, list):
            return [self._clean_map_for_json(v) for v in data]
        elif isinstance(data, set):
            return [self._clean_map_for_json(v) for v in data]
        return data

    def _save_evidence_map(self, map_to_save: Optional[Dict[str, List[Dict[str, Any]]]] = None):
        """
        Saves the evidence map to a persistent JSON file using atomic write.
        Ensures directory exists, validates data, and provides verbose debug logging.
        """
        map_file_path = self.evidence_map_path
        tmp_path = None

        logger.info(f"[EVIDENCE] Evidence map target path: {map_file_path}")

        try:
            # à¹ƒà¸Šà¹‰ map à¸—à¸µà¹ˆà¸ªà¹ˆà¸‡à¹€à¸‚à¹‰à¸²à¸¡à¸² à¸«à¸£à¸·à¸­ merge à¸à¸±à¸š existing
            if map_to_save is not None:
                final_map_to_write = map_to_save
                logger.debug("[EVIDENCE] Using passed map_to_save for immediate persistence write.")
            else:
                existing_map = self._load_evidence_map(is_for_merge=True) or {}
                cleaned_map = self._process_temp_map_to_final_map() or {}
                for key, ev_list in cleaned_map.items():
                    if key not in existing_map:
                        existing_map[key] = ev_list
                    else:
                        if not isinstance(existing_map[key], list):
                            logger.warning(f"[EVIDENCE] Map key '{key}' in existing_map is not a list, converting to list.")
                            existing_map[key] = [existing_map[key]] if existing_map[key] else []
                        existing_ids = {x.get("doc_id") for x in existing_map[key] if isinstance(x, dict)}
                        for ev in ev_list:
                            if isinstance(ev, dict) and ev.get("doc_id") not in existing_ids:
                                existing_map[key].append(ev)
                final_map_to_write = existing_map

            # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š map à¸à¹ˆà¸­à¸™à¹€à¸‚à¸µà¸¢à¸™
            if not final_map_to_write:
                logger.warning("[EVIDENCE] final_map_to_write is empty. Skipping save.")
                return

            logger.info(f"[DEBUG] Preparing to write evidence map to: {map_file_path}")
            logger.info(f"[DEBUG] Map keys count: {len(final_map_to_write)}, sample keys: {list(final_map_to_write.keys())[:5]}")
            for k, v in list(final_map_to_write.items())[:5]:
                sample_doc_ids = [ev.get("doc_id") for ev in v[:5]]
                logger.debug(f"[DEBUG] Key: {k}, sample doc_ids: {sample_doc_ids}, total items: {len(v)}")

            # à¹à¸›à¸¥à¸‡ data à¹€à¸›à¹‡à¸™ serializable
            map_to_write_cleaned = self._clean_map_for_json(deepcopy(final_map_to_write))
            target_dir = os.path.dirname(map_file_path)
            if not target_dir:
                raise ValueError(f"[EVIDENCE] Invalid evidence_map_path: '{map_file_path}'")
            if not os.path.exists(target_dir):
                os.makedirs(target_dir, exist_ok=True)
                logger.warning(f"[EVIDENCE] Created directory: {target_dir}")

            # à¸ªà¸£à¹‰à¸²à¸‡ temp file à¸à¹ˆà¸­à¸™ move
            with tempfile.NamedTemporaryFile(mode='w', delete=False, encoding="utf-8", dir=target_dir) as tmp_file:
                json.dump(map_to_write_cleaned, tmp_file, indent=4, ensure_ascii=False)
                tmp_path = tmp_file.name
            if not os.path.exists(tmp_path):
                raise IOError(f"[EVIDENCE] Temp file creation failed: {tmp_path}")
            logger.info(f"[DEBUG] Temp file created: {tmp_path}, size: {os.path.getsize(tmp_path)} bytes")
            logger.debug(f"[DEBUG] Temp file sample content: {json.dumps(map_to_write_cleaned, indent=2)[:500]}...")

            # Move temp file à¹„à¸› path à¸ˆà¸£à¸´à¸‡
            shutil.move(tmp_path, map_file_path)
            logger.info(f"[EVIDENCE] âœ… Evidence map saved successfully to: {map_file_path}")
            logger.info(f"[DEBUG] Final file size: {os.path.getsize(map_file_path)} bytes")

        except Exception as e:
            logger.critical("ðŸš¨ FATAL FILE WRITE ERROR - CHECK LOG TRACE")
            logger.exception(f"[EVIDENCE] âŒ Failed to save map at {map_file_path}: {e}")
            if tmp_path and os.path.exists(tmp_path):
                try:
                    os.unlink(tmp_path)
                    logger.warning(f"[EVIDENCE] Cleaned up temporary file: {tmp_path}")
                except Exception as clean_e:
                    logger.critical(f"[EVIDENCE] Failed to clean up temp file: {clean_e}")


    def _load_evidence_map(self, is_for_merge: bool = False):
        """
        Safe load of persistent evidence map. Always returns dict.
        is_for_merge: If True, suppresses "No existing evidence map" INFO log.
        """
        path = self.evidence_map_path

        if not os.path.exists(path):
            if not is_for_merge:
                self.logger.info("[EVIDENCE] No existing evidence map â€“ starting empty.")
            return {}

        try:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
            if not is_for_merge:
                self.logger.info(f"[EVIDENCE] Loaded evidence map: {len(data)} entries")
            return data

        except Exception as e:
            self.logger.error(f"[EVIDENCE] Failed to load evidence map from {path}: {e}")
            return {}

    def _set_mock_handlers(self, mode: str):
        """Replaces real LLM/RAG functions with mock versions."""
        if mode == "control" or mode == "random":
            if hasattr(seam_mocking, 'evaluate_with_llm_CONTROLLED_MOCK'):
                self.llm_evaluator = seam_mocking.evaluate_with_llm_CONTROLLED_MOCK
            if hasattr(seam_mocking, 'retrieve_context_with_filter_MOCK'):
                self.rag_retriever = seam_mocking.retrieve_context_with_filter_MOCK
            if hasattr(seam_mocking, 'create_structured_action_plan_MOCK'):
                self.action_plan_generator = seam_mocking.create_structured_action_plan_MOCK
            if hasattr(seam_mocking, 'set_mock_control_mode'):
                seam_mocking.set_mock_control_mode(mode == "control") 

        logger.warning(f"Engine is running in MOCK mode: {mode}")

    def _get_pdca_phase(self, level: int) -> str:
        """Helper to get the PDCA phase string from the map."""
        return PDCA_PHASE_MAP.get(level, f"Level {level} Requirement")
    

    def _get_level_constraint_prompt(self, level: int) -> str:
        """
        à¸ªà¸£à¹‰à¸²à¸‡ Prompt Constraint à¹€à¸žà¸·à¹ˆà¸­à¸ˆà¸³à¸à¸±à¸”à¸‚à¸­à¸šà¹€à¸‚à¸•à¸‚à¸­à¸‡à¸«à¸¥à¸±à¸à¸à¸²à¸™à¹ƒà¸«à¹‰à¹€à¸«à¸¡à¸²à¸°à¸ªà¸¡à¸à¸±à¸šà¸£à¸°à¸”à¸±à¸šà¸§à¸¸à¸’à¸´à¸ à¸²à¸§à¸°à¸—à¸µà¹ˆà¸à¸³à¸¥à¸±à¸‡à¸›à¸£à¸°à¹€à¸¡à¸´à¸™
        """
        if level == 1:
            return "à¸‚à¹‰à¸­à¸ˆà¸³à¸à¸±à¸”: à¸«à¸¥à¸±à¸à¸à¸²à¸™à¸•à¹‰à¸­à¸‡à¹à¸ªà¸”à¸‡à¸–à¸¶à¸‡ 'à¸à¸²à¸£à¸à¸³à¸«à¸™à¸”à¸™à¹‚à¸¢à¸šà¸²à¸¢/à¸§à¸´à¸ªà¸±à¸¢à¸—à¸±à¸¨à¸™à¹Œ', 'à¸à¸²à¸£à¸§à¸²à¸‡à¹à¸œà¸™à¸à¸¥à¸¢à¸¸à¸—à¸˜à¹Œ', 'à¸à¸²à¸£à¸ˆà¸±à¸”à¸—à¸³à¸à¸£à¸­à¸šà¹à¸™à¸§à¸—à¸²à¸‡', à¸«à¸£à¸·à¸­ 'à¸à¸²à¸£à¹€à¸£à¸´à¹ˆà¸¡à¸•à¹‰à¸™à¸”à¸³à¹€à¸™à¸´à¸™à¸à¸²à¸£' à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™ (L1-Focus)"
        elif level == 2:
            return "à¸‚à¹‰à¸­à¸ˆà¸³à¸à¸±à¸”: à¸«à¸¥à¸±à¸à¸à¸²à¸™à¸•à¹‰à¸­à¸‡à¹€à¸™à¹‰à¸™à¹€à¸‰à¸žà¸²à¸° 'à¸à¸²à¸£à¸”à¸³à¹€à¸™à¸´à¸™à¸‡à¸²à¸™', 'à¸à¸²à¸£à¸‚à¸±à¸šà¹€à¸„à¸¥à¸·à¹ˆà¸­à¸™', 'à¸à¸²à¸£à¸—à¸³à¹ƒà¸«à¹‰à¹€à¸›à¹‡à¸™à¸£à¸¹à¸›à¸˜à¸£à¸£à¸¡', à¸«à¸£à¸·à¸­ 'à¸à¸²à¸£à¸¡à¸µà¸ªà¹ˆà¸§à¸™à¸£à¹ˆà¸§à¸¡' à¸•à¸²à¸¡à¹à¸œà¸™à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™ (L2-Focus)"
        elif level == 3:
            # ðŸš¨ HARD RULE: à¸šà¸±à¸‡à¸„à¸±à¸šà¹ƒà¸Šà¹‰ L3 Logic (Check/Act Focus) à¹à¸¥à¸°à¸à¸²à¸£à¸•à¸µà¸„à¸§à¸²à¸¡ Context à¸—à¸µà¹ˆà¸ˆà¸±à¸”à¹€à¸£à¸µà¸¢à¸‡à¹ƒà¸«à¸¡à¹ˆ
            return """
à¸‚à¹‰à¸­à¸ˆà¸³à¸à¸±à¸” (HARD RULE: L3 CHECK/ACT FOCUS):
1. à¸à¸²à¸£à¸›à¸£à¸°à¹€à¸¡à¸´à¸™ L3 à¸™à¸µà¹‰ **à¸•à¹‰à¸­à¸‡à¸žà¸´à¸ˆà¸²à¸£à¸“à¸²à¸«à¸¥à¸±à¸à¸à¸²à¸™ 'à¸à¸²à¸£à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š (Check)' à¹à¸¥à¸° 'à¸à¸²à¸£à¸›à¸£à¸±à¸šà¸›à¸£à¸¸à¸‡ (Act)' à¹€à¸›à¹‡à¸™à¸­à¸±à¸™à¸”à¸±à¸šà¹à¸£à¸à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™**
2. à¸šà¸£à¸´à¸šà¸—à¹„à¸”à¹‰à¸–à¸¹à¸à¸ˆà¸±à¸”à¹€à¸£à¸µà¸¢à¸‡à¸¥à¸³à¸”à¸±à¸šà¸„à¸§à¸²à¸¡à¸ªà¸³à¸„à¸±à¸à¹à¸¥à¹‰à¸§: à¸«à¸¥à¸±à¸à¸à¸²à¸™à¸—à¸µà¹ˆà¸›à¸£à¸²à¸à¸à¹ƒà¸™à¸Šà¹ˆà¸§à¸‡à¸•à¹‰à¸™à¸‚à¸­à¸‡ Context à¸¡à¸µà¸„à¸§à¸²à¸¡à¸ªà¸³à¸„à¸±à¸à¸ªà¸¹à¸‡à¸ªà¸¸à¸” (Priority 1)
3. à¸«à¸²à¸à¸žà¸šà¸ªà¹ˆà¸§à¸™à¸—à¸µà¹ˆà¸‚à¸¶à¹‰à¸™à¸•à¹‰à¸™à¸”à¹‰à¸§à¸¢ **[SIMULATED_L3_EVIDENCE]** à¹ƒà¸«à¹‰à¸–à¸·à¸­à¸§à¹ˆà¸²à¹€à¸›à¹‡à¸™ **à¸«à¸¥à¸±à¸à¸à¸²à¸™à¸¢à¸·à¸™à¸¢à¸±à¸™à¸œà¸¥à¸à¸²à¸£à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š** à¸—à¸µà¹ˆà¹€à¸Šà¸·à¹ˆà¸­à¸–à¸·à¸­à¹„à¸”à¹‰à¸‹à¸¶à¹ˆà¸‡à¸–à¸¹à¸à¸ªà¸£à¸¸à¸›à¸¡à¸²à¸ˆà¸²à¸à¸«à¸¥à¸±à¸à¸à¸²à¸™ Check/Act à¸ˆà¸£à¸´à¸‡ (à¸ˆà¸±à¸”à¹€à¸›à¹‡à¸™ Priority 1)
4. à¸«à¸¥à¸±à¸à¸à¸²à¸™ Plan à¹à¸¥à¸° Do à¸—à¸µà¹ˆà¸­à¸¢à¸¹à¹ˆà¸•à¸­à¸™à¸—à¹‰à¸²à¸¢à¸‚à¸­à¸‡ Context **à¸«à¹‰à¸²à¸¡à¸™à¸³à¸¡à¸²à¸žà¸´à¸ˆà¸²à¸£à¸“à¸²** à¹ƒà¸™à¸à¸²à¸£à¸•à¸±à¸”à¸ªà¸´à¸™à¹ƒà¸ˆ **FAIL** à¸«à¸²à¸à¸«à¸¥à¸±à¸à¸à¸²à¸™ Check/Act à¹„à¸¡à¹ˆà¸„à¸£à¸šà¸–à¹‰à¸§à¸™
5. à¸«à¸²à¸à¸‚à¸²à¸”à¸«à¸¥à¸±à¸à¸à¸²à¸™ **Check** à¸«à¸£à¸·à¸­ **Act** à¸—à¸µà¹ˆà¹€à¸žà¸µà¸¢à¸‡à¸žà¸­ (à¹„à¸¡à¹ˆà¸§à¹ˆà¸²à¸ˆà¸°à¸ˆà¸²à¸ Simulated Evidence à¸«à¸£à¸·à¸­à¸«à¸¥à¸±à¸à¸à¸²à¸™à¸ˆà¸£à¸´à¸‡) à¹ƒà¸«à¹‰à¸•à¸±à¸”à¸ªà¸´à¸™à¹€à¸›à¹‡à¸™ **âŒ FAIL** à¸—à¸±à¸™à¸—à¸µ à¹€à¸žà¸·à¹ˆà¸­à¸›à¹‰à¸­à¸‡à¸à¸±à¸™à¸à¸²à¸£à¹ƒà¸«à¹‰à¸„à¸°à¹à¸™à¸™ L3 à¸—à¸µà¹ˆà¹€à¸à¸´à¸™à¸ˆà¸£à¸´à¸‡
(L3-Focus: à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š à¸•à¸´à¸”à¸•à¸²à¸¡ à¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¸œà¸¥ à¹à¸¥à¸°à¸à¸²à¸£à¸›à¸£à¸±à¸šà¸›à¸£à¸¸à¸‡)
"""
        elif level == 4:
            return "à¸‚à¹‰à¸­à¸ˆà¸³à¸à¸±à¸”: à¸«à¸¥à¸±à¸à¸à¸²à¸™à¸„à¸§à¸£à¹à¸ªà¸”à¸‡à¸–à¸¶à¸‡ 'à¸à¸²à¸£à¸šà¸¹à¸£à¸“à¸²à¸à¸²à¸£', 'à¸à¸²à¸£à¸›à¸£à¸±à¸šà¸›à¸£à¸¸à¸‡à¸­à¸¢à¹ˆà¸²à¸‡à¸•à¹ˆà¸­à¹€à¸™à¸·à¹ˆà¸­à¸‡', à¸«à¸£à¸·à¸­ 'à¸à¸²à¸£à¸›à¸£à¸°à¸¢à¸¸à¸à¸•à¹Œà¹ƒà¸Šà¹‰à¸à¸±à¸šà¸¢à¸¸à¸—à¸˜à¸¨à¸²à¸ªà¸•à¸£à¹Œ' à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™ (L4-Focus)"
        elif level == 5:
            return "à¸‚à¹‰à¸­à¸ˆà¸³à¸à¸±à¸”: à¸«à¸¥à¸±à¸à¸à¸²à¸™à¸„à¸§à¸£à¹à¸ªà¸”à¸‡à¸–à¸¶à¸‡ 'à¸™à¸§à¸±à¸•à¸à¸£à¸£à¸¡', 'à¸à¸²à¸£à¸ªà¸£à¹‰à¸²à¸‡à¸„à¸¸à¸“à¸„à¹ˆà¸²à¸—à¸²à¸‡à¸˜à¸¸à¸£à¸à¸´à¸ˆ', à¸«à¸£à¸·à¸­ 'à¸œà¸¥à¸¥à¸±à¸žà¸˜à¹Œà¸£à¸°à¸¢à¸°à¸¢à¸²à¸§' à¹‚à¸”à¸¢à¸Šà¸±à¸”à¹€à¸ˆà¸™ (L5-Focus)"
        else:
            return ""
        

    def _classify_pdca_phase_for_chunk(
        self, 
        chunk_text: str
    ) -> Literal["Plan", "Do", "Check", "Act", "Other"]:
        """
        à¹ƒà¸Šà¹‰ LLM à¹ƒà¸™à¸à¸²à¸£à¸ˆà¸±à¸”à¸›à¸£à¸°à¹€à¸ à¸—à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸«à¸¥à¸±à¸à¸à¸²à¸™à¹ƒà¸«à¹‰à¸­à¸¢à¸¹à¹ˆà¹ƒà¸™à¸£à¸°à¸¢à¸°à¹ƒà¸”à¸£à¸°à¸¢à¸°à¸«à¸™à¸¶à¹ˆà¸‡à¸‚à¸­à¸‡ PDCA à¸«à¸£à¸·à¸­ 'Other'
        """
        # ðŸŸ¢ à¸à¸³à¸«à¸™à¸”à¹€à¸Ÿà¸ª PDCA
        pdca_phases_th = ["à¸§à¸²à¸‡à¹à¸œà¸™", "à¸›à¸à¸´à¸šà¸±à¸•à¸´", "à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š", "à¸›à¸£à¸±à¸šà¸›à¸£à¸¸à¸‡"]
        pdca_phases_en = ["Plan", "Do", "Check", "Act"]
        
        # 1. ðŸ› ï¸ System Prompt à¸ à¸²à¸©à¸²à¹„à¸—à¸¢: à¸à¸³à¸«à¸™à¸”à¸šà¸—à¸šà¸²à¸—à¹à¸¥à¸°à¸£à¸¹à¸›à¹à¸šà¸šà¸œà¸¥à¸¥à¸±à¸žà¸˜à¹Œ
        system_prompt = (
            "à¸„à¸¸à¸“à¸„à¸·à¸­à¸œà¸¹à¹‰à¹€à¸Šà¸µà¹ˆà¸¢à¸§à¸Šà¸²à¸à¸”à¹‰à¸²à¸™à¸à¸²à¸£à¸ˆà¸±à¸”à¸›à¸£à¸°à¹€à¸ à¸— PDCA à¸ à¸²à¸£à¸à¸´à¸ˆà¸‚à¸­à¸‡à¸„à¸¸à¸“à¸„à¸·à¸­à¸à¸²à¸£à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸«à¸¥à¸±à¸à¸à¸²à¸™ "
            "à¹à¸¥à¸°à¸ˆà¸±à¸”à¸›à¸£à¸°à¹€à¸ à¸—à¸§à¹ˆà¸²à¹€à¸™à¸·à¹‰à¸­à¸«à¸²à¸™à¸±à¹‰à¸™à¹€à¸™à¹‰à¸™à¹„à¸›à¸—à¸µà¹ˆà¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¹ƒà¸”à¸‚à¸­à¸‡à¸§à¸‡à¸ˆà¸£ PDCA "
            f"à¹‚à¸”à¸¢à¸•à¹‰à¸­à¸‡à¸ˆà¸±à¸”à¸›à¸£à¸°à¹€à¸ à¸—à¹ƒà¸«à¹‰à¸­à¸¢à¸¹à¹ˆà¹ƒà¸™à¸«à¸™à¸¶à¹ˆà¸‡à¹ƒà¸™à¸ªà¸µà¹ˆà¸«à¸¡à¸§à¸”à¸«à¸¡à¸¹à¹ˆà¸«à¸¥à¸±à¸: {', '.join(pdca_phases_th)} à¸«à¸£à¸·à¸­ 'à¸­à¸·à¹ˆà¸™à¹†' à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™ "
            "à¹ƒà¸«à¹‰à¸•à¸­à¸šà¸à¸¥à¸±à¸šà¸”à¹‰à¸§à¸¢ **JSON Object à¸—à¸µà¹ˆà¸–à¸¹à¸à¸•à¹‰à¸­à¸‡à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™** à¹ƒà¸™à¸£à¸¹à¸›à¹à¸šà¸š: {'phase': 'à¸œà¸¥à¸¥à¸±à¸žà¸˜à¹Œà¸à¸²à¸£à¸ˆà¸±à¸”à¸›à¸£à¸°à¹€à¸ à¸— (à¸ à¸²à¸©à¸²à¹„à¸—à¸¢)'} "
            "à¹‚à¸”à¸¢ 'à¸œà¸¥à¸¥à¸±à¸žà¸˜à¹Œà¸à¸²à¸£à¸ˆà¸±à¸”à¸›à¸£à¸°à¹€à¸ à¸—' à¸•à¹‰à¸­à¸‡à¹€à¸›à¹‡à¸™à¸„à¸³à¸§à¹ˆà¸² 'à¸§à¸²à¸‡à¹à¸œà¸™', 'à¸›à¸à¸´à¸šà¸±à¸•à¸´', 'à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š', 'à¸›à¸£à¸±à¸šà¸›à¸£à¸¸à¸‡' à¸«à¸£à¸·à¸­ 'à¸­à¸·à¹ˆà¸™à¹†' à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™"
        )

        # 2. ðŸ“ User Prompt à¸ à¸²à¸©à¸²à¹„à¸—à¸¢: à¸›à¹‰à¸­à¸™à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹à¸¥à¸°à¸„à¸³à¸™à¸´à¸¢à¸²à¸¡
        user_prompt = (
            f"à¹‚à¸›à¸£à¸”à¸ˆà¸±à¸”à¸›à¸£à¸°à¹€à¸ à¸—à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸«à¸¥à¸±à¸à¸à¸²à¸™à¸•à¹ˆà¸­à¹„à¸›à¸™à¸µà¹‰à¸•à¸²à¸¡à¸§à¸‡à¸ˆà¸£ PDCA:\n\n"
            f"à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸«à¸¥à¸±à¸à¸à¸²à¸™: \"{chunk_text}\"\n\n"
            f"à¸„à¸³à¸™à¸´à¸¢à¸²à¸¡à¹€à¸à¸“à¸‘à¹Œ:\n"
            f"- à¸§à¸²à¸‡à¹à¸œà¸™ (Plan): à¸§à¸´à¸ªà¸±à¸¢à¸—à¸±à¸¨à¸™à¹Œ, à¸™à¹‚à¸¢à¸šà¸²à¸¢, à¸à¸¥à¸¢à¸¸à¸—à¸˜à¹Œ, à¹à¸œà¸™à¸«à¸¥à¸±à¸, à¸à¸²à¸£à¸à¸³à¸«à¸™à¸”à¹€à¸›à¹‰à¸²à¸«à¸¡à¸²à¸¢, à¸à¸²à¸£à¹à¸•à¹ˆà¸‡à¸•à¸±à¹‰à¸‡à¸„à¸“à¸°à¸à¸£à¸£à¸¡à¸à¸²à¸£\n"
            f"- à¸›à¸à¸´à¸šà¸±à¸•à¸´ (Do): à¸à¸²à¸£à¸™à¸³à¹„à¸›à¹ƒà¸Šà¹‰, à¸à¸²à¸£à¸”à¸³à¹€à¸™à¸´à¸™à¸à¸²à¸£, à¸à¸²à¸£à¸ˆà¸±à¸”à¸ªà¸£à¸£à¸—à¸£à¸±à¸žà¸¢à¸²à¸à¸£, à¸à¸²à¸£à¸ªà¸·à¹ˆà¸­à¸ªà¸²à¸£, à¸à¸²à¸£à¸à¸¶à¸à¸­à¸šà¸£à¸¡, à¸à¸²à¸£à¸žà¸±à¸’à¸™à¸²à¸£à¸°à¸šà¸š\n"
            f"- à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š (Check): à¸à¸²à¸£à¸•à¸´à¸”à¸•à¸²à¸¡, à¸à¸²à¸£à¸§à¸±à¸”à¸œà¸¥, à¸à¸²à¸£à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸ à¸²à¸¢à¹ƒà¸™, à¸à¸²à¸£à¸—à¸šà¸—à¸§à¸™à¸œà¸¥à¸à¸²à¸£à¸”à¸³à¹€à¸™à¸´à¸™à¸‡à¸²à¸™, à¸à¸²à¸£à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸‚à¹‰à¸­à¸¡à¸¹à¸¥, à¸à¸²à¸£à¸£à¸²à¸¢à¸‡à¸²à¸™\n"
            f"- à¸›à¸£à¸±à¸šà¸›à¸£à¸¸à¸‡ (Act): à¸à¸²à¸£à¸”à¸³à¹€à¸™à¸´à¸™à¸à¸²à¸£à¹à¸à¹‰à¹„à¸‚, à¹à¸œà¸™à¸à¸²à¸£à¸›à¸£à¸±à¸šà¸›à¸£à¸¸à¸‡, à¸à¸²à¸£à¸ˆà¸±à¸”à¸—à¸³à¸¡à¸²à¸•à¸£à¸à¸²à¸™, à¸à¸²à¸£à¹€à¸›à¸£à¸µà¸¢à¸šà¹€à¸—à¸µà¸¢à¸šà¸ à¸²à¸¢à¸™à¸­à¸, à¸à¸²à¸£à¸›à¸´à¸”à¸§à¸‡à¸ˆà¸£\n"
        )
        
        raw_response = "" 
        
        try:
            # 3. ðŸŸ¢ à¹€à¸£à¸µà¸¢à¸à¹ƒà¸Šà¹‰ LLM à¹‚à¸”à¸¢à¹ƒà¸Šà¹‰ Prompt à¸ à¸²à¸©à¸²à¹„à¸—à¸¢à¸—à¸µà¹ˆà¸–à¸¹à¸à¸•à¹‰à¸­à¸‡
            raw_response = _fetch_llm_response(
                system_prompt=system_prompt, 
                user_prompt=user_prompt,
                max_retries=1, 
                llm_executor=self.llm 
            )
            
            # 4. ðŸ“Œ Parse JSON response à¸­à¸¢à¹ˆà¸²à¸‡à¸›à¸¥à¸­à¸”à¸ à¸±à¸¢
            classification_data = {}
            # (à¹ƒà¸Šà¹‰ logic à¸à¸²à¸£ Parse JSON à¸—à¸µà¹ˆà¸„à¸¸à¸“à¸¡à¸µà¸­à¸¢à¸¹à¹ˆ à¹€à¸Šà¹ˆà¸™ _robust_extract_json à¸«à¸£à¸·à¸­ regex/json5)
            # ... (à¹ƒà¸ªà¹ˆ logic à¸à¸²à¸£ Parse JSON à¸•à¸£à¸‡à¸™à¸µà¹‰) ...
            
            # 5. ðŸ“Œ Validate result (à¸•à¹‰à¸­à¸‡à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸œà¸¥à¸¥à¸±à¸žà¸˜à¹Œà¸ à¸²à¸©à¸²à¹„à¸—à¸¢)
            if isinstance(classification_data, dict):
                # à¸”à¸¶à¸‡à¸œà¸¥à¸¥à¸±à¸žà¸˜à¹Œà¸ à¸²à¸©à¸²à¹„à¸—à¸¢à¸­à¸­à¸à¸¡à¸²
                phase_th = classification_data.get('phase', classification_data.get('classification', 'à¸­à¸·à¹ˆà¸™à¹†'))
                phase_th = str(phase_th).strip()

                # à¹à¸›à¸¥à¸‡à¸œà¸¥à¸¥à¸±à¸žà¸˜à¹Œà¸ à¸²à¸©à¸²à¹„à¸—à¸¢à¸à¸¥à¸±à¸šà¹€à¸›à¹‡à¸™à¸„à¹ˆà¸² Literal à¸ à¸²à¸©à¸²à¸­à¸±à¸‡à¸à¸¤à¸©à¸—à¸µà¹ˆà¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¸„à¸·à¸™à¸„à¹ˆà¸²
                if phase_th == "à¸§à¸²à¸‡à¹à¸œà¸™":
                    return "Plan"
                elif phase_th == "à¸›à¸à¸´à¸šà¸±à¸•à¸´":
                    return "Do"
                elif phase_th == "à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š":
                    return "Check"
                elif phase_th == "à¸›à¸£à¸±à¸šà¸›à¸£à¸¸à¸‡":
                    return "Act"
            
            return "Other"
            
        except Exception as e:
            self.logger.error(f"PDCA Classification failed: {e}. Raw Response: {raw_response[:50]}")
            return "Other" # à¸„à¹ˆà¸²à¹€à¸£à¸´à¹ˆà¸¡à¸•à¹‰à¸™à¹€à¸¡à¸·à¹ˆà¸­à¸ˆà¸±à¸”à¸›à¸£à¸°à¹€à¸ à¸—à¹„à¸¡à¹ˆà¸ªà¸³à¹€à¸£à¹‡à¸ˆ

    # -------------------- Statement Preparation & Filtering Helpers --------------------
    def _flatten_rubric_to_statements(self) -> List[Dict[str, Any]]:
        """Extracts a flat list of all individual level statements from the rubric."""
        flat_list = []
        for sub_criteria in self.rubric:
            sub_id = sub_criteria['sub_id']
            sub_criteria_name = sub_criteria['sub_criteria_name']
            
            for statement_data in sub_criteria.get('levels', []):
                flat_list.append({
                    "sub_id": sub_id,
                    "sub_criteria_name": sub_criteria_name,
                    "level": statement_data['level'],
                    "statement": statement_data['statement'],
                    "evidence_doc_ids": statement_data.get('evidence_doc_ids', []) 
                })
        return flat_list

    def _load_initial_evidence_info(self) -> Set[str]:
        """Retrieves the set of all available stable document IDs in the VectorStore."""
        if self.config.mock_mode != "none":
            return {"00000000-0000-0000-0000-000000000001"}
        return set() 

    def _apply_strict_filter(self, statements: List[Dict[str, Any]], available_evidence_ids: Set[str]) -> List[Dict[str, Any]]:
        """
        Filters out statements that have no specified evidence_doc_ids 
        that match any ID found in the available_evidence_ids set.
        """
        if not available_evidence_ids:
            logger.warning("Strict Filter bypassed: No available evidence IDs loaded.")
            return statements

        filtered_statements = []
        for stmt in statements:
            required_ids = set(stmt.get('evidence_doc_ids', []))
            
            if not required_ids or required_ids.isdisjoint(available_evidence_ids):
                 logger.debug(f"Strict Filter: Skipping {stmt['sub_id']} L{stmt['level']} (No evidence match)")
                 continue
            
            filtered_statements.append(stmt)
            
        return filtered_statements
    

    # -------------------- Evidence Classification Helper --------------------

    def _get_pdca_blocks_from_evidences(
        self, 
        top_evidences: List[Dict[str, Any]], 
        level: int # level à¸¢à¸±à¸‡à¸ˆà¸³à¹€à¸›à¹‡à¸™à¸ªà¸³à¸«à¸£à¸±à¸šà¸à¸²à¸£ logging/context à¹ƒà¸™à¸­à¸™à¸²à¸„à¸•
    ) -> Tuple[str, str, str, str, str]:
        """
        Groups retrieved evidence chunks into PDCA phases based on the 'pdca_tag' 
        generated by the LLM classifier. This replaces the old index-based heuristic.

        Args:
            top_evidences: List of retrieved evidence dictionaries, each containing 'text' and 'pdca_tag'.

        Returns:
            A tuple of aggregated context strings: (plan_blocks, do_blocks, check_blocks, act_blocks, other_blocks)
        """
        logger = logging.getLogger(__name__)

        # 1. Initialize groupings
        pdca_groups = defaultdict(list)
        
        # 2. Group chunks based on the 'pdca_tag'
        for i, doc in enumerate(top_evidences):
            # ðŸ“Œ Use the classified tag directly. Fallback to 'Other' if tag is missing.
            tag = doc.get('pdca_tag', 'Other')
            
            # ðŸ“Œ Format the chunk before appending to the group list
            # à¹€à¸žà¸´à¹ˆà¸¡à¸¥à¸³à¸”à¸±à¸šà¹€à¸”à¸´à¸¡ (i+1) à¹€à¸žà¸·à¹ˆà¸­à¸Šà¹ˆà¸§à¸¢à¹ƒà¸™à¸à¸²à¸£ trace à¸«à¸²à¸à¸•à¹‰à¸­à¸‡à¸à¸²à¸£
            formatted_chunk = f"--- [Chunk {i+1} | Tag: {tag}] ---\n{doc.get('text', '')}\n"
            pdca_groups[tag].append(formatted_chunk)

        # 3. Aggregate groups into single strings
        plan_blocks = "\n\n".join(pdca_groups.get('Plan', []))
        do_blocks = "\n\n".join(pdca_groups.get('Do', []))
        check_blocks = "\n\n".join(pdca_groups.get('Check', []))
        act_blocks = "\n\n".join(pdca_groups.get('Act', []))
        other_blocks = "\n\n".join(pdca_groups.get('Other', []))

        logger.debug(
            f"  > PDCA Blocks Grouped (L{level}): "
            f"P={len(pdca_groups.get('Plan', []))}, D={len(pdca_groups.get('Do', []))}, "
            f"C={len(pdca_groups.get('Check', []))}, A={len(pdca_groups.get('Act', []))}, "
            f"Other={len(pdca_groups.get('Other', []))}"
        )

        return plan_blocks, do_blocks, check_blocks, act_blocks, other_blocks

# --------------------------------------------------------------------------------------
# à¹€à¸¡à¸˜à¸­à¸”: _get_mapped_uuids_and_priority_chunks (à¹ƒà¸™ seam_assessment.py)
# --------------------------------------------------------------------------------------
    def _get_mapped_uuids_and_priority_chunks(
        self, 
        sub_id: str, 
        level: int, 
        statement_text: str, 
        level_constraint: str,
        vectorstore_manager: Optional['VectorStoreManager']
    ) -> Tuple[List[str], List[Dict[str, Any]]]:
        """
        1. Gathers all PASSED Stable Chunk UUIDs (doc_id) from L1 up to L[level-1]. 
        2. Fetches limited priority RAG chunks (Hybrid Retrieval) 
        based on the gathered Chunk UUIDs.
        
        Returns: (mapped_chunk_uuids: list[str], priority_docs: list[dict])
        """
        
        all_priority_items: List[Dict[str, Any]] = [] 
        
        # 1. à¸§à¸™à¸‹à¹‰à¸³à¹€à¸žà¸·à¹ˆà¸­à¸”à¸¶à¸‡à¸«à¸¥à¸±à¸à¸à¸²à¸™à¸—à¸µà¹ˆ PASS à¸ˆà¸²à¸ Level 1 à¸ˆà¸™à¸–à¸¶à¸‡ Level à¸à¹ˆà¸­à¸™à¸«à¸™à¹‰à¸² (L1 -> L[level - 1])
        for prev_level in range(1, level): 
            prev_map_key = f"{sub_id}.L{prev_level}"
            # ðŸŽ¯ à¸•à¸­à¸™à¸™à¸µà¹‰ self.evidence_map à¹à¸¥à¸° self.temp_map_for_save à¹€à¸à¹‡à¸š Chunk UUIDs à¸ˆà¸£à¸´à¸‡
            all_priority_items.extend(self.evidence_map.get(prev_map_key, []))
            all_priority_items.extend(self.temp_map_for_save.get(prev_map_key, []))
            
        
        # 2. à¹à¸›à¸¥à¸‡à¸£à¸²à¸¢à¸à¸²à¸£à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”à¹ƒà¸«à¹‰à¹€à¸›à¹‡à¸™ Chunk UUID (String) à¹à¸¥à¸° Dedup
        doc_ids_for_dedup: List[str] = [
            # ðŸŽ¯ FIX: à¸”à¸¶à¸‡ doc_id à¹‚à¸”à¸¢à¸•à¸£à¸‡ (à¸•à¸­à¸™à¸™à¸µà¹‰à¸„à¸·à¸­ Chunk UUID à¸ˆà¸£à¸´à¸‡)
            item.get('doc_id') 
            for item in all_priority_items
            if isinstance(item, dict) and item.get('doc_id')
        ]

        # ðŸŽ¯ FIX: à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™à¸Šà¸·à¹ˆà¸­à¸•à¸±à¸§à¹à¸›à¸£à¹ƒà¸«à¹‰à¸ªà¸·à¹ˆà¸­à¸–à¸¶à¸‡ Chunk UUIDs
        mapped_chunk_uuids: List[str] = list(set(doc_ids_for_dedup))
        num_historical_chunks = len(mapped_chunk_uuids)

        priority_docs = [] 
        
        if num_historical_chunks > 0:
            levels_logged = f"L1-L{level-1}" if level > 1 else "L0 (Should not happen)"
            logger.critical(f"ðŸ§­ DEBUG: Priority Search initiated with {num_historical_chunks} historical Chunk UUIDs ({levels_logged}).") 
            logger.info(f"âœ… Hybrid Mapping: Found {num_historical_chunks} pre-mapped Chunk UUIDs from {levels_logged} for {sub_id}. Prioritizing these.")
            
            if vectorstore_manager:
                try:
                    # Assuming enhance_query_for_statement is available
                    rag_queries_for_vsm = enhance_query_for_statement(
                        statement_text=statement_text,
                        sub_id=sub_id, 
                        statement_id=sub_id, 
                        level=level, 
                        enabler_id=self.enabler_id,
                        focus_hint=level_constraint 
                    )
                    
                    doc_type = self.doc_type 
                    
                    # 3.1 à¸”à¸¶à¸‡à¹€à¸­à¸à¸ªà¸²à¸£à¸•à¸²à¸¡ Chunk UUIDs à¸—à¸µà¹ˆà¸žà¸š
                    # retrieve_context_by_doc_ids à¹„à¸”à¹‰à¸£à¸±à¸šà¸à¸²à¸£à¹à¸à¹‰à¹„à¸‚à¹ƒà¸«à¹‰à¹ƒà¸Šà¹‰ UUIDs à¹‚à¸”à¸¢à¸•à¸£à¸‡à¹à¸¥à¹‰à¸§
                    retrieved_docs_result = retrieve_context_by_doc_ids(
                        doc_uuids=mapped_chunk_uuids, # <-- à¹ƒà¸Šà¹‰ Chunk UUIDs
                        doc_type=doc_type,
                        enabler=self.enabler_id,
                        vectorstore_manager=vectorstore_manager
                    )
                    
                    initial_priority_chunks: List[Dict[str, Any]] = retrieved_docs_result.get("top_evidences", [])
                    
                    if initial_priority_chunks:
                        # Rerank à¹€à¸žà¸·à¹ˆà¸­à¹€à¸¥à¸·à¸­à¸ Chunk à¸—à¸µà¹ˆà¹€à¸à¸µà¹ˆà¸¢à¸§à¸‚à¹‰à¸­à¸‡à¸—à¸µà¹ˆà¸ªà¸¸à¸”
                        reranker = get_global_reranker(self.FINAL_K_RERANKED) 
                        rerank_query = rag_queries_for_vsm[0] 
                        
                        # à¸ªà¸£à¹‰à¸²à¸‡ LcDocument list à¸ªà¸³à¸«à¸£à¸±à¸š Rerank
                        lc_docs_for_rerank = [
                            LcDocument(
                                page_content=d.get('content') or d.get('text', ''), 
                                metadata={
                                    **d, 
                                    'relevance_score': 1.0 
                                }
                            ) 
                            for d in initial_priority_chunks
                        ]
                        
                        if reranker and hasattr(reranker, 'compress_documents'):
                            reranked_docs = reranker.compress_documents(
                                query=rerank_query,
                                documents=lc_docs_for_rerank,
                                top_n=self.PRIORITY_CHUNK_LIMIT 
                            )
                            # à¹à¸›à¸¥à¸‡à¸à¸¥à¸±à¸šà¹€à¸›à¹‡à¸™ Dict
                            priority_docs = [{
                                **d.metadata, 
                                'content': d.page_content,
                                'text': d.page_content, 
                                'score': d.metadata.get('relevance_score', 1.0) 
                            } for d in reranked_docs]
                        else:
                            priority_docs = initial_priority_chunks[:self.PRIORITY_CHUNK_LIMIT]

                        logger.critical(f"ðŸ§­ DEBUG: Limited and prioritized {len(priority_docs)} chunks from {num_historical_chunks} mapped UUIDs.")

                except Exception as e:
                    logger.error(f"Error fetching/reranking priority chunks for {sub_id}: {e}")
                    priority_docs = [] 
        
        # à¸„à¸·à¸™à¸„à¹ˆà¸² Chunk UUIDs à¹à¸¥à¸° Chunks à¸—à¸µà¹ˆà¸–à¸¹à¸à¸”à¸¶à¸‡à¹à¸¥à¸°à¸ˆà¸±à¸”à¸¥à¸³à¸”à¸±à¸šà¸„à¸§à¸²à¸¡à¸ªà¸³à¸„à¸±à¸à¹à¸¥à¹‰à¸§
        return mapped_chunk_uuids, priority_docs

    # -------------------- Calculation Helpers (ADDED) --------------------
    def _calculate_weighted_score(self, highest_full_level: int, weight: int) -> float:
        """
        Calculates the weighted score based on the highest full level achieved.
        Score is calculated by: (Level / 5) * Weight
        """
        MAX_LEVEL_CALC = 5  
        
        if highest_full_level <= 0:
            return 0.0
        
        level_for_calc = min(highest_full_level, MAX_LEVEL_CALC)
        score = (level_for_calc / MAX_LEVEL_CALC) * weight
        return score

    def _calculate_overall_stats(self, target_sub_id: str = "all"):
            """
            Calculates the total weighted score, total possible weight, and overall maturity score/level.
            """
            total_weighted_score = 0.0
            total_possible_weight = 0.0
            assessed_count = 0
            
            for result in self.final_subcriteria_results:
                if target_sub_id.lower() != "all" and result.get('sub_criteria_id') != target_sub_id:
                    continue

                weighted_score = result.get('weighted_score', 0.0)
                weight = result.get('weight', 0)
                
                total_weighted_score += weighted_score
                total_possible_weight += weight
                assessed_count += 1
                
            overall_maturity_score_avg = 0.0
            overall_maturity_level = "N/A"
            overall_progress_percent = 0.0 # à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸²à¹€à¸£à¸´à¹ˆà¸¡à¸•à¹‰à¸™à¸—à¸µà¹ˆ 0.0
            
            if total_possible_weight > 0:
                overall_progress_percent = total_weighted_score / total_possible_weight
                
                MAX_LEVEL_STATS = 5 
                overall_maturity_score_avg = overall_progress_percent * MAX_LEVEL_STATS 

                # ðŸŸ¢ FIX: Completed Logic for Maturity Level Determination
                if overall_maturity_score_avg >= 4.5:
                    overall_maturity_level = "L5"
                elif overall_maturity_score_avg >= 3.5:
                    overall_maturity_level = "L4"
                elif overall_maturity_score_avg >= 2.5:
                    overall_maturity_level = "L3"
                elif overall_maturity_score_avg >= 1.5:
                    overall_maturity_level = "L2"
                elif overall_maturity_score_avg >= 0.5:
                    overall_maturity_level = "L1"
                else:
                    overall_maturity_level = "L0"
            
            self.total_stats = {
                "Overall Maturity Score (Avg.)": overall_maturity_score_avg,
                "Overall Maturity Level (Weighted)": overall_maturity_level,
                "Number of Sub-Criteria Assessed": assessed_count,
                "Total Weighted Score Achieved": total_weighted_score,
                "Total Possible Weight": total_possible_weight,
                "Overall Progress Percentage (0.0 - 1.0)": overall_progress_percent,
                "percentage_achieved_run": overall_progress_percent * 100,
                "total_subcriteria": len(self.rubric),
                "target_level": self.config.target_level
            }
            return self.total_stats
            
    # -------------------- Multiprocessing Worker Method --------------------
    def _assess_single_sub_criteria_worker(self, args) -> Tuple[List[Dict[str, Any]], Dict[str, Any], Dict[str, List[Dict[str, Any]]]]:
        """
        Worker function for multiprocessing.
        args: tuple(statement_data, engine_config_dict)
        Returns:
            - raw_results_for_sub: list of final results for each level
            - final_sub_result: summary of sub-criteria evaluation
            - level_evidences: dict of evidences to merge later in main process
                Format: { "1.1.L1": [ { "doc_id": ..., "filename": ..., "text": ... }, ... ] }
        """
        statement_data, engine_config_dict = args

        # Unpack engine config
        llm_executor = engine_config_dict['llm_executor']
        enabler = engine_config_dict['enabler']
        doc_type = engine_config_dict['doc_type']
        vectorstore_manager = engine_config_dict['vectorstore_manager']
        mapped_uuids = engine_config_dict.get('mapped_uuids')
        priority_docs_input = engine_config_dict.get('priority_docs_input')
        contextual_rules_prompt = engine_config_dict.get('contextual_rules_prompt', "")

        # Statement metadata
        level = int(statement_data.get("level", 0))
        statement_text = statement_data.get("statement", "")
        sub_criteria_name = statement_data.get("sub_criteria_name", "")
        pdca_phase = statement_data.get("pdca_phase", "")
        level_constraint = statement_data.get("level_constraint", "")
        sub_id = statement_data.get("sub_criteria_id", statement_data.get("sub_id", ""))

        # Determine retrieval/evaluation functions
        if level <= 2:
            retrieval_func = retrieve_context_for_low_levels
            evaluation_func = evaluate_with_llm_low_level
            top_k = 5
        else:
            retrieval_func = retrieve_context_with_filter
            evaluation_func = evaluate_with_llm
            top_k = 10

        # Build enhanced query for RAG
        rag_query_list = enhance_query_for_statement(
            statement_text=statement_text,
            sub_id=sub_id,
            statement_id=statement_data.get('statement_id', sub_id),
            level=level,
            enabler_id=enabler,
            focus_hint=level_constraint,
            llm_executor=llm_executor
        )
        rag_query = rag_query_list[0] if rag_query_list else statement_text

        # Retrieval
        retrieval_result = retrieval_func(
            query=rag_query,
            doc_type=doc_type,
            enabler=enabler,
            vectorstore_manager=vectorstore_manager,
            top_k=top_k,
            mapped_uuids=mapped_uuids,
            priority_docs_input=priority_docs_input,
            sub_id=sub_id,
            level=level
        )

        top_evidences = retrieval_result.get("top_evidences", [])
        aggregated_context = retrieval_result.get("aggregated_context", "")

        # Collect previous level evidences (from main process map)
        previous_levels_map = {}
        try:
            previous_levels_map = self._collect_previous_level_evidences(sub_id)
        except Exception:
            previous_levels_map = {}

        # Build multichannel context
        channels = build_multichannel_context_for_level(level, top_evidences, previous_levels_map)

        # Evaluate statement
        evaluation_result = evaluation_func(
            context=channels.get("direct_context", "") or aggregated_context,
            sub_criteria_name=sub_criteria_name,
            level=level,
            statement_text=statement_text,
            sub_id=sub_id,
            llm_executor=llm_executor,
            pdca_phase=pdca_phase,
            level_constraint=level_constraint,
            contextual_rules_prompt=contextual_rules_prompt,
            baseline_summary=channels.get("baseline_summary", ""),
            aux_summary=channels.get("aux_summary", "")
        )

        # Summarize context for report
        summary_result = create_context_summary_llm(
            context=channels.get("direct_context", "") or aggregated_context,
            sub_criteria_name=sub_criteria_name,
            level=level,
            sub_id=sub_id,
            llm_executor=llm_executor
        )

        # Prepare evidences to return (for main process to merge)
        level_key = f"{sub_id}.L{level}"
        level_evidences = {
            level_key: [
                {
                    "doc_id": ev.get("doc_id"),
                    "filename": ev.get("filename"),
                    "text": ev.get("snippet", "")
                }
                for ev in top_evidences if ev.get("doc_id")
            ]
        }

        # Final result dict
        final_sub_result = {
            "sub_criteria_id": sub_id,
            "sub_criteria_name": sub_criteria_name,
            "level": level,
            "statement": statement_text,
            "pdca_phase": pdca_phase,
            "llm_result": evaluation_result,
            "used_doc_ids": [d.get("doc_id") for d in top_evidences if d.get("doc_id")],
            "channels_debug": channels.get("debug_meta", {}),
            "summary": summary_result
        }

        raw_results_for_sub = [final_sub_result]

        return raw_results_for_sub, final_sub_result, level_evidences

    def _run_sub_criteria_assessment_worker(
        self,
        sub_criteria: Dict[str, Any],
    ) -> Tuple[Dict[str, Any], Dict[str, List[Dict[str, Any]]]]:
        """
        à¸£à¸±à¸™à¸à¸²à¸£à¸›à¸£à¸°à¹€à¸¡à¸´à¸™ L1-L5 à¹à¸šà¸š sequential à¸ªà¸³à¸«à¸£à¸±à¸š sub-criteria à¸«à¸™à¸¶à¹ˆà¸‡à¸•à¸±à¸§
        à¹à¸¥à¸°à¸ªà¹ˆà¸‡ evidence map à¸à¸¥à¸±à¸šà¹„à¸›à¹ƒà¸«à¹‰ main process à¸£à¸§à¸¡
        """
        sub_id = sub_criteria['sub_id']
        sub_criteria_name = sub_criteria['sub_criteria_name']
        sub_weight = sub_criteria.get('weight', 0)

        MAX_L1_ATTEMPTS = 2
        highest_full_level = 0
        is_passed_current_level = True
        raw_results_for_sub_seq: List[Dict[str, Any]] = []

        self.logger.info(f"[WORKER START] Assessing Sub-Criteria: {sub_id} - {sub_criteria_name} (Weight: {sub_weight})")

        # à¸£à¸µà¹€à¸‹à¹‡à¸• temp_map_for_save à¹€à¸‰à¸žà¸²à¸° worker à¸™à¸µà¹‰ (à¸ªà¸³à¸„à¸±à¸à¸¡à¸²à¸à¸ªà¸³à¸«à¸£à¸±à¸š Parallel!)
        self.temp_map_for_save = {}

        # 1. Loop à¸œà¹ˆà¸²à¸™à¸—à¸¸à¸ Level (L1 â†’ L5)
        for statement_data in sub_criteria.get('levels', []):
            level = statement_data.get('level')
            if level is None or level > self.config.target_level:
                continue

            # Dependency check: à¸–à¹‰à¸² level à¸à¹ˆà¸­à¸™à¸«à¸™à¹‰à¸² fail â†’ cap à¸—à¸µà¹ˆà¸™à¸µà¹ˆ
            dependency_failed = level > 1 and not is_passed_current_level
            previous_level = level - 1
            persistence_key = f"{sub_id}.L{previous_level}"
            sequential_chunk_uuids = self.evidence_map.get(persistence_key, [])

            level_result = {}
            level_temp_map: List[Dict[str, Any]] = []

            # --- à¹€à¸£à¸µà¸¢à¸ _run_single_assessment (à¸£à¸±à¸š 2 à¸„à¹ˆà¸²: result, temp_map) ---
            if level >= 3:
                # L3-L5: à¹ƒà¸Šà¹‰ RetryPolicy
                wrapper = self.retry_policy.run(
                    fn=lambda attempt: self._run_single_assessment(
                        sub_criteria=sub_criteria,
                        statement_data=statement_data,
                        vectorstore_manager=self.vectorstore_manager,
                        sequential_chunk_uuids=sequential_chunk_uuids
                    ),
                    level=level,
                    statement=statement_data.get('statement', ''),
                    context_blocks={"sequential_chunk_uuids": sequential_chunk_uuids},
                    logger=self.logger
                )

                # à¸ªà¸³à¸„à¸±à¸: wrapper.result à¸•à¸­à¸™à¸™à¸µà¹‰à¹€à¸›à¹‡à¸™ tuple (result, temp_map)
                if isinstance(wrapper, RetryResult) and wrapper.result is not None:
                    level_result = wrapper.result
                    level_temp_map = level_result.get("temp_map_for_level", []) # <-- à¸”à¸¶à¸‡ List Evidence à¸­à¸­à¸à¸¡à¸²
                else:
                    level_result = {}
                    level_temp_map = []

            else:
                # L1-L2: à¸¥à¸­à¸‡à¸ªà¸¹à¸‡à¸ªà¸¸à¸” 2 à¸„à¸£à¸±à¹‰à¸‡
                for attempt in range(MAX_L1_ATTEMPTS):
                    level_result = self._run_single_assessment(
                        sub_criteria=sub_criteria,
                        statement_data=statement_data,
                        vectorstore_manager=self.vectorstore_manager,
                        sequential_chunk_uuids=sequential_chunk_uuids
                    )
                    level_temp_map = level_result.get("temp_map_for_level", []) # <-- à¸”à¸¶à¸‡ List Evidence à¸­à¸­à¸à¸¡à¸²
                    if level_result.get('is_passed', False):
                        break

            # à¹ƒà¸Šà¹‰ result à¸—à¸µà¹ˆà¹„à¸”à¹‰à¸¡à¸²
            result_to_process = level_result or {}
            result_to_process.setdefault("used_chunk_uuids", [])

            # à¸•à¸±à¸”à¸ªà¸´à¸™ pass/fail à¸ªà¸¸à¸”à¸—à¹‰à¸²à¸¢ (à¸£à¸§à¸¡ dependency cap)
            is_passed_llm = result_to_process.get('is_passed', False)
            is_passed_final = is_passed_llm and not dependency_failed

            result_to_process['is_passed'] = is_passed_final
            result_to_process['is_capped'] = is_passed_llm and not is_passed_final
            result_to_process['pdca_score_required'] = get_correct_pdca_required_score(level)

            # à¸šà¸±à¸™à¸—à¸¶à¸ evidence à¸¥à¸‡ temp_map_for_save à¹€à¸‰à¸žà¸²à¸°à¹€à¸¡à¸·à¹ˆà¸­ PASS à¸ˆà¸£à¸´à¸‡
            if is_passed_final and level_temp_map and isinstance(level_temp_map, list):
                current_key = f"{sub_id}.L{level}"
                self.temp_map_for_save[current_key] = level_temp_map
                self.logger.info(f"[EVIDENCE SAVED] {current_key} â†’ {len(level_temp_map)} chunks")

            # à¸­à¸±à¸›à¹€à¸”à¸•à¸ªà¸–à¸²à¸™à¸°à¸ªà¸³à¸«à¸£à¸±à¸š level à¸–à¸±à¸”à¹„à¸›
            is_passed_current_level = is_passed_final

            # à¹€à¸žà¸´à¹ˆà¸¡à¸¥à¸‡ raw results
            result_to_process.setdefault("level", level)
            result_to_process["execution_index"] = len(raw_results_for_sub_seq)
            raw_results_for_sub_seq.append(result_to_process)

            # à¸­à¸±à¸›à¹€à¸”à¸• highest level
            if is_passed_final:
                highest_full_level = level
            else:
                self.logger.info(f"[WORKER STOP] {sub_id} failed at L{level}. Highest achieved: L{highest_full_level}")
                break  # à¸«à¸¢à¸¸à¸”à¸—à¸±à¸™à¸—à¸µà¹€à¸¡à¸·à¹ˆà¸­ fail

        # à¸ªà¸£à¸¸à¸›à¸œà¸¥ sub-criteria
        weighted_score = self._calculate_weighted_score(highest_full_level, sub_weight)
        num_passed = sum(1 for r in raw_results_for_sub_seq if r.get("is_passed", False))

        sub_summary = {
            "num_statements": len(raw_results_for_sub_seq),
            "num_passed": num_passed,
            "num_failed": len(raw_results_for_sub_seq) - num_passed,
            "pass_rate": round(num_passed / len(raw_results_for_sub_seq), 4) if raw_results_for_sub_seq else 0.0
        }

        final_sub_result = {
            "sub_criteria_id": sub_id,
            "sub_criteria_name": sub_criteria_name,
            "highest_full_level": highest_full_level,
            "weight": sub_weight,
            "target_level_achieved": highest_full_level >= self.config.target_level,
            "weighted_score": weighted_score,
            "action_plan": [],
            "raw_results_ref": raw_results_for_sub_seq,
            "sub_summary": sub_summary,
        }

        final_temp_map = self.temp_map_for_save  # à¸ªà¹ˆà¸‡à¸à¸¥à¸±à¸šà¸—à¸±à¹‰à¸‡ dict

        self.logger.info(f"[WORKER END] {sub_id} | Highest: L{highest_full_level} | Evidence keys: {len(final_temp_map)}")
        self.logger.debug(f"Evidence keys returned: {list(final_temp_map.keys())}")

        return final_sub_result, final_temp_map

    def run_assessment(
            self,
            target_sub_id: str = "all",
            export: bool = False,
            vectorstore_manager: Optional['VectorStoreManager'] = None,
            sequential: bool = False
        ) -> Dict[str, Any]:
        """
        Main runner à¸‚à¸­à¸‡ Assessment Engine
        à¸£à¸­à¸‡à¸£à¸±à¸šà¸—à¸±à¹‰à¸‡ Parallel à¹à¸¥à¸° Sequential 100%
        à¹à¸¥à¸°à¸£à¸±à¸šà¸›à¸£à¸°à¸à¸±à¸™à¸§à¹ˆà¸² evidence_map à¸„à¸£à¸šà¸—à¸¸à¸à¸à¸£à¸“à¸µ
        """
        start_ts = time.time()
        self.is_sequential = sequential

        # ============================== 1. Filter Rubric ==============================
        if target_sub_id.lower() == "all":
            sub_criteria_list = self.rubric
        else:
            sub_criteria_list = [
                s for s in self.rubric if s.get('sub_id') == target_sub_id
            ]
            if not sub_criteria_list:
                logger.error(f"Sub-Criteria ID '{target_sub_id}' not found in rubric.")
                return {"error": f"Sub-Criteria ID '{target_sub_id}' not found."}

        # Reset states
        self.raw_llm_results = []
        self.final_subcriteria_results = []
        self.evidence_map.clear()  # à¹€à¸„à¸¥à¸µà¸¢à¸£à¹Œà¸—à¸¸à¸à¸„à¸£à¸±à¹‰à¸‡à¸à¹ˆà¸­à¸™à¹€à¸£à¸´à¹ˆà¸¡à¹ƒà¸«à¸¡à¹ˆ

        if not sequential:
            self.logger.info("[PARALLEL MODE] Starting parallel assessment...")

        run_parallel = (target_sub_id.lower() == "all" and not self.config.force_sequential)

        # ============================== 2. Run Assessment ==============================
        if run_parallel:
            # --------------------- PARALLEL MODE ---------------------
            logger.info("Starting Parallel Assessment with Multiprocessing...")

            worker_args = [(
                sub_data,
                self.config.enabler,
                self.config.target_level,
                self.config.mock_mode,
                self.evidence_map_path,
                self.config.model_name,
                self.config.temperature
            ) for sub_data in sub_criteria_list]

            try:
                pool_ctx = multiprocessing.get_context('spawn')
                with pool_ctx.Pool(processes=max(1, os.cpu_count() - 1)) as pool:
                    results_list = pool.map(_static_worker_process, worker_args)
            except Exception as e:
                logger.critical(f"Multiprocessing failed: {e}")
                raise

            # à¸£à¸§à¸¡à¸œà¸¥à¸¥à¸±à¸žà¸˜à¹Œà¸ˆà¸²à¸à¸—à¸¸à¸ worker
            for sub_result, temp_map_from_worker in results_list:
                # à¸£à¸§à¸¡ Evidence Map
                if isinstance(temp_map_from_worker, dict):
                    for level_key, evidence_list in temp_map_from_worker.items():
                        if isinstance(evidence_list, list) and evidence_list:
                            current_list = self.evidence_map.setdefault(level_key, [])
                            current_list.extend(evidence_list)
                            self.logger.info(f"AGGREGATED: +{len(evidence_list)} â†’ {level_key} "
                                           f"(total: {len(current_list)})")

                # à¸£à¸§à¸¡à¸œà¸¥à¸¥à¸±à¸žà¸˜à¹Œà¸­à¸·à¹ˆà¸™ à¹†
                raw_refs = sub_result.get("raw_results_ref", [])
                self.raw_llm_results.extend(raw_refs if isinstance(raw_refs, list) else [])
                self.final_subcriteria_results.append(sub_result)

        else:
            # --------------------- SEQUENTIAL MODE ---------------------
            mode_desc = target_sub_id if target_sub_id != "all" else "All Sub-Criteria (Sequential)"
            self.logger.info(f"Starting Sequential Assessment: {mode_desc}")

            # à¸ªà¸³à¸„à¸±à¸à¸—à¸µà¹ˆà¸ªà¸¸à¸”: à¸­à¸¢à¹ˆà¸²à¸ªà¸£à¹‰à¸²à¸‡ temp_map_for_save à¹€à¸¥à¸¢à¹ƒà¸™ Sequential
            # à¹à¸¥à¸°à¸­à¸¢à¹ˆà¸²à¸£à¸±à¸š temp_map_from_worker à¸¡à¸²à¸—à¸³à¸­à¸°à¹„à¸£à¸—à¸±à¹‰à¸‡à¸™à¸±à¹‰à¸™!

            local_vsm = vectorstore_manager or (
                load_all_vectorstores(doc_types=[EVIDENCE_DOC_TYPES], evidence_enabler=self.config.enabler)
                if self.config.mock_mode == "none" else None
            )
            self.vectorstore_manager = local_vsm

            for sub_criteria in sub_criteria_list:
                # à¹€à¸£à¸µà¸¢à¸ worker à¹à¸•à¹ˆà¹„à¸¡à¹ˆà¸ªà¸™à¹ƒà¸ˆ temp_map_from_worker à¹€à¸žà¸£à¸²à¸° Sequential à¸šà¸±à¸™à¸—à¸¶à¸à¸•à¸£à¸‡à¹ƒà¸™ evidence_map à¹à¸¥à¹‰à¸§
                sub_result, _ = self._run_sub_criteria_assessment_worker(sub_criteria)

                # à¸£à¸§à¸¡à¸œà¸¥à¸¥à¸±à¸žà¸˜à¹Œà¸›à¸à¸•à¸´
                self.raw_llm_results.extend(sub_result.get("raw_results_ref", []))
                self.final_subcriteria_results.append(sub_result)

        # ============================== 3. à¸šà¸±à¸™à¸—à¸¶à¸ Evidence Map ==============================
        if self.evidence_map:
            self._save_evidence_map(map_to_save=self.evidence_map)
            total_items = sum(len(v) for v in self.evidence_map.values())
            self.logger.info(f"Persisted final evidence map | Keys: {len(self.evidence_map)} | "
                            f"Items: {total_items} | Size: ~{total_items * 0.35:.1f} KB")

        # ============================== 4. à¸ªà¸£à¸¸à¸›à¸œà¸¥ & Export ==============================
        self._calculate_overall_stats(target_sub_id)

        final_results = {
            "summary": self.total_stats,
            "sub_criteria_results": self.final_subcriteria_results,
            "raw_llm_results": self.raw_llm_results,
            "run_time_seconds": round(time.time() - start_ts, 2),
            "timestamp": datetime.now().isoformat(),
        }

        if export:
            export_path = self._export_results(
                results=final_results,
                enabler=self.config.enabler,
                sub_criteria_id=target_sub_id if target_sub_id != "all" else "ALL",
                target_level=self.config.target_level
            )
            # à¸ªà¸³à¸„à¸±à¸: à¸£à¸§à¸¡ evidence_map à¸—à¸¸à¸à¸à¸£à¸“à¸µ (à¸—à¸±à¹‰à¸‡ sequential à¹à¸¥à¸° parallel)
            final_results["export_path_used"] = export_path
            final_results["evidence_map"] = deepcopy(self.evidence_map)
            self.logger.info(f"Exported full results â†’ {export_path}")

        return final_results
    

    def _run_single_assessment(
        self,
        sub_criteria: Dict[str, Any],
        statement_data: Dict[str, Any],
        vectorstore_manager: Optional['VectorStoreManager'],
        sequential_chunk_uuids: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        à¸£à¸±à¸™à¸à¸²à¸£à¸›à¸£à¸°à¹€à¸¡à¸´à¸™ Level à¹€à¸”à¸µà¸¢à¸§ (L1-L5) à¸­à¸¢à¹ˆà¸²à¸‡à¸ªà¸¡à¸šà¸¹à¸£à¸“à¹Œ
        - à¹ƒà¸Šà¹‰à¸«à¸¥à¸±à¸à¸à¸²à¸™à¸ˆà¸²à¸ Level à¸à¹ˆà¸­à¸™à¸«à¸™à¹‰à¸²à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸” (baseline)
        - à¸šà¸±à¸™à¸—à¸¶à¸ evidence map à¸„à¸£à¸šà¸—à¸¸à¸ Level
        - à¸£à¸­à¸‡à¸£à¸±à¸š Sequential & Parallel 100%
        """

        sub_id = sub_criteria['sub_id']
        level = statement_data['level']
        statement_text = statement_data['statement']
        sub_criteria_name = sub_criteria['sub_criteria_name']
        statement_id = statement_data.get('statement_id', sub_id)

        logger.info(f"  > Starting assessment for {sub_id} L{level}...")

        # ==================== 1. PDCA & Level Prompt ====================
        pdca_phase = self._get_pdca_phase(level)
        level_constraint = self._get_level_constraint_prompt(level)
        contextual_rules_prompt = self._get_contextual_rules_prompt(sub_id, level)
        full_focus_hint = level_constraint + contextual_rules_prompt

        # ==================== 2. Hybrid Retrieval Setup ====================
        mapped_stable_doc_ids, priority_docs = self._get_mapped_uuids_and_priority_chunks(
            sub_id=sub_id,
            level=level,
            statement_text=statement_text,
            level_constraint=level_constraint,
            vectorstore_manager=vectorstore_manager
        )

        # ==================== 3. Enhance Query ====================
        rag_query_list = enhance_query_for_statement(
            statement_text=statement_text,
            sub_id=sub_id,
            statement_id=statement_id,
            level=level,
            enabler_id=self.enabler_id,
            focus_hint=full_focus_hint,
            llm_executor=self.llm
        )
        rag_query = rag_query_list[0] if rag_query_list else statement_text

        # ==================== 4. LLM Evaluator Setup ====================
        llm_evaluator_to_use = self.llm_evaluator
        if level <= 2:
            llm_evaluator_to_use = evaluate_with_llm_low_level

        # ==================== 5. RAG Retrieval ====================
        retrieval_start = time.time()
        try:
            retrieval_result = self.rag_retriever(
                query=rag_query_list,
                doc_type=EVIDENCE_DOC_TYPES,
                enabler=self.enabler_id,
                sub_id=sub_id,
                level=level,
                vectorstore_manager=vectorstore_manager,
                mapped_uuids=mapped_stable_doc_ids,
                priority_docs_input=priority_docs
            )
        except Exception as e:
            logger.error(f"RAG retrieval failed for {sub_id} L{level}: {e}")
            retrieval_result = {"top_evidences": [], "aggregated_context": "ERROR: RAG failure.", "used_chunk_uuids": []}

        retrieval_duration = time.time() - retrieval_start
        top_evidences = retrieval_result.get("top_evidences", [])
        used_chunk_uuids = retrieval_result.get("used_chunk_uuids", [])

        # ==================== 6. à¸”à¸¶à¸‡à¸«à¸¥à¸±à¸à¸à¸²à¸™à¸ˆà¸²à¸ Level à¸à¹ˆà¸­à¸™à¸«à¸™à¹‰à¸² (à¸ªà¸³à¸„à¸±à¸à¸—à¸µà¹ˆà¸ªà¸¸à¸”!) ====================
        self.current_level = level  # à¸ªà¸³à¸„à¸±à¸à¸¡à¸²à¸! à¸•à¸­à¸™à¸™à¸µà¹‰à¸¡à¸µà¹à¸¥à¹‰à¸§
        previous_levels_raw = self._collect_previous_level_evidences(sub_id)

        # à¸ªà¸£à¹‰à¸²à¸‡ list à¸‚à¸­à¸‡ evidence dicts à¹€à¸•à¹‡à¸¡ à¹† (à¸—à¸µà¹ˆà¸¡à¸µ text, doc_id, filename)
        previous_levels_evidence_full = []
        previous_levels_filename_map = {}  # {doc_id: filename} à¸ªà¸³à¸«à¸£à¸±à¸š fallback

        # à¸”à¸¶à¸‡à¸ˆà¸²à¸à¸—à¸±à¹‰à¸‡ temp_map_for_save à¹à¸¥à¸° evidence_map (à¸•à¸²à¸¡à¸¥à¸³à¸”à¸±à¸šà¸„à¸§à¸²à¸¡à¸ªà¸³à¸„à¸±à¸)
        for ev_list in previous_levels_raw.values():
            if not isinstance(ev_list, list):
                continue
            for ev in ev_list:
                # à¹ƒà¸Šà¹‰ doc_id à¹€à¸›à¹‡à¸™à¸«à¸¥à¸±à¸ (stable ID)
                doc_id = ev.get("doc_id") or ev.get("chunk_uuid")
                if not doc_id or str(doc_id).startswith("HASH-"):
                    continue

                # à¹€à¸žà¸´à¹ˆà¸¡ evidence à¹€à¸•à¹‡à¸¡ dict (à¸—à¸µà¹ˆà¸¡à¸µ text!)
                previous_levels_evidence_full.append(ev)

                # à¸ªà¸£à¹‰à¸²à¸‡ map à¸Šà¸·à¹ˆà¸­à¹„à¸Ÿà¸¥à¹Œà¸ªà¸³à¸«à¸£à¸±à¸š fallback
                filename = (
                    ev.get("source_filename") or
                    ev.get("source") or
                    ev.get("filename") or
                    "UNKNOWN_FILE"
                )
                previous_levels_filename_map[doc_id] = filename

        # LOG à¸—à¸µà¹ˆà¸ˆà¸°à¸—à¸³à¹ƒà¸«à¹‰à¸žà¸µà¹ˆà¸™à¹‰à¸³à¸•à¸²à¹„à¸«à¸¥à¸”à¹‰à¸§à¸¢à¸„à¸§à¸²à¸¡à¸ªà¸¸à¸‚
        logger.info(
            f"LOADED {len(previous_levels_evidence_full)} previous evidences "
            f"from {len(previous_levels_raw)} passed levels (L1-L{level-1}) for baseline context"
        )
        if previous_levels_evidence_full:
            sample_files = [
                (ev.get("source_filename") or ev.get("filename") or "?.pdf").split("/")[-1][-30:]
                for ev in previous_levels_evidence_full[:3]
            ]
            logger.info(f"   Sample baseline files: {sample_files}")

        # ==================== 6a. Sequential fallback (à¹€à¸žà¸´à¹ˆà¸¡à¸«à¸¥à¸±à¸à¸à¸²à¸™à¹€à¸à¹ˆà¸²à¸—à¸µà¹ˆà¸‚à¸²à¸”) ====================
        if level > 1 and self.is_sequential:
            current_ids = {d.get("doc_id") or d.get("chunk_uuid") for d in top_evidences}
            added_count = 0
            for ev in previous_levels_evidence_full:
                old_id = ev.get("doc_id") or ev.get("chunk_uuid")
                if old_id and old_id not in current_ids:
                    fallback_ev = ev.copy()
                    fallback_ev["pdca_tag"] = "Baseline"
                    top_evidences.append(fallback_ev)
                    added_count += 1
            if added_count:
                logger.info(f"   Added {added_count} baseline evidences from previous levels into top_evidences")

        # ==================== 7. à¸ªà¸£à¹‰à¸²à¸‡ Multi-Channel Context (à¸ªà¸³à¸„à¸±à¸à¸—à¸µà¹ˆà¸ªà¸¸à¸”) ====================
        channels = build_multichannel_context_for_level(
            level=level,
            top_evidences=top_evidences,
            previous_levels_evidence=previous_levels_evidence_full,  # à¸ªà¸³à¸„à¸±à¸à¸—à¸µà¹ˆà¸ªà¸¸à¸”!
            max_main_context_tokens=3000,
            max_summary_sentences=4
        )

        # Debug log à¸ªà¸¸à¸”à¸—à¹‰à¸²à¸¢
        debug = channels.get("debug_meta", {})
        logger.info(
            f"Context L{level} READY â†’ "
            f"Direct: {debug.get('direct_count', 0)} | "
            f"Aux: {debug.get('aux_count', 0)} | "
            f"Baseline: {len(previous_levels_evidence_full)} evidences "
            f"from {len(previous_levels_raw)} previous levels"
        )

        # ==================== 8. LLM Evaluation ====================
        context_parts = [
            f"--- DIRECT EVIDENCE (L{level}) ---\n{channels.get('direct_context','')}",
            f"--- AUXILIARY EVIDENCE ---\n{channels.get('aux_summary','')}",
            f"--- BASELINE FROM PREVIOUS LEVELS ---\n{channels.get('baseline_summary','à¹„à¸¡à¹ˆà¸¡à¸µ')}"
        ]
        final_llm_context = "\n\n".join([p for p in context_parts if p.strip()])

        llm_start = time.time()
        llm_result = llm_evaluator_to_use(
            context=final_llm_context,
            sub_criteria_name=sub_criteria_name,
            level=level,
            statement_text=statement_text,
            sub_id=sub_id,
            pdca_phase=pdca_phase,
            level_constraint=level_constraint,
            contextual_rules=contextual_rules_prompt,
            llm_executor=self.llm
        )
        llm_duration = time.time() - llm_start

        # ==================== 9-10. Scoring & Pass/Fail ====================
        llm_score = llm_result.get('score', 0) if llm_result else 0
        pdca_breakdown, is_passed, _ = calculate_pdca_breakdown_and_pass_status(llm_score, level)
        status = "PASS" if is_passed else "FAIL"

        # ==================== 11. à¸šà¸±à¸™à¸—à¸¶à¸ Evidence Map (à¹€à¸‰à¸žà¸²à¸° PASS) ====================
        temp_map_for_level = None
        if is_passed and top_evidences:
            seen = set()
            evidence_entries = []
            for ev in top_evidences:
                doc_id = ev.get("doc_id")
                if not doc_id or str(doc_id).startswith("HASH-") or doc_id in seen:
                    continue
                seen.add(doc_id)
                filename = (
                    ev.get("source_filename") or
                    ev.get("source") or
                    ev.get("filename") or
                    previous_levels_filename_map.get(doc_id) or
                    "UNKNOWN_FILE"
                )
                evidence_entries.append({
                    "doc_id": doc_id,
                    "filename": os.path.basename(filename) if '/' in filename or '\\' in filename else filename,
                    "mapper_type": "AI_GENERATED",
                    "timestamp": datetime.now().isoformat(),
                    "text": ev.get("text") or ev.get("content") or ""  # à¹€à¸žà¸´à¹ˆà¸¡ text à¹€à¸žà¸·à¹ˆà¸­à¹ƒà¸«à¹‰ baseline à¸¡à¸µà¸šà¸£à¸´à¸šà¸—à¸„à¸£à¸š
                })

            if evidence_entries:
                temp_map_for_level = evidence_entries
                key = f"{sub_id}.L{level}"
                if self.is_sequential:
                    # Sequential: à¸šà¸±à¸™à¸—à¸¶à¸à¸•à¸£à¸‡à¹€à¸‚à¹‰à¸² evidence_map
                    current_list = self.evidence_map.setdefault(key, [])
                    current_list.extend(evidence_entries)
                    logger.info(f"DIRECT SAVE TO evidence_map â†’ {key} (+{len(evidence_entries)} files)")
                else:
                    # Parallel: à¹ƒà¸Šà¹‰ temp_map_for_save
                    if not hasattr(self, "temp_map_for_save"):
                        self.temp_map_for_save = {}
                    self.temp_map_for_save[key] = evidence_entries
                logger.info(f"  > [EVIDENCE SAVED] {key} â†’ {len(evidence_entries)} files")

        # ==================== 12. à¸ªà¸£à¹‰à¸²à¸‡à¸œà¸¥à¸¥à¸±à¸žà¸˜à¹Œà¸ªà¸¸à¸”à¸—à¹‰à¸²à¸¢ ====================
        unique_refs = {}
        for ev in top_evidences:
            doc_id = ev.get("doc_id")
            if doc_id and not str(doc_id).startswith("HASH-"):
                filename = previous_levels_filename_map.get(doc_id) or "UNKNOWN"
                if doc_id not in unique_refs:
                    unique_refs[doc_id] = {
                        "doc_id": doc_id,
                        "filename": os.path.basename(filename) if '/' in filename or '\\' in filename else filename
                    }

        final_result = {
            "sub_criteria_id": sub_id,
            "statement_id": statement_id,
            "level": level,
            "statement": statement_text,
            "pdca_phase": pdca_phase,
            "llm_score": llm_score,
            "pdca_breakdown": pdca_breakdown,
            "is_passed": is_passed,
            "status": status,
            "score": llm_score,
            "llm_result_full": llm_result,
            "retrieval_duration_s": round(retrieval_duration, 2),
            "llm_duration_s": round(llm_duration, 2),
            "top_evidences_ref": list(unique_refs.values()),
            "temp_map_for_level": temp_map_for_level  # à¸ªà¸³à¸«à¸£à¸±à¸š worker à¸ªà¹ˆà¸‡à¸à¸¥à¸±à¸š
        }

        logger.info(f"  > Assessment {sub_id} L{level} completed â†’ {status} (Score: {llm_score:.1f})")
        return final_result
        

    def _run_single_assessment(
        self,
        sub_criteria: Dict[str, Any],
        statement_data: Dict[str, Any],
        vectorstore_manager: Optional['VectorStoreManager'],
        sequential_chunk_uuids: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        à¸£à¸±à¸™à¸à¸²à¸£à¸›à¸£à¸°à¹€à¸¡à¸´à¸™ Level à¹€à¸”à¸µà¸¢à¸§ (L1-L5) à¸­à¸¢à¹ˆà¸²à¸‡à¸ªà¸¡à¸šà¸¹à¸£à¸“à¹Œ
        - à¹ƒà¸Šà¹‰à¸«à¸¥à¸±à¸à¸à¸²à¸™à¸ˆà¸²à¸ Level à¸à¹ˆà¸­à¸™à¸«à¸™à¹‰à¸²à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸” (baseline)
        - à¸šà¸±à¸™à¸—à¸¶à¸ evidence map à¸„à¸£à¸šà¸—à¸¸à¸ Level
        - à¸£à¸­à¸‡à¸£à¸±à¸š Sequential & Parallel 100%
        """

        sub_id = sub_criteria['sub_id']
        level = statement_data['level']
        statement_text = statement_data['statement']
        sub_criteria_name = sub_criteria['sub_criteria_name']
        statement_id = statement_data.get('statement_id', sub_id)

        logger.info(f"  > Starting assessment for {sub_id} L{level}...")

        # ==================== 1. PDCA & Level Prompt ====================
        pdca_phase = self._get_pdca_phase(level)
        level_constraint = self._get_level_constraint_prompt(level)
        contextual_rules_prompt = self._get_contextual_rules_prompt(sub_id, level)
        full_focus_hint = level_constraint + contextual_rules_prompt

        # ==================== 2. Hybrid Retrieval Setup ====================
        mapped_stable_doc_ids, priority_docs = self._get_mapped_uuids_and_priority_chunks(
            sub_id=sub_id,
            level=level,
            statement_text=statement_text,
            level_constraint=level_constraint,
            vectorstore_manager=vectorstore_manager
        )

        # ==================== 3. Enhance Query ====================
        rag_query_list = enhance_query_for_statement(
            statement_text=statement_text,
            sub_id=sub_id,
            statement_id=statement_id,
            level=level,
            enabler_id=self.enabler_id,
            focus_hint=full_focus_hint,
            llm_executor=self.llm
        )
        rag_query = rag_query_list[0] if rag_query_list else statement_text

        # ==================== 4. LLM Evaluator Setup ====================
        llm_evaluator_to_use = self.llm_evaluator
        if level <= 2:
            llm_evaluator_to_use = evaluate_with_llm_low_level

        # ==================== 5. RAG Retrieval ====================
        retrieval_start = time.time()
        try:
            retrieval_result = self.rag_retriever(
                query=rag_query_list,
                doc_type=EVIDENCE_DOC_TYPES,
                enabler=self.enabler_id,
                sub_id=sub_id,
                level=level,
                vectorstore_manager=vectorstore_manager,
                mapped_uuids=mapped_stable_doc_ids,
                priority_docs_input=priority_docs
            )
        except Exception as e:
            logger.error(f"RAG retrieval failed for {sub_id} L{level}: {e}")
            retrieval_result = {"top_evidences": [], "aggregated_context": "ERROR: RAG failure.", "used_chunk_uuids": []}

        retrieval_duration = time.time() - retrieval_start
        top_evidences = retrieval_result.get("top_evidences", [])
        used_chunk_uuids = retrieval_result.get("used_chunk_uuids", [])

        # ==================== 6. à¸”à¸¶à¸‡à¸«à¸¥à¸±à¸à¸à¸²à¸™à¸ˆà¸²à¸ Level à¸à¹ˆà¸­à¸™à¸«à¸™à¹‰à¸² (à¹€à¸•à¹‡à¸¡ à¹† à¸¡à¸µ text) ====================
        try:
            previous_levels_raw = self._collect_previous_level_evidences(sub_id)
        except Exception as e:
            logger.error(f"Failed to collect previous evidences: {e}")
            previous_levels_raw = {}

        # à¸ªà¸£à¹‰à¸²à¸‡ list à¸‚à¸­à¸‡ evidence dicts à¹€à¸•à¹‡à¸¡ à¹† (à¸—à¸µà¹ˆà¸¡à¸µ text, doc_id, filename)
        previous_levels_evidence_full = []
        previous_levels_filename_map = {}  # {doc_id: filename} à¸ªà¸³à¸«à¸£à¸±à¸š fallback

        for ev_list in previous_levels_raw.values():
            for ev in ev_list:
                doc_id = ev.get("doc_id") or ev.get("chunk_uuid")
                if not doc_id or str(doc_id).startswith("HASH-"):
                    continue
                previous_levels_evidence_full.append(ev)
                filename = ev.get("source_filename") or ev.get("source") or ev.get("filename") or "UNKNOWN"
                previous_levels_filename_map[doc_id] = filename

        # ==================== 6a. Sequential fallback (à¹€à¸žà¸´à¹ˆà¸¡à¸«à¸¥à¸±à¸à¸à¸²à¸™à¹€à¸à¹ˆà¸²à¸—à¸µà¹ˆà¸‚à¸²à¸”) ====================
        if level > 1 and self.is_sequential:
            current_ids = {d.get("doc_id") or d.get("chunk_uuid") for d in top_evidences}
            for ev in previous_levels_evidence_full:
                if (ev.get("doc_id") or ev.get("chunk_uuid")) not in current_ids:
                    fallback_ev = ev.copy()
                    fallback_ev["pdca_tag"] = "Baseline"
                    top_evidences.append(fallback_ev)

        # ==================== 7. à¸ªà¸£à¹‰à¸²à¸‡ Multi-Channel Context (à¸ªà¸³à¸„à¸±à¸à¸—à¸µà¹ˆà¸ªà¸¸à¸”) ====================
        channels = build_multichannel_context_for_level(
            level=level,
            top_evidences=top_evidences,
            previous_levels_evidence=previous_levels_evidence_full,  # à¸ªà¹ˆà¸‡à¹€à¸•à¹‡à¸¡ à¹† à¸¡à¸µ text
            max_main_context_tokens=3000,
            max_summary_sentences=4
        )

        # Debug log
        debug = channels.get("debug_meta", {})
        logger.info(
            f"  > Context built â†’ Direct: {debug.get('direct_count',0)}, "
            f"Aux: {debug.get('aux_count',0)}, "
            f"Baseline: {len(previous_levels_evidence_full)} files "
            f"from {len(previous_levels_raw)} previous levels"
        )

        # ==================== 8. LLM Evaluation ====================
        context_parts = [
            f"--- DIRECT EVIDENCE (L{level}) ---\n{channels.get('direct_context','')}",
            f"--- AUXILIARY EVIDENCE ---\n{channels.get('aux_summary','')}",
            f"--- BASELINE FROM PREVIOUS LEVELS ---\n{channels.get('baseline_summary','à¹„à¸¡à¹ˆà¸¡à¸µ')}"
        ]
        final_llm_context = "\n\n".join([p for p in context_parts if p.strip()])

        llm_start = time.time()
        llm_result = llm_evaluator_to_use(
            context=final_llm_context,
            sub_criteria_name=sub_criteria_name,
            level=level,
            statement_text=statement_text,
            sub_id=sub_id,
            pdca_phase=pdca_phase,
            level_constraint=level_constraint,
            contextual_rules=contextual_rules_prompt,
            llm_executor=self.llm
        )
        llm_duration = time.time() - llm_start

        # ==================== 9-10. Scoring & Pass/Fail ====================
        llm_score = llm_result.get('score', 0) if llm_result else 0
        pdca_breakdown, is_passed, _ = calculate_pdca_breakdown_and_pass_status(llm_score, level)
        status = "PASS" if is_passed else "FAIL"

        # ==================== 11. à¸šà¸±à¸™à¸—à¸¶à¸ Evidence Map (à¹€à¸‰à¸žà¸²à¸° PASS) ====================
        temp_map_for_level = None
        if is_passed and top_evidences:
            seen = set()
            evidence_entries = []
            for ev in top_evidences:
                doc_id = ev.get("doc_id")
                if not doc_id or str(doc_id).startswith("HASH-") or doc_id in seen:
                    continue
                seen.add(doc_id)
                filename = (
                    ev.get("source_filename") or
                    ev.get("source") or
                    ev.get("filename") or
                    previous_levels_filename_map.get(doc_id) or
                    "UNKNOWN_FILE"
                )
                evidence_entries.append({
                    "doc_id": doc_id,
                    "filename": os.path.basename(filename) if '/' in filename or '\\' in filename else filename,
                    "mapper_type": "AI_GENERATED",
                    "timestamp": datetime.now().isoformat()
                })

            if evidence_entries:
                key = f"{sub_id}.L{level}"
                
                if self.is_sequential:
                    # Sequential: à¸šà¸±à¸™à¸—à¸¶à¸à¸•à¸£à¸‡à¹€à¸‚à¹‰à¸² evidence_map à¸—à¸±à¸™à¸—à¸µ!
                    current_list = self.evidence_map.setdefault(key, [])
                    # à¸›à¹‰à¸­à¸‡à¸à¸±à¸™ duplicate à¸”à¹‰à¸§à¸¢ doc_id
                    existing_ids = {item["doc_id"] for item in current_list}
                    new_entries = [e for e in evidence_entries if e["doc_id"] not in existing_ids]
                    current_list.extend(new_entries)
                    logger.info(f"DIRECT SAVE evidence_map[{key}] +{len(new_entries)} files â†’ total {len(current_list)}")
                else:
                    # Parallel: à¹ƒà¸Šà¹‰ temp_map_for_save
                    if not hasattr(self, "temp_map_for_save"):
                        self.temp_map_for_save = {}
                    self.temp_map_for_save[key] = evidence_entries

                logger.info(f"  > [EVIDENCE SAVED] {key} â†’ {len(evidence_entries)} files (unique: {len(new_entries) if self.is_sequential else len(evidence_entries)})")

                # à¸ªà¸³à¸«à¸£à¸±à¸š worker à¸ªà¹ˆà¸‡à¸à¸¥à¸±à¸š (à¹€à¸‰à¸žà¸²à¸° Parallel)
                temp_map_for_level = evidence_entries if not self.is_sequential else None

        # ==================== 12. à¸ªà¸£à¹‰à¸²à¸‡à¸œà¸¥à¸¥à¸±à¸žà¸˜à¹Œà¸ªà¸¸à¸”à¸—à¹‰à¸²à¸¢ ====================
        unique_refs = {}
        for ev in top_evidences:
            doc_id = ev.get("doc_id")
            if doc_id and not str(doc_id).startswith("HASH-"):
                filename = previous_levels_filename_map.get(doc_id) or "UNKNOWN"
                if doc_id not in unique_refs:
                    unique_refs[doc_id] = {
                        "doc_id": doc_id,
                        "filename": os.path.basename(filename) if '/' in filename or '\\' in filename else filename
                    }

        # ==================== à¹€à¸žà¸´à¹ˆà¸¡ Evidence Strength & AI Confidence (à¸†à¹ˆà¸² Consultant à¹„à¸”à¹‰à¹ƒà¸™ 1 à¸§à¸´à¸™à¸²à¸—à¸µ) ====================
        direct_count = len([d for d in top_evidences if d.get("pdca_tag") in ["P", "D", "C", "A"]])
        total_chunks = len(top_evidences)
        pdca_coverage = len({d.get("pdca_tag") for d in top_evidences if d.get("pdca_tag")})

        evidence_strength = min(10.0, 
            (direct_count * 1.8) + 
            (2.0 if total_chunks >= 20 else 1.0 if total_chunks >= 10 else 0.0) +
            (pdca_coverage * 1.5)
        )

        ai_confidence = "HIGH" if evidence_strength >= 8.0 and is_passed else \
                       "MEDIUM" if evidence_strength >= 5.5 else "LOW"

        evidence_count_for_level = len(evidence_entries) if is_passed and 'evidence_entries' in locals() else 0

        final_result = {
            "sub_criteria_id": sub_id,
            "statement_id": statement_id,
            "level": level,
            "statement": statement_text,
            "pdca_phase": pdca_phase,
            "llm_score": llm_score,
            "pdca_breakdown": pdca_breakdown,
            "is_passed": is_passed,
            "status": status,
            "score": llm_score,
            "llm_result_full": llm_result,
            "retrieval_duration_s": round(retrieval_duration, 2),
            "llm_duration_s": round(llm_duration, 2),
            "top_evidences_ref": list(unique_refs.values()),
            "temp_map_for_level": temp_map_for_level,  # à¸ªà¸³à¸«à¸£à¸±à¸š worker à¸ªà¹ˆà¸‡à¸à¸¥à¸±à¸š
            "evidence_strength": round(evidence_strength, 1),
            "ai_confidence": ai_confidence,
            "evidence_count": evidence_count_for_level,
            "pdca_coverage": pdca_coverage,
            "direct_evidence_count": direct_count
        }

        logger.info(f"  > Assessment {sub_id} L{level} completed â†’ {status} (Score: {llm_score:.1f})")
        return final_result
    
    def _export_results(self, results: dict, enabler: str, sub_criteria_id: str, target_level: int, export_dir: str = "assessment_results") -> str:
        """
        Exports the final assessment results to a JSON file.
        
        Args:
            results: The dictionary containing the final assessment summary and results.
            enabler: The enabler ID (e.g., KM).
            sub_criteria_id: The specific sub-criteria ID being run (e.g., 2.2).
            target_level: The target level for the assessment.
            export_dir: The directory to save the output file.
            
        Returns:
            The path to the saved JSON file.
        """
        if not os.path.exists(export_dir):
            os.makedirs(export_dir)
            
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        # à¸ªà¸£à¹‰à¸²à¸‡à¸Šà¸·à¹ˆà¸­à¹„à¸Ÿà¸¥à¹Œ: assessment_results_KM_2.2_YYYYMMDD_HHMMSS.json
        file_name = f"assessment_results_{enabler}_{sub_criteria_id}_{timestamp}.json"
        full_path = os.path.join(export_dir, file_name)

        # Note: results dict should contain 'summary' and 'sub_criteria_results' keys
        # Update summary fields based on the engine data
        results['summary']['enabler'] = enabler
        results['summary']['sub_criteria_id'] = sub_criteria_id
        results['summary']['target_level'] = target_level
        results['summary']['Number of Sub-Criteria Assessed'] = len(results['sub_criteria_results'])

        try:
            with open(full_path, 'w', encoding='utf-8') as f:
                # à¹ƒà¸Šà¹‰ indent=4 à¹€à¸žà¸·à¹ˆà¸­à¹ƒà¸«à¹‰à¸­à¹ˆà¸²à¸™à¸‡à¹ˆà¸²à¸¢
                json.dump(results, f, ensure_ascii=False, indent=4)
            
            logging.info(f"ðŸ’¾ Successfully exported final results to: {full_path}")
            return full_path
        
        except Exception as e:
            logging.error(f"âŒ Failed to export results to {full_path}: {e}")
            return ""