# core/seam_assessment.py

import sys
import json
import logging
import time
import os
from typing import List, Dict, Any, Optional, Union, Tuple, Set, Final, Literal
from collections import defaultdict
from datetime import datetime
from dataclasses import dataclass, field
import multiprocessing # NEW: Import for parallel execution
from core.llm_data_utils import enhance_query_for_statement
import pathlib, uuid
from langchain_core.documents import Document as LcDocument

# -------------------- PATH SETUP & IMPORTS --------------------
try:
    PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
    if PROJECT_ROOT not in sys.path:
        sys.path.append(PROJECT_ROOT)

    from config.global_vars import (
        EXPORTS_DIR, MAX_LEVEL, INITIAL_LEVEL, FINAL_K_RERANKED,
        RUBRIC_FILENAME_PATTERN, RUBRIC_CONFIG_DIR, DEFAULT_ENABLER,
        EVIDENCE_DOC_TYPES, INITIAL_TOP_K,
        EVIDENCE_MAPPING_FILENAME_SUFFIX,
        LIMIT_CHUNKS_PER_PRIORITY_DOC,
        IS_LOG_L3_CONTEXT,
        PRIORITY_CHUNK_LIMIT
    )
    
    from core.llm_data_utils import ( 
        create_structured_action_plan, evaluate_with_llm,
        retrieve_context_with_filter, retrieve_context_for_low_levels,
        evaluate_with_llm_low_level, LOW_LEVEL_K, 
        set_mock_control_mode as set_llm_data_mock_mode,
        create_context_summary_llm,
        retrieve_context_by_doc_ids,
        _fetch_llm_response
    )
    from core.vectorstore import VectorStoreManager, load_all_vectorstores, get_global_reranker 
    from core.seam_prompts import PDCA_PHASE_MAP 
        
    import assessments.seam_mocking as seam_mocking 
 
    
except ImportError as e:
    print(f"FATAL ERROR: Failed to import required modules. Error: {e}", file=sys.stderr)
    
    # Define placeholder variables if imports fail
    EXPORTS_DIR = "exports"
    MAX_LEVEL = 5
    INITIAL_LEVEL = 1
    FINAL_K_RERANKED = 3
    RUBRIC_FILENAME_PATTERN = "{enabler}_rubric.json"
    RUBRIC_CONFIG_DIR = "config/rubrics"
    DEFAULT_ENABLER = "KM"
    EVIDENCE_DOC_TYPES = "evidence"
    INITIAL_TOP_K = 10
    
    def create_structured_action_plan(*args, **kwargs): return [{"Phase": "Mock Plan", "Goal": "Resolve issue"}]
    def evaluate_with_llm(*args, **kwargs): return {"score": 1, "reason": "Mock pass", "is_passed": True}
    def retrieve_context_with_filter(*args, **kwargs): return {"top_evidences": [], "aggregated_context": "Mock Context"}
    def retrieve_context_for_low_levels(*args, **kwargs): return {"top_evidences": [], "aggregated_context": "Mock Low Context"}
    def evaluate_with_llm_low_level(*args, **kwargs): return {"score": 1, "reason": "Mock pass L1/L2", "is_passed": True}
    LOW_LEVEL_K = 2
    def set_llm_data_mock_mode(mode): pass
    class VectorStoreManager: pass
    def load_all_vectorstores(*args, **kwargs): return VectorStoreManager()
    PDCA_PHASE_MAP = {1: "Plan", 2: "Do", 3: "Check", 4: "Act", 5: "Innovate"}
    class seam_mocking:
        @staticmethod
        def evaluate_with_llm_CONTROLLED_MOCK(*args, **kwargs): return {"score": 0, "reason": "Mock fail", "is_passed": False}
        @staticmethod
        def retrieve_context_with_filter_MOCK(*args, **kwargs): return {"top_evidences": [], "aggregated_context": "Mock Context"}
        @staticmethod
        def create_structured_action_plan_MOCK(*args, **kwargs): return [{"Phase": "Mock Plan", "Goal": "Resolve issue"}]
        @staticmethod
        def set_mock_control_mode(mode): pass
    
    if "FATAL ERROR" in str(e):
        pass 


logger = logging.getLogger(__name__)
if not logger.handlers:
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# =================================================================
# üü¢ FIX: Helper Function for PDCA Calculation (Priority 1 Part 2 & Priority 2)
# NOTE: ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡πÅ‡∏õ‡∏•‡∏á llm_score (1-5) ‡πÄ‡∏õ‡πá‡∏ô PDCA Breakdown ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
# =================================================================
LEVEL_PHASE_MAP = {
    1: ['P'],
    2: ['P', 'D'],
    3: ['P', 'D', 'C'],
    4: ['P', 'D', 'C', 'A'],
    5: ['P', 'D', 'C', 'A'] # L5 ‡πÉ‡∏ä‡πâ P, D, C, A ‡πÄ‡∏ä‡πà‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö L4 ‡πÅ‡∏ï‡πà‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏ï‡πá‡∏°‡∏≠‡∏≤‡∏à‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô
}

# ----------------------------------------------------------------------
# NEW CONSTANT: ‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô PDCA ‡∏ó‡∏µ‡πà '‡∏ú‡πà‡∏≤‡∏ô' ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ Level (Achieved Score)
# ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Achieved Score (Sum of P,D,C,A) ‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö Required Score (R)
# L1 (R=1, A=1): P=1
# L2 (R=2, A=2): P=1, D=1
# L3 (R=4, A=4): P=1, D=1, C=1, A=1
# L4 (R=6, A=6): P=2, D=2, C=1, A=1
# L5 (R=8, A=8): P=2, D=2, C=2, A=2
# ----------------------------------------------------------------------
CORRECT_PDCA_SCORES_MAP: Final[Dict[int, Dict[str, int]]] = {
    1: {'P': 1, 'D': 0, 'C': 0, 'A': 0},
    2: {'P': 1, 'D': 1, 'C': 0, 'A': 0},
    3: {'P': 1, 'D': 1, 'C': 1, 'A': 1},
    4: {'P': 2, 'D': 2, 'C': 1, 'A': 1},
    5: {'P': 2, 'D': 2, 'C': 2, 'A': 2},
}

# üü¢ NOTE: ‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£ Global ‡∏´‡∏£‡∏∑‡∏≠ Config Flag ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏¥‡∏î/‡∏õ‡∏¥‡∏î‡πÇ‡∏´‡∏°‡∏î‡∏ô‡∏µ‡πâ
# ‡πÄ‡∏ä‡πà‡∏ô: IS_L3_DEBUG_TEST = True 
# ‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏™‡πà‡∏á‡∏Ñ‡πà‡∏≤‡∏ô‡∏µ‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà build_simulated_l3_evidence (‡πÄ‡∏ä‡πà‡∏ô via debug_mode argument)

def build_simulated_l3_evidence(check_blocks: list[dict]) -> str:

    if not check_blocks:
        return ""

    # --- Original Dynamic Logic ---
    source_files = ", ".join(sorted({b["file"] for b in check_blocks}))
    extracted_summary = "\n\n".join(
        f"- ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå {b['file']}:\n{b['content'][:600]}"
        for b in check_blocks
    )

    return f"""
[SIMULATED_L3_EVIDENCE]
‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö (Check Phase) ‡∏û‡∏ö‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå: {source_files}
... (‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏Ç‡∏≠‡∏á‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏î‡∏¥‡∏°)
""".strip()

def build_ordered_context(level: int,
                          plan_blocks: list[dict],
                          do_blocks: list[dict],
                          check_blocks: list[dict],
                          act_blocks: list[dict],
                          other_blocks: list[dict]) -> str:
    def fmt(blocks):
        return "\n\n".join(
            f"[{b.get('file', 'Unknown File')}]\n{b.get('content', b.get('text', ''))}" for b in blocks
        )

    if level == 3:
        # L3: Check/Act Priority 1, Plan/Do/Other ‡∏ï‡πà‡∏≠‡∏ó‡πâ‡∏≤‡∏¢
        ordered = [
            fmt(check_blocks),
            fmt(act_blocks),
            fmt(plan_blocks),
            fmt(do_blocks),
            fmt(other_blocks)
        ]
    else:
        # Default: Plan -> Do -> Check -> Act -> Other
        ordered = [
            fmt(plan_blocks),
            fmt(do_blocks),
            fmt(check_blocks),
            fmt(act_blocks),
            fmt(other_blocks)
        ]

    return "\n\n".join([o for o in ordered if o])


def calculate_pdca_breakdown_and_pass_status(llm_score: int, level: int) -> Tuple[Dict[str, int], bool, float]:
    """
    ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì PDCA breakdown, is_passed status, ‡πÅ‡∏•‡∏∞ raw_pdca_score (Achieved Score) 
    ‡πÇ‡∏î‡∏¢‡πÅ‡∏õ‡∏•‡∏á‡∏à‡∏≤‡∏Å llm_score (1-5) ‡πÅ‡∏•‡∏∞ Level ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô 
    
    ‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£:
    - L1 ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ llm_score >= 1
    - L2 ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ llm_score >= 2
    - L3 ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ llm_score >= 3
    - L4 ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ llm_score >= 4
    - L5 ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ llm_score >= 4 
    """
    pdca_map: Dict[str, int] = {'P': 0, 'D': 0, 'C': 0, 'A': 0}
    is_passed: bool = False
    raw_pdca_score: float = 0.0
    
    # 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ PASS (‡πÉ‡∏ä‡πâ Logic ‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏Å‡∏≥‡∏´‡∏ô‡∏î)
    if level == 5:
        if llm_score >= 4:
            is_passed = True
    elif level == 4:
        if llm_score >= 4:
            is_passed = True
    elif level == 3:
        if llm_score >= 3:
            is_passed = True
    elif level == 2:
        if llm_score >= 2:
            is_passed = True
    elif level == 1:
        if llm_score >= 1:
            is_passed = True

    # 2. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì PDCA Breakdown ‡πÅ‡∏•‡∏∞ raw_pdca_score (Achieved Score)
    if is_passed:
        # *** REVISED LOGIC: ‡πÉ‡∏ä‡πâ CORRECT_PDCA_SCORES_MAP ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô P, D, C, A ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ***
        correct_scores = CORRECT_PDCA_SCORES_MAP.get(level, pdca_map) 
        pdca_map.update(correct_scores)
        
        # raw_pdca_score (Achieved Score) ‡∏à‡∏∞‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö Required Score (R) ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ú‡πà‡∏≤‡∏ô
        raw_pdca_score = float(sum(pdca_map.values()))
    
    return pdca_map, is_passed, raw_pdca_score

def get_correct_pdca_required_score(level: int) -> int:
    """
    ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤ Required Score (R) ‡∏ï‡∏≤‡∏° Level ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå SE-AM:
    L1=1, L2=2, L3=4, L4=6, L5=8
    """
    # ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ô‡∏µ‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß
    if level == 1:
        return 1
    elif level == 2:
        return 2
    elif level == 3:
        return 4
    elif level == 4:
        return 6
    elif level == 5:
        return 8
    # ‡∏Å‡∏£‡∏ì‡∏µ Level ‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î
    return 8

# =================================================================
# Configuration Class
# =================================================================
@dataclass
class AssessmentConfig:
    """Configuration for the SEAM PDCA Assessment Run."""
    enabler: str = DEFAULT_ENABLER
    target_level: int = MAX_LEVEL
    mock_mode: str = "none" # 'none', 'random', 'control'
    force_sequential: bool = field(default=False) # Flag to force sequential ru


# =================================================================
# SEAM Assessment Engine (PDCA Focused)
# =================================================================
class SEAMPDCAEngine:
    
    # üéØ Mapping for RAG Query Augmentation at Level 1 (Plan)
    ENABLER_L1_AUGMENTATION = {
        "KM": "‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢ ‡∏ß‡∏¥‡∏™‡∏±‡∏¢‡∏ó‡∏±‡∏®‡∏ô‡πå ‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå ‡πÅ‡∏ú‡∏ô‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏∏‡πà‡∏á‡∏°‡∏±‡πà‡∏ô",
        "HCM": "‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏• ‡πÅ‡∏ú‡∏ô‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡∏ô ‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏• ‡∏Å‡∏≤‡∏£‡∏°‡∏µ‡∏™‡πà‡∏ß‡∏ô‡∏£‡πà‡∏ß‡∏°‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£",
        "DT": "‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏î‡∏¥‡∏à‡∏¥‡∏ó‡∏±‡∏• ‡πÅ‡∏ú‡∏ô‡πÅ‡∏°‡πà‡∏ö‡∏ó‡∏î‡∏¥‡∏à‡∏¥‡∏ó‡∏±‡∏• ‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á IT ‡πÅ‡∏ú‡∏ô‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ",
        "SP": "‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£ ‡πÅ‡∏ú‡∏ô‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£ ‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå ‡πÅ‡∏ú‡∏ô‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå",
        "DEFAULT": "‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢ ‡∏ß‡∏¥‡∏™‡∏±‡∏¢‡∏ó‡∏±‡∏®‡∏ô‡πå ‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå ‡πÅ‡∏ú‡∏ô‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏∏‡πà‡∏á‡∏°‡∏±‡πà‡∏ô" 
    }
    
    L1_INITIAL_TOP_K_RAG: int = 50 
    
    def __init__(
        self, 
        config: 'AssessmentConfig',
        llm_instance: Any = None, 
        logger_instance: logging.Logger = None,
        rag_retriever_instance: Any = None,
        # üü¢ FIX #1: ‡πÄ‡∏û‡∏¥‡πà‡∏° doc_type ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÅ‡∏Å‡πâ AttributeError ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤
        doc_type: str = EVIDENCE_DOC_TYPES, 
        # üü¢ FIX #2: ‡πÄ‡∏û‡∏¥‡πà‡∏° vectorstore_manager ‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏Å‡∏¥‡∏î TypeError ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
        vectorstore_manager: Optional['VectorStoreManager'] = None 
    ):

            self.config = config
            self.enabler_id = config.enabler
            self.target_level = config.target_level
            self.rubric = self._load_rubric()
            
            # üü¢ FIX #3: ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤ vectorstore_manager ‡πÅ‡∏•‡∏∞ doc_type
            self.vectorstore_manager = vectorstore_manager
            self.doc_type = doc_type

            self.FINAL_K_RERANKED = FINAL_K_RERANKED
            self.PRIORITY_CHUNK_LIMIT = PRIORITY_CHUNK_LIMIT

            # üü¢ NEW: ‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö LLM ‡πÅ‡∏•‡∏∞ Logger Instance
            self.llm = llm_instance           # ‚¨ÖÔ∏è ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç AttributeError: 'llm'
            self.logger = logger_instance if logger_instance is not None else logging.getLogger(__name__)

            # üü¢ FIX: Disable Strict Filter (Permanent Bypass)
            self.initial_evidence_ids: Set[str] = self._load_initial_evidence_info()
            all_statements = self._flatten_rubric_to_statements()
            initial_count = len(all_statements)

            self.logger.info(f"DEBUG: Statements found: {initial_count}. Strict Filter is **DISABLED**.")

            # all_statements = self._apply_strict_filter(all_statements, self.initial_evidence_ids) 
            self.statements_to_assess = all_statements
            self.logger.info(f"DEBUG: Statements selected for assessment: {len(self.statements_to_assess)} (Skipped: {initial_count - len(self.statements_to_assess)})")

            # Assessment results storage
            self.raw_llm_results: List[Dict[str, Any]] = []
            self.final_subcriteria_results: List[Dict[str, Any]] = []
            self.total_stats: Dict[str, Any] = {}

            # üìå NEW: Persistent Mapping Configuration
        
            # 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏ö‡∏ö Dynamic: [enabler]_evidence_mapping_new.json
            map_filename = f"{self.enabler_id.lower()}{EVIDENCE_MAPPING_FILENAME_SUFFIX}"
            
            # 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏û‡∏≤‡∏ò‡πÅ‡∏ö‡∏ö‡πÄ‡∏ï‡πá‡∏°: [RUBRIC_CONFIG_DIR]/km_evidence_mapping_new.json
            # NOTE: ‡πÉ‡∏ä‡πâ RUBRIC_CONFIG_DIR ‡∏ã‡∏∂‡πà‡∏á‡∏Ñ‡∏ß‡∏£‡∏ä‡∏µ‡πâ‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå config
            self.evidence_map_path = os.path.join(RUBRIC_CONFIG_DIR, map_filename)
            
            # 3. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Attribute ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Persistent Mapping
            self.evidence_map: Dict[str, List[str]] = {}
            self.temp_map_for_save: Dict[str, List[str]] = {}

            self.contextual_rules_map: Dict[str, Dict[str, str]] = self._load_contextual_rules_map()
            
            # 4. ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤‡πÄ‡∏°‡∏ò‡∏≠‡∏î‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ self.evidence_map_path)
            self._load_evidence_map() 
            
            self.logger.info(f"Persistent Map Path set to: {self.evidence_map_path}")

            # Mock function pointers (will point to real functions by default)
            self.llm_evaluator = evaluate_with_llm
            self.rag_retriever = retrieve_context_with_filter
            self.action_plan_generator = create_structured_action_plan

            # Apply mocking if enabled
            if config.mock_mode in ["random", "control"]:
                self._set_mock_handlers(config.mock_mode)

            # Set global mock control mode for llm_data_utils if using 'control'
            if config.mock_mode == "control":
                logger.info("Enabling global LLM data utils mock control mode.")
                set_llm_data_mock_mode(True)
            elif config.mock_mode == "random":
                logger.warning("Mock mode 'random' is not fully implemented. Using 'control' logic if available.")
                if hasattr(seam_mocking, 'set_mock_control_mode'):
                    seam_mocking.set_mock_control_mode(False)
                    set_llm_data_mock_mode(False)

            self.logger.info(f"Engine initialized for Enabler: {self.enabler_id}, Mock Mode: {config.mock_mode}")

    # -------------------- Initialization Helpers --------------------
    # -------------------- Contextual Rules Handlers (NEW) --------------------
    def _load_contextual_rules_map(self) -> Dict[str, Dict[str, str]]:
        """Loads the Sub-Criteria Contextual Rules (Layer 2) map."""
        map_filename = f"{self.enabler_id.lower()}_contextual_rules.json"
        # üìå NOTE: ‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£ global PROJECT_ROOT ‡πÅ‡∏•‡∏∞ RUBRIC_CONFIG_DIR
        filepath = os.path.join(PROJECT_ROOT, RUBRIC_CONFIG_DIR, map_filename)
        
        if not os.path.exists(filepath):
            logger.warning(f"‚ö†Ô∏è Contextual Rules map not found at {filepath}. Using empty map.")
            return {}

        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
                logger.info(f"‚úÖ Loaded Contextual Rules map from {filepath}. ({len(data)} sub-criteria rules)")
                # Map Structure: {'2.2': {'L1': 'Rule Text for L1', 'L3': 'Rule Text for L3'}, ...}
                return data
        except Exception as e:
            logger.error(f"‚ùå Failed to load Contextual Rules map. Error: {e}")
            return {}

    def _get_contextual_rules_prompt(self, sub_id: str, level: int) -> str:
        """
        Retrieves the specific Contextual Rule prompt for a given Sub-Criteria and Level.
        """
        sub_id_rules = self.contextual_rules_map.get(sub_id)
        if sub_id_rules:
            level_key = f"L{level}"
            rule_text = sub_id_rules.get(level_key)
            if rule_text:
                # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏™‡πà‡πÉ‡∏ô Prompt
                return f"\n--- ‡∏Å‡∏é‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏¢‡πà‡∏≠‡∏¢ ({sub_id} L{level}) ---\n‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Ñ‡∏ß‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ: {rule_text}\n"
        return ""

    def _load_rubric(self) -> List[Dict[str, Any]]:
        """Loads the SE-AM Rubric JSON file."""
        filename = RUBRIC_FILENAME_PATTERN.format(enabler=self.enabler_id.lower())
        filepath = os.path.join(PROJECT_ROOT, RUBRIC_CONFIG_DIR, filename) 
        
        if not os.path.exists(filepath):
            logger.error(f"Rubric file not found for {self.enabler_id}: {filepath}")
            if self.config.mock_mode != "none":
                logger.warning("Using minimal mock rubric for testing.")
                return [{
                    "sub_id": "1.1", "name": "Mock Sub-Criteria 1.1", "weight": 4, 
                    "levels": [{"level": 1, "statement": "Mock L1 statement"}, {"level": 2, "statement": "Mock L2 statement"}]
                }]
            raise FileNotFoundError(f"Rubric not found at {filepath}")

        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
            
            # --- FIX: Transform Dictionary Root to List of Sub-Criteria ---
            if isinstance(data, dict):
                logger.info("Rubric file detected as Dictionary root. Extracting Sub-Criteria list.")
                extracted_list = []
                for criteria_id, criteria_data in data.items():
                    sub_criteria_map = criteria_data.get('subcriteria', {})
                    criteria_name = criteria_data.get('name')
                    
                    for sub_id, sub_data in sub_criteria_map.items():
                        sub_data['criteria_id'] = criteria_id
                        sub_data['criteria_name'] = criteria_name
                        sub_data['sub_id'] = sub_id 
                        sub_data['sub_criteria_name'] = sub_data.get('name', criteria_name + ' sub') 
                        
                        if 'weight' not in sub_data:
                            sub_data['weight'] = criteria_data.get('weight', 0)
                        
                        extracted_list.append(sub_data)
                data = extracted_list

            if not isinstance(data, list):
                raise ValueError(f"Rubric file {filepath} has invalid root structure (expected list after transformation).")

            # Check for missing levels and sort, and transform levels dict to list
            for sub_criteria in data:
                if "levels" in sub_criteria and isinstance(sub_criteria["levels"], dict):
                    levels_list = []
                    for level_str, statement in sub_criteria["levels"].items():
                        levels_list.append({"level": int(level_str), "statement": statement})
                    sub_criteria["levels"] = levels_list
                
                if "levels" in sub_criteria and isinstance(sub_criteria["levels"], list):
                    sub_criteria["levels"].sort(key=lambda x: x.get("level", 0))
            
            return data

    # -------------------- Persistent Mapping Handlers --------------------
    def _load_evidence_map(self) -> Dict[str, List[str]]:
        """Loads persistent evidence mapping from the dynamic file path."""
        evidence_map = {}
        if os.path.exists(self.evidence_map_path):
            try:
                with open(self.evidence_map_path, 'r', encoding='utf-8') as f:
                    evidence_map = json.load(f)
                logger.info(f"‚úÖ Loaded persistent evidence map from {self.evidence_map_path}. ({len(evidence_map)} entries)")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Failed to load evidence map. Starting with empty map. Error: {e}")
        else:
            logger.info(f"üÜï Persistent evidence map file not found. Starting with empty map.")
            
        # üìå ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï self.evidence_map ‡πÉ‡∏ô __init__ (‡∏ó‡πà‡∏≤‡∏ô‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏´‡πâ‡πÄ‡∏°‡∏ò‡∏≠‡∏î‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÄ‡∏≠‡∏á)
        self.evidence_map = evidence_map # ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏ï‡∏≤‡∏°‡πÇ‡∏Ñ‡πâ‡∏î‡πÉ‡∏ô __init__ ‡∏Ç‡∏≠‡∏á‡∏ó‡πà‡∏≤‡∏ô
        return evidence_map

    def _save_evidence_map(self, new_passed_map: Dict[str, List[str]]):
        """Saves the combined evidence mapping (self.evidence_map + new_passed_map) to the dynamic file path."""
        
        # 1. ‡∏£‡∏ß‡∏°‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏° (self.evidence_map) + ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå PASS ‡πÉ‡∏´‡∏°‡πà (new_passed_map)
        # üü¢ FIX: ‡πÉ‡∏ä‡πâ Argument ‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤
        final_map = self.evidence_map.copy() 
        final_map.update(new_passed_map) # <-- ‡πÉ‡∏ä‡πâ Argument ‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏°‡∏≤‡∏à‡∏≤‡∏Å run_assessment

        if not final_map:
            logger.info("No evidence passed during run to save.")
            return
            
        try:
            # 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á Directory (‡∏´‡∏≤‡∏Å RUBRIC_CONFIG_DIR ‡πÑ‡∏°‡πà‡∏°‡∏µ)
            os.makedirs(os.path.dirname(self.evidence_map_path), exist_ok=True)
            
            # 3. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå
            with open(self.evidence_map_path, 'w', encoding='utf-8') as f:
                json.dump(final_map, f, indent=4, ensure_ascii=False) 
            logger.info(f"üíæ Successfully saved {len(final_map)} entries to persistent map at {self.evidence_map_path}.")
        except Exception as e:
            logger.error(f"‚ùå Failed to save evidence map. Error: {e}")

    def _set_mock_handlers(self, mode: str):
        """Replaces real LLM/RAG functions with mock versions."""
        if mode == "control" or mode == "random":
            if hasattr(seam_mocking, 'evaluate_with_llm_CONTROLLED_MOCK'):
                self.llm_evaluator = seam_mocking.evaluate_with_llm_CONTROLLED_MOCK
            if hasattr(seam_mocking, 'retrieve_context_with_filter_MOCK'):
                self.rag_retriever = seam_mocking.retrieve_context_with_filter_MOCK
            if hasattr(seam_mocking, 'create_structured_action_plan_MOCK'):
                self.action_plan_generator = seam_mocking.create_structured_action_plan_MOCK
            if hasattr(seam_mocking, 'set_mock_control_mode'):
                seam_mocking.set_mock_control_mode(mode == "control") 

        logger.warning(f"Engine is running in MOCK mode: {mode}")

    def _get_pdca_phase(self, level: int) -> str:
        """Helper to get the PDCA phase string from the map."""
        return PDCA_PHASE_MAP.get(level, f"Level {level} Requirement")
    

    def _get_level_constraint_prompt(self, level: int) -> str:
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á Prompt Constraint ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Å‡∏±‡∏ö‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ß‡∏∏‡∏í‡∏¥‡∏†‡∏≤‡∏ß‡∏∞‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô
        """
        if level == 1:
            return "‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏ñ‡∏∂‡∏á '‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢/‡∏ß‡∏¥‡∏™‡∏±‡∏¢‡∏ó‡∏±‡∏®‡∏ô‡πå', '‡∏Å‡∏≤‡∏£‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå', '‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏ó‡∏≥‡∏Å‡∏£‡∏≠‡∏ö‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á', ‡∏´‡∏£‡∏∑‡∏≠ '‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£' ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (L1-Focus)"
        elif level == 2:
            return "‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ô‡πâ‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞ '‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô', '‡∏Å‡∏≤‡∏£‡∏Ç‡∏±‡∏ö‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô', '‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡∏ò‡∏£‡∏£‡∏°', ‡∏´‡∏£‡∏∑‡∏≠ '‡∏Å‡∏≤‡∏£‡∏°‡∏µ‡∏™‡πà‡∏ß‡∏ô‡∏£‡πà‡∏ß‡∏°' ‡∏ï‡∏≤‡∏°‡πÅ‡∏ú‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (L2-Focus)"
        elif level == 3:
            # üö® HARD RULE: ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏ä‡πâ L3 Logic (Check/Act Focus) ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏° Context ‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡∏°‡πà
            return """
‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î (HARD RULE: L3 CHECK/ACT FOCUS):
1. ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô L3 ‡∏ô‡∏µ‡πâ **‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô '‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö (Check)' ‡πÅ‡∏•‡∏∞ '‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á (Act)' ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô**
2. ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÅ‡∏•‡πâ‡∏ß: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏ï‡πâ‡∏ô‡∏Ç‡∏≠‡∏á Context ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î (Priority 1)
3. ‡∏´‡∏≤‡∏Å‡∏û‡∏ö‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ **[SIMULATED_L3_EVIDENCE]** ‡πÉ‡∏´‡πâ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô **‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö** ‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠‡πÑ‡∏î‡πâ‡∏ã‡∏∂‡πà‡∏á‡∏ñ‡∏π‡∏Å‡∏™‡∏£‡∏∏‡∏õ‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô Check/Act ‡∏à‡∏£‡∏¥‡∏á (‡∏à‡∏±‡∏î‡πÄ‡∏õ‡πá‡∏ô Priority 1)
4. ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô Plan ‡πÅ‡∏•‡∏∞ Do ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏ï‡∏≠‡∏ô‡∏ó‡πâ‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á Context **‡∏´‡πâ‡∏≤‡∏°‡∏ô‡∏≥‡∏°‡∏≤‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤** ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à **FAIL** ‡∏´‡∏≤‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô Check/Act ‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô
5. ‡∏´‡∏≤‡∏Å‡∏Ç‡∏≤‡∏î‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô **Check** ‡∏´‡∏£‡∏∑‡∏≠ **Act** ‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠ (‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏à‡∏≤‡∏Å Simulated Evidence ‡∏´‡∏£‡∏∑‡∏≠‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á) ‡πÉ‡∏´‡πâ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÄ‡∏õ‡πá‡∏ô **‚ùå FAIL** ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô L3 ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡∏à‡∏£‡∏¥‡∏á
(L3-Focus: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏° ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏• ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á)
"""
        elif level == 4:
            return "‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ñ‡∏∂‡∏á '‡∏Å‡∏≤‡∏£‡∏ö‡∏π‡∏£‡∏ì‡∏≤‡∏Å‡∏≤‡∏£', '‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á', ‡∏´‡∏£‡∏∑‡∏≠ '‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå' ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (L4-Focus)"
        elif level == 5:
            return "‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ñ‡∏∂‡∏á '‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°', '‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡πà‡∏≤‡∏ó‡∏≤‡∏á‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à', ‡∏´‡∏£‡∏∑‡∏≠ '‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß' ‡πÇ‡∏î‡∏¢‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô (L5-Focus)"
        else:
            return ""
        

    def _classify_pdca_phase_for_chunk(
        self, 
        chunk_text: str
    ) -> Literal["Plan", "Do", "Check", "Act", "Other"]:
        """
        ‡πÉ‡∏ä‡πâ LLM ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏∞‡∏¢‡∏∞‡πÉ‡∏î‡∏£‡∏∞‡∏¢‡∏∞‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏Ç‡∏≠‡∏á PDCA ‡∏´‡∏£‡∏∑‡∏≠ 'Other'
        """
        # üü¢ ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏ü‡∏™ PDCA
        pdca_phases_th = ["‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô", "‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥", "‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö", "‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á"]
        pdca_phases_en = ["Plan", "Do", "Check", "Act"]
        
        # 1. üõ†Ô∏è System Prompt ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢: ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó‡πÅ‡∏•‡∏∞‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        system_prompt = (
            "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó PDCA ‡∏†‡∏≤‡∏£‡∏Å‡∏¥‡∏à‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô "
            "‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ß‡πà‡∏≤‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏ô‡πâ‡∏ô‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡πÉ‡∏î‡∏Ç‡∏≠‡∏á‡∏ß‡∏á‡∏à‡∏£ PDCA "
            f"‡πÇ‡∏î‡∏¢‡∏ï‡πâ‡∏≠‡∏á‡∏à‡∏±‡∏î‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏´‡∏ô‡∏∂‡πà‡∏á‡πÉ‡∏ô‡∏™‡∏µ‡πà‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏´‡∏•‡∏±‡∏Å: {', '.join(pdca_phases_th)} ‡∏´‡∏£‡∏∑‡∏≠ '‡∏≠‡∏∑‡πà‡∏ô‡πÜ' ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô "
            "‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡∏î‡πâ‡∏ß‡∏¢ **JSON Object ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô** ‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö: {'phase': '‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó (‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢)'} "
            "‡πÇ‡∏î‡∏¢ '‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó' ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ '‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô', '‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥', '‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö', '‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á' ‡∏´‡∏£‡∏∑‡∏≠ '‡∏≠‡∏∑‡πà‡∏ô‡πÜ' ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô"
        )

        # 2. üìù User Prompt ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢: ‡∏õ‡πâ‡∏≠‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏¥‡∏¢‡∏≤‡∏°
        user_prompt = (
            f"‡πÇ‡∏õ‡∏£‡∏î‡∏à‡∏±‡∏î‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ‡∏ï‡∏≤‡∏°‡∏ß‡∏á‡∏à‡∏£ PDCA:\n\n"
            f"‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô: \"{chunk_text}\"\n\n"
            f"‡∏Ñ‡∏≥‡∏ô‡∏¥‡∏¢‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå:\n"
            f"- ‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô (Plan): ‡∏ß‡∏¥‡∏™‡∏±‡∏¢‡∏ó‡∏±‡∏®‡∏ô‡πå, ‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢, ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå, ‡πÅ‡∏ú‡∏ô‡∏´‡∏•‡∏±‡∏Å, ‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢, ‡∏Å‡∏≤‡∏£‡πÅ‡∏ï‡πà‡∏á‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡∏ì‡∏∞‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£\n"
            f"- ‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥ (Do): ‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡πÑ‡∏õ‡πÉ‡∏ä‡πâ, ‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£, ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏™‡∏£‡∏£‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£, ‡∏Å‡∏≤‡∏£‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£, ‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏≠‡∏ö‡∏£‡∏°, ‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏£‡∏∞‡∏ö‡∏ö\n"
            f"- ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö (Check): ‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°, ‡∏Å‡∏≤‡∏£‡∏ß‡∏±‡∏î‡∏ú‡∏•, ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏†‡∏≤‡∏¢‡πÉ‡∏ô, ‡∏Å‡∏≤‡∏£‡∏ó‡∏ö‡∏ó‡∏ß‡∏ô‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô, ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•, ‡∏Å‡∏≤‡∏£‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô\n"
            f"- ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á (Act): ‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç, ‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á, ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏ó‡∏≥‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô, ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏†‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å, ‡∏Å‡∏≤‡∏£‡∏õ‡∏¥‡∏î‡∏ß‡∏á‡∏à‡∏£\n"
        )
        
        raw_response = "" 
        
        try:
            # 3. üü¢ ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ LLM ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Prompt ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
            raw_response = _fetch_llm_response(
                system_prompt=system_prompt, 
                user_prompt=user_prompt,
                max_retries=1, 
                llm_executor=self.llm 
            )
            
            # 4. üìå Parse JSON response ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
            classification_data = {}
            # (‡πÉ‡∏ä‡πâ logic ‡∏Å‡∏≤‡∏£ Parse JSON ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà ‡πÄ‡∏ä‡πà‡∏ô _robust_extract_json ‡∏´‡∏£‡∏∑‡∏≠ regex/json5)
            # ... (‡πÉ‡∏™‡πà logic ‡∏Å‡∏≤‡∏£ Parse JSON ‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ) ...
            
            # 5. üìå Validate result (‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢)
            if isinstance(classification_data, dict):
                # ‡∏î‡∏∂‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏≠‡∏≠‡∏Å‡∏°‡∏≤
                phase_th = classification_data.get('phase', classification_data.get('classification', '‡∏≠‡∏∑‡πà‡∏ô‡πÜ'))
                phase_th = str(phase_th).strip()

                # ‡πÅ‡∏õ‡∏•‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤ Literal ‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡∏ó‡∏µ‡πà‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤
                if phase_th == "‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô":
                    return "Plan"
                elif phase_th == "‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥":
                    return "Do"
                elif phase_th == "‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö":
                    return "Check"
                elif phase_th == "‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á":
                    return "Act"
            
            return "Other"
            
        except Exception as e:
            self.logger.error(f"PDCA Classification failed: {e}. Raw Response: {raw_response[:50]}")
            return "Other" # ‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏î‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à

    # -------------------- Statement Preparation & Filtering Helpers --------------------
    def _flatten_rubric_to_statements(self) -> List[Dict[str, Any]]:
        """Extracts a flat list of all individual level statements from the rubric."""
        flat_list = []
        for sub_criteria in self.rubric:
            sub_id = sub_criteria['sub_id']
            sub_criteria_name = sub_criteria['sub_criteria_name']
            
            for statement_data in sub_criteria.get('levels', []):
                flat_list.append({
                    "sub_id": sub_id,
                    "sub_criteria_name": sub_criteria_name,
                    "level": statement_data['level'],
                    "statement": statement_data['statement'],
                    "evidence_doc_ids": statement_data.get('evidence_doc_ids', []) 
                })
        return flat_list

    def _load_initial_evidence_info(self) -> Set[str]:
        """Retrieves the set of all available stable document IDs in the VectorStore."""
        if self.config.mock_mode != "none":
            return {"00000000-0000-0000-0000-000000000001"}
        return set() 

    def _apply_strict_filter(self, statements: List[Dict[str, Any]], available_evidence_ids: Set[str]) -> List[Dict[str, Any]]:
        """
        Filters out statements that have no specified evidence_doc_ids 
        that match any ID found in the available_evidence_ids set.
        """
        if not available_evidence_ids:
            logger.warning("Strict Filter bypassed: No available evidence IDs loaded.")
            return statements

        filtered_statements = []
        for stmt in statements:
            required_ids = set(stmt.get('evidence_doc_ids', []))
            
            if not required_ids or required_ids.isdisjoint(available_evidence_ids):
                 logger.debug(f"Strict Filter: Skipping {stmt['sub_id']} L{stmt['level']} (No evidence match)")
                 continue
            
            filtered_statements.append(stmt)
            
        return filtered_statements
    
    # -------------------- Evidence Classification Helper --------------------
    # ‡πÉ‡∏ô class SEAMPDCAEngine:
    # -------------------- Evidence Classification Helper --------------------

    def _get_pdca_blocks_from_evidences(
        self, 
        top_evidences: List[Dict[str, Any]], 
        level: int # level ‡∏¢‡∏±‡∏á‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ logging/context ‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï
    ) -> Tuple[str, str, str, str, str]:
        """
        Groups retrieved evidence chunks into PDCA phases based on the 'pdca_tag' 
        generated by the LLM classifier. This replaces the old index-based heuristic.

        Args:
            top_evidences: List of retrieved evidence dictionaries, each containing 'text' and 'pdca_tag'.

        Returns:
            A tuple of aggregated context strings: (plan_blocks, do_blocks, check_blocks, act_blocks, other_blocks)
        """
        logger = logging.getLogger(__name__)

        # 1. Initialize groupings
        pdca_groups = defaultdict(list)
        
        # 2. Group chunks based on the 'pdca_tag'
        for i, doc in enumerate(top_evidences):
            # üìå Use the classified tag directly. Fallback to 'Other' if tag is missing.
            tag = doc.get('pdca_tag', 'Other')
            
            # üìå Format the chunk before appending to the group list
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÄ‡∏î‡∏¥‡∏° (i+1) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ trace ‡∏´‡∏≤‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
            formatted_chunk = f"--- [Chunk {i+1} | Tag: {tag}] ---\n{doc.get('text', '')}\n"
            pdca_groups[tag].append(formatted_chunk)

        # 3. Aggregate groups into single strings
        plan_blocks = "\n\n".join(pdca_groups.get('Plan', []))
        do_blocks = "\n\n".join(pdca_groups.get('Do', []))
        check_blocks = "\n\n".join(pdca_groups.get('Check', []))
        act_blocks = "\n\n".join(pdca_groups.get('Act', []))
        other_blocks = "\n\n".join(pdca_groups.get('Other', []))

        logger.debug(
            f"  > PDCA Blocks Grouped (L{level}): "
            f"P={len(pdca_groups.get('Plan', []))}, D={len(pdca_groups.get('Do', []))}, "
            f"C={len(pdca_groups.get('Check', []))}, A={len(pdca_groups.get('Act', []))}, "
            f"Other={len(pdca_groups.get('Other', []))}"
        )

        return plan_blocks, do_blocks, check_blocks, act_blocks, other_blocks

    def _get_mapped_uuids_and_priority_chunks(
                    self, 
                    sub_id: str, 
                    level: int, 
                    statement_text: str, 
                    level_constraint: str,
                    vectorstore_manager: Optional['VectorStoreManager']
                ) -> Tuple[List[str], List[Dict[str, Any]]]:
                    """
                    1. Gathers all PASSED Stable Doc IDs from L1 up to L[level-1]. 
                    2. Fetches limited priority RAG chunks (Hybrid Retrieval) 
                    based on the gathered doc_ids.
                    
                    Returns: (mapped_stable_doc_ids: list[str], priority_docs: list[dict])
                    """
                    
                    all_priority_items: List[Dict[str, Any]] = [] 
                    
                    # 1. ‡∏ß‡∏ô‡∏ã‡πâ‡∏≥‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà PASS ‡∏à‡∏≤‡∏Å Level 1 ‡∏à‡∏ô‡∏ñ‡∏∂‡∏á Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ (L1 -> L[level - 1])
                    for prev_level in range(1, level): 
                        prev_map_key = f"{sub_id}.L{prev_level}"
                        # 1. Get UUIDs/Items from the Persistent Map
                        all_priority_items.extend(self.evidence_map.get(prev_map_key, []))
                        # 2. Get UUIDs/Items from the Temporary Map
                        all_priority_items.extend(self.temp_map_for_save.get(prev_map_key, []))
                        
                    
                    # 2. ‡πÅ‡∏õ‡∏•‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô Stable Document ID (String) ‡πÅ‡∏•‡∏∞ Dedup
                    doc_ids_for_dedup: List[str] = [
                        # Item ‡∏Ñ‡∏ß‡∏£‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô Dict ‡πÄ‡∏™‡∏°‡∏≠‡∏ï‡∏≤‡∏° Logic ‡πÉ‡∏ô Save on PASS (‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ .get('doc_id'))
                        item.get('doc_id') 
                        for item in all_priority_items
                        if isinstance(item, dict)
                    ]

                    # ‡∏•‡∏ö‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ã‡πâ‡∏≥ (Dedup) ‡πÅ‡∏•‡∏∞ ‡∏Å‡∏£‡∏≠‡∏á‡∏Ñ‡πà‡∏≤ None ‡∏≠‡∏≠‡∏Å
                    # ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠ **Stable Document IDs** (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà Chunk UUIDs)
                    mapped_stable_doc_ids: List[str] = [uid for uid in list(set(doc_ids_for_dedup)) if uid is not None]
                    num_historical_docs = len(mapped_stable_doc_ids)

                    priority_docs = [] 
                    
                    if num_historical_docs > 0:
                        levels_logged = f"L1-L{level-1}" if level > 1 else "L0 (Should not happen)"
                        logger.critical(f"üß≠ DEBUG: Priority Search initiated with {num_historical_docs} historical Stable Doc IDs ({levels_logged}).") 
                        logger.info(f"‚úÖ Hybrid Mapping: Found {num_historical_docs} pre-mapped Stable Doc IDs from {levels_logged} for {sub_id}. Prioritizing these.")
                        
                        if vectorstore_manager:
                            try:
                                # üü¢ FIX: ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ enhance_query_for_statement ‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
                                # Note: ‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ sub_id ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏Ç‡∏≠‡∏á statement_id ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Signature ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
                                rag_queries_for_vsm = enhance_query_for_statement(
                                    statement_text=statement_text,
                                    sub_id=sub_id, # FIX: ID ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏¢‡πà‡∏≠‡∏¢ (e.g., "1.1")
                                    statement_id=sub_id, # ‡πÉ‡∏ä‡πâ sub_id ‡πÄ‡∏õ‡πá‡∏ô statement_id ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß 
                                    level=level, 
                                    enabler_id=self.enabler_id,
                                    focus_hint=level_constraint 
                                )
                                
                                # -------------------- 3. Fetch Limited Priority Chunks --------------------
                                # üìå NEW LOGIC: ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å VSM ‡∏ï‡∏≤‡∏° Stable Doc IDs ‡πÅ‡∏•‡∏∞‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
                                # ‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ query ‡πÅ‡∏£‡∏Å‡∏Ç‡∏≠‡∏á Multi-Query ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Reranker (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•)
                                
                                doc_type = self.doc_type # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡∏ñ‡∏π‡∏Å‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÉ‡∏ô self
                                
                                # 3.1 ‡∏î‡∏∂‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ï‡∏≤‡∏° Stable Doc IDs ‡∏ó‡∏µ‡πà‡∏û‡∏ö
                                retrieved_docs_result = retrieve_context_by_doc_ids(
                                    doc_uuids=mapped_stable_doc_ids,
                                    doc_type=doc_type,
                                    enabler=self.enabler_id,
                                    vectorstore_manager=vectorstore_manager
                                )
                                
                                initial_priority_chunks: List[Dict[str, Any]] = retrieved_docs_result.get("top_evidences", [])
                                
                                if initial_priority_chunks:
                                    # 3.2 ‡πÉ‡∏ä‡πâ Reranker ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô chunks ‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡πÑ‡∏î‡πâ (‡πÄ‡∏ä‡πà‡∏ô 5-10 chunks)
                                    reranker = get_global_reranker(self.FINAL_K_RERANKED) # FINAL_K_RERANKED ‡πÄ‡∏õ‡πá‡∏ô K ‡∏ï‡∏±‡∏ß‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
                                    rerank_query = rag_queries_for_vsm[0] # ‡πÉ‡∏ä‡πâ Query ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Rerank
                                    
                                    # ‡πÅ‡∏õ‡∏•‡∏á Dict Chunks ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô LcDocument ‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤ Reranker
                                    lc_docs_for_rerank = [
                                        LcDocument(
                                            page_content=d.get('content') or d.get('text', ''), # ‡πÉ‡∏ä‡πâ 'content' ‡∏´‡∏£‡∏∑‡∏≠ 'text'
                                            metadata={
                                                **d, # ‡πÄ‡∏Å‡πá‡∏ö metadata ‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
                                                'relevance_score': 1.0 # ‡∏ï‡∏±‡πâ‡∏á score ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
                                            }
                                        ) 
                                        for d in initial_priority_chunks
                                    ]
                                    
                                    if reranker and hasattr(reranker, 'compress_documents'):
                                        # Rerank ‡πÅ‡∏•‡∏∞‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô chunks
                                        reranked_docs = reranker.compress_documents(
                                            query=rerank_query,
                                            documents=lc_docs_for_rerank,
                                            top_n=self.PRIORITY_CHUNK_LIMIT # üìå ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡∏°‡∏µ self.PRIORITY_CHUNK_LIMIT
                                        )
                                        # ‡πÅ‡∏õ‡∏•‡∏á‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô Dict (‡πÄ‡∏û‡∏¥‡πà‡∏° score ‡∏à‡∏≤‡∏Å reranker)
                                        priority_docs = [{
                                            **d.metadata, 
                                            'content': d.page_content,
                                            'text': d.page_content, # ‡πÄ‡∏û‡∏¥‡πà‡∏° 'text' ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö compatibility
                                            'score': d.metadata.get('relevance_score', 1.0) # Score ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å Reranker
                                        } for d in reranked_docs]
                                    else:
                                        # Fallback: ‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î chunks ‡∏á‡πà‡∏≤‡∏¢ ‡πÜ ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ Reranker
                                        priority_docs = initial_priority_chunks[:self.PRIORITY_CHUNK_LIMIT]

                                    logger.critical(f"üß≠ DEBUG: Limited and prioritized {len(priority_docs)} chunks from {num_historical_docs} docs.")

                            except Exception as e:
                                logger.error(f"Error fetching/reranking priority chunks for {sub_id}: {e}")
                                # ‡∏´‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏£‡∏∑‡∏≠ Rerank ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ mapped_stable_doc_ids ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏Å‡∏£‡∏≠‡∏á‡πÉ‡∏ô RAG
                                priority_docs = [] 
                    
                    # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ Stable Doc IDs ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏Å‡∏£‡∏≠‡∏á‡πÉ‡∏ô RAG ‡πÅ‡∏•‡∏∞ Priority Chunks ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡∏Å‡∏±‡∏î‡πÅ‡∏•‡πâ‡∏ß
                    return mapped_stable_doc_ids, priority_docs

    # -------------------- Calculation Helpers (ADDED) --------------------
    def _calculate_weighted_score(self, highest_full_level: int, weight: int) -> float:
        """
        Calculates the weighted score based on the highest full level achieved.
        Score is calculated by: (Level / 5) * Weight
        """
        MAX_LEVEL_CALC = 5  
        
        if highest_full_level <= 0:
            return 0.0
        
        level_for_calc = min(highest_full_level, MAX_LEVEL_CALC)
        score = (level_for_calc / MAX_LEVEL_CALC) * weight
        return score

    def _calculate_overall_stats(self, target_sub_id: str = "all"):
            """
            Calculates the total weighted score, total possible weight, and overall maturity score/level.
            """
            total_weighted_score = 0.0
            total_possible_weight = 0.0
            assessed_count = 0
            
            for result in self.final_subcriteria_results:
                if target_sub_id.lower() != "all" and result.get('sub_criteria_id') != target_sub_id:
                    continue

                weighted_score = result.get('weighted_score', 0.0)
                weight = result.get('weight', 0)
                
                total_weighted_score += weighted_score
                total_possible_weight += weight
                assessed_count += 1
                
            overall_maturity_score_avg = 0.0
            overall_maturity_level = "N/A"
            overall_progress_percent = 0.0 # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏ó‡∏µ‡πà 0.0
            
            if total_possible_weight > 0:
                overall_progress_percent = total_weighted_score / total_possible_weight
                
                MAX_LEVEL_STATS = 5 
                overall_maturity_score_avg = overall_progress_percent * MAX_LEVEL_STATS 

                # üü¢ FIX: Completed Logic for Maturity Level Determination
                if overall_maturity_score_avg >= 4.5:
                    overall_maturity_level = "L5"
                elif overall_maturity_score_avg >= 3.5:
                    overall_maturity_level = "L4"
                elif overall_maturity_score_avg >= 2.5:
                    overall_maturity_level = "L3"
                elif overall_maturity_score_avg >= 1.5:
                    overall_maturity_level = "L2"
                elif overall_maturity_score_avg >= 0.5:
                    overall_maturity_level = "L1"
                else:
                    overall_maturity_level = "L0"
            
            self.total_stats = {
                "Overall Maturity Score (Avg.)": overall_maturity_score_avg,
                "Overall Maturity Level (Weighted)": overall_maturity_level,
                "Number of Sub-Criteria Assessed": assessed_count,
                "Total Weighted Score Achieved": total_weighted_score,
                "Total Possible Weight": total_possible_weight,
                "Overall Progress Percentage (0.0 - 1.0)": overall_progress_percent,
                "percentage_achieved_run": overall_progress_percent * 100,
                "total_subcriteria": len(self.rubric),
                "target_level": self.config.target_level
            }
            return self.total_stats
            
    # -------------------- Multiprocessing Worker Method --------------------
    @staticmethod
    # üìå ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠ Logic ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏¢‡πâ‡∏≤‡∏¢‡πÑ‡∏õ‡πÉ‡∏ô _run_single_assessment ‡∏´‡∏£‡∏∑‡∏≠ _assess_single_statement_logic
    def _assess_single_sub_criteria_worker( # ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡πá‡∏ô _assess_single_statement_logic
        self, 
        statement_data: Dict[str, Any], 
        llm_executor: Any, 
        sub_id: str,
        enabler: str,
        doc_type: str,
        # üü¢ ‡∏£‡∏±‡∏ö VSM Instance ‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
        vectorstore_manager: Any, 
        # üü¢ NEW: ‡∏£‡∏±‡∏ö mapped_uuids ‡πÅ‡∏•‡∏∞ priority_docs_input ‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤
        mapped_uuids: Optional[List[str]] = None, 
        priority_docs_input: Optional[List[Any]] = None,
        # üü¢ NEW: ‡∏£‡∏±‡∏ö contextual_rules_prompt ‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤
        contextual_rules_prompt: str = "" 
    ) -> Dict[str, Any]:
        """
        Worker function to assess a single statement (sub-criteria level) by:
        1. Determining RAG strategy (Low-level or Standard) based on the level.
        2. Retrieving context (Hybrid RAG).
        3. Evaluating context using the appropriate LLM prompt.
        4. Summarizing the result.
        """
        
        # 1. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        level = int(statement_data.get("level", 0))
        statement_text = statement_data.get("statement", "")
        sub_criteria_name = statement_data.get("sub_criteria_name", "")
        pdca_phase = statement_data.get("pdca_phase", "")
        level_constraint = statement_data.get("level_constraint", "")

        # 2. üéØ ‡∏Å‡∏≥‡∏´‡∏ô‡∏î K ‡πÅ‡∏•‡∏∞ RAG Function ‡∏ï‡∏≤‡∏° Level
        # (‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤ LOW_LEVEL_K, STANDARD_K, INITIAL_TOP_K ‡∏ñ‡∏π‡∏Å Import ‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡∏Ñ‡∏•‡∏≤‡∏™)
        LOW_LEVEL_K = 3      
        STANDARD_K = 30      
        INITIAL_TOP_K = 100  
        
        if level <= 2:
            # L1, L2: Low-Level (Reduced K, Simplified Prompt)
            top_k = LOW_LEVEL_K # ‡πÉ‡∏ä‡πâ K ‡∏ô‡πâ‡∏≠‡∏¢
            retrieval_func = retrieve_context_for_low_levels # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Å‡∏≤‡∏£ import ‡∏à‡∏≤‡∏Å llm_data_utils
            evaluation_func = evaluate_with_llm_low_level # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Å‡∏≤‡∏£ import ‡∏à‡∏≤‡∏Å llm_data_utils
        else:
            # L3, L4, L5: Standard Level (High K, Full Prompt)
            top_k = STANDARD_K # ‡πÉ‡∏ä‡πâ K ‡∏™‡∏π‡∏á ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏≤‡∏Å (‡∏£‡∏ß‡∏° 300 ‡πÑ‡∏ü‡∏•‡πå)
            retrieval_func = retrieve_context_with_filter # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Å‡∏≤‡∏£ import ‡∏à‡∏≤‡∏Å llm_data_utils
            evaluation_func = evaluate_with_llm # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Å‡∏≤‡∏£ import ‡∏à‡∏≤‡∏Å llm_data_utils
            
        logger.info(f"Retrieval strategy for {sub_id} L{level}: K={top_k}, Function={retrieval_func.__name__}")

        # 3. üîç RAG: ‡∏î‡∏∂‡∏á Context (‡πÉ‡∏ä‡πâ Hybrid RAG)
        try:
            # 3.1 ‡∏™‡∏£‡πâ‡∏≤‡∏á Queries (Multi-Query)
            focus_hint = f"Focus: {pdca_phase}, Level Constraint: {level_constraint}"
            queries_list = enhance_query_for_statement(
                statement_text=statement_text,      # 1. ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° Statement
                sub_id=sub_id,                      # 2. FIX: ID ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏¢‡πà‡∏≠‡∏¢ (e.g., "1.1")
                statement_id=sub_id,                # 3. ‡πÉ‡∏ä‡πâ sub_id ‡πÄ‡∏õ‡πá‡∏ô statement_id ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß
                level=level,                        # 4. Level
                enabler_id=enabler,                 # 5. Enabler ID
                focus_hint=focus_hint,              # 6. Focus Hint
                llm_executor=llm_executor           # 7. NEW: ‡∏™‡πà‡∏á LLM Executor ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ (Optional)
            )
                    
            # 3.2 ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô Retrieval ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
            retrieval_result = retrieval_func(
                query=queries_list, # ‡∏™‡πà‡∏á Multi-Query
                doc_type=doc_type,
                enabler=enabler,
                vectorstore_manager=vectorstore_manager, # ‡πÉ‡∏ä‡πâ VSM ‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏°‡∏≤
                top_k=top_k,
                initial_k=INITIAL_TOP_K, 
                # üü¢ ‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠ Hybrid Arguments
                mapped_uuids=mapped_uuids, 
                priority_docs_input=priority_docs_input,
                sub_id=sub_id,
                level=level
            )
            
            context = retrieval_result.get("context", "")
            # üü¢ ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• UUIDs ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ Action Plan/Export)
            used_chunk_uuids = retrieval_result.get("used_chunk_uuids", [])
            
        except Exception as e:
            logger.exception(f"RAG failed for {sub_id} L{level}: {e}")
            # ‚ùå Fallback ‡∏Å‡∏£‡∏ì‡∏µ RAG ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ 0 ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏• Error
            return {
                "sub_id": sub_id, "level": level, "is_passed": False, 
                "score": 0, "reason": f"RAG Error: {e.__class__.__name__}",
                "P_Plan_Score": 0, "D_Do_Score": 0, "C_Check_Score": 0, "A_Act_Score": 0,
                "used_chunk_uuids": [],
                "summary": "RAG process failed.",
                "suggestion": "Check RAG configuration or source documents."
            }

        # 4. üß† LLM Evaluation: ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        evaluation_result = evaluation_func(
            context=context,
            sub_criteria_name=sub_criteria_name,
            level=level,
            statement_text=statement_text,
            sub_id=sub_id,
            llm_executor=llm_executor,
            pdca_phase=pdca_phase, 
            level_constraint=level_constraint,
            contextual_rules_prompt=contextual_rules_prompt 
        )
        
        # 5. üìù Summarization 
        summary_result = create_context_summary_llm( # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Å‡∏≤‡∏£ import ‡∏à‡∏≤‡∏Å llm_data_utils
            context=context,
            sub_criteria_name=sub_criteria_name,
            level=level,
            sub_id=sub_id,
            llm_executor=llm_executor 
        )

        # 6. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏£‡∏ß‡∏°
        final_result = {
            "sub_id": sub_id,
            "level": level,
            "statement": statement_text,
            "pdca_phase": pdca_phase,
            "context": context,
            "used_chunk_uuids": used_chunk_uuids, 
            **evaluation_result, 
            **summary_result     
        }
        
        return final_result

    def _export_results(self, results: dict, enabler: str, sub_criteria_id: str, target_level: int, export_dir: str = "assessment_results") -> str:
        """
        Exports the final assessment results to a JSON file.
        
        Args:
            results: The dictionary containing the final assessment summary and results.
            enabler: The enabler ID (e.g., KM).
            sub_criteria_id: The specific sub-criteria ID being run (e.g., 2.2).
            target_level: The target level for the assessment.
            export_dir: The directory to save the output file.
            
        Returns:
            The path to the saved JSON file.
        """
        if not os.path.exists(export_dir):
            os.makedirs(export_dir)
            
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå: assessment_results_KM_2.2_YYYYMMDD_HHMMSS.json
        file_name = f"assessment_results_{enabler}_{sub_criteria_id}_{timestamp}.json"
        full_path = os.path.join(export_dir, file_name)

        # Note: results dict should contain 'summary' and 'sub_criteria_results' keys
        # Update summary fields based on the engine data
        results['summary']['enabler'] = enabler
        results['summary']['sub_criteria_id'] = sub_criteria_id
        results['summary']['target_level'] = target_level
        results['summary']['Number of Sub-Criteria Assessed'] = len(results['sub_criteria_results'])

        try:
            with open(full_path, 'w', encoding='utf-8') as f:
                # ‡πÉ‡∏ä‡πâ indent=4 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢
                json.dump(results, f, ensure_ascii=False, indent=4)
            
            logging.info(f"üíæ Successfully exported final results to: {full_path}")
            return full_path
        
        except Exception as e:
            logging.error(f"‚ùå Failed to export results to {full_path}: {e}")
            return ""
        
    
    # -------------------- Main Execution --------------------
    def run_assessment(
        self, 
        target_sub_id: str = "all", 
        export: bool = False, 
        vectorstore_manager: Optional['VectorStoreManager'] = None
    ) -> Dict[str, Any]:
        """
        Main runner for the assessment engine.
        Implements sequential maturity check (L1 -> L2 -> L3...) and multiprocessing.
        """
        start_ts = time.time()
        MAX_L1_ATTEMPTS = 2
        MAX_LEVEL = 5 # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤ MAX_LEVEL ‡∏Ñ‡∏∑‡∏≠ 5

        
        # 1. Filter Rubric based on target_sub_id
        if target_sub_id.lower() == "all":
            sub_criteria_list = self.rubric
        else:
            sub_criteria_list = [s for s in self.rubric if s.get('sub_id') == target_sub_id]
            if not sub_criteria_list:
                logger.error(f"Sub-Criteria ID '{target_sub_id}' not found in rubric.")
                return {"error": f"Sub-Criteria ID '{target_sub_id}' not found."}

        # Reset storage
        self.raw_llm_results = []
        self.final_subcriteria_results = []
        
        # üü¢ Core Logic Switch for Parallel Execution
        run_parallel = (target_sub_id.lower() == "all" and not self.config.force_sequential)
        
        if run_parallel:
            logger.info("Starting Parallel Assessment (All Sub-Criteria) with Multiprocessing Pool...")
            
            sub_criteria_data_list = sub_criteria_list 
            engine_config_dict = self.config.__dict__ 
            worker_args = [(sub_data, engine_config_dict) for sub_data in sub_criteria_data_list]
            
            try:
                # üü¢ FIX: Set context to 'forkserver' or 'spawn' for robust multiprocessing initialization 
                if sys.platform != "win32":
                    # NOTE: ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤ self._assess_single_sub_criteria_worker ‡∏ñ‡∏π‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Å‡∏±‡∏ö multiprocessing
                    mp_context = multiprocessing.get_context('spawn')
                    pool = mp_context.Pool(processes=max(1, os.cpu_count() - 1))
                else:
                    pool = multiprocessing.Pool(processes=max(1, os.cpu_count() - 1))
                    
                with pool:
                    # üìå NOTE: _assess_single_sub_criteria_worker ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤‡πÉ‡∏ô‡πÇ‡∏à‡∏ó‡∏¢‡πå‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ ‡∏°‡∏µ‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö starmap
                    # ‡πÇ‡∏Ñ‡πâ‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏∂‡∏á‡∏ñ‡∏π‡∏Å‡∏Ñ‡∏á‡πÑ‡∏ß‡πâ‡∏ï‡∏≤‡∏°‡πÄ‡∏î‡∏¥‡∏° ‡πÅ‡∏ï‡πà‡∏Ñ‡∏ß‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á
                    results_tuples = pool.starmap(self._assess_single_sub_criteria_worker, worker_args)
                    
            except Exception as e:
                logger.critical(f"Multiprocessing Pool Execution Failed: {e}")
                logger.exception("FATAL: Multiprocessing pool failed to execute worker functions.")
                raise
            
            for raw_results_for_sub, final_sub_result in results_tuples:
                self.raw_llm_results.extend(raw_results_for_sub) 
                self.final_subcriteria_results.append(final_sub_result)

        else:
            run_mode_desc = target_sub_id if target_sub_id.lower() != 'all' else 'All Sub-Criteria (Forced Sequential)'
            logger.info(f"Starting Sequential Assessment for: {run_mode_desc}")
            
            # üü¢ FIX: Initialize local_vsm (‡πÅ‡∏Å‡πâ NameError)
            local_vsm = vectorstore_manager 
            
            if self.config.mock_mode == "none":
                logger.info("Sequential run: Re-instantiating VectorStoreManager locally in main process for robustness.")
                try:
                    # NOTE: load_all_vectorstores ‡∏ï‡πâ‡∏≠‡∏á‡∏ñ‡∏π‡∏Å Import
                    local_vsm = load_all_vectorstores(
                        doc_types=[EVIDENCE_DOC_TYPES], 
                        evidence_enabler=self.config.enabler
                    )
                except Exception as e:
                    logger.error(f"FATAL: Local VSM Re-instantiation Failed for Sequential Run: {e}")
                    raise
            
            if self.config.mock_mode == "none" and not local_vsm:
                logger.error("VectorStoreManager is required for sequential execution in non-mock mode.")
                raise ValueError("VSM missing in sequential non-mock mode.")

            for sub_criteria in sub_criteria_list:
                sub_id = sub_criteria['sub_id']
                sub_criteria_name = sub_criteria['sub_criteria_name']
                sub_weight = sub_criteria.get('weight', 0)
                
                logger.info(f"\n[START] Assessing Sub-Criteria: {sub_id} - {sub_criteria_name} (Weight: {sub_weight})")
                
                highest_full_level = INITIAL_LEVEL - 1 
                is_passed_current_level = True
                raw_results_for_sub_seq = [] 
                
                # üü¢ NEW: Persistent Mapping for Sequential Flow (Hybrid RAG)
                # ‡πÄ‡∏Å‡πá‡∏ö Chunk UUIDs ‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡πÉ‡∏ô Level ‡∏ó‡∏µ‡πà PASS ‡πÅ‡∏•‡πâ‡∏ß {level: [chunk_uuids]}
                passed_chunk_uuids_map: Dict[int, List[str]] = {} 
                # -----------------------------------------------
                
                for statement_data in sub_criteria.get('levels', []):
                    level = statement_data.get('level')
                    
                    if level is None or level > self.config.target_level:
                        continue 
                    
                    # ------------------ Action #5: Sequential Softening (MODIFIED) ------------------
                    # Track dependency status *before* running current level (used for Capping later)
                    dependency_failed = level > 1 and not is_passed_current_level
                    
                    if dependency_failed:
                        logger.warning(f"  > L{level-1} failed. **Continuing** to assess L{level} for detailed scoring.")
                    
                    # üü¢ NEW: ‡∏î‡∏∂‡∏á Chunk UUIDs ‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏à‡∏≤‡∏Å Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ (L[level-1])
                    previous_level = level - 1
                    sequential_chunk_uuids = passed_chunk_uuids_map.get(previous_level, []) 
                    # -----------------------------------------------

                    # üìå NEW LOGIC: Conditional Retry for Level 1 
                    max_attempts = MAX_L1_ATTEMPTS
                    final_result_for_level = None
                    
                    for attempt in range(max_attempts):
                        
                        if level == 1 and attempt > 0:
                            logger.warning(f"  > üîÑ RETRYING {sub_id} L1 (Attempt {attempt+1}/{MAX_L1_ATTEMPTS})...")
                        
                        # üìå FIX: ‡∏™‡πà‡∏á sequential_chunk_uuids ‡πÄ‡∏Ç‡πâ‡∏≤ _run_single_assessment
                        result = self._run_single_assessment(
                            sub_criteria=sub_criteria,
                            statement_data=statement_data,
                            vectorstore_manager=local_vsm,
                            # üü¢ ‡∏™‡πà‡∏á Chunk UUIDs ‡∏à‡∏≤‡∏Å L[level-1] ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ
                            sequential_chunk_uuids=sequential_chunk_uuids 
                        )
                        
                        is_passed_llm_raw = result.get('is_passed', False)
                        
                        if is_passed_llm_raw:
                            final_result_for_level = result
                            break
                        
                        # üü¢ MODIFIED BREAK CONDITION: ‡∏´‡∏¢‡∏∏‡∏î‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ñ‡∏∂‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ (‡πÅ‡∏•‡∏∞ FAIL) ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
                        if attempt == max_attempts - 1:
                            final_result_for_level = result
                            break 
                        # -----------------

                    # ----------------- END RETRY LOGIC -----------------
                    
                    result_to_process = final_result_for_level # Use the final result of the level's attempts

                    # ------------------ üü¢ Action #1: PDCA Scoring & Capping (FIXED LOGIC) ------------------
                    try:
                        # 1. Retrieve the PASS status calculated in _run_single_assessment (where PDCA logic is correct)
                        is_passed_llm_calculated = result_to_process.get('is_passed', False)
                        
                        # Use the calculated pass status as the default
                        is_passed_level_check = is_passed_llm_calculated

                        # NOTE: get_correct_pdca_required_score ‡∏ï‡πâ‡∏≠‡∏á‡∏ñ‡∏π‡∏Å Import
                        result_to_process['pdca_score_required'] = get_correct_pdca_required_score(level)
                        
                        # 2. Apply Capping/Penalty if Dependency Failed (Action #5 Capping)
                        if dependency_failed:
                            # If dependency failed, the effective pass status for the sequential flow MUST be FAIL/CAPPED
                            is_passed_level_check = False # FAIL for dependency tracking
                            
                            if is_passed_llm_calculated:
                                logger.warning(f"  > L{level} CAPPED. Dependency L{level-1} failed. Score/PDCA values remain for reporting, but final pass status for sequencing is FAIL.")

                        is_capped = is_passed_llm_calculated and not is_passed_level_check
                        result_to_process['is_capped'] = is_capped

                        # 3. Update the result structure (Important for the final JSON export)
                        result_to_process['is_passed'] = is_passed_level_check # Update with dependency-aware status
                        
                        # 4. Update status trackers and **Save Hybrid RAG Map**
                        is_passed_current_level = is_passed_level_check # Update tracker for the next iteration
                        
                        # üìå NEW LOGIC: ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Chunk UUIDs ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ (used_chunk_uuids) ‡∏•‡∏á‡πÉ‡∏ô Map
                        if is_passed_level_check:
                            used_uuids = result_to_process.get('used_chunk_uuids')
                            if used_uuids:
                                passed_chunk_uuids_map[level] = used_uuids
                                logger.info(f"  > L{level} passed. Saved {len(used_uuids)} chunk UUIDs for L{level+1} Sequential Hybrid RAG.")

                    except Exception as e:
                        logger.error(f"Error checking dependency status/processing result for {sub_id} L{level}: {e}")
                        is_passed_current_level = False # Default fail if dependency check errors

                    # 5. Append the PROCESSED result
                    self.raw_llm_results.append(result_to_process)
                    raw_results_for_sub_seq.append(result_to_process)
                    # ------------------ üü¢ /Action #1 (FIXED) ------------------
                    
                    if is_passed_current_level:
                        highest_full_level = level
                
                # -------------------- FINALIZE SUB-CRITERIA (Sequential) --------------------
                
                target_plan_level = highest_full_level + 1
                action_plan = []
                
                # üìå Logic ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Action Plan 
                if target_plan_level <= MAX_LEVEL and highest_full_level < self.config.target_level: 
                    logger.info(f"  > Generating Action Plan: Target L{target_plan_level}...")
                    
                    failed_statements_for_plan = [
                        r for r in raw_results_for_sub_seq
                        if r.get("level") == target_plan_level
                    ]
                    
                    if failed_statements_for_plan:
                        try:
                            # NOTE: self.action_plan_generator ‡∏ï‡πâ‡∏≠‡∏á‡∏ñ‡∏π‡∏Å‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÑ‡∏ß‡πâ
                            action_plan = self.action_plan_generator(
                                failed_statements_for_plan, 
                                sub_id=sub_id, 
                                target_level=target_plan_level,
                                llm_executor=self.llm  # üü¢ NEW: ‡∏™‡πà‡∏á LLM instance ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ
                                # ‡∏•‡∏ö enabler= ‡∏≠‡∏≠‡∏Å ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô def ‡∏Ç‡∏≠‡∏á create_structured_action_plan ‡πÅ‡∏•‡πâ‡∏ß
                            )
                        except Exception as e:
                            logger.error(f"Action Plan Generation failed for {sub_id}: {e}")
                            action_plan = [{"Phase": "ERROR", "Goal": "Action Plan generation failed."}]
                    
                # üìå 1. Calculate Weighted Score
                weighted_score = self._calculate_weighted_score(highest_full_level, sub_weight)

                # üìå 2. Generate Final Result Object
                final_sub_result = {
                    "sub_criteria_id": sub_id,
                    "sub_criteria_name": sub_criteria_name,
                    "highest_full_level": highest_full_level,
                    "weight": sub_weight,
                    "target_level_achieved": highest_full_level >= self.config.target_level,
                    "weighted_score": weighted_score,
                    "action_plan": action_plan,
                    "raw_results_ref": raw_results_for_sub_seq 
                }
                self.final_subcriteria_results.append(final_sub_result)
                
                logger.critical(f"[END] Sub-Criteria {sub_id} completed. Highest Level: L{highest_full_level} (Score: {weighted_score:.2f})")
                
                # üìå Auto-Saving temporary evidence map (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ Logic ‡πÉ‡∏ô‡∏Ñ‡∏•‡∏≤‡∏™)
                if self.temp_map_for_save:
                    logger.info(f"üíæ Auto-Saving temporary evidence map after {sub_id} completion...")
                    # NOTE: _save_evidence_map ‡∏ï‡πâ‡∏≠‡∏á‡∏ñ‡∏π‡∏Å‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÑ‡∏ß‡πâ
                    self._save_evidence_map(self.temp_map_for_save)

        # 6. Calculate Overall Statistics & Finalize
        self._calculate_overall_stats(target_sub_id)

        final_results = {
            "summary": self.total_stats,
            "sub_criteria_results": self.final_subcriteria_results,
            "raw_llm_results": self.raw_llm_results,
            "run_time_seconds": time.time() - start_ts,
            "timestamp": datetime.now().isoformat(),
        }
        
        if export:
            # NOTE: self._export_results ‡∏ï‡πâ‡∏≠‡∏á‡∏ñ‡∏π‡∏Å‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÑ‡∏ß‡πâ
            export_path = self._export_results(
                results=final_results,
                enabler=self.config.enabler,
                sub_criteria_id=target_sub_id,
                target_level=self.config.target_level
            )
            final_results["export_path_used"] = export_path
        
        return final_results

# -------------------- Core Assessment Logic --------------------
    def _run_single_assessment(
        self,
        sub_criteria: Dict[str, Any],
        statement_data: Dict[str, Any],
        vectorstore_manager: Optional['VectorStoreManager'],
        # üü¢ NEW: ‡∏£‡∏±‡∏ö Chunk UUIDs ‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏à‡∏≤‡∏Å Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ (L[level-1])
        sequential_chunk_uuids: Optional[List[str]] = None 
    ) -> Dict[str, Any]:
        """Runs RAG retrieval and LLM evaluation for a single statement (Level)."""
        sub_id = sub_criteria['sub_id']
        level = statement_data['level']
        statement_text = statement_data['statement']
        sub_criteria_name = sub_criteria['sub_criteria_name']

        statement_id = statement_data.get('statement_id', sub_id)
        
        logger.info(f"  > Starting assessment for {sub_id} L{level}...")

        # 1. Determine PDCA Phase and LEVEL CONSTRAINT
        pdca_phase = self._get_pdca_phase(level)
        level_constraint = self._get_level_constraint_prompt(level)
        
        contextual_rules_prompt = self._get_contextual_rules_prompt(sub_id, level)
        
        full_focus_hint = level_constraint + contextual_rules_prompt
        
        # -------------------- üõë NEW LOGIC START: Hybrid Retrieval (Helper Call) üõë --------------------
        # 1. Hybrid Retrieval: Fetch mapped Stable Doc IDs and priority chunks from VSM
        # üìå ‡∏£‡∏±‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏õ‡πá‡∏ô Stable Doc IDs ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏°‡∏õ‡πÑ‡∏ß‡πâ ‡πÅ‡∏•‡∏∞ Priority Docs
        mapped_stable_doc_ids, priority_docs = self._get_mapped_uuids_and_priority_chunks(
            sub_id=sub_id,
            level=level,
            statement_text=statement_text,
            level_constraint=level_constraint, 
            vectorstore_manager=vectorstore_manager
            # ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏™‡πà‡∏á sequential_chunk_uuids ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ Helper ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å Map ‡πÄ‡∏≠‡∏á
        )
        # -------------------- üõë NEW LOGIC END üõë --------------------
        
        # 2. RAG Retrieval SETUP (Pre-Query Enhancement)
        # üü¢ FIX #1: ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ enhance_query_for_statement ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡πà‡∏á 'level' ‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏ö List[str]
        rag_query_list = enhance_query_for_statement(
            statement_text=statement_text,      # 1. statement_text
            sub_id=sub_id,                      # 2. FIX: ID ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏¢‡πà‡∏≠‡∏¢ (e.g., "1.1")
            statement_id=statement_id,          # 3. FIX: ID Statement ‡∏¢‡πà‡∏≠‡∏¢ (e.g., "1.1.2" ‡∏´‡∏£‡∏∑‡∏≠ Fallback ‡πÄ‡∏õ‡πá‡∏ô "1.1")
            level=level,                        # 4. Level
            enabler_id=self.enabler_id,         # 5. Enabler ID
            focus_hint=full_focus_hint,         # 6. Focus Hint
            llm_executor=self.llm     # 7. LLM Executor (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡πÉ‡∏ô self)
        )
            
        # üìå ‡πÉ‡∏ä‡πâ query ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ Log/‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏• (rag_query)
        rag_query = rag_query_list[0] if rag_query_list else statement_text 

        current_final_k = FINAL_K_RERANKED
        current_rag_retriever = self.rag_retriever 
        current_llm_evaluator = self.llm_evaluator 
        initial_k_to_use = INITIAL_TOP_K

        # üü¢ PHASE 2 OPTIMIZATION: Use specialized retrieval/evaluation for L1/L2
        # NOTE: L1_INITIAL_TOP_K_RAG ‡∏ï‡πâ‡∏≠‡∏á‡∏ñ‡∏π‡∏Å‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÉ‡∏ô self (‡πÄ‡∏ä‡πà‡∏ô self.config.L1_INITIAL_TOP_K_RAG)
        if level <= 2:
            current_llm_evaluator = evaluate_with_llm_low_level
            current_final_k = LOW_LEVEL_K 
            initial_k_to_use = getattr(self, 'L1_INITIAL_TOP_K_RAG', INITIAL_TOP_K)
        else:
            current_final_k = FINAL_K_RERANKED

        # 2. RAG Retrieval EXECUTION
        
        retrieval_start = time.time()
        
        if self.config.mock_mode == "none" and not vectorstore_manager:
            logger.error(f"Cannot run RAG for {sub_id} L{level}: VectorstoreManager is None in non-mock mode.")
            # üìå FIX: ‡πÄ‡∏û‡∏¥‡πà‡∏° used_chunk_uuids ‡πÄ‡∏õ‡πá‡∏ô List ‡∏ß‡πà‡∏≤‡∏á
            retrieval_result = {"top_evidences": [], "aggregated_context": "ERROR: No vectorstore manager.", "used_chunk_uuids": []}
        else:
            # üü¢ NEW LOGIC: ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤ mapped_uuids ‡πÅ‡∏•‡∏∞ priority_docs_input ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ RAG Retriever
            
            # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á Limited Chunks ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (priority_docs ‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏á)
            if priority_docs:
                # 1. ‡∏™‡πà‡∏á Chunks ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏à‡∏≥‡∏Å‡∏±‡∏î‡πÑ‡∏õ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
                retrieval_map_uuids = None
                retrieval_priority_docs = priority_docs
            else:
                # 2. ‡∏ñ‡πâ‡∏≤‡∏î‡∏∂‡∏á Limited Chunks ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ: ‡πÉ‡∏´‡πâ RAG Retriever ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Hybrid Search ‡πÄ‡∏≠‡∏á
                # üìå FIX: ‡∏™‡πà‡∏á Dict ‡∏ó‡∏µ‡πà‡∏°‡∏µ Stable Doc IDs ‡πÅ‡∏•‡∏∞ Chunk UUIDs ‡∏à‡∏≤‡∏Å L[level-1]
                retrieval_map_uuids = {
                    "stable_doc_ids": mapped_stable_doc_ids,
                    "sequential_chunk_uuids": sequential_chunk_uuids or []
                }
                retrieval_priority_docs = None

            try:
                retrieval_result = current_rag_retriever(
                    query=rag_query_list, # üìå ‡∏™‡πà‡∏á List[str] (Multi-Query)
                    doc_type=EVIDENCE_DOC_TYPES, 
                    enabler=self.enabler_id,     
                    top_k=current_final_k,
                    initial_k=initial_k_to_use,
                    sub_id=sub_id, 
                    level=level,
                    vectorstore_manager=vectorstore_manager,
                    # üìå ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå: ‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô
                    mapped_uuids=retrieval_map_uuids, 
                    priority_docs_input=retrieval_priority_docs 
                )
            except Exception as e:
                logger.error(f"RAG retrieval failed for {sub_id} L{level}: {e}")
                # üìå FIX: ‡πÄ‡∏û‡∏¥‡πà‡∏° used_chunk_uuids ‡πÄ‡∏õ‡πá‡∏ô List ‡∏ß‡πà‡∏≤‡∏á
                retrieval_result = {"top_evidences": [], "aggregated_context": "ERROR: RAG failure.", "used_chunk_uuids": []}
        
        retrieval_duration = time.time() - retrieval_start
        aggregated_context = retrieval_result.get("aggregated_context", "")
        top_evidences = retrieval_result.get("top_evidences", [])
        # üü¢ NEW: ‡∏î‡∏∂‡∏á used_chunk_uuids ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å Reranked/‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏à‡∏£‡∏¥‡∏á
        used_chunk_uuids = retrieval_result.get("used_chunk_uuids", []) 

        logger.info(f"    - Retrieval found {len(top_evidences)} evidences in {retrieval_duration:.2f}s (K={current_final_k}).")

        # -------------------- CONTEXT ORDERING LOGIC --------------------
        # ------------------ Action #6: PDCA Content Classification (NEW) ------------------
        # üü¢ ‡∏ï‡∏¥‡∏î‡∏õ‡πâ‡∏≤‡∏¢ PDCA Tag ‡πÉ‡∏´‡πâ‡∏Å‡∏±‡∏ö Chunk ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å Reranked/‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô Context
        for doc in top_evidences:
            chunk_text = doc.get('text', '')
            if chunk_text:
                # üìå ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ Classifier ‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°
                pdca_tag = self._classify_pdca_phase_for_chunk(chunk_text) 
                doc['pdca_tag'] = pdca_tag 
            else:
                doc['pdca_tag'] = "Other"
        
        logger.info(f"  > ‚úÖ PDCA Content Tagging complete for {len(top_evidences)} evidences.")
        # ------------------ /Action #6 ------------------

        # 1) CLASSIFY PDCA BLOCKS FROM EVIDENCE
        plan_blocks, do_blocks, check_blocks, act_blocks, other_blocks = \
            self._get_pdca_blocks_from_evidences(top_evidences, level)

        final_context_for_llm = aggregated_context  # default

        # 2) APPLY L3 3-TIER ORDERING
        if level >= 3: 
            
            # üìå ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏à‡∏∞‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô C ‡∏´‡∏£‡∏∑‡∏≠ A ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Å‡∏±‡∏ö PDCA Loop 
            has_check_or_act = check_blocks or act_blocks
            
            if not has_check_or_act:
                logger.warning(f"‚ö†Ô∏è L{level}: No Check/Act blocks detected. Skipping custom ordering.")
            else:
                logger.critical(f"üö® Activating L{level} Content-Based Reordering.")

                # A. Build simulated evidence (Priority 1) - KEEP FOR NOW
                # simulated_evidence_context = build_simulated_l3_evidence(check_blocks)
                simulated_evidence_context=""
                
                # üü¢ NEW: ‡πÄ‡∏û‡∏¥‡πà‡∏° SAFETY CAP ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Simulated Evidence
                # if len(simulated_evidence_context) > MAX_SIMULATED_CONTEXT_LEN:
                #     logger.warning(f"‚ö†Ô∏è L{level} Simulated Context capped from {len(simulated_evidence_context)} to {MAX_SIMULATED_CONTEXT_LEN} chars.")
                #     simulated_evidence_context = simulated_evidence_context[:MAX_SIMULATED_CONTEXT_LEN]
                
                # if IS_LOG_L3_CONTEXT:
                #     logger.info(f"üü¢ L{level} simulated evidence created and merged: {len(simulated_evidence_context)} chars.")

                # B. Content-Based Ordered Context (‡πÉ‡∏ä‡πâ Blocks ‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ï‡∏≤‡∏° Tag ‡πÅ‡∏•‡πâ‡∏ß)
                final_context_for_llm = build_ordered_context(
                    level=level,
                    # simulated_l3=simulated_evidence_context, 
                    plan_blocks=plan_blocks,
                    do_blocks=do_blocks,
                    check_blocks=check_blocks,
                    act_blocks=act_blocks,
                    other_blocks=other_blocks
                )

                logger.info(f"    - L{level} context reordered successfully based on PDCA Tags.")
        
        # ‡∏™‡πà‡∏á‡∏Ñ‡πà‡∏≤‡∏ï‡πà‡∏≠‡πÉ‡∏´‡πâ LLM
        aggregated_context = final_context_for_llm

        # -------------------- CONTEXT ORDERING LOGIC END --------------------

        # 3. LLM Evaluation
        llm_start = time.time()
        llm_result = None # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ None ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏ô‡∏ó‡∏≤‡∏ô
        try:
            llm_result = current_llm_evaluator(
                context=aggregated_context,
                sub_criteria_name=sub_criteria_name,
                level=level,
                statement_text=statement_text,
                sub_id=sub_id,
                pdca_phase=pdca_phase,
                level_constraint=level_constraint,
                contextual_rules=contextual_rules_prompt, # üü¢ NEW: ‡∏™‡πà‡∏á‡∏Å‡∏é‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏Ç‡πâ‡∏≤ LLM
                llm_executor=self.llm
            )
        except Exception as e:
            logger.error(f"LLM evaluation failed for {sub_id} L{level}: {e}")
            llm_result = {"score": 0, "reason": f"LLM Fatal Error: {e}", "is_passed": False}

        llm_duration = time.time() - llm_start

        # üåü NEW STEP 3.5: LLM Context Summary (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô)
        try:
            # üìå NOTE: ‡πÉ‡∏ä‡πâ aggregated_context ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å RAG ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡∏∏‡∏õ
            summary_result = create_context_summary_llm(
                context=aggregated_context,
                sub_criteria_name=sub_criteria_name,
                level=level,
                sub_id=sub_id,
                llm_executor=self.llm # ‚¨ÖÔ∏è ‡∏™‡πà‡∏á LLM Instance ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ
            )
            # ‡∏î‡∏∂‡∏á summary text ‡πÅ‡∏•‡∏∞‡πÄ‡∏Å‡πá‡∏ö object ‡πÄ‡∏ï‡πá‡∏°‡πÑ‡∏ß‡πâ
            llm_summary_text = summary_result.get("summary", "N/A (LLM Summary Failed)")
            summary_for_save = summary_result
            
        except Exception as e:
            logger.error(f"Context summarization failed for {sub_id} L{level}: {e}")
            llm_summary_text = "ERROR: LLM summary failed. Using raw context."
            summary_for_save = {"summary": llm_summary_text, "suggestion_for_next_level": str(e)}

        # üü¢ FIX: Calculate PDCA breakdown and final pass status based on llm_score (Priority 1 Part 2 & Priority 2)
        # -------------------- üõë ‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î 'NoneType' üõë --------------------
        llm_score = 0
        if llm_result is not None and isinstance(llm_result, dict):
            llm_score = llm_result.get('score', 0)
        else:
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡∏Å LLM ‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô None ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà Dictionary
            self.logger.error(f"LLM returned None or invalid result for assessment {sub_id} L{level}. Setting score=0.")
        # -------------------- üõë ‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç üõë --------------------

        # üìå ‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏•‡πâ‡∏ß
        pdca_breakdown, is_passed, raw_pdca_score = calculate_pdca_breakdown_and_pass_status(
            llm_score=llm_score, 
            level=level
        )
        
        pass_status = "‚úÖ PASS" if is_passed else "‚ùå FAIL"
        
        # üìå Save on PASS Logic (Auto-Persistence - Idea 2)
        # ‡πÉ‡∏ä‡πâ map_key = f"{sub_id}.L{level}" ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Level ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
        map_key_current = f"{sub_id}.L{level}"
        if is_passed:
            
            # üü¢ FIX: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏õ‡πá‡∏ô List ‡∏Ç‡∏≠‡∏á Dictionary {doc_id, filename}
            # ‡∏î‡∏∂‡∏á UUIDs/Info ‡∏à‡∏≤‡∏Å Context ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å Reranked/‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏à‡∏£‡∏¥‡∏á (‡∏à‡∏≤‡∏Å top_evidences)
            uuids_to_save = []
            
            # üü¢ NEW LOGIC: ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å doc_id ‡πÅ‡∏•‡∏∞ filename ‡πÄ‡∏õ‡πá‡∏ô dictionary
            for doc in top_evidences:
                doc_id = doc.get('doc_id', None)
                source_filename = doc.get('source_filename', doc.get('source', None)) # ‡πÉ‡∏ä‡πâ 'source' ‡πÄ‡∏õ‡πá‡∏ô fallback
                
                if doc_id is not None:
                    uuids_to_save.append({
                        "doc_id": doc_id,
                        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ mapping file ‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô
                        "filename": source_filename,
                        "mapper_type": "AI_RAG", # ‚¨ÖÔ∏è ‡πÄ‡∏û‡∏¥‡πà‡∏° Field ‡∏ô‡∏µ‡πâ
                        "priority": True,    
                        "timestamp": datetime.now().isoformat() # ‚¨ÖÔ∏è ‡πÄ‡∏û‡∏¥‡πà‡∏° Field ‡∏ô‡∏µ‡πâ
                    })
            
            if uuids_to_save:
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° Log (‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï)
                is_new_mapping = map_key_current not in self.evidence_map
                
                # ... (‡πÇ‡∏Ñ‡πâ‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏û‡∏¥‡∏°‡∏û‡πå Log ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°) ...
                # ‡πÉ‡∏ä‡πâ sys.stderr/sys.stdout ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏û‡∏¥‡∏°‡∏û‡πå Log (‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô)
                print(f"\n[MAP üíæ {map_key_current}] ‚úÖ PASS: Saved {len(uuids_to_save)} evidence info to temp map. Details:", file=sys.stderr)
                
                # ‡πÅ‡∏™‡∏î‡∏á‡πÅ‡∏Ñ‡πà‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏£‡∏∑‡∏≠ ID ‡∏ó‡∏µ‡πà‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢
                for i, doc in enumerate(top_evidences[:3]): # ‡πÅ‡∏™‡∏î‡∏á 3 ‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å
                    doc_id = doc.get('doc_id', 'N/A')
                    source = doc.get('source_filename', doc.get('source', 'N/A')) # <--- ‡πÉ‡∏ä‡πâ 'source' ‡πÄ‡∏õ‡πá‡∏ô fallback
                    score = doc.get('score', 0.0)
                    
                    # ‡πÉ‡∏ä‡πâ stderr ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏¢‡∏Å‡∏à‡∏≤‡∏Å Log ‡∏õ‡∏Å‡∏ï‡∏¥
                    print(f"  > [Top {i+1} | Score: {score:.3f}] File: **{source}** (ID: {doc_id})", file=sys.stderr)
                
                # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å/‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï Mapping ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß (‡∏à‡∏∞ OVERWRITE ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Key ‡∏ô‡∏±‡πâ‡∏ô)
                self.temp_map_for_save[map_key_current] = uuids_to_save
                
                action_desc = "üÜï Temporarily stored new mapping" if is_new_mapping else "üíæ Updated temporary mapping"
                logger.info(f"{action_desc} for {map_key_current} after successful PASS. ({len(uuids_to_save)} evidence items)")

        result = {
            "sub_criteria_id": sub_id,
            "sub_criteria_name": sub_criteria_name,
            "level": level,
            "statement": statement_text,
            "pdca_phase": pdca_phase,
            "llm_score": llm_score,
            "reason": llm_result.get('reason', 'N/A'),
            "is_passed": is_passed, # üü¢ FIX: ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏à‡∏≤‡∏Å PDCA Logic
            "pdca_breakdown": pdca_breakdown, # üü¢ NEW FIELD
            "raw_pdca_score": raw_pdca_score, # üü¢ NEW FIELD
            "rag_query": rag_query,
            "retrieval_duration_s": retrieval_duration,
            "llm_duration_s": llm_duration,
            "retrieved_evidences_count": len(top_evidences),
            "retrieved_full_source_info": top_evidences,
            "aggregated_context_used": aggregated_context,
            # ‚úÖ NEW FIELD: ‡∏™‡∏£‡∏∏‡∏õ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡πÇ‡∏î‡∏¢ LLM (‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°)
            "llm_summarized_context": llm_summary_text, 
            # ‚úÖ NEW FIELD: ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå LLM Summary ‡πÄ‡∏ï‡πá‡∏° (‡∏£‡∏ß‡∏° suggestion)
            "llm_summary_full_result": summary_for_save,
            # üü¢ NEW: ‡∏™‡πà‡∏á UUIDs ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ Hybrid RAG ‡∏Ç‡∏≠‡∏á Level ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ)
            "used_chunk_uuids": used_chunk_uuids 
        }

        logger.info(f"    - Result: {pass_status} ({llm_score}/1) in {llm_duration:.2f}s. Reason: {llm_result.get('reason', 'N/A')[:50]}...")

        return result