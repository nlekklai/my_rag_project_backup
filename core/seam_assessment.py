# core/seam_assessment.py

import sys
import json
import logging
import time
import os
from typing import List, Dict, Any, Optional, Union, Tuple, Set, Final, Literal
from collections import defaultdict, OrderedDict
from datetime import datetime
from dataclasses import dataclass, field
import multiprocessing # NEW: Import for parallel execution
from functools import partial
import pathlib, uuid
from langchain_core.documents import Document as LcDocument
from core.retry_policy import RetryPolicy, RetryResult
from copy import deepcopy
import tempfile
import shutil
from .json_extractor import _robust_extract_json
from filelock import FileLock  # ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á: pip install filelock
import re
import hashlib
import copy
from database import init_db
from database import db_update_task_status as update_db
from pydantic import BaseModel
import random  # Added for shuffle

# -------------------- PATH SETUP & IMPORTS --------------------
try:
    PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
    if PROJECT_ROOT not in sys.path:
        sys.path.insert(0, PROJECT_ROOT)

    # 1. Import Constants ‡∏à‡∏≤‡∏Å global_vars
    from config.global_vars import (
        MAX_LEVEL,
        EVIDENCE_DOC_TYPES,
        RERANK_THRESHOLD,
        MAX_EVI_STR_CAP,
        DEFAULT_LLM_MODEL_NAME,
        LLM_TEMPERATURE,
        MIN_RETRY_SCORE,
        MAX_PARALLEL_WORKERS,
        PDCA_PRIORITY_ORDER,
        TARGET_DEVICE,
        PDCA_PHASE_MAP,
        INITIAL_TOP_K,
        FINAL_K_RERANKED,
        MAX_CHUNKS_PER_FILE,
        MAX_CHUNKS_PER_BLOCK,
        MATURITY_LEVEL_GOALS
    )
    
    # 2. Import Logic Functions
    from core.llm_data_utils import ( 
        create_structured_action_plan, evaluate_with_llm,
        retrieve_context_with_filter, retrieve_context_for_low_levels,
        evaluate_with_llm_low_level, LOW_LEVEL_K, 
        set_mock_control_mode as set_llm_data_mock_mode,
        create_context_summary_llm,
        retrieve_context_by_doc_ids,
        _fetch_llm_response,
        build_multichannel_context_for_level,
        _get_emergency_fallback_plan,
        _check_and_handle_empty_context
    )
    from core.vectorstore import VectorStoreManager, load_all_vectorstores, get_global_reranker 
    from core.action_plan_schema import ActionPlanActions, ActionPlanResult

    # 3. üéØ Import Path Utilities
    from utils.path_utils import (
        get_mapping_file_path, 
        get_evidence_mapping_file_path, 
        get_contextual_rules_file_path,
        get_doc_type_collection_key,
        get_assessment_export_file_path,
        get_export_dir,
        get_rubric_file_path,
        load_evidence_mapping,
        _n
    )

    import assessments.seam_mocking as seam_mocking 
    
except ImportError as e:
    # -------------------- Modernized Fallback Code --------------------
    print(f"‚ö†Ô∏è WARNING: Import failed, using dynamic fallback for Mac. Error: {e}", file=sys.stderr)
    
    # Fallback Constants
    EXPORTS_DIR = "exports"
    MAX_LEVEL = 5
    INITIAL_LEVEL = 1
    QA_FINAL_K = 3
    RUBRIC_FILENAME_PATTERN = "{tenant}_{enabler}_rubric.json"
    DEFAULT_ENABLER = "KM"
    EVIDENCE_DOC_TYPES = "evidence"
    INITIAL_TOP_K = 10

    # üìå Placeholder functions for path_utils (‡∏ä‡∏µ‡πâ‡πÄ‡∏Ç‡πâ‡∏≤ data_store ‡∏ï‡∏£‡∏á‡πÜ)
    def _n(s): return str(s).lower().strip()

    def get_mapping_file_path(doc_type, tenant, year=None, enabler=None):
        t = _n(tenant)
        if _n(doc_type) == "evidence":
            return f"data_store/{t}/mapping/{year}/{t}_{year}_{_n(enabler)}_doc_id_mapping.json"
        return f"data_store/{t}/mapping/{t}_{_n(doc_type)}_doc_id_mapping.json"

    def get_evidence_mapping_file_path(tenant, year, enabler):
        t = _n(tenant)
        return f"data_store/{t}/mapping/{year}/{t}_{year}_{_n(enabler)}_evidence_mapping.json"

    def get_contextual_rules_file_path(tenant, enabler):
        t = _n(tenant)
        return f"data_store/{t}/config/{t}_{_n(enabler)}_contextual_rules.json"

    def get_rubric_file_path(tenant, enabler):
        t = _n(tenant)
        return f"data_store/{t}/config/{t}_{_n(enabler)}_rubric.json"

    def load_evidence_mapping(tenant="pea", year=2568, enabler="KM"):
        path = get_evidence_mapping_file_path(tenant, year, enabler)
        if os.path.exists(path):
            try:
                with open(path, "r", encoding="utf-8") as f: return json.load(f)
            except: return {}
        return {}

    # Mock Logic Functions
    def create_structured_action_plan(*args, **kwargs): return []
    def evaluate_with_llm(*args, **kwargs): return {"score": 0, "reason": "Import Error Fallback", "is_passed": False}
    def retrieve_context_with_filter(*args, **kwargs): return {"top_evidences": [], "aggregated_context": ""}
    def retrieve_context_for_low_levels(*args, **kwargs): return {"top_evidences": [], "aggregated_context": ""}
    def evaluate_with_llm_low_level(*args, **kwargs): return {"score": 0, "is_passed": False}
    def set_llm_data_mock_mode(mode): pass
    def build_multichannel_context_for_level(*args, **kwargs): return ""
    
    class VectorStoreManager: pass
    def load_all_vectorstores(*args, **kwargs): return None
    
    class ActionPlanActions:
        @staticmethod
        def generate(*args, **kwargs): return []
    
    class ActionPlanResult:
        def __init__(self): self.success = False

    PDCA_PHASE_MAP = {
        1: "Plan (‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢)",
        2: "Do (‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡πÅ‡∏ú‡∏ô‡πÑ‡∏õ‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡πÅ‡∏•‡∏∞‡∏Ç‡∏±‡∏ö‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô)",
        3: "Check (‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•)",
        4: "Act (‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°)",
        5: "Sustainability (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏±‡πà‡∏á‡∏¢‡∏∑‡∏ô‡πÅ‡∏•‡∏∞‡∏ï‡πâ‡∏ô‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏î‡∏µ)"
    }

    MATURITY_LEVEL_GOALS = {
        1: "‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô ‡∏°‡∏µ‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢ ‡∏´‡∏£‡∏∑‡∏≠‡∏°‡∏µ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô",
        2: "‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡∏ö‡∏ö ‡∏°‡∏µ‡∏Ñ‡∏ì‡∏∞‡∏ó‡∏≥‡∏á‡∏≤‡∏ô",
        3: "‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á ‡πÅ‡∏•‡∏∞‡πÄ‡∏´‡πá‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô",
        4: "‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ä‡∏¥‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥ ‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°",
        5: "‡πÄ‡∏ô‡πâ‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏±‡πà‡∏á‡∏¢‡∏∑‡∏ô‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏ô‡πÅ‡∏ö‡∏ö (Role Model)"
    }

    def _get_emergency_fallback_plan(sub_id, name, level, *args, **kwargs):
        return {"summary": f"Fallback plan for {sub_id}", "steps": []}
    
    class seam_mocking:
        @staticmethod
        def set_mock_control_mode(mode): pass
    
    def create_context_summary_llm(*args, **kwargs): 
        return {"summary": "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡∏∏‡∏õ‡πÑ‡∏î‡πâ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏ö‡∏ö‡πÇ‡∏´‡∏•‡∏î Module ‡∏û‡∏±‡∏á", "coaching": "‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£ Import"}
    
    def _fetch_llm_response(*args, **kwargs): 
        return "{}"

    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÉ‡∏ô _run_single_assessment ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ
    MAX_EVI_STR_CAP = 10.0
    RERANK_THRESHOLD = 0.35

    if "FATAL ERROR" in str(e):
        pass 
# ----------------------------------------------------------------------

logger = logging.getLogger(__name__)
if not logger.handlers:
    logging.basicConfig(level=logging.DEBUG, format="%(asctime)s - %(levelname)s - %(message)s")

def get_actual_score(ev: dict) -> float:
    """
    [v2026.1 - ROBUST SCORING]
    - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤ '0.0 or score' ‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ Logic ‡πÄ‡∏û‡∏µ‡πâ‡∏¢‡∏ô
    - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á Dict ‡πÅ‡∏•‡∏∞ Langchain Document Object
    """
    if not ev:
        return 0.0

    # 1. ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö Top-level
    # ‡πÉ‡∏ä‡πâ next(...) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏Ñ‡πà‡∏≤‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà None
    score_keys = ["relevance_score", "rerank_score", "score"]
    
    # ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å Dict ‡∏´‡∏£‡∏∑‡∏≠ Object attribute
    val = None
    for key in score_keys:
        val = ev.get(key) if isinstance(ev, dict) else getattr(ev, key, None)
        if val is not None:
            return float(val)

    # 2. Fallback ‡πÑ‡∏õ‡∏ó‡∏µ‡πà Metadata (‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡πÉ‡∏ô ChromaDB)
    meta = ev.get("metadata", {}) if isinstance(ev, dict) else getattr(ev, "metadata", {})
    if meta:
        for key in score_keys:
            val = meta.get(key)
            if val is not None:
                return float(val)

    return 0.0

def _static_worker_process(worker_input_tuple: Tuple) -> Any:
    """
    [ULTIMATE WORKER v2026.3] Isolated Execution for Parallel Assessment
    ---------------------------------------------------------------------
    - ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ Process (Zero Memory Leak)
    - ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏ä‡πâ Context ‡∏à‡∏≤‡∏Å‡πÅ‡∏°‡πà‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥
    - ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏£‡∏∞‡∏ö‡∏ö‡∏û‡∏±‡∏á‡∏î‡πâ‡∏ß‡∏¢ Fallback Dictionary
    """

    # 1. üìÇ PATH SETUP
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏≠‡∏á‡πÄ‡∏´‡πá‡∏ô‡πÇ‡∏°‡∏î‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Import AssessmentConfig ‡πÅ‡∏•‡∏∞ Engine ‡πÑ‡∏î‡πâ
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
    if project_root not in sys.path:
        sys.path.append(project_root)
        
    worker_logger = logging.getLogger(f"Worker_{os.getpid()}")

    # 2. üì¶ ROBUST UNPACKING
    try:
        (
            sub_criteria_data, enabler, target_level, mock_mode, 
            evidence_map_path, model_name, temperature,
            min_retry_score, max_retrieval_attempts, document_map, 
            action_plan_model, year, tenant
        ) = worker_input_tuple
        
        sub_id = sub_criteria_data.get('sub_id', 'UNKNOWN')
        worker_logger.info(f"‚öôÔ∏è PID:{os.getpid()} | Starting: {sub_id} ({tenant}/{year})")
        
    except Exception as e:
        return {"error": f"Worker unpacking failed: {str(e)}", "status": "critical_failure"}
        
    # 3. üèóÔ∏è RECONSTRUCT ISOLATED ENGINE
    try:
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Config ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Worker ‡∏ï‡∏±‡∏ß‡∏ô‡∏µ‡πâ
        worker_config = AssessmentConfig(
            enabler=enabler,
            tenant=tenant,
            year=int(year) if year else None,     
            target_level=target_level,
            mock_mode=mock_mode,
            model_name=model_name, 
            temperature=temperature,
            min_retry_score=min_retry_score,            
            max_retrieval_attempts=max_retrieval_attempts 
        )

        # ‡∏Ñ‡∏∑‡∏ô‡∏ä‡∏µ‡∏û Engine (‡∏à‡∏∞‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà __init__ ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏á Patch ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡πÑ‡∏õ)
        worker_instance = SEAMPDCAEngine(
            config=worker_config, 
            evidence_map_path=evidence_map_path, 
            llm_instance=None,              
            vectorstore_manager=None,       
            logger_instance=worker_logger,
            document_map=document_map,      
            ActionPlanActions=action_plan_model
        )
    except Exception as e:
        worker_logger.error(f"‚ùå Worker initialization failed for {sub_id}: {e}")
        return {"sub_id": sub_id, "error": f"Init Error: {str(e)}", "status": "failed"}

    # 4. ‚ö° EXECUTE & TIME TRACKING
    try:
        start_time = time.time()
        
        # ‡∏£‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏£‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠ (Core Logic)
        result = worker_instance._run_sub_criteria_assessment_worker(sub_criteria_data)
        
        elapsed = time.time() - start_time
        worker_logger.info(f"‚úÖ PID:{os.getpid()} | Finished: {sub_id} in {elapsed:.2f}s")
        
        return result
        
    except Exception as e:
        worker_logger.error(f"‚ùå Execution error for {sub_id}: {str(e)}")
        return {
            "sub_id": sub_id,
            "error": str(e),
            "status": "failed",
            "execution_time": 0
        }
    
# =================================================================
# Configuration Class
# =================================================================
@dataclass
class AssessmentConfig:
    """Configuration for the SEAM PDCA Assessment Run."""
    
    # ------------------ 1. Assessment Context ------------------
    enabler: str = None
    tenant: str = None
    year: int = None  # üëà ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å DEFAULT_YEAR ‡πÄ‡∏õ‡πá‡∏ô None
    target_level: int = MAX_LEVEL
    mock_mode: str = "none" # 'none', 'random', 'control'
    force_sequential: bool = field(default=False) # Flag ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏£‡∏±‡∏ô‡πÅ‡∏ö‡∏ö Sequential

    # ------------------ 2. LLM Configuration (Configurable) ------------------
    # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Default ‡∏à‡∏≤‡∏Å global_vars.py
    model_name: str = DEFAULT_LLM_MODEL_NAME 
    temperature: float = LLM_TEMPERATURE

    # ------------------ 3. Adaptive RAG Retrieval Configuration ------------------
    # üü¢ NEW: ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô Rerank ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Adaptive Loop (MIN_RETRY_SCORE)
    # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Default 0.65 ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏û‡∏ö‡πÉ‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î Logic
    min_retry_score: float = MIN_RETRY_SCORE
    # üü¢ NEW: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≠‡∏ö‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á Adaptive RAG Loop (MAX_RETRIEVAL_ATTEMPTS)
    # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Default 3
    max_retrieval_attempts: int = 3
    
    # ------------------ 4. Export Configuration ------------------
    export_output: bool = field(default=False) # Flag ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏¥‡∏î/‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£ Export ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    export_path: str = "" # Path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå Export (‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å)


# =================================================================
# SEAM Assessment Engine (PDCA Focused) - Full Revise v2026
# =================================================================
class SEAMPDCAEngine:
    
    def __init__(
        self, 
        config: AssessmentConfig,
        llm_instance: Any = None, 
        logger_instance: logging.Logger = None,
        rag_retriever_instance: Any = None,
        doc_type: str = None, 
        vectorstore_manager: Optional['VectorStoreManager'] = None,
        evidence_map_path: Optional[str] = None,
        document_map: Optional[Dict[str, str]] = None,
        is_parallel_all_mode: bool = False,
        sub_id: str = 'all',
        **kwargs  
    ):
        """
        [ULTIMATE REVISE v2026.3] SEAM Assessment Engine Constructor
        ‡πÄ‡∏ô‡πâ‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏ô‡∏ó‡∏≤‡∏ô (Resilience), ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢ (Sanity Check) ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        """
        # -------------------------------------------------------
        # 1. Logger Setup (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö 1 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£ Trace Error)
        # -------------------------------------------------------
        self.doc_type = doc_type or getattr(config, 'doc_type', EVIDENCE_DOC_TYPES)
        clean_dt = str(self.doc_type).strip().lower()
        log_year = config.year if clean_dt == EVIDENCE_DOC_TYPES.lower() else "general"

        if logger_instance is not None:
            self.logger = logger_instance
        else:
            self.logger = logging.getLogger(__name__).getChild(
                f"Engine|{config.enabler}|{config.tenant}/{log_year}"
            )

        self.logger.info(f"üöÄ Initializing SEAMPDCAEngine: {config.enabler} ({config.tenant}/{log_year})")

        # -------------------------------------------------------
        # 2. Patch: Sanity Check & Core Configuration
        # -------------------------------------------------------
        self.config = config
        
        # [CRITICAL PATCH] ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏á‡∏≤‡∏ô
        if not self.config.enabler or not self.config.tenant:
            self.logger.critical("‚ùå Mandatory Config Missing: enabler and tenant must be provided!")
            raise ValueError("Enabler and Tenant are required for SEAMPDCAEngine.")

        self.enabler_id = config.enabler
        self.target_level = config.target_level
        self.sub_id = sub_id
        self.llm = llm_instance
        self.vectorstore_manager = vectorstore_manager
        self.is_parallel_all_mode = is_parallel_all_mode
        self.is_sequential = getattr(config, 'force_sequential', True)
        self.results = {}

        # -------------------------------------------------------
        # 3. Database & System Warm-up
        # -------------------------------------------------------
        try:
            init_db()  # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô 'no such table' ‡∏´‡∏£‡∏∑‡∏≠ Schema mismatch
            self.logger.info("üìÇ Database Schema verified/initialized.")
        except Exception as e:
            self.logger.error(f"‚ö†Ô∏è DB Init Warning: {e} (Check if tables already exist)")

        # -------------------------------------------------------
        # 4. Data Loading (Rubric, Rules & Policies)
        # -------------------------------------------------------
        # ‡πÇ‡∏´‡∏•‡∏î Rubric ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô AttributeError ‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô
        self.rubric = self._load_rubric()
        
        # ‡πÇ‡∏´‡∏•‡∏î‡∏Å‡∏é Contextual Rules ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö PDCA Logic
        self.contextual_rules_map = self._load_contextual_rules_map()
        
        self.retry_policy = RetryPolicy(
            max_attempts=3,
            base_delay=2.0,
            jitter=True,
            exponential_backoff=True,
        )

        # ‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô (Global Constants)
        self.RERANK_THRESHOLD = RERANK_THRESHOLD
        self.MAX_EVI_STR_CAP = MAX_EVI_STR_CAP

        # -------------------------------------------------------
        # 5. Mapping & Evidence Persistence Setup
        # -------------------------------------------------------
        # 5.1 Evidence Mapping (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏∑‡∏ö‡∏Ñ‡πâ‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á)
        self.evidence_map = {}
        if clean_dt == EVIDENCE_DOC_TYPES.lower():
            self.evidence_map_path = evidence_map_path or get_evidence_mapping_file_path(
                tenant=self.config.tenant, 
                year=self.config.year, 
                enabler=self.enabler_id
            )
            self.evidence_map = self._load_evidence_map()
            self.logger.info(f"üìä Evidence Mapping: Loaded {len(self.evidence_map)} keys.")

        # 5.2 Document Mapping (ID -> Filename ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏≥ Report/Audit)
        loaded_map = document_map or {}
        if not loaded_map:
            is_evi_mode = (clean_dt == EVIDENCE_DOC_TYPES.lower())
            mapping_path = get_mapping_file_path(
                self.doc_type, 
                tenant=self.config.tenant, 
                year=self.config.year if is_evi_mode else None,
                enabler=self.enabler_id if is_evi_mode else None
            )

            if os.path.exists(mapping_path):
                try:
                    with open(mapping_path, 'r', encoding='utf-8') as f:
                        raw_data = json.load(f)
                    loaded_map = {k: v.get("file_name", k) for k, v in raw_data.items()}
                    self.logger.info(f"üéØ Document Mapping: Loaded {len(loaded_map)} entries.")
                except Exception as e:
                    self.logger.error(f"‚ùå Error parsing mapping file: {e}")

        self.doc_id_to_filename_map = loaded_map
        self.document_map = loaded_map
        self.temp_map_for_save = {}

        # -------------------------------------------------------
        # 6. Lazy Engine Initialization (VSM & LLM)
        # -------------------------------------------------------
        if self.llm is None: self._initialize_llm_if_none()
        if self.vectorstore_manager is None: self._initialize_vsm_if_none()

        # ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ ID Mapping ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô VectorStore (‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô)
        if self.vectorstore_manager:
            try:
                self.vectorstore_manager._load_doc_id_mapping()
            except: pass

        # -------------------------------------------------------
        # 7. Function Registry (Pointers)
        # -------------------------------------------------------
        self.llm_evaluator = evaluate_with_llm
        self.rag_retriever = retrieve_context_with_filter
        self.create_structured_action_plan = create_structured_action_plan
        self.ActionPlanActions = ActionPlanActions
        self.action_plan_model = ActionPlanResult

        self.logger.info(f"‚úÖ Engine Initialized: Ready for Assessment (Sub-ID: {self.sub_id})")
    

    # =================================================================
    # DB Proxy Methods
    # =================================================================
    def db_update_task_status(self, record_id: str, progress: int, message: str, status: str = "RUNNING"):
        """
        Wrapper ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏ú‡πà‡∏≤‡∏ô Database Module
        - record_id: ID ‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÉ‡∏ô‡∏£‡∏≠‡∏ö‡∏ô‡∏±‡πâ‡∏ô
        - progress: 0-100
        - message: ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞
        """
        if not record_id: return
        try:
            # ‡πÉ‡∏ä‡πâ update_db ‡∏ó‡∏µ‡πà alias ‡∏°‡∏≤‡∏à‡∏≤‡∏Å database.db_update_task_status
            update_db(record_id, progress, message, status=status)
            self.logger.debug(f"[DB-PROGRESS] {record_id}: {progress}% - {message}")
        except Exception as e:
            self.logger.error(f"‚ùå DB Update Error: {e}")


    def get_rule_content(self, sub_id: str, level: int, key_type: str):
        """
        [ULTIMATE RULE ENGINE v2026.3]
        ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏à‡∏≤‡∏Å Contextual Rules ‡πÅ‡∏ö‡∏ö‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ä‡∏±‡πâ‡∏ô
        - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Priority: Specific Level > Sub-ID Root > Global Defaults
        """
        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏é‡∏Ç‡∏≠‡∏á Sub-ID ‡∏ô‡∏±‡πâ‡∏ô‡πÜ
        rule = self.contextual_rules_map.get(sub_id, {})
        level_key = f"L{level}"
        
        # 1. ü•á Priority 1: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏∞‡∏î‡∏±‡∏ö Level ‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á (e.g., 1.1 -> L3)
        level_data = rule.get(level_key, {})
        if key_type in level_data:
            return level_data[key_type]
        
        # 2. ü•à Priority 2: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏∞‡∏î‡∏±‡∏ö Sub-ID Root (‡∏Å‡∏é‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô‡∏ó‡∏∏‡∏Å Level ‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏ô‡∏±‡πâ‡∏ô)
        if key_type in rule:
            return rule[key_type]
        
        # 3. ü•â Priority 3: Global Defaults (‡∏Ñ‡πà‡∏≤‡∏Å‡∏•‡∏≤‡∏á‡∏Ç‡∏≠‡∏á Enabler)
        # ‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏•‡∏∏‡πà‡∏° keywords ‡πÅ‡∏•‡∏∞ required_phases ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô
        defaults = self.contextual_rules_map.get("_enabler_defaults", {})
        if key_type in defaults:
            return defaults[key_type]
            
        # 4. üõ°Ô∏è Fallback: ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        if key_type == "require_phase":
            return None # ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ caller ‡πÄ‡∏ä‡πá‡∏Ñ if phase is not None ‡πÑ‡∏î‡πâ
        if "keywords" in key_type:
            return [] # ‡∏Ñ‡∏∑‡∏ô list ‡∏ß‡πà‡∏≤‡∏á‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô loop ‡∏û‡∏±‡∏á
            
        return ""


    def _initialize_llm_if_none(self):
        """Initializes LLM instance if self.llm is None."""
        if self.llm is None:
            self.logger.warning("‚ö†Ô∏è Initializing LLM: model=%s, temperature=%s", 
                                self.config.model_name, self.config.temperature)
            try:
                # üü¢ FIX: Import ‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ create_llm_instance
                from models.llm import create_llm_instance 
                self.llm = create_llm_instance( 
                    model_name=self.config.model_name,
                    temperature=self.config.temperature
                )
                self.logger.info("‚úÖ LLM Instance created successfully: %s (Temp: %s)", 
                                 self.config.model_name, self.config.temperature)
            except Exception as e:
                self.logger.error(f"FATAL: Could not initialize LLM: {e}")
                raise

    def _initialize_vsm_if_none(self):
        """
        Initializes VectorStoreManager with Smart Year Selection.
        - If Evidence: Priority 1 = Specific Year, Priority 2 = Root Fallback.
        - If Document: Priority 1 = Root (General), Priority 2 = Specific Year Fallback.
        """
        if self.vectorstore_manager is not None:
            return

        # 1. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ç‡∏≠‡∏á DocType
        clean_dt = str(self.doc_type or getattr(self.config, 'doc_type', EVIDENCE_DOC_TYPES)).strip().lower()
        is_evidence = (clean_dt == EVIDENCE_DOC_TYPES.lower())
        
        self.logger.info(f"üöÄ Loading vectorstore(s) for DocType: '{clean_dt}' (Mode: {'Evidence' if is_evidence else 'General'})")

        try:
            target_enabler = str(self.enabler_id).lower() if self.enabler_id else None
            
            # üéØ [SMART SELECTION] ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å
            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô evidence ‡πÉ‡∏´‡πâ‡∏´‡∏≤‡∏ï‡∏≤‡∏°‡∏õ‡∏µ (2568) ‡∏Å‡πà‡∏≠‡∏ô ‡πÅ‡∏ï‡πà‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô document ‡πÉ‡∏´‡πâ‡∏´‡∏≤‡∏ó‡∏µ‡πà Root (None) ‡∏Å‡πà‡∏≠‡∏ô
            primary_year = self.config.year if is_evidence else None
            secondary_year = None if is_evidence else self.config.year

            # 2. First Attempt: ‡πÇ‡∏´‡∏•‡∏î‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
            self.vectorstore_manager = load_all_vectorstores(
                doc_types=[clean_dt], 
                enabler_filter=target_enabler, 
                tenant=self.config.tenant, 
                year=primary_year       
            )
            
            def count_retrievers(vsm):
                if vsm and hasattr(vsm, '_multi_doc_retriever') and vsm._multi_doc_retriever:
                    return len(vsm._multi_doc_retriever._all_retrievers)
                return 0

            len_retrievers = count_retrievers(self.vectorstore_manager)
            
            # 3. Second Attempt (Fallback): ‡∏ñ‡πâ‡∏≤‡∏´‡∏≤‡πÅ‡∏ö‡∏ö‡πÅ‡∏£‡∏Å‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ ‡πÉ‡∏´‡πâ‡∏™‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏´‡∏≤‡∏≠‡∏µ‡∏Å‡πÅ‡∏ö‡∏ö
            if len_retrievers == 0:
                lookup_label = f"year {secondary_year}" if secondary_year else "tenant root"
                self.logger.info(f"‚ö†Ô∏è No collections found in primary path, searching in {lookup_label}...")
                
                self.vectorstore_manager = load_all_vectorstores(
                    doc_types=[clean_dt], 
                    enabler_filter=target_enabler, 
                    tenant=self.config.tenant, 
                    year=secondary_year       
                )
                len_retrievers = count_retrievers(self.vectorstore_manager)

            # 4. Post-Load Process
            if self.vectorstore_manager and len_retrievers > 0:
                self.vectorstore_manager._load_doc_id_mapping() 
                self.logger.info(f"‚úÖ MultiDocRetriever loaded with {len_retrievers} collections.") 
            else:
                # 5. Final Error Handling
                expected_p = f"data_store/{self.config.tenant}/vectorstore/{primary_year or 'root'}"
                self.logger.error(f"‚ùå FATAL: 0 vector store collections loaded. Please check folder: {expected_p}")
                raise ValueError(f"No vector collections found for '{target_enabler}' in {self.config.tenant}")

        except Exception as e:
            self.logger.error(f"‚ùå FATAL: Could not initialize VectorStoreManager: {str(e)}")
            raise

    def get_cumulative_rules(self, sub_id: str, current_level: int) -> Dict[str, Any]:
        """
        [REVISED CUMULATIVE RULES ENGINE v2026.8 - PRIORITY & SMART ACCUMULATION]
        ----------------------------------------------------------------------------
        - ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏Å‡∏é‡∏™‡∏∞‡∏™‡∏°‡∏à‡∏≤‡∏Å L1 ‚Üí current_level ‡πÇ‡∏î‡∏¢‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Å‡∏±‡∏ö level ‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤
        - ‡πÉ‡∏ä‡πâ OrderedDict ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏•‡∏≥‡∏î‡∏±‡∏ö (specific > default)
        - Required phases ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏à‡∏≤‡∏Å level ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î (maturity-driven)
        - ‡πÅ‡∏¢‡∏Å instructions ‡πÄ‡∏õ‡πá‡∏ô dict {level: rule} + string ‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        - Logging ‡πÅ‡∏¢‡∏Å level: info ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö summary, debug ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö detail
        - Fallback ‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ rules ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö sub_id ‡∏ô‡∏±‡πâ‡∏ô

        Args:
            sub_id (str): ‡∏£‡∏´‡∏±‡∏™ sub-criteria ‡πÄ‡∏ä‡πà‡∏ô "1.2"
            current_level (int): ‡∏£‡∏∞‡∏î‡∏±‡∏ö maturity ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô (1-5)

        Returns:
            Dict[str, Any]: {
                "plan_keywords": List[str],
                "do_keywords": List[str],
                "check_keywords": List[str],
                "act_keywords": List[str],
                "required_phases": List[str],          # sorted, unique
                "level_specific_instructions": Dict[int, str],
                "all_instructions": str,               # ‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö prompt
                "source_summary": str                  # ‡∏™‡∏£‡∏∏‡∏õ‡∏ß‡πà‡∏≤‡∏°‡∏≤‡∏à‡∏≤‡∏Å level ‡πÑ‡∏´‡∏ô‡∏ö‡πâ‡∏≤‡∏á
            }
        """
        # 1. ‡∏î‡∏∂‡∏á defaults ‡πÄ‡∏õ‡πá‡∏ô‡∏ê‡∏≤‡∏ô (fallback ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ rules ‡πÄ‡∏â‡∏û‡∏≤‡∏∞)
        defaults = self.contextual_rules_map.get('_enabler_defaults', {})
        
        # ‡πÉ‡∏ä‡πâ OrderedDict ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏•‡∏≥‡∏î‡∏±‡∏ö: default ‚Üí L1 ‚Üí L2 ‚Üí ... ‚Üí current_level
        cum_keywords = {
            "plan": OrderedDict((k, None) for k in defaults.get('plan_keywords', [])),
            "do":   OrderedDict((k, None) for k in defaults.get('do_keywords', [])),
            "check": OrderedDict((k, None) for k in defaults.get('check_keywords', [])),
            "act":  OrderedDict((k, None) for k in defaults.get('act_keywords', []))
        }

        required_phases: Set[str] = set()
        level_specific_instructions: Dict[int, str] = {}
        source_levels: List[int] = []

        # 2. ‡∏î‡∏∂‡∏á rules ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡∏≠‡∏á sub_id
        sub_rules = self.contextual_rules_map.get(sub_id, {})
        if not sub_rules:
            logger.warning(f"[RULE_CUMULATIVE] No specific rules for {sub_id} ‚Üí using defaults only")
        
        # 3. ‡∏™‡∏∞‡∏™‡∏°‡∏à‡∏≤‡∏Å L1 ‡∏ñ‡∏∂‡∏á current_level (‡πÉ‡∏´‡πâ level ‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤‡∏ó‡∏±‡∏ö‡∏•‡∏≥‡∏î‡∏±‡∏ö)
        for lv in range(1, current_level + 1):
            lv_key = f"L{lv}"
            level_rule = sub_rules.get(lv_key, {})
            
            if not level_rule:
                continue  # ‡∏Ç‡πâ‡∏≤‡∏° level ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ rules
            
            source_levels.append(lv)
            
            # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï keywords (level ‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤‡∏ó‡∏±‡∏ö default ‡πÅ‡∏•‡∏∞ level ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤)
            for phase, key_name in [("plan", "plan_keywords"), ("do", "do_keywords"),
                                   ("check", "check_keywords"), ("act", "act_keywords")]:
                new_kws = level_rule.get(key_name, [])
                for kw in new_kws:
                    cum_keywords[phase][kw] = None  # OrderedDict ‡∏à‡∏∞‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏•‡∏≥‡∏î‡∏±‡∏ö
            
            # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï required phases (union ‡∏ó‡∏∏‡∏Å level)
            if 'require_phase' in level_rule:
                required_phases.update(level_rule['require_phase'])
            
            # ‡πÄ‡∏Å‡πá‡∏ö specific rule ‡∏ï‡∏≤‡∏° level
            specific = level_rule.get('specific_contextual_rule')
            if specific:
                level_specific_instructions[lv] = specific.strip()

        # 4. ‡πÅ‡∏õ‡∏•‡∏á OrderedDict ‚Üí list (‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏•‡∏≥‡∏î‡∏±‡∏ö)
        result_keywords = {phase: list(cum_keywords[phase].keys()) for phase in cum_keywords}

        # 5. Required phases: ‡πÉ‡∏ä‡πâ‡∏à‡∏≤‡∏Å level ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å (‡∏ñ‡πâ‡∏≤ L5 ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ P,D,C,A ‚Üí ‡πÉ‡∏ä‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)
        # ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡∏Ñ‡∏á union ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
        final_required = sorted(list(required_phases)) if required_phases else []

        # 6. ‡∏£‡∏ß‡∏° instructions ‡πÄ‡∏õ‡πá‡∏ô string (‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ LLM ‡πÇ‡∏ü‡∏Å‡∏±‡∏™‡∏ñ‡∏π‡∏Å‡∏à‡∏∏‡∏î)
        instructions_lines = ["‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏£‡∏∞‡∏î‡∏±‡∏ö Maturity:"]
        for lv in sorted(level_specific_instructions.keys()):
            prefix = "üéØ " if lv == current_level else "‚úÖ " # ‡πÄ‡∏ô‡πâ‡∏ô‡πÄ‡∏•‡πÄ‡∏ß‡∏•‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
            instructions_lines.append(f"{prefix}‡∏£‡∏∞‡∏î‡∏±‡∏ö L{lv}: {level_specific_instructions[lv]}")
        all_instructions = "\n".join(instructions_lines)

        # 7. ‡∏™‡∏£‡∏∏‡∏õ source ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö debug
        source_str = f"from levels {source_levels}" if source_levels else "defaults only"

        # 8. Logging (info ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö production monitoring, debug ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö trace)
        logger.info(
            f"[RULE_CUMULATIVE] {sub_id} L{current_level} | "
            f"Keywords (dedup): P={len(result_keywords['plan'])} | "
            f"D={len(result_keywords['do'])} | C={len(result_keywords['check'])} | "
            f"A={len(result_keywords['act'])} | Required={final_required} | "
            f"Instructions={len(level_specific_instructions)} | Source={source_str}"
        )

        if logger.isEnabledFor(logging.DEBUG):
            logger.debug(
                f"[RULE_DETAIL] {sub_id} L{current_level} | "
                f"Plan keywords sample: {result_keywords['plan'][:5]}... | "
                f"Required phases accumulated from levels: {sorted(source_levels)}"
            )

        # 9. Return structure ‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢
        return {
            "plan_keywords": result_keywords["plan"],
            "do_keywords": result_keywords["do"],
            "check_keywords": result_keywords["check"],
            "act_keywords": result_keywords["act"],
            "required_phases": final_required,
            "level_specific_instructions": level_specific_instructions,  # Dict[int, str]
            "all_instructions": all_instructions,                       # string ‡∏£‡∏ß‡∏°
            "source_summary": source_str,
            "accumulated_levels": sorted(source_levels)
        }


    def validate_accumulative_pass(self, llm_result: Dict[str, Any], target_phases: List[str]) -> Tuple[bool, str]:
        """
        ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Phase ‡∏ó‡∏µ‡πà AI ‡∏ï‡∏£‡∏ß‡∏à‡πÄ‡∏à‡∏≠ ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà Level ‡∏ô‡∏±‡πâ‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏∞‡∏™‡∏°‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        """
        # ‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Phase ‡∏ó‡∏µ‡πà AI ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô > 0 (‡∏Ñ‡∏∑‡∏≠‡πÄ‡∏à‡∏≠‡∏à‡∏£‡∏¥‡∏á)
        pdca_breakdown = llm_result.get('pdca_breakdown', {})
        found_phases = {phase for phase, score in pdca_breakdown.items() if score > 0}
        required_set = set(target_phases)

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô Subset (Required ‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô Found)
        is_subset = required_set.issubset(found_phases)
        
        if not is_subset:
            missing = required_set - found_phases
            error_msg = f"‡∏Ç‡∏≤‡∏î‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏ô‡πÄ‡∏ü‡∏™‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: {', '.join(missing)} (‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏™‡∏∞‡∏™‡∏° Maturity)"
            return False, error_msg
        
        return True, ""
    

    def _check_maturity_consistency(
        self, 
        sub_id: str, 
        current_level: int, 
        top_evidences: List[Dict[str, Any]],
        llm_pdca_breakdown: Dict[str, float]
    ) -> Dict[str, Any]:
        """
        ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡πÄ‡∏ä‡∏¥‡∏á Maturity:
        1. Phase ‡∏ï‡πâ‡∏≠‡∏á‡∏Ñ‡∏£‡∏ö‡∏ï‡∏≤‡∏° JSON require_phase (‡∏™‡∏∞‡∏™‡∏°)
        2. ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô (Rerank) ‡∏Ç‡∏≠‡∏á Phase ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏ï‡πâ‡∏≠‡∏á‡∏ñ‡∏∂‡∏á‡πÄ‡∏Å‡∏ì‡∏ë‡πå
        """
        # ‡∏î‡∏∂‡∏á‡∏Å‡∏é‡∏™‡∏∞‡∏™‡∏°‡∏à‡∏≤‡∏Å JSON
        cum_rules = self.get_cumulative_rules(sub_id, current_level)
        required_phases = set(cum_rules['phases'])
        
        # ‡∏î‡∏∂‡∏á Phase ‡∏ó‡∏µ‡πà AI ‡∏ï‡∏£‡∏ß‡∏à‡πÄ‡∏à‡∏≠‡∏à‡∏£‡∏¥‡∏á (score > 0)
        found_phases = {p for p, score in llm_pdca_breakdown.items() if score > 0}
        
        # 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Phase Gap
        missing_phases = required_phases - found_phases
        
        # 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏£‡∏≤‡∏¢ Phase (Critical Evidence Check)
        # ‡πÄ‡∏ä‡πà‡∏ô ‡∏ñ‡πâ‡∏≤ L3 require 'D' ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô D ‡∏ó‡∏µ‡πà Rerank Score ‡∏™‡∏π‡∏á‡∏û‡∏≠
        low_quality_phases = []
        for phase in required_phases:
            max_score = max([
                doc.get('rerank_score', 0.0) 
                for doc in top_evidences 
                if doc.get('pdca_tag') == phase
            ], default=0.0)
            
            if max_score < 0.4: # Threshold ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà "‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠‡πÑ‡∏î‡πâ"
                low_quality_phases.append(phase)

        return {
            "is_consistent": len(missing_phases) == 0 and len(low_quality_phases) == 0,
            "missing_phases": list(missing_phases),
            "low_quality_phases": low_quality_phases,
            "required_phases": list(required_phases)
        }

    def post_process_llm_result(
        self,
        llm_output: Any,
        level: int,
        sub_id: str = None
    ) -> Dict[str, Any]:
        """
        [POST-PROCESS v2026.Expert - FULL INTEGRATION]
        - FIXED: ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô PDCA ‡πÄ‡∏õ‡πá‡∏ô 0 (‡πÄ‡∏û‡∏¥‡πà‡∏° Force Mapping ‡∏à‡∏≤‡∏Å Extraction)
        - FIXED: Floor Rescue Mapping (‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÉ‡∏´‡πâ‡∏ñ‡∏∂‡∏á‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏ñ‡πâ‡∏≤‡πÇ‡∏î‡∏ô‡∏™‡∏±‡πà‡∏á‡∏ú‡πà‡∏≤‡∏ô)
        - FEATURE: Maturity-based Threshold Validation
        """
        log_prefix = f"{sub_id or 'Unknown'} L{level}"
        
        # 1. üõ†Ô∏è JSON Repair & Unpacking
        # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á String JSON ‡πÅ‡∏•‡∏∞ Dict (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Tuple Error ‡∏à‡∏≤‡∏Å‡∏ï‡πâ‡∏ô‡∏ó‡∏≤‡∏á)
        if isinstance(llm_output, tuple):
            llm_output = llm_output[0] if len(llm_output) > 0 else {}

        if isinstance(llm_output, str):
            try:
                # Clean up problematic symbols common in LLM outputs
                cleaned_str = re.sub(r'(\d+\.?\d*)\s*[\+\-]\s*(\d+\.?\d*)\s*=\s*(\d+\.?\d*)', r'\3', llm_output)
                cleaned_str = cleaned_str.strip().replace(",\n}", "\n}").replace(",}", "}")
                llm_output = json.loads(cleaned_str)
            except Exception as e:
                self.logger.error(f"‚ùå JSON Repair failed for {log_prefix}: {str(e)}")
                return {"is_passed": False, "score": 0.0, "reason": "Hard JSON Parsing Error"}

        if not isinstance(llm_output, dict):
            return {"is_passed": False, "score": 0.0, "reason": "Invalid Output Format"}

        # 2. üõ°Ô∏è Floor Rescue Awareness
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Engine ‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏ô‡πâ‡∏≤ (Single Assessment) ‡∏™‡∏±‡πà‡∏á Override ‡πÉ‡∏´‡πâ‡∏ú‡πà‡∏≤‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        is_overridden = llm_output.get('is_passed', False)

        # 3. üìä Score & PDCA Extraction (Heuristic Recovery)
        # ‡πÅ‡∏°‡∏õ‡∏ä‡∏∑‡πà‡∏≠ Key ‡∏ó‡∏µ‡πà AI ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏ï‡∏≠‡∏ö‡∏°‡∏≤‡∏ú‡∏¥‡∏î‡πÄ‡∏û‡∏µ‡πâ‡∏¢‡∏ô‡πÉ‡∏´‡πâ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô P-D-C-A
        extraction_map = {
            "P": ["P_Plan_Score", "score_p", "Plan_Score"],
            "D": ["D_Do_Score", "score_d", "Do_Score"],
            "C": ["C_Check_Score", "score_c", "Check_Score"],
            "A": ["A_Act_Score", "score_a", "Act_Score"]
        }

        pdca_results = {"P": 0.0, "D": 0.0, "C": 0.0, "A": 0.0}

        for phase, possible_keys in extraction_map.items():
            # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏î‡∏∂‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡πÜ Key ‡∏ó‡∏µ‡πà AI ‡∏°‡∏±‡∏Å‡∏à‡∏∞‡∏ä‡∏≠‡∏ö‡πÉ‡∏ä‡πâ
            score = 0.0
            for key in possible_keys:
                val = llm_output.get(key)
                if val is not None:
                    try:
                        score = float(val)
                        break
                    except: continue
            
            # üõ°Ô∏è Protection: ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏õ‡πá‡∏ô 0 ‡πÅ‡∏ï‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° Extraction ‡∏¢‡∏≤‡∏ß‡πÜ (‡πÅ‡∏õ‡∏•‡∏ß‡πà‡∏≤‡πÄ‡∏à‡∏≠‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÅ‡∏ï‡πà AI ‡∏•‡∏∑‡∏°‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô)
            # ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏Å‡∏π‡πâ‡∏Ñ‡∏∑‡∏ô‡πÉ‡∏´‡πâ 0.5 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô (‡πÄ‡∏â‡∏û‡∏≤‡∏∞ L1-L2 ‡∏´‡∏£‡∏∑‡∏≠ Overridden)
            ext_text = str(llm_output.get(f"Extraction_{phase}", "")).strip()
            if score == 0.0 and len(ext_text) > 15 and "‡πÑ‡∏°‡πà‡∏û‡∏ö" not in ext_text:
                if is_overridden or level <= 2:
                    self.logger.info(f"üõ°Ô∏è [RECOVERY] Found evidence text for {phase} in {log_prefix}. Assigning 0.5")
                    score = 0.5
            
            pdca_results[phase] = score

        # 4. ‚öñÔ∏è Maturity Threshold Calculation
        p, d, c, a = pdca_results["P"], pdca_results["D"], pdca_results["C"], pdca_results["A"]
        pdca_sum = round(p + d + c + a, 2)
        
        # ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ï‡∏≤‡∏°‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏Å (Threshold)
        threshold_map = {1: 1.0, 2: 2.0, 3: 4.0, 4: 6.0, 5: 8.0}
        threshold = threshold_map.get(level, 2.0)

        # 5. üèÅ Final Decision Logic
        is_passed = pdca_sum >= threshold
        fail_reason = llm_output.get('reason') or llm_output.get('fail_reason') or ""

        # ‡∏Å‡∏é‡∏ú‡πà‡∏≠‡∏ô‡∏õ‡∏£‡∏ô L1-L2 ‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏£‡∏ì‡∏µ Floor Rescue
        if is_overridden or (level <= 2 and p > 0 and d > 0):
            is_passed = True
            # ‡∏ñ‡πâ‡∏≤‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ "‡∏ú‡πà‡∏≤‡∏ô" ‡πÅ‡∏ï‡πà‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°‡πÑ‡∏°‡πà‡∏ñ‡∏∂‡∏á‡πÄ‡∏Å‡∏ì‡∏ë‡πå ‡πÉ‡∏´‡πâ‡∏õ‡∏±‡∏î‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡πÉ‡∏´‡πâ‡πÄ‡∏ó‡πà‡∏≤‡πÄ‡∏Å‡∏ì‡∏ë‡πå (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ UI ‡πÇ‡∏ä‡∏ß‡πå‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß)
            if pdca_sum < threshold:
                pdca_sum = threshold

        # ‡πÄ‡∏Ç‡πâ‡∏°‡∏á‡∏ß‡∏î L3+ (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ Check/Act)
        if not is_overridden:
            if level >= 3 and c <= 0:
                is_passed = False
                fail_reason = f"Level 3+ requires Check (C). Current C: {c}"
            if level >= 4 and a <= 0:
                is_passed = False
                fail_reason = f"Level 4+ requires Act (A). Current A: {a}"

        # 6. üì¶ Sync & Return Object
        # ‡∏£‡∏ß‡∏°‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡πâ‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏µ‡πà Router/UI ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
        llm_output.update({
            "score": round(pdca_sum, 2),
            "is_passed": is_passed,
            "reason": fail_reason,
            "pdca_breakdown": pdca_results, # ‚ú® ‡∏ï‡∏±‡∏ß‡∏ô‡∏µ‡πâ‡πÅ‡∏´‡∏•‡∏∞‡∏ó‡∏µ‡πà UI ‡∏à‡∏∞‡πÄ‡∏≠‡∏≤‡πÑ‡∏õ‡∏ß‡∏≤‡∏î‡∏Å‡∏£‡∏≤‡∏ü
            "status": "PASSED" if is_passed else "FAILED"
        })

        self.logger.info(
            f"üéØ [POST-PROCESS] {log_prefix} | Final: {llm_output['score']} | "
            f"P:{p} D:{d} C:{c} A:{a} | Passed: {is_passed} | Overridden: {is_overridden}"
        )

        return llm_output

    def _check_contextual_rule_condition(
        self, 
        condition: Dict[str, Any], 
        sub_id: str, 
        level: int, 
        top_evidences: List[Dict[str, Any]]
    ) -> bool:
        """
        [ADAPTIVE GATE v2026] 
        - ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å '‡∏™‡∏±‡πà‡∏á‡∏ï‡∏Å' ‡πÄ‡∏õ‡πá‡∏ô '‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ñ‡∏≥‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô'
        - ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏´‡πá‡∏ô‡∏ú‡∏• Gap Analysis ‡∏Ñ‡∏£‡∏ö‡∏ó‡∏∏‡∏Å‡πÄ‡∏•‡πÄ‡∏ß‡∏•
        """
        self.logger.info(f"üîç [VALIDATION GATE] Analyzing L{level} for {sub_id}")
        
        # 1. ‡πÄ‡∏ä‡πá‡∏Ñ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á (Maturity Check)
        if level > 1:
            prev_level = level - 1
            is_prev_passed = False
            
            # ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å Memory ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ß‡πâ‡πÉ‡∏ô _run_single_assessment
            if hasattr(self, 'level_details_map') and str(prev_level) in self.level_details_map:
                is_prev_passed = self.level_details_map[str(prev_level)].get('is_passed', False)
            
            if not is_prev_passed:
                # üí° ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å return False ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏â‡∏µ‡∏î Warning ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô Context ‡πÅ‡∏ó‡∏ô
                self.logger.warning(f"‚ö†Ô∏è [GAP DETECTED] L{prev_level} is not passed. L{level} might be considered invalid by auditor.")
                # ‡πÄ‡∏£‡∏≤‡πÉ‡∏´‡πâ True ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ LLM ‡πÑ‡∏î‡πâ‡∏≠‡πà‡∏≤‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô L2 ‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏Å‡πà‡∏≠‡∏ô
        
        # 2. ‡πÄ‡∏ä‡πá‡∏Ñ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥
        min_docs = condition.get('min_evidences', 1)
        if len(top_evidences) < min_docs:
            self.logger.warning(f"‚ö†Ô∏è [LOW EVIDENCE] Found only {len(top_evidences)} docs. Required: {min_docs}")

        return True # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏£‡∏±‡∏ô‡πÑ‡∏õ‡∏à‡∏ô‡∏ñ‡∏∂‡∏á L5    

    def _is_previous_level_passed(self, sub_id: str, level: int) -> bool:
        """
        Helper: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡∏ú‡πà‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≤‡∏Å self.assessment_results_map
        (‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö check_passed_levels ‡πÉ‡∏ô Contextual Rule)
        """
        # Key ‡πÉ‡∏ô assessment_results_map ‡∏°‡∏±‡∏Å‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô '3.1.L1', '3.1.L2'
        key = f"{sub_id}.L{level}"
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏≠‡∏á Level ‡∏ô‡∏±‡πâ‡∏ô‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà ‡πÅ‡∏•‡∏∞‡∏°‡∏µ is_passed ‡πÄ‡∏õ‡πá‡∏ô True
        result = self.assessment_results_map.get(key)
        
        return result is not None and result.get('is_passed', False)

    def _expand_context_with_neighbor_pages(self, top_evidences: List[Any], collection_name: str) -> List[Any]:
        """
        [SMART EXPAND v3 - Low Log] ‡∏î‡∏∂‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏ñ‡∏±‡∏î‡πÑ‡∏õ (‡πÅ‡∏•‡∏∞‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á 1 ‡∏´‡∏ô‡πâ‡∏≤) ‡∏´‡∏≤‡∏Å‡∏û‡∏ö Check
        - ‡∏•‡∏î log ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏ô‡πâ‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á debug ‡πÑ‡∏î‡πâ
        - ‡∏£‡∏ß‡∏° summary log ‡∏ï‡∏≠‡∏ô‡∏à‡∏ö
        """
        if not self.vectorstore_manager or not top_evidences:
            return top_evidences

        expanded_evidences = list(top_evidences)
        seen_keys = set()
        added_pages = 0
        added_chunks = 0
        failed_pages = set()  # ‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ü‡∏•‡πå+‡∏´‡∏ô‡πâ‡∏≤ ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà log ‡∏ã‡πâ‡∏≥

        check_triggers = [
            "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏∂‡∏á‡∏û‡∏≠‡πÉ‡∏à", "‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô", "‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•", "‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô", "score", "kpi", "3.41",
            "‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•", "‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏•", "‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î", "‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå", "‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô", "‡∏ú‡∏•‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô"
        ]

        for doc in top_evidences:
            text = (doc.get('text') or doc.get('page_content') or "").lower()
            meta = doc.metadata if hasattr(doc, 'metadata') else doc.get('metadata', {})
            page_label = meta.get("page_label")
            doc_uuid = meta.get("stable_doc_uuid")

            try:
                current_page = int("".join(filter(str.isdigit, str(page_label))))
            except (ValueError, TypeError):
                continue

            if not any(k in text for k in check_triggers):
                continue

            # ‡∏î‡∏∂‡∏á ¬±1 ‡πÅ‡∏•‡∏∞ +2 ‡∏´‡∏ô‡πâ‡∏≤ (‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á + ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ)
            for offset in [-1, 1, 2]:
                target_page = current_page + offset
                if target_page < 1:
                    continue  # ‡∏Ç‡πâ‡∏≤‡∏°‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡∏¥‡∏î‡∏•‡∏ö
                cache_key = f"{doc_uuid}_{target_page}"

                if cache_key in seen_keys:
                    continue

                neighbor_chunks = self.vectorstore_manager.get_chunks_by_page(
                    collection_name=collection_name,
                    stable_doc_uuid=doc_uuid,
                    page_label=str(target_page)
                )

                if neighbor_chunks:
                    for nc in neighbor_chunks:
                        new_doc = {
                            "text": f"[Act Context - Page {target_page} (‡∏à‡∏≤‡∏Å Check ‡∏ó‡∏µ‡πà‡∏´‡∏ô‡πâ‡∏≤ {current_page})]:\n{nc.page_content}",
                            "page_content": nc.page_content,
                            "metadata": nc.metadata,
                            "pdca_tag": "Act",
                            "is_supplemental": True,
                            "rerank_score": doc.get('rerank_score', 0.0)
                        }
                        expanded_evidences.append(new_doc)
                    seen_keys.add(cache_key)
                    added_pages += 1
                    added_chunks += len(neighbor_chunks)
                else:
                    fail_key = f"{doc_uuid}_{target_page}"
                    if fail_key not in failed_pages:
                        self.logger.debug(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏ô‡πâ‡∏≤ {target_page} ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå {doc_uuid}")
                        failed_pages.add(fail_key)

        # Summary log ‡πÅ‡∏Ñ‡πà‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
        if added_pages > 0:
            self.logger.info(f"Act-Hook completed: Added {added_pages} pages ({added_chunks} chunks) from Check triggers")
        else:
            self.logger.debug("Act-Hook: No additional neighbor pages found")

        return expanded_evidences
        
    def _resolve_evidence_filenames(self, evidence_entries: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        [ULTIMATE FIX] ‡∏ú‡∏™‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡∏Ç‡∏≠‡∏á‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏î‡∏¥‡∏° + ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏â‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Trace ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà
        """
        resolved_entries = []
        for entry in evidence_entries:
            resolved_entry = deepcopy(entry)
            doc_id = resolved_entry.get("doc_id", "")
            content_raw = resolved_entry.get('content')
            level_origin = resolved_entry.get('level', 'N/A')
            
            # ‡∏î‡∏∂‡∏á‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏° (Page Label ‡∏à‡∏≤‡∏Å Ingest ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤‡∏î‡∏¥‡∏ö)
            page_label = resolved_entry.get("page_label") or resolved_entry.get("page") or "N/A"

            # 1. AI Generated Reference (‡∏°‡πÇ‡∏ô‡∏°‡∏≤)
            if str(doc_id).startswith("UNKNOWN-"):
                resolved_entry["filename"] = "AI-GENERATED-REF"
                resolved_entry["page"] = "N/A"
                resolved_entries.append(resolved_entry)
                continue

            # 2. ‡πÄ‡∏Ñ‡∏™‡∏õ‡∏Å‡∏ï‡∏¥: ‡∏°‡∏µ doc_id ‡πÅ‡∏•‡∏∞‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô Map
            if doc_id and doc_id in self.doc_id_to_filename_map:
                mapped_name = self.doc_id_to_filename_map[doc_id]
                resolved_entry["filename"] = mapped_name
                resolved_entry["display_source"] = f"{mapped_name} (‡∏´‡∏ô‡πâ‡∏≤ {page_label})"
            
            # 3. ‡πÄ‡∏Ñ‡∏™‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡πÉ‡∏ô Map (Fallback)
            elif doc_id:
                resolved_entry["filename"] = f"DOC-{str(doc_id)[:8]}"
                resolved_entry["display_source"] = f"‡∏£‡∏´‡∏±‡∏™ {str(doc_id)[:8]} (‡∏´‡∏ô‡πâ‡∏≤ {page_label})"

            # 4. ‡πÄ‡∏Ñ‡∏™‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏≤‡∏¢ (The "Skipping" Case)
            else:
                if not content_raw:
                    # üü¢ ‡∏£‡∏∞‡∏ö‡∏∏ Level ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡∏Ñ‡∏ß‡∏£‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡πÅ‡∏Å‡πâ PDF ‡∏ó‡∏µ‡πà‡∏Ç‡πâ‡∏≠‡πÑ‡∏´‡∏ô
                    self.logger.warning(f"‚ö†Ô∏è [Data Gap] Level {level_origin}: Entry ‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤ (Skipped)")
                    continue 
                
                resolved_entry["filename"] = "UNMAPPED-DOCUMENT"
                preview = str(content_raw)[:30].replace('\n', ' ')
                self.logger.debug(f"üîç Unmapped Content Preview: {preview}...")

            resolved_entries.append(resolved_entry)
        return resolved_entries
        
    
    # -------------------- Contextual Rules Handlers (FIXED) --------------------
    def _load_contextual_rules_map(self) -> Dict[str, Any]:
        """
        [FINAL REVISED v2026.5] ‡πÇ‡∏´‡∏•‡∏î Contextual Rules ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
        - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö multi-tenant ‡πÅ‡∏•‡∏∞ multi-enabler ‡πÄ‡∏ï‡πá‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö
        - ‡∏°‡∏µ fallback ‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
        - ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Maturity (L1-L5) ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏Ç‡πâ‡∏°‡∏á‡∏ß‡∏î
        - Logging ‡∏ó‡∏µ‡πà‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏ä‡πà‡∏ß‡∏¢ debug
        - ‡πÑ‡∏°‡πà raise exception (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ engine ‡∏•‡πâ‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏£‡∏∞‡∏ö‡∏ö)
        """
        # 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á path ‡∏ï‡∏≤‡∏° tenant + enabler
        try:
            filepath = get_contextual_rules_file_path(
                tenant=self.config.tenant,
                enabler=self.enabler_id
            )
            self.logger.debug(f"üîç Attempting to load contextual rules from: {filepath}")
        except Exception as e:
            self.logger.error(f"‚ùå FATAL: Failed to generate rules file path: {e}")
            return {"_enabler_defaults": {}}

        # 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if not os.path.exists(filepath):
            self.logger.warning(
                f"‚ö†Ô∏è Contextual Rules file not found: {filepath}\n"
                f"   ‚Üí Using only global defaults (if available from fallback)."
            )
            return {"_enabler_defaults": {}}

        # 3. ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞ parse JSON
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
        except json.JSONDecodeError as e:
            self.logger.error(f"‚ùå JSON Decode Error in {filepath}: {e} (line {e.lineno}, col {e.colno})")
            return {"_enabler_defaults": {}}
        except Exception as e:
            self.logger.error(f"‚ùå Unexpected error reading {filepath}: {e}")
            return {"_enabler_defaults": {}}

        # 4. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        if not isinstance(data, dict):
            self.logger.error(f"‚ùå Invalid rules format in {filepath}: Expected dict, got {type(data)}")
            return {"_enabler_defaults": {}}

        # 5. ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô criteria ‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Maturity Structure
        sub_criteria_keys = [k for k in data.keys() if not k.startswith("_")]
        num_criteria = len(sub_criteria_keys)

        if num_criteria == 0:
            self.logger.warning(f"‚ö†Ô∏è No sub-criteria found in {filepath} (only defaults).")
        else:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ Maturity Levels (L1, L2, ...) ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            sample_sub = sub_criteria_keys[0]
            sub_data = data[sample_sub]
            level_keys = [k for k in sub_data.keys() if k.startswith("L") and len(k) >= 2]
            
            if level_keys:
                detected_levels = sorted(level_keys)
                self.logger.info(
                    f"‚úÖ Maturity-Based Rules loaded successfully!\n"
                    f"   File: {filepath}\n"
                    f"   Criteria: {num_criteria} (e.g., {sample_sub})\n"
                    f"   Levels detected: {', '.join(detected_levels)}"
                )
            else:
                self.logger.warning(
                    f"‚ö†Ô∏è Rules for '{sample_sub}' in {filepath} do not contain Maturity Levels (L1-L5).\n"
                    f"   Falling back to flat structure or defaults."
                )

        # 6. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö _enabler_defaults
        if "_enabler_defaults" in data:
            defaults = data["_enabler_defaults"]
            if isinstance(defaults, dict) and any(k.endswith("_keywords") for k in defaults.keys()):
                self.logger.info("‚úÖ Global PDCA Keywords (_enabler_defaults) loaded.")
            else:
                self.logger.warning("‚ö†Ô∏è _enabler_defaults exists but has invalid structure.")
        else:
            self.logger.info("‚ÑπÔ∏è No _enabler_defaults section found. Using empty defaults.")

        # 7. ‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô data ‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
        self.logger.info(f"‚úÖ Contextual Rules fully loaded from {filepath} ({num_criteria} criteria).")
        return data

    # ----------------------------------------------------------------------
    # üéØ FINAL FIX 2.3: Manual Map Reload Function (inside SEAMPDCAEngine)
    # ----------------------------------------------------------------------
    def _collect_previous_level_evidences(self, sub_id: str, current_level: int) -> Dict[str, List[Dict]]:
        """
        ‡∏î‡∏∂‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏à‡∏≤‡∏Å Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ (L1 ‚Üí L2, L2 ‚Üí L3 ‡∏Ø‡∏•‡∏Ø) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô Baseline Context
        
        [FINAL REVISED 2026] - Robust Hydration & UUID Normalization
        - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö UUID ‡∏ó‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö‡∏°‡∏µ‡∏Ç‡∏µ‡∏î (-) ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡∏µ‡∏î
        - ‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏¢‡∏∞ (fallback_doc_id, N/A) ‡∏≠‡∏≠‡∏Å‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£
        - ‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á Metadata (PDCA Tag, Source, Page)
        """
        
        # 1. ‡∏Ç‡πâ‡∏≤‡∏° Hydration ‡πÉ‡∏ô Full Parallel Mode ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß
        if getattr(self, 'is_parallel_all_mode', False):
            self.logger.info("FULL PARALLEL MODE: Skipping hydration")
            return {}

        # 2. ‡∏Ñ‡∏±‡∏î‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á Level ‡∏ó‡∏µ‡πà‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡πÉ‡∏ô Sub-Criteria ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
        collected = {}
        for key, ev_list in self.evidence_map.items():
            if key.startswith(f"{sub_id}.L") and isinstance(ev_list, list) and ev_list:
                try:
                    level_num = int(key.split(".L")[-1])
                    if level_num < current_level:
                        collected[key] = ev_list
                except (ValueError, IndexError):
                    continue

        if not collected:
            self.logger.info(f"No previous level evidences found for {sub_id} L{current_level}")
            return {}

        # 3. ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏° IDs ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á (Valid IDs only)
        stable_ids = set()
        for ev_list in collected.values():
            for ev in ev_list:
                sid = ev.get("stable_doc_uuid") or ev.get("doc_id")
                # ‡∏Å‡∏£‡∏≠‡∏á‡∏Ñ‡πà‡∏≤‡∏Ç‡∏¢‡∏∞‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏´‡∏•‡∏∏‡∏î‡∏°‡∏≤‡∏à‡∏≤‡∏Å LLM Response ‡πÉ‡∏ô‡∏£‡∏≠‡∏ö‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤
                if sid and isinstance(sid, str) and sid not in ["N/A", "fallback_doc_id", "None", ""]:
                    stable_ids.add(sid)

        if not stable_ids:
            self.logger.warning(f"‚ö†Ô∏è No valid IDs to hydrate for {sub_id} L{current_level}")
            return collected

        # 4. ‡∏î‡∏∂‡∏á Full Text Chunks ‡∏à‡∏≤‡∏Å Vector Store (Bulk Hydration)
        vsm = self.vectorstore_manager
        try:
            # ‡∏î‡∏∂‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏°‡∏≤‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥ Mapping ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
            full_chunks = vsm.get_documents_by_id(list(stable_ids), self.doc_type, self.enabler_id) 
            self.logger.info(f"HYDRATION: Retrieved {len(full_chunks)} chunks from VSM for mapping")
        except Exception as e:
            self.logger.error(f"Hydration failed in VSM call: {e}")
            return collected

        # 5. ‡∏™‡∏£‡πâ‡∏≤‡∏á Chunk Map (Key Optimization: ‡πÄ‡∏Å‡πá‡∏ö‡∏ó‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö‡∏°‡∏µ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡∏µ‡∏î)
        chunk_map = {}
        for chunk in full_chunks:
            meta = getattr(chunk, "metadata", {})
            # ‡πÄ‡∏Å‡πá‡∏ö UUID ‡∏ó‡∏∏‡∏Å‡∏•‡∏π‡∏Å‡πÄ‡∏•‡πà‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Match ‡πÄ‡∏à‡∏≠ 100%
            potential_ids = [
                meta.get("chunk_uuid"),
                meta.get("stable_doc_uuid"),
                meta.get("doc_id")
            ]
            for pid in potential_ids:
                if pid and isinstance(pid, str):
                    chunk_map[pid] = {"text": chunk.page_content, "metadata": meta}
                    chunk_map[pid.replace("-", "")] = {"text": chunk.page_content, "metadata": meta}

        # 6. ‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏ü‡∏∑‡πâ‡∏ô‡∏ü‡∏π‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ (Hydration Loop)
        hydrated = {}
        restored_count = 0
        total_items = sum(len(v) for v in collected.values())

        for key, ev_list in collected.items():
            new_list = []
            for ev in ev_list:
                new_ev = ev.copy()
                data = None
                
                # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° IDs ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô Map
                sid_raw = ev.get("stable_doc_uuid") or ev.get("doc_id") or ""
                sid_clean = sid_raw.replace("-", "")
                cid_raw = ev.get("chunk_uuid") or ""
                cid_clean = cid_raw.replace("-", "")

                # TRY 1: Match ‡∏î‡πâ‡∏ß‡∏¢ Chunk UUID (‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î)
                data = chunk_map.get(cid_raw) or chunk_map.get(cid_clean)

                # TRY 2: Match ‡∏î‡πâ‡∏ß‡∏¢ Stable Doc UUID / Doc ID (Fallback)
                if not data:
                    data = chunk_map.get(sid_raw) or chunk_map.get(sid_clean)

                if data:
                    new_ev["text"] = data["text"]
                    # Merge Metadata ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ
                    merged_meta = data["metadata"].copy()
                    merged_meta.update(new_ev) # ‡πÉ‡∏´‡πâ‡∏Ñ‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏°‡πÉ‡∏ô Evidence ‡πÄ‡∏Å‡πà‡∏á‡∏Å‡∏ß‡πà‡∏≤
                    new_ev = merged_meta
                    
                    new_ev["is_baseline"] = True 
                    restored_count += 1
                else:
                    # ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô Error ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô ID ‡∏à‡∏£‡∏¥‡∏á‡πÅ‡∏ï‡πà‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠
                    if sid_raw not in ["N/A", "fallback_doc_id", ""]:
                        self.logger.error(f"‚ùå HYDRATION FAILURE: {sid_raw[:8]}... (File: {ev.get('source_filename', 'Unknown')})")
                    new_ev["is_baseline"] = False
                    new_ev["page_label"] = ev.get("page_label") or ev.get("page") or "N/A"
                
                new_list.append(new_ev)
            hydrated[key] = new_list
                
        self.logger.info(f"‚úÖ BASELINE HYDRATED: {restored_count}/{total_items} chunks restored successfully")
        return hydrated

    def _get_contextual_rules_prompt(self, sub_id: str, level: int) -> str:
        """
        Retrieves the specific Contextual Rule prompt for a given Sub-Criteria and Level,
        ‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á‡∏Å‡∏≤‡∏£ Inject ‡∏Å‡∏é L5 ‡∏û‡∏¥‡πÄ‡∏®‡∏© ‡∏´‡∏≤‡∏Å Level == 5
        """
        sub_id_rules = self.contextual_rules_map.get(sub_id)
        rule_text = ""
        
        # 1. ‡∏î‡∏∂‡∏á‡∏Å‡∏é‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏¢‡πà‡∏≠‡∏¢ (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
        if sub_id_rules:
            level_key = f"L{level}"
            specific_rule = sub_id_rules.get(level_key)
            if specific_rule:
                rule_text += f"\n--- ‡∏Å‡∏é‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏¢‡πà‡∏≠‡∏¢ ({sub_id} L{level}) ---\n‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Ñ‡∏ß‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ: {specific_rule}\n"
        
        # 2. **INJECT L5 SPECIAL RULE (Safe Injection)**
        # ‡πÉ‡∏™‡πà‡∏Å‡∏é Bonus 2.0 ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô L5 ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏ö‡∏Å‡∏ß‡∏ô L3/L4
        if level == 5:
            l5_bonus_rule = """
            \n--- L5 SPECIAL RULE (Innovation & Sustainability) ---
            * **L5 PASS Condition (‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö):** ‡∏´‡∏≤‡∏Å Level ‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠ **L5** ‡∏ó‡πà‡∏≤‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏° PDCA (P+D+C+A) ‡∏ï‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô L3/L4 ‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏ô‡∏û‡∏ö‡∏Å‡πà‡∏≠‡∏ô (‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î 8.0)
            * **‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç Bonus 2.0:** ‡∏´‡∏≤‡∏Å‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏° PDCA ‡πÑ‡∏î‡πâ **‚â• 7.0** **‡πÅ‡∏•‡∏∞** ‡∏ó‡πà‡∏≤‡∏ô‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠ **‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 1 ‡∏ä‡∏¥‡πâ‡∏ô** ‡πÉ‡∏ô Context ‡∏ó‡∏µ‡πà‡πÅ‡∏™‡∏î‡∏á‡∏ñ‡∏∂‡∏á:
                * (a) ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏• KM / ‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏° (Innovation Award)
                * (b) ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡∏ó‡∏µ‡πà‡∏ß‡∏±‡∏î‡πÑ‡∏î‡πâ (ROI, Productivity, Cost Saving)
                * (c) ‡∏Å‡∏≤‡∏£‡πÄ‡∏ú‡∏¢‡πÅ‡∏û‡∏£‡πà/‡∏Å‡∏≤‡∏£‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å (External Recognition/Publication)
            * **‡πÉ‡∏´‡πâ‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡πÉ‡∏´‡πâ Bonus Score 2.0 ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ **Score ‚â• 9.0** ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡πâ‡∏á **is_passed=true** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏∞‡∏ó‡πâ‡∏≠‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏•‡∏¥‡∏® (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£ Reset ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô)
            """
            rule_text += l5_bonus_rule
            
        return rule_text

    def _load_rubric(self) -> Dict[str, Any]:
        """ Loads the SEAM rubric JSON file using path_utils. """
        
        # üéØ FIX: ‡πÉ‡∏ä‡πâ get_rubric_file_path ‡∏à‡∏≤‡∏Å path_utils ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Path ‡πÄ‡∏≠‡∏á
        filepath = None # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô UnboundLocalError
        
        try:
            # 1. ‡∏£‡∏±‡∏ö Path ‡∏à‡∏≤‡∏Å path_utils ‡∏ã‡∏∂‡πà‡∏á‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏ä‡∏µ‡πâ‡πÑ‡∏õ‡∏ó‡∏µ‡πà 'config/' ‡πÅ‡∏•‡πâ‡∏ß
            filepath = get_rubric_file_path(
                tenant=self.config.tenant,
                enabler=self.enabler_id
            )
        except Exception as e:
            # ‡∏î‡∏±‡∏Å‡∏à‡∏±‡∏ö Exception ‡∏´‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏î‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô path_utils
            self.logger.error(f"‚ùå FATAL: Error calling get_rubric_file_path: {e}")
            return {} 

        # 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if filepath is None or not os.path.exists(filepath):
            self.logger.error(f"‚ö†Ô∏è Rubric file not found at expected path: {filepath}")
            return {}

        # 3. ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå JSON
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
            self.logger.info(f"‚úÖ Rubric loaded successfully from: {filepath}")
            return data
        except json.JSONDecodeError:
            self.logger.error(f"‚ùå Error decoding Rubric JSON. File might be corrupted: {filepath}")
            return {}
        except Exception as e:
            self.logger.error(f"‚ùå Error loading Rubric file from {filepath}: {e}")
            return {}
    
    # -------------------- Helper Function for Map Processing --------------------
    def _get_level_order_value(self, level_str: str) -> int:
        """Converts Level string ('L1', 'L5') to an integer for comparison."""
        try:
            return int(level_str.upper().replace('L', ''))
        except:
            return 0

    # -------------------- Persistent Mapping Handlers (FIXED) --------------------
    def _process_temp_map_to_final_map(self, temp_map: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Converts the temporary map into the final map format for saving, 
        and filters out temporary/unresolvable evidence IDs.
        """
        working_map = temp_map or self.temp_map_for_save or {}
        final_map_for_save = {}
        total_cleaned_items = 0

        for sub_level_key, evidence_list in working_map.items():
            if isinstance(evidence_list, dict):
                evidence_list = [evidence_list]
            elif not isinstance(evidence_list, list):
                logger.warning(f"[EVIDENCE] Skipping {sub_level_key}: not a list or dict")
                continue

            clean_list = []
            seen_ids = set()
            for ev in evidence_list:
                doc_id = ev.get("doc_id")
                
                if not doc_id:
                    continue
                
                # 1. FIX: ‡∏Å‡∏£‡∏≠‡∏á ID ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß (TEMP-) ‡∏≠‡∏≠‡∏Å
                if doc_id.startswith("TEMP-"):
                    # ID ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏õ‡∏•‡∏á‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô Stable Document ID ‡πÑ‡∏î‡πâ ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÉ‡∏ä‡πâ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ
                    logger.debug(f"[EVIDENCE] Filtering out unresolvable TEMP- ID: {doc_id} for {sub_level_key}.")
                    continue 
                
                # 2. Logic ‡πÄ‡∏î‡∏¥‡∏°: ‡∏Å‡∏£‡∏≠‡∏á HASH- (Placeholder) ‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ã‡πâ‡∏≥
                if doc_id.startswith("HASH-") or doc_id in seen_ids:
                    continue
                    
                seen_ids.add(doc_id)
                clean_list.append(ev)
                total_cleaned_items += 1 

            if clean_list:
                final_map_for_save[sub_level_key] = clean_list

        logger.info(f"[EVIDENCE] Processed {len(final_map_for_save)} sub-level keys with total {total_cleaned_items} evidence items")
        return final_map_for_save

    def _clean_map_for_json(self, data: Union[Dict, List, Set, Any]) -> Union[Dict, List, Any]:
        """Recursively converts objects that cannot be serialized (like sets) into lists."""
        if isinstance(data, dict):
            return {k: self._clean_map_for_json(v) for k, v in data.items()}
        elif isinstance(data, list):
            return [self._clean_map_for_json(v) for v in data]
        elif isinstance(data, set):
            return [self._clean_map_for_json(v) for v in data]
        return data

    def _clean_temp_entries(self, evidence_map: Dict[str, List[Any]]) -> Dict[str, List[Dict]]:
        """
        ‡∏Å‡∏£‡∏≠‡∏á TEMP-, HASH-, ‡πÅ‡∏•‡∏∞ Unknown ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å evidence map ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô 'str' object has no attribute 'get' ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ã‡πà‡∏≠‡∏°‡πÅ‡∏ã‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        """
        if not evidence_map or not isinstance(evidence_map, dict):
            return {}

        cleaned_map = {}
        total_removed = 0
        total_unknown_fixed = 0
        total_invalid_type = 0

        for key, entries in evidence_map.items():
            if not isinstance(entries, list):
                continue
                
            valid_entries = []
            for entry in entries:
                # üõ°Ô∏è Defense: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
                if not isinstance(entry, dict):
                    if isinstance(entry, str) and entry.strip():
                        # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏Å‡∏π‡πâ‡∏Ñ‡∏∑‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô string
                        entry = {"doc_id": entry, "filename": "Unknown", "relevance_score": 0.0}
                    else:
                        total_invalid_type += 1
                        continue

                doc_id = entry.get("doc_id")
                if doc_id is None:
                    total_removed += 1
                    continue
                
                doc_id_str = str(doc_id)

                # 1. ‡∏Å‡∏£‡∏≠‡∏á TEMP- ‡πÅ‡∏•‡∏∞ HASH-
                if doc_id_str.startswith("TEMP-") or doc_id_str.startswith("HASH-"):
                    total_removed += 1
                    continue

                # 2. ‡∏Å‡∏£‡∏≠‡∏á Unknown
                if not doc_id_str or doc_id_str.lower() == "unknown":
                    total_removed += 1
                    continue

                # 3. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Filename
                filename = str(entry.get("filename", "")).strip()
                if not filename or filename.lower() in ["unknown", "none", "unknown_file.pdf", "n/a"]:
                    short_id = doc_id_str[:8]
                    entry["filename"] = f"‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á_{short_id}.pdf"
                    total_unknown_fixed += 1
                else:
                    try:
                        entry["filename"] = os.path.basename(filename)
                    except:
                        entry["filename"] = filename

                valid_entries.append(entry)

            if valid_entries:
                cleaned_map[key] = valid_entries

        return cleaned_map

    def _save_evidence_map(self, map_to_save: Optional[Dict[str, List[Dict[str, Any]]]] = None):
        """
        ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å evidence map ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢ 100% 
        [REVISED 2026] - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö UUID v5, ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô ID 'fallback', ‡πÅ‡∏•‡∏∞‡∏ó‡∏≥ Atomic Write
        """
        try:
            map_file_path = get_evidence_mapping_file_path(
                tenant=self.config.tenant,
                year=self.config.year,
                enabler=self.enabler_id
            )
        except Exception as e:
            self.logger.critical(f"[EVIDENCE] FATAL: ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Path ‡πÑ‡∏î‡πâ: {e}")
            raise

        lock_path = map_file_path + ".lock"
        tmp_path = None

        self.logger.info(f"[EVIDENCE] Preparing atomic save ‚Üí {map_file_path}")

        try:
            os.makedirs(os.path.dirname(map_file_path), exist_ok=True)

            with FileLock(lock_path, timeout=60):
                # 1. ‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å (Merge Logic)
                if map_to_save is not None:
                    final_map_to_write = map_to_save
                else:
                    # ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤‡∏à‡∏≤‡∏Å Disk ‡∏°‡∏≤ Merge ‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏ô Memory
                    existing_map = self._load_evidence_map(is_for_merge=True) or {}
                    runtime_map = deepcopy(self.evidence_map)
                    final_map_to_write = existing_map

                    for key, new_entries in runtime_map.items():
                        current_entries = final_map_to_write.setdefault(key, [])
                        
                        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Index ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ä‡πá‡∏Ñ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô (Key: Cleaned UUID)
                        entry_map = {}
                        for e in current_entries:
                            if isinstance(e, dict):
                                # üü¢ [FIX] ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î ID ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô Key ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö
                                raw_id = e.get("chunk_uuid") or e.get("doc_id") or "N/A"
                                clean_id = str(raw_id).replace("-", "").lower()
                                entry_map[clean_id] = e
                        
                        for new_entry in new_entries:
                            if not isinstance(new_entry, dict): continue
                            
                            # ‡∏î‡∏∂‡∏á ID ‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏´‡∏°‡πà
                            raw_new_id = new_entry.get("chunk_uuid") or new_entry.get("doc_id") or "N/A"
                            clean_new_id = str(raw_new_id).replace("-", "").lower()

                            # üü¢ [FIX] ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏¢‡∏∞‡∏´‡∏£‡∏∑‡∏≠ 'fallback' ‡∏´‡∏•‡∏∏‡∏î‡πÄ‡∏Ç‡πâ‡∏≤‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
                            if clean_new_id in ["na", "n/a", "fallback", "none", ""]:
                                continue

                            new_score = new_entry.get("relevance_score", 0.0)

                            # ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ ID ‡∏ô‡∏µ‡πâ‡πÉ‡∏ô Database ‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡∏±‡∏ß‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏î‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤ -> ‡πÉ‡∏´‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å
                            if clean_new_id not in entry_map:
                                entry_map[clean_new_id] = new_entry
                            else:
                                old_entry = entry_map[clean_new_id]
                                old_score = old_entry.get("relevance_score", 0.0)
                                
                                # ‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Metadata ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏´‡∏≤‡∏Å‡∏ï‡∏±‡∏ß‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ (‡πÄ‡∏ä‡πà‡∏ô ‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤)
                                if "page" not in new_entry or new_entry["page"] in ["N/A", None]:
                                    new_entry["page"] = old_entry.get("page")
                                if "page_label" not in new_entry:
                                    new_entry["page_label"] = old_entry.get("page_label")
                                        
                                if new_score >= old_score:
                                    entry_map[clean_new_id] = new_entry

                        final_map_to_write[key] = list(entry_map.values())

                if not final_map_to_write:
                    self.logger.warning("[EVIDENCE] Nothing to save (empty map).")
                    return

                # 2. ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå
                final_map_to_write = self._clean_temp_entries(final_map_to_write)
                for key, entries in final_map_to_write.items():
                    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á (‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏≠‡∏¢‡∏π‡πà‡∏ö‡∏ô)
                    entries.sort(key=lambda x: x.get("relevance_score", 0.0), reverse=True)

                # 3. üõ°Ô∏è ATOMIC WRITE (‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏û‡∏±‡∏á‡∏Ç‡∏ì‡∏∞‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô)
                with tempfile.NamedTemporaryFile(
                    mode='w', delete=False, encoding="utf-8", dir=os.path.dirname(map_file_path)
                ) as tmp_file:
                    cleaned_data = self._clean_map_for_json(final_map_to_write)
                    json.dump(cleaned_data, tmp_file, indent=4, ensure_ascii=False)
                    tmp_path = tmp_file.name

                # ‡∏¢‡πâ‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß‡∏°‡∏≤‡∏ó‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏£‡∏¥‡∏á (‡πÄ‡∏õ‡πá‡∏ô Atomic Operation ‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö OS)
                shutil.move(tmp_path, map_file_path)
                tmp_path = None

                self.logger.info(f"[EVIDENCE] SAVED SUCCESSFULLY! Total Keys: {len(final_map_to_write)}")

        except Exception as e:
            self.logger.critical("[EVIDENCE] FATAL ERROR DURING ATOMIC SAVE")
            self.logger.exception(e)
            raise
        finally:
            # ‡πÄ‡∏Å‡πá‡∏ö‡∏Å‡∏ß‡∏≤‡∏î‡πÑ‡∏ü‡∏•‡πå Lock ‡πÅ‡∏•‡∏∞‡πÑ‡∏ü‡∏•‡πå Temp ‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏≤‡∏á‡∏≠‡∏¢‡∏π‡πà
            if os.path.exists(lock_path):
                try: os.unlink(lock_path)
                except: pass
            if tmp_path and os.path.exists(tmp_path):
                try: os.unlink(tmp_path)
                except: pass

    def merge_evidence_mappings(self, results_list: List[Any]) -> Dict[str, List[Dict]]:
        """
        [REVISED METHOD v2026.1.15]
        - ‡∏¢‡πâ‡∏≤‡∏¢‡πÄ‡∏Ç‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô Method ‡∏Ç‡∏≠‡∏á Class SEAMPDCAEngine ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢
        - ‡πÉ‡∏ä‡πâ Logic ‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡∏∞ Tuple (level_id, evidence_map) ‡∏Ç‡∏≠‡∏á‡∏û‡∏µ‡πà 100%
        - ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ã‡πâ‡∏≥‡∏î‡πâ‡∏ß‡∏¢ Indexing ‡∏Ç‡∏≠‡∏á doc_id/chunk_uuid
        """
        merged_mapping = {}
        
        self.logger.info(f"üß¨ Starting to merge evidence mappings from {len(results_list)} levels...")

        for item in results_list:
            # üéØ ‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô map ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏à‡∏≤‡∏Å Tuple (level_id, evidence_map)
            # ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á item ‡∏õ‡∏Å‡∏ï‡∏¥‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô: (1, {"L1": [...], "L2": [...]})
            temp_map = item[1] if isinstance(item, tuple) and len(item) == 2 else {}
            
            # ‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà item ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà tuple ‡πÅ‡∏ï‡πà‡πÄ‡∏õ‡πá‡∏ô dict ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß (‡∏Å‡∏±‡∏ô‡∏û‡∏•‡∏≤‡∏î)
            if not temp_map and isinstance(item, dict):
                temp_map = item

            if not temp_map: 
                continue

            for level_key, evidence_list in temp_map.items():
                if level_key not in merged_mapping:
                    merged_mapping[level_key] = []
                
                # üõ°Ô∏è ‡∏™‡∏£‡πâ‡∏≤‡∏á Index ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏±‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ã‡πâ‡∏≥ (Unique ID)
                existing_ids = {
                    str(e.get('doc_id') or e.get('chunk_uuid')) 
                    for e in merged_mapping[level_key]
                }
                
                for new_ev in evidence_list:
                    new_id = str(new_ev.get('doc_id') or new_ev.get('chunk_uuid'))
                    if new_id not in existing_ids:
                        merged_mapping[level_key].append(new_ev)
                        existing_ids.add(new_id)
        
        self.logger.info(f"‚úÖ Merging completed. Levels detected: {list(merged_mapping.keys())}")
        return merged_mapping

    def _load_evidence_map(self, is_for_merge: bool = False) -> Dict[str, List[Dict[str, Any]]]:
        """
        ‡πÇ‡∏´‡∏•‡∏î evidence map ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
        is_for_merge = True ‚Üí ‡πÑ‡∏°‡πà log "No existing map" (‡πÉ‡∏ä‡πâ‡πÉ‡∏ô save)
        """
        try:
            path = get_evidence_mapping_file_path(
                tenant=self.config.tenant,
                year=self.config.year,
                enabler=self.enabler_id
            )
        except Exception as e:
            self.logger.error(f"[EVIDENCE] FATAL: ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏î‡πâ: {e}")
            return {}

        if not os.path.exists(path):
            if not is_for_merge:
                self.logger.info("[EVIDENCE] No existing evidence map found ‚Äì starting fresh.")
            return {}

        try:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
            if not is_for_merge:
                total_items = sum(len(v) for v in data.values() if isinstance(v, list))
                self.logger.info(f"[EVIDENCE] Loaded evidence map: {len(data)} keys, {total_items} items from {path}")
            return data
        except Exception as e:
            self.logger.error(f"[EVIDENCE] Failed to load evidence map from {path}: {e}")
            return {}


    def _set_mock_handlers(self, mode: str):
        """Replaces real LLM/RAG functions with mock versions."""
        if mode == "control" or mode == "random":
            if hasattr(seam_mocking, 'evaluate_with_llm_CONTROLLED_MOCK'):
                self.llm_evaluator = seam_mocking.evaluate_with_llm_CONTROLLED_MOCK
            if hasattr(seam_mocking, 'retrieve_context_with_filter_MOCK'):
                self.rag_retriever = seam_mocking.retrieve_context_with_filter_MOCK
            if hasattr(seam_mocking, 'create_structured_action_plan_MOCK'):
                self.action_plan_generator = seam_mocking.create_structured_action_plan_MOCK
            if hasattr(seam_mocking, 'set_mock_control_mode'):
                seam_mocking.set_mock_control_mode(mode == "control") 

        logger.warning(f"Engine is running in MOCK mode: {mode}")

    def _get_pdca_phase(self, level: int) -> str:
        """Helper to get the PDCA phase string from the map."""
        return PDCA_PHASE_MAP.get(level, f"Level {level} Requirement")

    def _get_level_constraint_prompt(self, sub_id: str, level: int) -> str:
        """
        [ADAPTIVE AUDIT GUIDELINE v2026] 
        - ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å '‡∏Å‡∏é‡πÄ‡∏´‡∏•‡πá‡∏Å' ‡πÄ‡∏õ‡πá‡∏ô '‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤'
        - ‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏´‡πâ AI ‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á (Alignment) ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏ö‡∏ú‡∏¥‡∏î
        """
        required_phases = self.get_rule_content(sub_id, level, "require_phase") or []
        specific_rule = self.get_rule_content(sub_id, level, "specific_contextual_rule") or ""
        
        level_name = PDCA_PHASE_MAP.get(level, f"Level {level}")
        level_goal = MATURITY_LEVEL_GOALS.get(level, "")

        prompt_lines = [
            f"\n### [‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô: {sub_id} | {level_name}] ###",
            f"üéØ ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: {level_goal}",
            "---"
        ]
        
        # üìå 1. ‡∏Å‡∏≤‡∏£‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡πÄ‡∏ü‡∏™ (‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å '‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô' ‡πÄ‡∏õ‡πá‡∏ô '‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á')
        if required_phases:
            phase_labels = {"P": "‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô (Plan)", "D": "‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥ (Do)", "C": "‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö (Check)", "A": "‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á (Act)"}
            readable = [phase_labels.get(p, p) for p in required_phases]
            prompt_lines.append(f"üîç ‡∏≠‡∏á‡∏Ñ‡πå‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡∏û‡∏ö: {', '.join(readable)}")
            prompt_lines.append("   - ‡πÇ‡∏õ‡∏£‡∏î‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ß‡πà‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏™‡∏∞‡∏ó‡πâ‡∏≠‡∏ô‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°‡πÉ‡∏ô‡πÄ‡∏ü‡∏™‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà")
            prompt_lines.append("   - ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏ô‡πÄ‡∏ü‡∏™‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç ‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏∏‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏Ç‡∏≤‡∏î‡∏´‡∏≤‡∏¢‡πÉ‡∏ô‡∏ä‡πà‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤")

        # üõë 2. ‡∏Å‡∏é‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠ (‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å '‡∏Å‡∏é‡πÄ‡∏´‡∏•‡πá‡∏Å' ‡πÄ‡∏õ‡πá‡∏ô '‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏û‡∏¥‡πÄ‡∏®‡∏©')
        if specific_rule:
            prompt_lines.append(f"üí° ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ: \"{specific_rule}\"")
            prompt_lines.append("   - ‡πÉ‡∏ä‡πâ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏Ç‡∏≠‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤")
            prompt_lines.append("   - ‡∏´‡∏≤‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡πÄ‡∏à‡∏ï‡∏ô‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏Ç‡∏≠‡∏á‡πÄ‡∏Å‡∏ì‡∏ë‡πå ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ï‡∏≤‡∏°‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°")

        # ‚öñÔ∏è 3. ‡πÄ‡∏û‡∏¥‡πà‡∏° Instruction ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏ò‡∏£‡∏£‡∏° (The Fair Guard)
        prompt_lines.append("\n‚öñÔ∏è [‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô]")
        prompt_lines.append("- ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ï‡∏≤‡∏°‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏£‡∏¥‡∏á (Substance over Form) ‡∏≠‡∏¢‡πà‡∏≤‡∏õ‡∏±‡∏î‡∏ï‡∏Å‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ Keyword ‡∏ï‡∏£‡∏á‡∏ï‡∏±‡∏ß")
        prompt_lines.append("- ‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Å‡πâ‡∏≥‡∏Å‡∏∂‡πà‡∏á ‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Å‡∏±‡∏ö '‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£' ‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ Coaching Insight ‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô")
        
        return "\n".join(prompt_lines)

    # -------------------- Statement Preparation & Filtering Helpers --------------------
    def _flatten_rubric_to_statements(self) -> List[Dict[str, Any]]:
        """
        Transforms the hierarchical rubric structure loaded in self.rubric
        into a flat list of statements ready for assessment.
        """
        if not self.rubric:
            self.logger.warning("Cannot flatten rubric: self.rubric is empty.")
            return []
            
        data = deepcopy(self.rubric)
        extracted_list = []
        
        if not isinstance(data, dict):
             self.logger.error("Rubric data structure is invalid (expected dict of criteria).")
             return []
             
        # for criteria_id, criteria_data in data.items():
        for criteria_id, criteria_data in data.get('criteria', {}).items():
            # üéØ FIX 1: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Criteria Data
            if not isinstance(criteria_data, dict):
                self.logger.warning(f"Skipping malformed criteria entry: {criteria_id} (not a dict).")
                continue
                
            sub_criteria_map = criteria_data.get('subcriteria', {})
            criteria_name = criteria_data.get('name')
            
            # üéØ FIX 2: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Sub-criteria Map
            if not isinstance(sub_criteria_map, dict):
                 self.logger.warning(f"Skipping criteria {criteria_id}: 'subcriteria' is not a dictionary.")
                 continue

            for sub_id, sub_data in sub_criteria_map.items():
                
                # üéØ FIX 3: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ sub_data ‡πÄ‡∏õ‡πá‡∏ô Dictionary ‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô TypeError)
                if not isinstance(sub_data, dict):
                    self.logger.warning(
                        f"Skipping malformed sub-criteria entry: {criteria_id}.{sub_id} "
                        f"is not a dictionary (found type: {type(sub_data).__name__})."
                    )
                    continue
                
                # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤ Metadata ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
                sub_data['criteria_id'] = criteria_id
                sub_data['criteria_name'] = criteria_name
                sub_data['sub_id'] = sub_id 
                sub_data['sub_criteria_name'] = sub_data.get('name', criteria_name + ' sub')
                if 'weight' not in sub_data:
                    sub_data['weight'] = criteria_data.get('weight', 0)
                extracted_list.append(sub_data)

        # Re-check and re-sort levels
        final_list = []
        for sub_criteria in extracted_list: 
            
            # üéØ FIX 4: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á Level ‡∏à‡∏≤‡∏Å Dict ‡∏ó‡∏µ‡πà‡∏°‡∏µ Key ‡πÄ‡∏õ‡πá‡∏ô String ‡πÄ‡∏õ‡πá‡∏ô List
            if "levels" in sub_criteria and isinstance(sub_criteria["levels"], dict):
                levels_list = []
                for level_str, statement in sub_criteria["levels"].items():
                    try:
                        # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÅ‡∏õ‡∏•‡∏á Level Key ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô Integer
                        level_int = int(level_str)
                        if isinstance(statement, str):
                            levels_list.append({"level": level_int, "statement": statement})
                        else:
                            self.logger.warning(f"Level {level_str} statement in {sub_criteria.get('sub_id')} is not a string.")
                    except ValueError:
                        self.logger.error(f"Invalid level key '{level_str}' found in {sub_criteria.get('sub_id', 'UNKNOWN_ID')}. Skipping.")
                        continue
                        
                sub_criteria["levels"] = levels_list
            
            # ‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏£‡∏∞‡∏î‡∏±‡∏ö
            if "levels" in sub_criteria and isinstance(sub_criteria["levels"], list):
                 sub_criteria["levels"].sort(key=lambda x: x.get("level", 0))
                 final_list.append(sub_criteria)
            else:
                 self.logger.warning(f"Sub-criteria {sub_criteria.get('sub_id', 'UNKNOWN_ID')} missing 'levels' list.")


        return final_list

    
    # -------------------- Evidence Classification Helper (Robust 2026) --------------------
    def _get_mapped_uuids_and_priority_chunks(
        self,
        sub_id: str,
        level: int,
        statement_text: str = "",
        level_constraint: Optional[Any] = None, # üü¢ ‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô Optional ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Error Missing Argument
        vectorstore_manager: Optional['VectorStoreManager'] = None,
        evidence_map: Optional[Dict] = None 
    ) -> Tuple[List[str], List[Dict]]:
        """
        [DYNAMIC CONTINUITY v2026.6.2] - ROBUST SIGNATURE
        ----------------------------------------------
        - Fix: ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÅ‡∏ö‡∏ö‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Error 'missing 1 required positional argument'
        - Logic: ‡∏î‡∏∂‡∏á Baseline ‡∏à‡∏≤‡∏Å Memory/Map ‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á (Inheritance)
        """
        from copy import deepcopy
        priority_chunks = []
        mapped_stable_ids = []

        # 1. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ä‡πâ Evidence Map (‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: 1. Argument ‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏°‡∏≤ -> 2. Class Attribute -> 3. Empty Dict)
        target_map = evidence_map if evidence_map is not None else getattr(self, 'evidence_map', {})

        # 2. üß† [AUTO-HISTORY & INHERITANCE]
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡∏ú‡πà‡∏≤‡∏ô‡πÉ‡∏ô Level ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏≥‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡πÉ‡∏ô Level ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
        for key, evidences in target_map.items():
            if key.startswith(f"{sub_id}.L") and isinstance(evidences, list):
                try:
                    lvl_in_key = int(key.split(".L")[-1])
                    # ‡∏Å‡∏é Inheritance: ‡πÉ‡∏ä‡πâ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏à‡∏≤‡∏Å Level ‡∏ó‡∏µ‡πà <= ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
                    if lvl_in_key <= level:
                        history_items = deepcopy(evidences)
                        for item in history_items:
                            item["is_baseline"] = True
                            # ‡∏ö‡∏π‡∏™‡∏ï‡πå‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡∏≠‡∏á RAG
                            item["rerank_score"] = max(item.get("rerank_score", 0.0), 0.85)
                        priority_chunks.extend(history_items)
                except (ValueError, IndexError):
                    continue

        # 3. üîç [SEMANTIC HINTING] ‡∏Å‡∏£‡∏ì‡∏µ L1 ‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡πÉ‡∏ô Map
        if not priority_chunks and level == 1:
            rule_config = getattr(self, 'contextual_rules_map', {}).get(sub_id, {}).get(str(level), {})
            hints = rule_config.get("plan_keywords", [])[:2]
            if hints and vectorstore_manager:
                self.logger.info(f"üîé L1 Discovery: Searching using hints: {hints}")
                try:
                    discovery_result = vectorstore_manager.quick_search(
                        query=f"{sub_id} {' '.join(hints)}",
                        top_k=5
                    )
                    for chunk in discovery_result:
                        chunk["rerank_score"] = 0.85 # ‡∏ö‡∏π‡∏™‡∏ï‡πå‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠‡∏à‡∏≤‡∏Å Keyword Rule
                        priority_chunks.append(chunk)
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Quick search failed: {e}")

        if not priority_chunks:
            return [], []

        # 4. üíß [ROBUST HYDRATION] ‡πÄ‡∏ï‡∏¥‡∏°‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ Full Text ‡∏à‡∏≤‡∏Å Vector DB
        try:
            priority_chunks = self._robust_hydrate_documents_for_priority_chunks(
                chunks_to_hydrate=priority_chunks,
                vsm=vectorstore_manager
            )
        except Exception as e:
            self.logger.error(f"‚ùå Hydration failed in priority module: {e}")

        # 5. üéØ [ID SYNC] ‡∏™‡∏Å‡∏±‡∏î UUID ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ Filter ‡∏Ç‡∏≠‡∏á Main Retriever
        seen_ids = set()
        for chunk in priority_chunks:
            sid = chunk.get("stable_doc_uuid") or chunk.get("doc_id")
            if sid and isinstance(sid, str):
                if sid not in seen_ids and len(sid) >= 32:
                    mapped_stable_ids.append(sid)
                    seen_ids.add(sid)

        self.logger.info(f"‚úÖ Continuity Ready: {len(priority_chunks)} priority chunks | Sub:{sub_id} L{level}")
        return mapped_stable_ids, priority_chunks
    
    # -------------------- Calculation Helpers (ADDED) --------------------
    def _calculate_weighted_score(self, highest_full_level: int, weight: int) -> float:
        """
        Calculates the weighted score based on the highest full level achieved.
        Score is calculated by: (Level / MAX_LEVEL) * Weight
        """
        # üéØ ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å MAX_LEVEL_CALC ‡πÄ‡∏õ‡πá‡∏ô MAX_LEVEL ‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏°‡∏≤‡∏à‡∏≤‡∏Å global_vars
        from config.global_vars import MAX_LEVEL  
        
        if highest_full_level <= 0:
            return 0.0
        
        # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô (‡πÄ‡∏ä‡πà‡∏ô ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ data ‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î)
        level_for_calc = min(highest_full_level, MAX_LEVEL)
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ñ‡πà‡∏ß‡∏á‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏ï‡∏≤‡∏°‡πÄ‡∏û‡∏î‡∏≤‡∏ô‡πÄ‡∏•‡πÄ‡∏ß‡∏•‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
        score = (level_for_calc / MAX_LEVEL) * weight
        return score

    def _calculate_overall_stats(self, target_sub_id: str):
        """
        [ULTIMATE STATS v2026.4] Weighted Maturity & Coaching Analytics
        ------------------------------------------------------
        - ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Maturity Score ‡πÅ‡∏•‡∏∞ Progress %
        - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Soft Gaps ‡πÅ‡∏•‡∏∞ Strength Points ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Coaching Report
        """
        from config.global_vars import MAX_LEVEL
        results = self.final_subcriteria_results
        
        # 1. üõ°Ô∏è Safety Guard
        if not results:
            self.total_stats = {
                "overall_avg_score": 0.0,
                "overall_level_label": "L0",
                "record_id": self.current_record_id,
                "status": "No Data"
            }
            return

        # 2. ‚öñÔ∏è ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ñ‡πà‡∏ß‡∏á‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å
        total_weighted_score_achieved = sum(r.get('weighted_score', 0.0) for r in results)
        total_possible_weight = sum(r.get('weight', 0.0) for r in results)

        # 3. üìä Maturity Score & Progress
        overall_avg_score = 0.0
        if total_possible_weight > 0:
            overall_avg_score = round((total_weighted_score_achieved / total_possible_weight) * MAX_LEVEL, 2)
        
        max_possible_points = total_possible_weight * MAX_LEVEL
        progress_percent = 0.0
        if max_possible_points > 0:
            progress_percent = round((total_weighted_score_achieved / max_possible_points) * 100, 2)

        # 4. üè∑Ô∏è Maturity Level Label (Audit Logic)
        avg_full_level = sum(r.get('highest_full_level', 0) for r in results) / len(results)
        final_level = int(avg_full_level) 
        overall_level_label = f"L{min(max(final_level, 0), MAX_LEVEL)}"

        # 5. üí° [NEW] Coaching Metrics (‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏à‡∏∏‡∏î‡πÅ‡∏Ç‡πá‡∏á‡πÅ‡∏•‡∏∞‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏±‡∏í‡∏ô‡∏≤)
        total_strengths = 0
        total_coaching_needs = 0
        
        for r in results:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏•‡πÄ‡∏ß‡∏•
            details = r.get('level_details', {})
            for lvl_data in details.values():
                insight = lvl_data.get('coaching_insight', "")
                if "‡∏à‡∏∏‡∏î‡πÅ‡∏Ç‡πá‡∏á" in insight or "üåü" in insight:
                    total_strengths += 1
                if "‡∏Ç‡πâ‡∏≠‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥" in insight or "üí°" in insight:
                    total_coaching_needs += 1

        # 6. üìù ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏™‡∏£‡∏∏‡∏õ
        self.total_stats = {
            "overall_avg_score": min(overall_avg_score, float(MAX_LEVEL)),
            "overall_level_label": overall_level_label,
            "total_weighted_score": round(total_weighted_score_achieved, 2),
            "total_possible_weight": total_possible_weight,
            "progress_percent": progress_percent,
            "gap_to_full": round(total_possible_weight - total_weighted_score_achieved, 2),
            "assessed_count": len(results),
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏°‡∏¥‡∏ï‡∏¥‡∏Å‡∏≤‡∏£ Coaching ‡πÉ‡∏ô Stats
            "coaching_metrics": {
                "total_strength_points": total_strengths,
                "total_improvement_areas": total_coaching_needs
            },
            "enabler": self.config.enabler,
            "record_id": self.current_record_id,
            "assessed_at": datetime.now().isoformat()
        }

        # 7. üì¢ Logging
        self.logger.info(f"üèÜ Overall: {overall_level_label} ({overall_avg_score}/{MAX_LEVEL})")
        self.logger.info(f"üí° Analytics: Strengths[{total_strengths}] | Needs Improvement[{total_coaching_needs}]")


    def _export_results(self, results: dict, sub_criteria_id: str, **kwargs) -> str:
        """
        [ULTIMATE EXPORTER v2026.4 - PRODUCTION READY]
        ---------------------------
        - üõ°Ô∏è JSON Safe: ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Pydantic objects ‡πÅ‡∏•‡∏∞ ActionPlan
        - üìÇ Hierarchical Storage: ‡πÄ‡∏Å‡πá‡∏ö‡πÅ‡∏¢‡∏Å Tenant/Year/Enabler
        - üí° Coaching Summary: ‡∏£‡∏ß‡∏° Insight ‡∏ó‡∏∏‡∏Å Level ‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
        - üìä Audit Summary: ‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÅ‡∏•‡∏∞‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢
        """

        # 1. ‚öôÔ∏è ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        record_id = kwargs.get("record_id", getattr(self, "current_record_id", "no_id"))
        enabler = self.config.enabler
        tenant = self.config.tenant
        year = str(self.config.year)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # 2. üìÅ ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Path
        export_dir = os.path.join("data_store", tenant, "exports", year, enabler)
        try:
            os.makedirs(export_dir, exist_ok=True)
            file_name = f"assessment_{enabler}_{record_id}_{sub_criteria_id}_{timestamp}.json"
            full_path = os.path.join(export_dir, file_name)
        except Exception as e:
            self.logger.error(f"‚ùå Directory creation failed: {e}")
            full_path = f"assessment_fallback_{record_id}.json"

        # 3. üõ†Ô∏è JSON Serialization Helper (‡∏à‡∏∏‡∏î‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç)
        def json_serializable(obj):
            """‡πÅ‡∏õ‡∏•‡∏á Object ‡∏û‡∏¥‡πÄ‡∏®‡∏©‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô Type ‡∏ó‡∏µ‡πà JSON ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö"""
            if isinstance(obj, datetime):
                return obj.isoformat()
            if isinstance(obj, BaseModel): # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Pydantic (ActionPlan)
                return obj.model_dump()
            if hasattr(obj, '__dict__'):
                return obj.__dict__
            return str(obj)

        # 4. üìä ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Summary
        if 'summary' not in results:
            results['summary'] = {}
        
        summary = results['summary']
        sub_res_list = results.get('sub_criteria_results', [])

        # 5. üí° ‡∏™‡∏Å‡∏±‡∏î Coaching Insights ‡∏à‡∏≤‡∏Å‡∏£‡∏≤‡∏¢‡πÄ‡∏•‡πÄ‡∏ß‡∏• (‡∏£‡∏ß‡∏°‡∏®‡∏π‡∏ô‡∏¢‡πå)
        all_coaching_insights = []
        for sub_res in sub_res_list:
            details = sub_res.get('level_details', {})
            for lvl, data in details.items():
                insight = data.get('coaching_insight', "")
                if insight:
                    all_coaching_insights.append({
                        "id": sub_res.get('sub_id'),
                        "level": lvl,
                        "insight": insight
                    })
        
        summary['coaching_summary'] = all_coaching_insights

        # 6. üìë ‡∏ù‡∏±‡∏á Identity Metadata
        results['metadata'] = {
            "record_id": record_id,
            "tenant": tenant,
            "year": year,
            "enabler": enabler,
            "model_used": getattr(self.config, "model_name", "unknown"),
            "target_level": self.config.target_level,
            "export_at": datetime.now().isoformat()
        }

        # 7. üìà ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô (Audit Summary)
        if str(sub_criteria_id).lower() != "all" and len(sub_res_list) > 0:
            # ‡∏Å‡∏£‡∏ì‡∏µ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏£‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠ (‡πÄ‡∏ä‡πà‡∏ô 1.1)
            main_res = sub_res_list[0]
            summary.update({
                "highest_pass_level": main_res.get('highest_full_level', 0),
                "achieved_weight": round(main_res.get('weighted_score', 0.0), 2),
                "total_weight": main_res.get('weight', 0.0),
                "is_target_achieved": main_res.get('target_level_achieved', False)
            })
        else:
            # ‡∏Å‡∏£‡∏ì‡∏µ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏° (All)
            all_pass_levels = [r.get('highest_full_level', 0) for r in sub_res_list]
            total_achieved = sum(r.get('weighted_score', 0.0) for r in sub_res_list)
            total_possible = sum(r.get('weight', 0.0) for r in sub_res_list)
            
            summary.update({
                "highest_pass_level_overall": max(all_pass_levels) if all_pass_levels else 0,
                "total_achieved_weight": round(total_achieved, 2),
                "total_possible_weight": round(total_possible, 2),
                "overall_percentage": round((total_achieved / total_possible * 100), 2) if total_possible > 0 else 0.0,
                "total_subcriteria_assessed": len(sub_res_list)
            })

        # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Action Plan Items ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        summary['total_action_plan_items'] = sum(
            len(r.get('action_plan', [])) 
            for r in sub_res_list 
            if isinstance(r.get('action_plan'), list)
        )

        # 8. üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏î‡πâ‡∏ß‡∏¢ Safety Serializer
        try:
            with open(full_path, 'w', encoding='utf-8') as f:
                # ‡πÉ‡∏ä‡πâ default=json_serializable ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Error
                json.dump(results, f, ensure_ascii=False, indent=4, default=json_serializable)
            
            self.logger.info(f"üíæ EXPORT SUCCESS: {full_path}")
            return full_path
            
        except Exception as e:
            self.logger.error(f"‚ùå Export failed at write stage: {str(e)}")
            # Fallback: ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö‡∏î‡∏¥‡∏ö‡∏´‡∏≤‡∏Å JSON ‡∏û‡∏±‡∏á
            try:
                fallback_path = full_path.replace(".json", "_emergency_dump.txt")
                with open(fallback_path, 'w', encoding='utf-8') as f:
                    f.write(str(results))
                return fallback_path
            except:
                return ""
        
    def _create_error_result(
        self,
        level: int,
        error_message: str,
        start_time: float,
        retrieval_duration: float,
        sub_id: str,
        statement_id: str,
        statement_text: str,
        llm_duration: float = 0.0,
    ) -> Dict[str, Any]:
        """
        Generates a standard error dictionary for when RAG or LLM fails.
        """
        # NOTE: Assumes self._get_pdca_phase is defined
        try:
            pdca_phase = self._get_pdca_phase(level) 
        except Exception:
            pdca_phase = "N/A"
            
        pdca_breakdown = {p: 0 for p in ['P', 'D', 'C', 'A']}
        total_duration = time.time() - start_time

        self.logger.error(f"FATAL ERROR RESULT for {sub_id} L{level}: {error_message}")

        return {
            "sub_criteria_id": sub_id,
            "statement_id": statement_id,
            "level": level,
            "statement": statement_text,
            "pdca_phase": pdca_phase,
            "llm_score": 0.0,
            "pdca_breakdown": pdca_breakdown,
            "is_passed": False,
            "status": "FAIL",
            "score": 0.0,
            "llm_result_full": {"error": error_message, "details": "Assessment skipped due to critical failure."},
            "retrieval_duration_s": round(retrieval_duration, 2),
            "llm_duration_s": round(llm_duration, 2),
            "top_evidences_ref": [],
            "temp_map_for_level": [],
            "evidence_strength": 0.0,
            "ai_confidence": "LOW",
            "evidence_count": 0,
            "pdca_coverage": 0.0,
            "direct_evidence_count": 0,
            "rag_query": statement_text,
            "full_context_meta": {"error_type": "Critical Failure"},
            # üü¢ NEW: Relevant Score Gate Metadata (Set to default error values)
            "max_relevant_score": 0.0,
            "max_relevant_source": "ERROR_HANDLING",
            "is_evidence_strength_capped": False,
            "max_evidence_strength_used": 0.0,
            "total_run_time_s": round(total_duration, 2)
        }

    def _save_level_evidences_and_calculate_strength(
        self,
        level_temp_map: List[Dict[str, Any]],
        sub_id: str,
        level: int,
        llm_result: Dict[str, Any],
        highest_rerank_score: float = 0.0
    ) -> float:
        """ [REVISED v2026.12] - ‡πÉ‡∏ä‡πâ LLM Tagging ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ñ‡∏≤‡∏ß‡∏£ """
        import hashlib
        import os
        from datetime import datetime

        map_key = f"{sub_id}.L{level}"
        new_evidence_list: List[Dict[str, Any]] = []
        seen_ids = set()

        self.logger.info(f"üíæ [EVI SAVE] Processing {map_key} | Count: {len(level_temp_map)}")

        for chunk in level_temp_map:
            # 1. ‡∏à‡∏±‡∏î‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Handle both dict & LangChain Doc)
            meta = chunk.get("metadata", {}) if isinstance(chunk, dict) else getattr(chunk, "metadata", {})
            text = chunk.get("text") if isinstance(chunk, dict) else getattr(chunk, "page_content", "")
            
            if not text.strip(): continue

            # 2. Stable ID Generation (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ã‡πâ‡∏≥)
            c_uuid = str(chunk.get("chunk_uuid") or meta.get("chunk_uuid") or hashlib.sha256(text.encode()).hexdigest()[:16])
            d_uuid = str(chunk.get("stable_doc_uuid") or meta.get("stable_doc_uuid") or "doc-unknown")
            unique_key = f"{d_uuid}:{c_uuid}"
            if unique_key in seen_ids: continue
            seen_ids.add(unique_key)

            # 3. ‚ú® CRITICAL FIX: ‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏∏ PDCA Tag (‡πÉ‡∏ä‡πâ LLM ‡πÅ‡∏ó‡∏ô Regex)
            # ‡∏î‡∏∂‡∏á Tag ‡πÄ‡∏î‡∏¥‡∏°‡∏à‡∏≤‡∏Å Metadata ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ (‡∏à‡∏≤‡∏Å‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏ô Assessment)
            pdca_tag = chunk.get("pdca_tag") or meta.get("pdca_tag")
            
            # ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ Tag ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡πá‡∏ô Other ‡πÉ‡∏´‡πâ Re-tag ‡∏î‡πâ‡∏ß‡∏¢ LLM (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πä‡∏∞‡∏Å‡πà‡∏≠‡∏ô‡∏•‡∏á DB)
            if not pdca_tag or pdca_tag == "Other":
                fname = os.path.basename(str(meta.get("source") or "Unknown"))
                # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡πÄ‡∏Å‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏à‡∏π‡∏ô‡πÑ‡∏ß‡πâ
                pdca_tag = self._get_semantic_tag(text, sub_id, level, filename=fname)

            # 4. Normalize ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô
            source_raw = meta.get("source") or "Unknown"
            
            evidence_entry = {
                "sub_id": sub_id,
                "level": level,
                "relevance_score": float(chunk.get("rerank_score") or chunk.get("score") or 0.5),
                "doc_id": d_uuid,
                "stable_doc_uuid": d_uuid,
                "chunk_uuid": c_uuid,
                "source": source_raw,
                "source_filename": os.path.basename(str(source_raw)),
                "page": str(meta.get("page_label") or meta.get("page") or "N/A"),
                "pdca_tag": pdca_tag,
                "text_preview": text[:300].replace("\n", " ") + "...",
                "status": "PASS" if llm_result.get("is_passed", False) else "FAIL",
                "timestamp": datetime.now().isoformat(),
            }
            new_evidence_list.append(evidence_entry)

        # 5. ‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Strength
        if new_evidence_list:
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á Memory Map ‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏ö
            self.evidence_map.setdefault(map_key, []).extend(new_evidence_list)
            self.temp_map_for_save.setdefault(map_key, []).extend(new_evidence_list)

            # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Tag ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥ Log Summary
            counts = {"P": 0, "D": 0, "C": 0, "A": 0, "Other": 0}
            for ev in new_evidence_list: counts[ev['pdca_tag']] += 1

            # üìä Strength Calculation ( rerank + pdca_richness )
            # ‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Å‡∏±‡∏ö Rerank Score 60% ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á PDCA 40%
            unique_tags = {ev['pdca_tag'] for ev in new_evidence_list if ev['pdca_tag'] in "PDCA"}
            coverage_score = len(unique_tags) / 4.0 # ‡∏û‡∏ö‡∏Å‡∏µ‡πà‡∏´‡∏°‡∏ß‡∏î‡∏à‡∏≤‡∏Å 4 ‡∏´‡∏°‡∏ß‡∏î
            
            final_strength = round((highest_rerank_score * 0.6) + (coverage_score * 0.4), 2)

            self.logger.info(
                f"‚úÖ [SAVED] {map_key}: {len(new_evidence_list)} chunks | "
                f"P:{counts['P']} D:{counts['D']} C:{counts['C']} A:{counts['A']} | "
                f"Strength: {final_strength:.2f}"
            )
            return final_strength
            
        return 0.0
    
    def _calculate_evidence_strength_cap(
        self,
        top_evidences: List[Any],
        level: int,
        highest_rerank_score: Optional[float] = None
    ) -> Dict[str, Any]:
        """
        [PROTECTED v2026] ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ç‡∏µ‡∏î‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô
        - ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô Dictionary ‡πÄ‡∏™‡∏°‡∏≠‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Error ‡πÉ‡∏ô‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ
        """
        try:
            # ‚öôÔ∏è Configuration
            threshold = getattr(self, "RERANK_THRESHOLD", 0.35)
            cap_value = getattr(self, "MAX_EVI_STR_CAP", 5.0)
            
            # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ñ‡πà‡∏≤ None ‡∏´‡∏£‡∏∑‡∏≠ String ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏•‡∏Ç
            try:
                baseline_score = float(highest_rerank_score) if highest_rerank_score is not None else 0.0
            except (ValueError, TypeError):
                baseline_score = 0.0

            max_score_found = baseline_score
            max_score_source = "Adaptive_RAG_Loop"
            
            if not isinstance(top_evidences, list):
                top_evidences = []

            score_keys = ["rerank_score", "score", "relevance_score", "_rerank_score_force"]

            # üîç Scan Metadata
            for idx, doc in enumerate(top_evidences, 1):
                current_score = 0.0
                if isinstance(doc, dict):
                    metadata = doc.get("metadata") or {}
                    current_doc_source = metadata.get("file_name") or metadata.get("source") or f"Doc_{idx}"
                else:
                    metadata = getattr(doc, "metadata", {}) or {}
                    current_doc_source = getattr(doc, "source", f"Doc_{idx}")

                for key in score_keys:
                    val = metadata.get(key) if isinstance(metadata, dict) else None
                    if val is None and isinstance(doc, dict): val = doc.get(key)
                    
                    if val is not None:
                        try:
                            temp_s = float(val)
                            if 0.0 < temp_s <= 1.0:
                                current_score = temp_s
                                break
                        except: continue

                if current_score > max_score_found:
                    max_score_found = current_score
                    max_score_source = str(current_doc_source)

            is_capped = max_score_found < threshold
            max_evi_str_for_prompt = float(cap_value) if is_capped else 10.0

            # üìä Internal Log
            status_icon = "üö®" if is_capped else "‚úÖ"
            self.logger.info(
                f"{status_icon} Evi Str {'CAPPED' if is_capped else 'FULL'} L{level}: "
                f"Best {max_score_found:.4f} from '{max_score_source}' (Threshold: {threshold})"
            )

            return {
                "is_capped": bool(is_capped),
                "max_evi_str_for_prompt": float(max_evi_str_for_prompt),
                "top_score": round(float(max_score_found), 4),
                "max_score_source": str(max_score_source)
            }

        except Exception as e:
            self.logger.error(f"‚ùå Critical Fallback in _calculate_evidence_strength_cap: {e}")
            return {"is_capped": False, "max_evi_str_for_prompt": 10.0, "top_score": 0.0, "max_score_source": "Fallback"}
    
    def calculate_audit_confidence(self, matched_chunks: List[Any]) -> Dict[str, Any]:
        """
        [ULTIMATE AUDIT CONFIDENCE v2026.3]
        - Quality Gate: ‡∏Å‡∏£‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà Score ‡∏ï‡πà‡∏≥‡∏ó‡∏¥‡πâ‡∏á
        - Independence: ‡πÄ‡∏ä‡πá‡∏Ñ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏ï‡∏≠‡∏ö‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á)
        - Coverage: ‡πÄ‡∏ä‡πá‡∏Ñ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡∏ß‡∏á‡∏à‡∏£ PDCA
        - Traceability: ‡πÄ‡∏ä‡πá‡∏Ñ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ç‡∏≠‡∏á Metadata (‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå/‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤)
        """
        if not matched_chunks:
            return {
                "level": "NONE", "reason": "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö",
                "source_count": 0, "coverage_ratio": 0, "traceability_score": 0
            }

        # 0. üõ°Ô∏è Quality Filter (‡πÉ‡∏ä‡πâ Threshold 0.40 ‡∏ï‡∏≤‡∏°‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô SE-AM)
        valid_chunks = []
        for doc in matched_chunks:
            # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á Dict ‡πÅ‡∏•‡∏∞ Langchain Document
            meta = doc.metadata if hasattr(doc, 'metadata') else doc.get('metadata', {})
            score = meta.get('rerank_score') or meta.get('score') or 1.0
            
            if score >= 0.40:
                valid_chunks.append(doc)
        
        if not valid_chunks:
            return {
                "level": "LOW", "reason": "‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ï‡πà‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á (Relevance) ‡∏ï‡πà‡∏≥‡πÄ‡∏Å‡∏¥‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠‡πÑ‡∏î‡πâ",
                "source_count": 0, "coverage_ratio": 0, "traceability_score": 0
            }

        # 1. üìÇ Source Independence (‡∏ô‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô)
        unique_sources = set()
        for doc in valid_chunks:
            meta = doc.metadata if hasattr(doc, 'metadata') else doc.get('metadata', {})
            # ‡πÉ‡∏ä‡πâ Priority ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô _normalize_meta
            src = (meta.get('source_filename') or meta.get('filename') or 
                   meta.get('file_name') or meta.get('source'))
            if src:
                unique_sources.add(os.path.basename(str(src)))
        
        independence_score = len(unique_sources)
        
        # 2. üß© PDCA Coverage (‡πÄ‡∏ä‡πá‡∏Ñ‡∏°‡∏¥‡∏ï‡∏¥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô)
        pdca_map = {"P": False, "D": False, "C": False, "A": False}
        for doc in valid_chunks:
            meta = doc.metadata if hasattr(doc, 'metadata') else doc.get('metadata', {})
            tag = str(meta.get('pdca_tag', '')).upper()
            if tag in pdca_map:
                pdca_map[tag] = True
        
        found_tags_count = sum(pdca_map.values())
        coverage_ratio = found_tags_count / 4
        
        # 3. üîç Traceability (‡∏Ñ‡∏ß‡∏≤‡∏°‡πÇ‡∏õ‡∏£‡πà‡∏á‡πÉ‡∏™‡∏Ç‡∏≠‡∏á‡πÅ‡∏´‡∏•‡πà‡∏á‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á)
        traceable_count = 0
        for doc in valid_chunks:
            meta = doc.metadata if hasattr(doc, 'metadata') else doc.get('metadata', {})
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏´‡∏ô‡πâ‡∏≤ 0 ‡∏´‡∏≤‡∏¢)
            has_page = any([
                meta.get('page_label') is not None,
                meta.get('page') is not None,
                meta.get('page_number') is not None
            ])
            has_file = any([meta.get('source_filename'), meta.get('filename'), meta.get('file_name')])
            
            if has_page and has_file:
                traceable_count += 1
        
        traceability_score = traceable_count / len(valid_chunks) if valid_chunks else 0

        # 4. ‚öñÔ∏è Decision Matrix (Gated Audit Logic)
        if coverage_ratio <= 0.25 or independence_score < 1:
            confidence = "LOW"
            desc = "‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏±‡πà‡∏ô‡∏ï‡πà‡∏≥: ‡∏Ç‡∏≤‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏´‡∏£‡∏∑‡∏≠‡∏°‡∏µ‡∏°‡∏¥‡∏ï‡∏¥ PDCA ‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠"
        elif independence_score < 3 or coverage_ratio < 0.75:
            confidence = "MEDIUM"
            desc = "‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏±‡πà‡∏ô‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á: ‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏™‡∏ô‡∏±‡∏ö‡∏™‡∏ô‡∏∏‡∏ô‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏´‡∏•‡πà‡∏á ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡∏Ç‡∏≤‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡πÉ‡∏ô‡∏°‡∏¥‡∏ï‡∏¥ PDCA"
        else:
            confidence = "HIGH"
            desc = "‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏±‡πà‡∏ô‡∏™‡∏π‡∏á: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏£‡∏ö‡∏ß‡∏á‡∏à‡∏£ PDCA ‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏à‡∏≤‡∏Å‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢ (Cross-Check)"

        # üö® Penalty: ‡∏•‡∏î‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏´‡∏≤‡∏Å‡∏£‡∏∞‡∏ö‡∏∏‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á (‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤) ‡πÑ‡∏°‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
        if traceability_score < 0.50 and confidence != "LOW":
            confidence = "MEDIUM" if confidence == "HIGH" else "LOW"
            desc += " (‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Å‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ï‡πà‡∏≥)"

        return {
            "level": confidence, 
            "reason": desc, 
            "source_count": independence_score,
            "coverage_ratio": coverage_ratio,
            "traceability_score": round(traceability_score, 2),
            "pdca_found": [k for k, v in pdca_map.items() if v]
        }

        

    def _robust_hydrate_documents_for_priority_chunks(
        self,
        chunks_to_hydrate: List[Dict],
        vsm: Optional['VectorStoreManager'],
        current_sub_id: Optional[str] = None,
        level: Optional[int] = None
    ) -> List[Dict]:
        """
        [ULTIMATE HYDRATION v2026.12]
        - ‡∏î‡∏∂‡∏á Full Text ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Priority Chunks ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ LLM ‡πÄ‡∏´‡πá‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô
        - ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏õ‡πÉ‡∏ä‡πâ LLM-based Tagging ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡∏ó‡∏∏‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö (L1-L5)
        - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏∞‡∏ö‡∏ö Boost Score ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ AI ‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Å‡∏±‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ô‡∏µ‡πâ
        """
        from collections import defaultdict
        
        active_sub_id = current_sub_id or getattr(self, 'sub_id', 'unknown')
        if not chunks_to_hydrate:
            self.logger.debug(f"‚ÑπÔ∏è [HYDRATION] No chunks to hydrate for {active_sub_id} L{level}")
            return []

        # 1. üè∑Ô∏è Helper: ‡∏à‡∏±‡∏î‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏î‡πâ‡∏ß‡∏¢ LLM (‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö Core Assessment)
        def _safe_classify(text: str, filename: str = "") -> str:
            try:
                # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ LLM Tagging ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏õ‡∏£‡∏±‡∏ö‡∏à‡∏π‡∏ô‡πÑ‡∏ß‡πâ (Self-contained within class)
                tag = self._get_semantic_tag(
                    text=text, 
                    sub_id=active_sub_id, 
                    level=level or 1,
                    filename=filename
                )
                return tag if tag in {"P", "D", "C", "A"} else "Other"
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è PDCA classify failed in hydration: {e}")
                return "Other"

        # 2. üìè Helper: ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞ Scoring Boost
        def _standardize_chunk(chunk: Dict, score: float):
            chunk.setdefault("is_baseline", True)
            text = chunk.get("text", "").strip()
            meta = chunk.get("metadata", {})
            
            if text:
                fname = os.path.basename(str(meta.get("source") or meta.get("file_name") or "Unknown"))
                # ‚ú® ‡πÉ‡∏ä‡πâ LLM Tagging ‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ‡πÄ‡∏•‡∏¢
                chunk["pdca_tag"] = _safe_classify(text, filename=fname)
                
                # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö Boost ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÉ‡∏´‡πâ‡∏™‡∏π‡∏á ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ä‡∏ô‡∏∞ Chunk ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö
                chunk["rerank_score"] = max(float(chunk.get("rerank_score", 0.0)), score)
                chunk["score"] = max(float(chunk.get("score", 0.0)), score)
            return chunk

        # 3. üîë ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏° IDs
        stable_ids = {
            sid for c in chunks_to_hydrate
            if (sid := (c.get("stable_doc_uuid") or c.get("doc_id") or c.get("chunk_uuid")))
        }

        if not stable_ids or not vsm:
            boosted = [_standardize_chunk(c.copy(), 0.9) for c in chunks_to_hydrate]
            return self._guarantee_text_key(boosted)

        # 4. üõ∞Ô∏è Fetch Full Documents (Hydration Process)
        stable_id_map = defaultdict(list)
        try:
            retrieved_docs = vsm.get_documents_by_id(
                list(stable_ids), doc_type=self.doc_type, enabler=self.config.enabler
            )
            for doc in retrieved_docs:
                sid = doc.metadata.get("stable_doc_uuid") or doc.metadata.get("doc_id")
                if sid:
                    stable_id_map[sid].append({"text": doc.page_content, "metadata": doc.metadata})
        except Exception as e:
            self.logger.error(f"‚ùå [HYDRATION] VSM Fetch Error: {e}")
            return self._guarantee_text_key([_standardize_chunk(c.copy(), 0.9) for c in chunks_to_hydrate])

        # 5. üíß Hydrate & Dedup
        hydrated_priority_docs = []
        seen_signatures = set()
        SAFE_META_KEYS = {"source", "file_name", "page", "page_label", "page_number"}

        for chunk in chunks_to_hydrate:
            new_chunk = chunk.copy()
            sid = new_chunk.get("stable_doc_uuid") or new_chunk.get("doc_id")

            hydrated = False
            if sid and sid in stable_id_map:
                # ‡∏î‡∏∂‡∏á Full Text ‡∏°‡∏≤‡∏ó‡∏±‡∏ö Snippet ‡∏™‡∏±‡πâ‡∏ô‡πÜ
                best_match = stable_id_map[sid][0]
                new_chunk["text"] = best_match["text"]
                meta = best_match.get("metadata", {})
                new_chunk.update({k: v for k, v in meta.items() if k in SAFE_META_KEYS})
                hydrated = True

            # ‡∏ó‡∏≥ Standardize + Tagging (Score 1.0 ‡∏ñ‡πâ‡∏≤‡∏î‡∏∂‡∏á‡πÄ‡∏ï‡πá‡∏°‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à / 0.85 ‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ‡∏Ç‡∏≠‡∏á‡πÄ‡∏î‡∏¥‡∏°)
            new_chunk = _standardize_chunk(new_chunk, score=1.0 if hydrated else 0.85)

            # Check Signature ‡∏Å‡∏±‡∏ô‡∏ã‡πâ‡∏≥
            sig = (sid, new_chunk.get("chunk_uuid"), new_chunk.get("text", "")[:100])
            if sig not in seen_signatures:
                seen_signatures.add(sig)
                hydrated_priority_docs.append(new_chunk)

        self.logger.info(f"‚úÖ [HYDRATION] Complete: {len(hydrated_priority_docs)} priority chunks ready.")
        return self._guarantee_text_key(hydrated_priority_docs)

    def _guarantee_text_key(
        self,
        chunks: List[Dict],
        total_count: int = 0,
        restored_count: int = 0
    ) -> List[Dict]:
        """
        Guarantee 'text' key exists in every chunk
        """
        final_chunks = []

        for chunk in chunks:
            if "text" not in chunk or not isinstance(chunk["text"], str):
                chunk["text"] = ""
                cid = str(chunk.get("chunk_uuid", "N/A"))
                self.logger.debug(f"Guaranteed 'text' key for chunk (ID: {cid[:8]})")
            final_chunks.append(chunk)

        if total_count > 0:
            baseline_count = sum(1 for c in final_chunks if c.get("is_baseline"))
            self.logger.info(
                f"HYDRATION FINAL: Restored {restored_count}/{total_count} "
                f"(Baseline={baseline_count}, Total final={len(final_chunks)})"
            )

        return final_chunks


    def _normalize_meta(self, c: Dict) -> Tuple[str, str]:
        """
        [REVISED] ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡∏∞‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏£‡∏∞‡∏ö‡∏ö Priority Fallback
        - ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤ 0 ‡∏´‡∏≤‡∏¢ (Index-based)
        - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏à‡∏≤‡∏Å Ingest ‡πÉ‡∏´‡∏°‡πà ‡πÅ‡∏•‡∏∞ Reranker Output
        - ‡πÄ‡∏ô‡πâ‡∏ô page_label ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏ö‡∏ô UI
        """
        if not isinstance(c, dict):
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡πá‡∏ô LangChain Document ‡πÉ‡∏´‡πâ‡∏î‡∏∂‡∏á metadata ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤
            if hasattr(c, "metadata"):
                meta = c.metadata
                # ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á dict ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÇ‡∏Ñ‡πâ‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏•‡πà‡∏≤‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏î‡πâ
                c = {"metadata": meta}
            else:
                return "Unknown Source", "N/A"

        # ‡∏î‡∏∂‡∏á metadata ‡∏°‡∏≤‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÑ‡∏ß‡πâ (‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏Å‡∏£‡∏ì‡∏µ None ‡∏´‡∏£‡∏∑‡∏≠ Dict)
        meta = c.get("metadata") or {}

        # 1. üîç ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå (Source Name Priority)
        source_priority = [
            c.get("source_filename"),
            meta.get("source_filename"),
            c.get("filename"),
            meta.get("source"),
            meta.get("file_name"),
            c.get("id") 
        ]
        
        source = "Unknown"
        for s in source_priority:
            if s and str(s).strip():
                source = str(s).strip()
                break

        # 2. üîç ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤ (Page Label Priority)
        page_keys = ["page_label", "page", "page_number"]
        found_page = None
        for key in page_keys:
            val_root = c.get(key)
            if val_root is not None and str(val_root).strip().lower() != "n/a":
                found_page = val_root
                break
            val_meta = meta.get(key)
            if val_meta is not None and str(val_meta).strip().lower() != "n/a":
                found_page = val_meta
                break

        # 3. ‚ú® Final Cleaning & UI Formatting
        clean_source = os.path.basename(source) if "/" in source or "\\" in source else source
        if found_page is not None:
            page_str = str(found_page).strip()
            clean_page = page_str if page_str.lower() != "n/a" else "N/A"
        else:
            clean_page = "N/A"

        return clean_source, clean_page
    
    
    def _get_pdca_blocks_from_evidences(self, evidences, baseline_evidences, level, sub_id, contextual_rules_map, record_id=None) -> Dict[str, str]:
        """ ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞ Tagging ‡πÉ‡∏´‡πâ‡πÑ‡∏≠‡∏Ñ‡∏≠‡∏ô ‚úÖ ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏• """
        pdca_groups = defaultdict(list)
        seen_texts = set()

        for chunk in (evidences or []):
            txt = chunk.get("text", "").strip()
            if not txt or txt in seen_texts: continue
            seen_texts.add(txt)

            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å LLM Tagging
            tag = self._get_semantic_tag(text=txt, sub_id=sub_id, level=level, filename=chunk.get("source_filename", "Unknown"))
            final_tag = tag if tag in {"P", "D", "C", "A"} else ("P" if level == 1 else "Other")
            
            chunk["pdca_tag"] = final_tag # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡∏≠‡∏∑‡πà‡∏ô‡∏£‡∏π‡πâ
            pdca_groups[final_tag].append(chunk)

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Dictionary ‡∏Ç‡∏≠‡∏á Blocks
        return {
            "Plan": self._create_text_block_from_chunks("Plan", pdca_groups["P"]),
            "Do": self._create_text_block_from_chunks("Do", pdca_groups["D"]),
            "Check": self._create_text_block_from_chunks("Check", pdca_groups["C"]),
            "Act": self._create_text_block_from_chunks("Act", pdca_groups["A"]),
            "Other": self._create_text_block_from_chunks("Other", pdca_groups["Other"])
        }

    def _create_text_block_from_chunks(self, tag, chunks):
        """ ‡∏ï‡∏±‡∏ß‡∏ä‡πà‡∏ß‡∏¢‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å chunk """
        if not chunks: return ""
        parts = [f"### [{tag} Evidence]\n{c.get('text','')}" for c in chunks[:5]]
        return "\n\n".join(parts)


    def _generate_action_plan_safe(
        self, 
        sub_id: str, 
        name: str, 
        enabler: str, 
        results: List[Dict]
    ) -> Any:
        """
        [ULTIMATE REVISE 2026.5] - STRATEGIC ACTION PLAN GENERATOR
        -----------------------------------------------------------
        - ‡∏ö‡∏π‡∏£‡∏ì‡∏≤‡∏Å‡∏≤‡∏£ Coaching Insight ‡πÅ‡∏•‡∏∞ Soft Gap ‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà Roadmap
        - ‡πÅ‡∏¢‡∏Å Mode ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏°‡∏Ç‡πâ‡∏ô‡∏Ç‡∏≠‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô (Sustain / Refinement / Gap)
        - ‡πÉ‡∏ä‡πâ Logic ‡∏ã‡πà‡∏≠‡∏°‡∏£‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô (Foundation Repair) ‡∏Å‡πà‡∏≠‡∏ô‡∏¢‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏™‡∏π‡πà‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢
        """
        try:
            self.logger.info(f"üõ†Ô∏è Preparing Strategic Action Plan for {sub_id} - {name}")
            
            # 1. ‡∏Ñ‡∏±‡∏î‡∏Å‡∏£‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ (Data Enrichment)
            to_recommend = []
            has_major_gap = False
            
            for r in results:
                is_passed = r.get('is_passed', False)
                strength = r.get('score', 0.0) # ‡πÉ‡∏ä‡πâ score ‡∏£‡∏≤‡∏¢‡∏î‡πâ‡∏≤‡∏ô‡∏°‡∏≤‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤
                coaching = r.get('coaching_insight', '').strip()
                reason = r.get('reason', '').strip()
                level = r.get('level', 0)

                # ‡∏£‡∏ß‡∏° Coaching Insight ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ AI
                enhanced_reason = reason
                if coaching:
                    enhanced_reason += f"\n[Coaching Insight & Soft Gap]: {coaching}"

                # ‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà Action Plan:
                # - ‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô (FAILED/GAP)
                # - ‡∏ú‡πà‡∏≤‡∏ô‡πÅ‡∏ï‡πà‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ï‡πà‡∏≥ (Weak Evidence)
                # - ‡∏°‡∏µ Coaching Insight (‡∏°‡∏µ‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡πà‡∏≠‡∏¢‡∏≠‡∏î)
                if not is_passed:
                    has_major_gap = True
                    to_recommend.append({
                        "level": level,
                        "reason": enhanced_reason,
                        "recommendation_type": "FAILED"
                    })
                elif is_passed and (strength < 1.0 or coaching):
                    to_recommend.append({
                        "level": level,
                        "reason": enhanced_reason,
                        "recommendation_type": "QUALITY_REFINEMENT" if strength < 1.0 else "SUSTAIN_ADVICE"
                    })

            # 2. ‡∏Å‡∏£‡∏ì‡∏µ‡∏ú‡πà‡∏≤‡∏ô‡∏´‡∏°‡∏î‡πÅ‡∏•‡∏∞‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏î‡∏µ‡∏°‡∏≤‡∏Å (No recommendation needed)
            if not to_recommend:
                return {
                    "status": "EXCELLENT", 
                    "message": "‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡πÅ‡∏ú‡∏ô‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏ö‡∏ö‡∏á‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏°‡πÅ‡∏Ç‡πá‡∏á‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏ó‡∏∏‡∏Å‡∏ß‡∏á‡∏à‡∏£ PDCA ‡πÅ‡∏•‡πâ‡∏ß",
                    "coaching_summary": "‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏ô‡πÅ‡∏ö‡∏ö (Best Practice)"
                }

            # 3. ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÇ‡∏´‡∏°‡∏î‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô Action Plan
            # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô (FAILED) ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ ACTION_PLAN_PROMPT (Remediation Mode)
            # ‡∏ñ‡πâ‡∏≤‡∏ú‡πà‡∏≤‡∏ô‡∏´‡∏°‡∏î‡πÅ‡∏ï‡πà‡∏°‡∏µ‡∏à‡∏∏‡∏î‡πÄ‡∏ï‡∏¥‡∏°‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ QUALITY_REFINEMENT_PROMPT
            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô Level 5 ‡∏´‡∏°‡∏î‡πÅ‡∏•‡πâ‡∏ß ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ EXCELLENCE_ADVICE_PROMPT
            
            target_level = self.config.target_level if hasattr(self, 'config') else 5
            
            # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Argument ‡∏´‡∏•‡∏±‡∏Å
            action_plan_args = {
                "recommendation_statements": to_recommend,
                "sub_id": sub_id,
                "sub_criteria_name": name,
                "enabler": enabler,
                "target_level": target_level,
                "llm_executor": self.llm,
                "logger": self.logger
            }

            # 4. ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏¢‡∏ô‡∏ï‡πå‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô (Ref. create_structured_action_plan ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ)
            # ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡πÉ‡∏ô create_structured_action_plan ‡∏à‡∏∞‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ä‡πâ PromptTemplate 
            # ‡∏ï‡∏≤‡∏° Mode ‡∏ó‡∏µ‡πà‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏à‡∏≤‡∏Å recommendation_statements
            
            roadmap = create_structured_action_plan(**action_plan_args)

            # 5. ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á
            if isinstance(roadmap, list) and len(roadmap) > 0:
                self.logger.info(f"‚úÖ Action Plan generated with {len(roadmap)} phases")
                return roadmap
            else:
                return _get_emergency_fallback_plan(sub_id, name, target_level, not has_major_gap, False, enabler)

        except Exception as e:
            self.logger.error(f"‚ö†Ô∏è Action Plan Generation Failed: {str(e)}", exc_info=True)
            return {
                "status": "ERROR",
                "message": f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥: {str(e)}",
                "fallback_plan": _get_emergency_fallback_plan(sub_id, name, 5, False, False, enabler)
            }
    
    def _prepare_worker_tuple(self, sub_data: Dict, document_map: Optional[Dict]) -> Tuple:
        return (
            sub_data,                          # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏ì‡∏ë‡πå (1.1, 1.2...)
            self.config.enabler,               # KM / IT / ...
            self.config.target_level,          # ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ L5
            self.config.mock_mode,             # none/random
            self.evidence_map_path,            # ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Å‡πá‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô
            self.config.model_name,            # llama3.1:70b ‡∏´‡∏£‡∏∑‡∏≠ 8b
            self.config.temperature,           # 0.0
            getattr(self.config, 'min_retry_score', 0.65),
            getattr(self.config, 'max_retrieval_attempts', 3),
            document_map or self.document_map, # ‡πÅ‡∏ú‡∏ô‡∏ú‡∏±‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
            getattr(self, 'ActionPlanActions', None),
            self.config.year,                  # 2567
            self.config.tenant                 # pea
        )

    def _integrate_worker_results(self, sub_result: Dict, temp_map: Dict):
        # 1. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÅ‡∏ú‡∏ô‡∏ú‡∏±‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô (Evidence Map)
        if isinstance(temp_map, dict):
            for level_key, evidence_list in temp_map.items():
                # ‡πÅ‡∏õ‡∏•‡∏á UUID ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏£‡∏¥‡∏á ‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Metadata
                resolved_list = self._resolve_evidence_filenames(evidence_list)
                self._normalize_evidence_metadata(resolved_list)
                
                # ‡∏¢‡∏±‡∏î‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏£‡∏∞‡πÄ‡∏õ‡πã‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á Main Process
                if level_key not in self.evidence_map:
                    self.evidence_map[level_key] = []
                self.evidence_map[level_key].extend(resolved_list)
        
        # 2. ‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏î‡∏¥‡∏ö‡∏à‡∏≤‡∏Å LLM
        if sub_result:
            self.raw_llm_results.extend(sub_result.get("raw_results_ref", []))
            self.final_subcriteria_results.append(sub_result)

    # ----------------------------------------------------------------------
    # üöÄ CORE WORKER: Assessment Execution (REVISED v2026.1.14 - FINAL STABLE)
    # ----------------------------------------------------------------------
    def _run_sub_criteria_assessment_worker(
        self,
        sub_criteria: Dict[str, Any],
        vectorstore_manager: Optional['VectorStoreManager'] = None
    ) -> Tuple[Dict[str, Any], Dict[str, List[Dict[str, Any]]]]:
        """
        [ADVANCED AUDITOR MODE v2026.1.14]
        - FIXED: 'tuple' object has no attribute 'get' (Robust Result Handling)
        - IMPROVED: PDCA Breakdown mapping directly from LLM response
        - OPTIMIZED: Prevention of redundant audit starts during retries
        """
        # 1. INITIALIZATION
        MAX_RETRY_ATTEMPTS = 2
        sub_id = sub_criteria['sub_id']
        sub_criteria_name = sub_criteria['sub_criteria_name']
        sub_weight = sub_criteria.get('weight', 0)
        
        current_enabler = getattr(self.config, 'enabler', 'KM')
        vsm = vectorstore_manager or getattr(self, 'vectorstore_manager', None)
        
        current_sequential_pass_level = 0 
        first_failure_level = None 
        raw_results_for_sub_seq: List[Dict[str, Any]] = []
        level_details_map = {} 
        start_ts = time.time() 

        self.logger.info(f"üßµ [WORKER START] {sub_id} | Mode: Phase-Based Sequential")
        all_rules_for_sub = getattr(self, 'contextual_rules_map', {}).get(sub_id, {})

        # -----------------------------------------------------------
        # 2. EVALUATION LOOP (L1 ‚Üí Target Level)
        # -----------------------------------------------------------
        levels_to_assess = sorted(sub_criteria.get('levels', []), key=lambda x: x.get('level', 0))

        for statement_data in levels_to_assess:
            level = statement_data.get('level')
            if level is None or level > self.config.target_level:
                continue
            
            # --- [EXECUTION with RETRY & SAFE UNPACKING] ---
            level_result = {}
            for attempt_num in range(1, MAX_RETRY_ATTEMPTS + 1):
                try:
                    # üîç ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏£‡∏≤‡∏¢‡πÄ‡∏•‡πÄ‡∏ß‡∏•
                    raw_res = self._run_single_assessment(
                        sub_criteria=sub_criteria,
                        statement_data=statement_data,
                        vectorstore_manager=vsm,
                        attempt=attempt_num,
                        record_id=self.current_record_id,
                        evidence_map=self.evidence_map,
                        **all_rules_for_sub.get(str(level), {})
                    )

                    # ‚ú® [CRITICAL FIX] ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Error Tuple ‡πÅ‡∏ö‡∏ö‡πÄ‡∏ö‡πá‡∏î‡πÄ‡∏™‡∏£‡πá‡∏à
                    if isinstance(raw_res, tuple):
                        # ‡∏ñ‡πâ‡∏≤‡∏™‡πà‡∏á‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô (dict, map) ‡∏´‡∏£‡∏∑‡∏≠ (dict,) ‡πÉ‡∏´‡πâ‡πÅ‡∏Å‡∏∞‡πÄ‡∏≠‡∏≤‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å
                        level_result = raw_res[0] if len(raw_res) > 0 else {}
                    elif isinstance(raw_res, dict):
                        level_result = raw_res
                    else:
                        self.logger.warning(f"‚ö†Ô∏è Unknown response format from L{level}: {type(raw_res)}")
                        level_result = {}

                    # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                    if level_result and "is_passed" in level_result:
                        break
                        
                except Exception as e:
                    self.logger.error(f"‚ùå {sub_id} L{level} Attempt {attempt_num} failed: {str(e)}")
                    if attempt_num == MAX_RETRY_ATTEMPTS:
                        level_result = {
                            "level": level, 
                            "is_passed": False, 
                            "reason": f"System Error: {str(e)}", 
                            "score": 0.0
                        }

            # Fallback level in case of failure
            if 'level' not in level_result: level_result['level'] = level

            # --- [SEQUENTIAL & GAP LOGIC] ---
            is_passed_llm = level_result.get('is_passed', False)
            level_result['raw_is_passed'] = is_passed_llm 

            if not is_passed_llm and first_failure_level is None:
                first_failure_level = level
                level_result.update({"display_status": "FAILED", "gap_type": "PRIMARY_GAP"})
            elif is_passed_llm and first_failure_level is not None:
                level_result.update({"display_status": "PASSED (CAPPED)", "gap_type": "SEQUENTIAL_GAP", "is_passed": False})
            elif not is_passed_llm and first_failure_level is not None:
                level_result.update({"display_status": "FAILED (GAP)", "gap_type": "COMPOUND_GAP"})
            else:
                current_sequential_pass_level = level
                level_result.update({"display_status": "PASSED", "gap_type": "NONE"})

            # --- [DATA MAPPING for UI & DASHBOARD] ---
            # ‡∏™‡∏Å‡∏±‡∏î‡∏Ñ‡πà‡∏≤ PDCA ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ‡∏´‡∏ô‡πâ‡∏≤ Dashboard (Router)
            pdca_raw = level_result.get('pdca_breakdown', {})
            pdca_final = {
                "P": float(pdca_raw.get('P', 0.0)),
                "D": float(pdca_raw.get('D', 0.0)),
                "C": float(pdca_raw.get('C', 0.0)),
                "A": float(pdca_raw.get('A', 0.0))
            }

            level_details_map[str(level)] = {
                "level": level,
                "is_passed": level_result.get('is_passed', False),
                "raw_is_passed": level_result.get('raw_is_passed', False),
                "score": level_result.get('score', 0.0),
                "pdca_breakdown": pdca_final, # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ß‡∏≤‡∏î‡∏Å‡∏£‡∏≤‡∏ü PDCA
                "reason": level_result.get('reason', ""),
                "summary_thai": level_result.get('summary_thai', ""),
                "coaching_insight": level_result.get('coaching_insight', ""),
                "display_status": level_result.get("display_status", "UNKNOWN"),
                "gap_type": level_result.get("gap_type", "NONE"),
                "evidences": level_result.get('temp_map_for_level', []), # ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå PDF/PNG
                "audit_confidence": level_result.get('audit_confidence', {})
            }
            raw_results_for_sub_seq.append(level_result)

        # -----------------------------------------------------------
        # 3. FINAL SYNTHESIS
        # -----------------------------------------------------------
        action_plan_result = self._generate_action_plan_safe(
            sub_id, sub_criteria_name, current_enabler, raw_results_for_sub_seq
        )
        
        weighted_score = round(self._calculate_weighted_score(current_sequential_pass_level, sub_weight), 2)
        current_sub_map = {k: v for k, v in self.evidence_map.items() if k.startswith(f"{sub_id}.")}

        final_output = {
            "sub_id": sub_id,
            "sub_criteria_id": sub_id,
            "sub_criteria_name": sub_criteria_name,
            "highest_pass_level": current_sequential_pass_level, 
            "level_details": level_details_map,
            "weight": sub_weight,
            "weighted_score": weighted_score,
            "target_level_achieved": current_sequential_pass_level >= self.config.target_level,
            "action_plan": action_plan_result, 
            "raw_results_ref": raw_results_for_sub_seq,
            "worker_duration_s": round(time.time() - start_ts, 2)
        }

        return final_output, current_sub_map

    def run_assessment(
        self,
        target_sub_id: str = "all",
        export: bool = False,
        vectorstore_manager: Optional['VectorStoreManager'] = None,
        sequential: bool = False,
        document_map: Optional[Dict[str, str]] = None,
        record_id: str = None,
    ) -> Dict[str, Any]:
        """
        [ULTIMATE ASSEMBLY v2026.6 - INTEGRATED]
        ‡∏£‡∏∞‡∏ö‡∏ö Orchestrator ‡∏ó‡∏µ‡πà‡∏ú‡∏™‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡∏à‡∏≤‡∏Å Main ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏â‡∏•‡∏≤‡∏î‡∏à‡∏≤‡∏Å PDCA Branch
        """
        start_ts = time.time()
        self.is_sequential = sequential
        self.current_record_id = record_id 

        # ============================== 1. ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô ==============================
        all_statements = self._flatten_rubric_to_statements()
        is_all = str(target_sub_id).lower() == "all"
        sub_criteria_list = all_statements if is_all else [
            s for s in all_statements if str(s.get('sub_id')).lower() == str(target_sub_id).lower()
        ]

        if not sub_criteria_list:
            return self._create_failed_result(record_id, f"Criteria '{target_sub_id}' not found", start_ts)

        self.logger.info(f"üéØ Target: {target_sub_id} | Record ID: {record_id}")

        # ============================== 2. ‡∏£‡∏∞‡∏ö‡∏ö Resumption (Load Baseline) ==============================
        # ‡∏î‡∏∂‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡∏´‡∏≤‡πÄ‡∏à‡∏≠ (Evidence Map) ‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏ä‡πà‡∏ß‡∏¢‡∏ï‡∏±‡πâ‡∏á‡∏ï‡πâ‡∏ô‡πÉ‡∏´‡πâ RAG
        self.evidence_map = {}
        loaded_data = self._load_evidence_map()
        if loaded_data:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö record_id ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢ (Origin Main Logic)
            if isinstance(loaded_data, dict) and loaded_data.get("record_id") == record_id:
                self.evidence_map = loaded_data.get("evidence_map", {})
                self.logger.info(f"üîÑ Resumed Evidence Map: {len(self.evidence_map)} keys loaded")

        # ============================== 3. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô (Parallel vs Sequential) ==============================
        max_workers = int(os.environ.get('MAX_PARALLEL_WORKERS', 4))
        run_parallel = is_all and not sequential
        
        self.raw_llm_results = []
        self.final_subcriteria_results = []
        results_list = []

        # ============================== 4. Execution Phase ==============================
        if run_parallel:
            # --- ‡πÇ‡∏´‡∏°‡∏î‡∏£‡∏±‡∏ô‡∏Ç‡∏ô‡∏≤‡∏ô (Parallel) ---
            self.logger.info(f"üöÄ Starting Parallel Assessment (Workers: {max_workers})")
            # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Argument ‡πÉ‡∏´‡πâ Worker (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ record_id ‡∏ï‡∏¥‡∏î‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢)
            worker_args = [self._prepare_worker_tuple(s, document_map) for s in sub_criteria_list]
            try:
                ctx = multiprocessing.get_context('spawn')
                with ctx.Pool(processes=max_workers) as pool:
                    results_list = pool.map(_static_worker_process, worker_args)
            except Exception as e:
                self.logger.critical(f"‚ùå Parallel execution failed: {e}")
                raise
        else:
            # --- ‡πÇ‡∏´‡∏°‡∏î‡∏£‡∏±‡∏ô‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (Sequential) ---
            self.logger.info(f"üßµ Starting Sequential Assessment: {target_sub_id}")
            vsm = vectorstore_manager or self._init_local_vsm()
            for sub_criteria in sub_criteria_list:
                # ‡∏™‡πà‡∏á record_id ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ _run_single_assessment ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ
                res = self._run_sub_criteria_assessment_worker(sub_criteria, vsm)
                results_list.append(res)

        # ============================== 5. Integration (Merge & Normalize) ==============================
        # ‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å Worker ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á Metadata
        new_merged_map = self.merge_evidence_mappings(results_list)
        
        for key, evidences in new_merged_map.items():
            # ‚ú® Normalize Metadata ‡∏Å‡πà‡∏≠‡∏ô‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å (New PDCA Branch Logic)
            self._normalize_evidence_metadata(evidences)
            
            if key not in self.evidence_map:
                self.evidence_map[key] = evidences
            else:
                # ‡∏Å‡∏±‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ã‡πâ‡∏≥ (Unique by doc_id/chunk_uuid)
                existing_ids = {str(e.get('doc_id') or e.get('chunk_uuid')) for e in self.evidence_map[key]}
                for ev in evidences:
                    if str(ev.get('doc_id') or ev.get('chunk_uuid')) not in existing_ids:
                        self.evidence_map[key].append(ev)

        # ‡πÅ‡∏¢‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå LLM ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏ó‡∏≥‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô
        for res in results_list:
            if isinstance(res, tuple) and len(res) == 2:
                sub_result, _ = res
                self.raw_llm_results.extend(sub_result.get("raw_results_ref", []))
                self.final_subcriteria_results.append(sub_result)

        # ============================== 6. Persistence (Save Baseline) ==============================
        if self.evidence_map:
            try:
                save_payload = {
                    "record_id": record_id,
                    "evidence_map": self.evidence_map,
                    "timestamp": datetime.now().isoformat()
                }
                self._save_evidence_map(map_to_save=save_payload)
                self.logger.info(f"‚úÖ Baseline Saved for Record: {record_id}")
            except Exception as e:
                self.logger.error(f"‚ùå Persistence failed: {e}")

        # ============================== 7. Final Summary & Analytics ==============================
        self._calculate_overall_stats(target_sub_id)
        
        # ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏° Coaching Insights ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏•‡πà‡∏°‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô (New Branch Feature)
        all_insights = []
        for res in self.final_subcriteria_results:
            details = res.get('level_details', {})
            for lvl, data in details.items():
                if data.get('coaching_insight'):
                    all_insights.append({
                        "sub_id": res.get('sub_id'),
                        "level": int(lvl),
                        "text": data['coaching_insight'],
                        "status": "passed" if data.get('is_passed') else "failed"
                    })
        self.total_stats['global_coaching_brief'] = all_insights

        return {
            "record_id": record_id,
            "summary": self.total_stats,
            "sub_criteria_results": self.final_subcriteria_results,
            "run_time_seconds": round(time.time() - start_ts, 2),
            "timestamp": datetime.now().isoformat(),
            "export_path": self._export_results(self.final_subcriteria_results, target_sub_id, record_id) if export else None
        }

    def _merge_worker_results(self, sub_result: Dict[str, Any], temp_map: Dict[str, List[Dict]]):
        """
        [INTERNAL HELPER] ‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å Worker ‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á Engine
        - ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£ Normalize Metadata ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡∏∞‡∏´‡∏ô‡πâ‡∏≤‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
        - ‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå LLM ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡πÇ‡∏¢‡∏á‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô (Evidence Map)
        """
        if not sub_result:
            return

        # 1. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Evidence Map (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏ô‡∏û‡∏ö)
        if temp_map and isinstance(temp_map, dict):
            for level_key, evidence_list in temp_map.items():
                if not evidence_list:
                    continue
                
                # Normalize Metadata ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ (‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå/‡∏´‡∏ô‡πâ‡∏≤) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö seam_prompts.py
                self._normalize_evidence_metadata(evidence_list)
                
                # ‡∏ô‡∏≥‡πÑ‡∏õ‡∏£‡∏ß‡∏°‡πÉ‡∏ô map ‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á Engine
                if level_key not in self.evidence_map:
                    self.evidence_map[level_key] = []
                self.evidence_map[level_key].extend(evidence_list)

        # 2. ‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏î‡∏¥‡∏ö‡∏à‡∏≤‡∏Å LLM (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Ref. ‡πÅ‡∏•‡∏∞ Debug)
        if "raw_results_ref" in sub_result:
            self.raw_llm_results.extend(sub_result["raw_results_ref"])

        # 3. ‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏£‡∏≤‡∏¢‡πÄ‡∏Å‡∏ì‡∏ë‡πå (Final Results per Sub-ID)
        self.final_subcriteria_results.append(sub_result)
        
        self.logger.debug(f"‚úÖ Merged results for {sub_result.get('sub_criteria_id', 'Unknown')}")
    
    def _run_expert_re_evaluation(
        self,
        sub_id: str,
        level: int,
        statement_text: str,
        context: str,
        first_attempt_reason: str,
        missing_tags: Union[List[str], Set[str]],
        highest_rerank_score: float,
        sub_criteria_name: str,
        llm_evaluator_to_use: Any,
        base_kwargs: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        [EXPERT LOOP v2026.45] 
        - ‡∏£‡∏∞‡∏ö‡∏ö‡∏≠‡∏∏‡∏ó‡∏ò‡∏£‡∏ì‡πå‡∏û‡∏¥‡πÄ‡∏®‡∏©‡πÄ‡∏°‡∏∑‡πà‡∏≠ Rerank score ‡∏™‡∏π‡∏á (>0.75) ‡πÅ‡∏ï‡πà LLM ‡πÉ‡∏´‡πâ‡∏ï‡∏Å
        - ‡πÉ‡∏ä‡πâ 'Substance over Form' ‡∏Å‡∏£‡∏∞‡∏ï‡∏∏‡πâ‡∏ô‡πÉ‡∏´‡πâ AI ‡∏°‡∏≠‡∏á‡∏´‡∏≤‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°‡∏à‡∏£‡∏¥‡∏á‡πÅ‡∏ó‡∏ô Keyword
        """
        self.logger.info(f"üîç [EXPERT-RE-EVAL] Triggered for {sub_id} L{level} | Rerank: {highest_rerank_score:.3f}")

        missing_str = ", ".join(sorted(set(missing_tags))) if missing_tags else "PDCA ‡∏ó‡∏µ‡πà‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô"

        hint_msg = f"""
        --- ‚ö†Ô∏è [EXPERT RE-ASSESSMENT NOTICE] ---
        ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏£‡∏≠‡∏ö‡πÅ‡∏£‡∏Å: "‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô" (‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•: {first_attempt_reason[:150]}...)
        
        ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏ö‡∏ö‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏∂‡∏Å:
        - ‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å (Highest Rerank: {highest_rerank_score:.4f})
        - ‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÉ‡∏ô‡∏£‡∏≠‡∏ö‡πÅ‡∏£‡∏Å: {missing_str}
        
        ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç:
        ‡πÇ‡∏õ‡∏£‡∏î‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÅ‡∏ö‡∏ö 'Substance over Form' ‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á 
        ‡∏´‡∏≤‡∏Å‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Ç‡∏≠‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ñ‡∏∂‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏à‡∏£‡∏¥‡∏á (Do) ‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏° (Check) ‡πÅ‡∏°‡πâ‡∏à‡∏∞‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡∏ï‡∏≤‡∏° Keyword ‡πÄ‡∏õ‡πä‡∏∞ 
        ‡∏ó‡πà‡∏≤‡∏ô‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ï‡∏≤‡∏°‡∏£‡∏∞‡∏î‡∏±‡∏ö Maturity ‡∏à‡∏£‡∏¥‡∏á‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡πÉ‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÑ‡∏î‡πâ
        """

        expert_kwargs = base_kwargs.copy()
        expert_kwargs["context"] = f"{context}\n\n{hint_msg}"
        expert_kwargs["sub_criteria_name"] = f"{sub_criteria_name} (Expert Re-assessment)"
        
        # ‡∏£‡∏≠‡∏ö‡∏™‡∏≠‡∏á‡πÉ‡∏ä‡πâ temperature 0 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
        expert_kwargs["temperature"] = 0.0

        try:
            re_eval_result = llm_evaluator_to_use(**expert_kwargs)
            
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏ß‡πà‡∏≤‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡∏∏‡∏ó‡∏ò‡∏£‡∏ì‡πå
            re_eval_result["is_expert_evaluated"] = True
            re_eval_result["original_fail_reason"] = first_attempt_reason
            
            if re_eval_result.get("is_passed", False):
                self.logger.info(f"üõ°Ô∏è [EXPERT-OVERRIDE] {sub_id} L{level} REVERSED to PASSED!")
                re_eval_result["reason"] = f"[Expert Pass]: {re_eval_result.get('reason', '')}"
            
            return re_eval_result
        except Exception as e:
            self.logger.error(f"üõë Expert Eval Error: {str(e)}")
            return {"is_passed": False, "score": 0.0, "reason": f"Expert Eval Failure: {str(e)}"}
    
    def _apply_diversity_filter(self, evidences: List[Dict], level: int) -> List[Dict]:
        if not evidences:
            return []

        sorted_evidences = sorted(
            evidences,
            key=lambda x: x.get('rerank_score', 0) or x.get('priority_score', 0),
            reverse=True
        )

        if level <= 2:
            return sorted_evidences[:20]  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 15 ‡πÄ‡∏õ‡πá‡∏ô 20 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏°‡∏µ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡πÄ‡∏´‡πá‡∏ô D

        diverse_results = []
        file_counts = defaultdict(int)
        per_file_limit = 5  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 4 ‡πÄ‡∏õ‡πá‡∏ô 5
        max_total = 30  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 25 ‡πÄ‡∏õ‡πá‡∏ô 30

        for ev in sorted_evidences:
            source = ev.get('metadata', {}).get('source_filename') or 'Unknown'
            source = os.path.basename(str(source))
            file_counts[source] += 1

            if file_counts[source] <= per_file_limit:
                diverse_results.append(ev)

            if len(diverse_results) >= max_total:
                break

        return diverse_results
    
    def _normalize_evidence_metadata(self, evidence_list: List[Dict[str, Any]]):
        """
        ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÉ‡∏´‡πâ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á Flattened ‡πÅ‡∏•‡∏∞ Nested Metadata 
        ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô (Export) ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà Error
        """
        for ev in evidence_list:
            if not isinstance(ev, dict):
                continue
                
            # 1. ‡∏î‡∏∂‡∏á Metadata ‡∏´‡∏•‡∏±‡∏Å (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
            meta = ev.get("metadata", {}) if isinstance(ev.get("metadata"), dict) else {}
            
            # 2. ‡∏õ‡∏£‡∏±‡∏ö Source (‡πÄ‡∏ä‡πá‡∏Ñ‡∏ó‡∏±‡πâ‡∏á‡∏ô‡∏≠‡∏Å‡πÅ‡∏•‡∏∞‡πÉ‡∏ô meta)
            raw_source = ev.get("source") or meta.get("source") or ev.get("source_filename") or meta.get("source_filename")
            ev["source"] = os.path.basename(str(raw_source)) if raw_source else "Unknown_File"
            
            # 3. ‡∏õ‡∏£‡∏±‡∏ö Page (‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô String ‡πÄ‡∏™‡∏°‡∏≠‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô JSON Error)
            raw_page = ev.get("page") or meta.get("page") or meta.get("page_label") or "N/A"
            ev["page"] = str(raw_page)
            
            # 4. ‡∏õ‡∏£‡∏±‡∏ö Relevance Score (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏´‡∏•‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠ Key)
            raw_score = ev.get("relevance_score") or ev.get("score") or meta.get("rerank_score") or 0.0
            try:
                ev["relevance_score"] = float(raw_score)
            except (ValueError, TypeError):
                ev["relevance_score"] = 0.0
            
            # 5. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö ID ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏¢‡∏á‡πÉ‡∏¢ Database)
            if not ev.get("stable_doc_uuid"):
                ev["stable_doc_uuid"] = ev.get("doc_id") or meta.get("stable_doc_uuid") or "unknown_uuid"
            
            # 6. ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ü‡∏¥‡∏•‡∏î‡πå‡πÅ‡∏™‡∏î‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏ö‡∏ö‡∏™‡∏ß‡∏¢‡πÜ (Optional - ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏ä‡∏ß‡πå‡πÉ‡∏ô Log)
            ev["source_filename"] = ev["source"]

        return evidence_list

    def relevance_score_fn(self, evidence: Dict[str, Any], sub_id: str, level: int) -> float:
        if not evidence:
            return 0.0

        # 1. Rerank (45%)
        rerank_raw = evidence.get('rerank_score') or evidence.get('score') or 0.0
        rerank_score = float(rerank_raw) if isinstance(rerank_raw, (int, float)) else 0.0
        normalized_rerank = min(max(rerank_score, 0.0), 1.0)

        # 2. ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        text = (evidence.get('text') or evidence.get('page_content') or '').lower().strip()
        meta = evidence.get('metadata', {}) if isinstance(evidence.get('metadata'), dict) else {}
        filename = (meta.get('source') or meta.get('source_filename') or '').lower()

        # 3. Cumulative rules
        cum_rules = self.get_cumulative_rules(sub_id, level)

        # 4. Source Grading
        source_bonus = 0.0
        primary = ["‡∏°‡∏ï‡∏¥", "‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å", "‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á", "‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®", "‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢", "‡πÅ‡∏ú‡∏ô‡πÅ‡∏°‡πà‡∏ö‡∏ó", "‡∏°‡∏ï‡∏¥‡∏ö‡∏≠‡∏£‡πå‡∏î"]
        secondary = ["assessment report", "‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô", "‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•", "‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏•"]
        if any(p in filename for p in primary):
            source_bonus += 0.25
        if any(p in filename for p in secondary):
            source_bonus -= 0.15

        # 5. Keyword Score (35%)
        target_kws = set()
        if level <= 2:
            target_kws.update(cum_rules.get('plan_keywords', []) + cum_rules.get('do_keywords', []))
        else:
            target_kws.update(cum_rules.get('check_keywords', []) + cum_rules.get('act_keywords', []))

        match_count = sum(1 for kw in target_kws if kw.lower() in text)
        expected = max(1, len(target_kws) * 0.25)
        keyword_score = min((match_count / expected) ** 0.5, 1.0)
        keyword_score = max(keyword_score, 0.15 if match_count >= 1 else 0.0)

        # 6. PDCA Tag Bonus (‡∏™‡∏°‡∏î‡∏∏‡∏• 0.30)
        pdca_bonus = 0.0
        pdca_tag = evidence.get('pdca_tag') or meta.get('pdca_tag')
        if pdca_tag and str(pdca_tag).upper() in {'P', 'D', 'C', 'A'}:
            pdca_bonus = 0.30

        # 7. Neighbor Bonus
        neighbor_bonus = 0.20 if evidence.get('is_neighbor', False) or meta.get('is_neighbor', False) else 0.0

        # 8. Specific Rule Bonus
        specific_rule = cum_rules.get('specific_contextual_rule', '').lower()
        rule_bonus = 0.20 if specific_rule and specific_rule in text else 0.0

        # 9. ‡∏£‡∏ß‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô (45% Rerank + 35% Keyword + 20% Bonuses)
        final_score = (
            0.45 * normalized_rerank +
            0.35 * keyword_score +
            source_bonus + pdca_bonus + neighbor_bonus + rule_bonus
        )

        # 10. Min floor ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö rerank ‡∏™‡∏π‡∏á (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô miss good evidence)
        if normalized_rerank > 0.80:
            final_score = max(final_score, 0.40)

        final_score = min(max(final_score, 0.0), 1.0)

        # 11. Logging (‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ ‡πÑ‡∏°‡πà‡πÄ‡∏â‡∏û‡∏≤‡∏∞ 1.2)
        if normalized_rerank > 0.75:
            self.logger.info(
                f"[HIGH-RERANK] {sub_id} L{level} | "
                f"rerank={rerank_score:.4f} | kw={keyword_score:.4f} | "
                f"pdca_bonus={pdca_bonus:.3f} | final={final_score:.4f} | "
                f"tag={pdca_tag} | text={text[:100]}..."
            )

        self.logger.debug(
            f"[{sub_id} L{level}] RelScore: {final_score:.4f} | Rerank: {normalized_rerank:.4f} | "
            f"KW: {keyword_score:.4f} | Src: {source_bonus:.3f} | PDCA: {pdca_bonus:.3f}"
        )

        return final_score

    def enhance_query_for_statement(
        self,
        statement_text: str,
        sub_id: str,
        statement_id: str,
        level: int,
        focus_hint: str = "",
    ) -> List[str]:
        """
        [REVISED STRATEGIC v2026] 
        ‡πÄ‡∏ô‡πâ‡∏ô‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Bias ‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏£‡∏Å ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏°‡∏ä‡∏±‡∏î‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö Maturity ‡∏™‡∏π‡∏á
        """
        logger = logging.getLogger(__name__)
        
        # 1. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏≠‡∏±‡∏ï‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô (Anchors)
        enabler_id = getattr(self.config, 'enabler', 'KM').upper()
        id_anchor = f"{enabler_id} {sub_id}"
        tenant_name = getattr(self.config, 'tenant', 'PEA').upper()
        
        # 2. ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏î‡∏¥‡∏ö Keywords ‡∏à‡∏≤‡∏Å Rules ‡πÅ‡∏ö‡∏ö Cumulative
        raw_kws = []
        
        # ‡∏î‡∏∂‡∏á‡∏Å‡∏é‡πÄ‡∏´‡∏•‡πá‡∏Å‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡∏Ç‡πâ‡∏≠
        must_list = self.get_rule_content(sub_id, level, "must_include_keywords")
        if isinstance(must_list, list): raw_kws.extend(must_list)
        
        # üü¢ Strategic Selection: ‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ï‡∏≤‡∏°‡∏£‡∏∞‡∏î‡∏±‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏ô‡∏µ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏´‡∏ô‡πâ‡∏≤ 1-2
        if level <= 2:
            # L1-L2: ‡πÄ‡∏ô‡πâ‡∏ô‡πÅ‡∏ú‡∏ô‡πÅ‡∏•‡∏∞‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢
            raw_kws.extend(self.get_rule_content(sub_id, 1, "plan_keywords") or [])
            raw_kws.extend(self.get_rule_content(sub_id, 2, "do_keywords") or [])
        else:
            # L3-L5: ‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡πÅ‡∏•‡∏∞‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå (‚ùå ‡∏à‡∏á‡πÉ‡∏à‡πÑ‡∏°‡πà‡πÄ‡∏≠‡∏≤ plan_keywords)
            raw_kws.extend(self.get_rule_content(sub_id, 2, "do_keywords") or [])
            raw_kws.extend(self.get_rule_content(sub_id, 3, "check_keywords") or [])
            if level >= 4:
                raw_kws.extend(self.get_rule_content(sub_id, 4, "act_keywords") or [])
            if level >= 5:
                raw_kws.extend(self.get_rule_content(sub_id, 5, "act_keywords") or [])

        # ‡∏•‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ã‡πâ‡∏≥‡πÅ‡∏•‡∏∞‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≥‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ Query ‡∏Å‡∏£‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢
        clean_kws_list = sorted(list(set(str(k).strip() for k in raw_kws if k)))
        keywords_str = " ".join(clean_kws_list[:12])

        # 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î Queries ‡πÅ‡∏ö‡∏ö Diversified
        queries = []
        # ‡∏ï‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏¢‡∏≤‡∏¢ '‡πÄ‡∏ä‡πà‡∏ô' ‡∏≠‡∏≠‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Embedding ‡∏à‡∏±‡∏ö‡πÉ‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô
        clean_stmt = statement_text.split("‡πÄ‡∏ä‡πà‡∏ô")[0].strip()

        # Query 1: Core Precision (Anchor + Statement + Keywords)
        queries.append(f"{id_anchor} {clean_stmt} {keywords_str}")

        # Query 2: Evidence Type Targeting (‡∏£‡∏∞‡∏ö‡∏∏‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô)
        if level <= 2:
            queries.append(f"‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏® ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á ‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥ {id_anchor} {keywords_str}")
        else:
            # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏´‡∏≤‡∏†‡∏≤‡∏Ñ‡∏ú‡∏ô‡∏ß‡∏Å‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏•
            queries.append(f"‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏• KPI ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥ ‡∏†‡∏≤‡∏Ñ‡∏ú‡∏ô‡∏ß‡∏Å ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÅ‡∏ô‡∏ö‡∏ó‡πâ‡∏≤‡∏¢ {id_anchor} {keywords_str}")

        # Query 3: Organization Context (Tenant Specific)
        queries.append(f"{tenant_name} {id_anchor} {clean_stmt}")

        # Query 4: PDCA Synonyms (‡∏à‡∏≤‡∏Å Global Vars)
        synonyms = ""
        try:
            from config.global_vars import PDCA_LEVEL_SYNONYMS
            synonyms = PDCA_LEVEL_SYNONYMS.get(level, "")
        except ImportError:
            fallback = {1: "‡πÅ‡∏ú‡∏ô", 2: "‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥", 3: "‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô", 4: "‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á", 5: "‡∏¢‡∏±‡πà‡∏á‡∏¢‡∏∑‡∏ô"}
            synonyms = fallback.get(level, "")
        
        if synonyms:
            queries.append(f"{id_anchor} {synonyms} {keywords_str}")

        # Query 5: Advanced Maturity (L4-L5 ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô)
        if level >= 4:
            queries.append(f"‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏° Best Practice ‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö Lesson Learned {id_anchor}")

        # 4. Final Processing & Truncation
        final_queries = []
        seen = set()
        for q in queries:
            q_norm = " ".join(q.split()[:25]) # ‡∏à‡∏≥‡∏Å‡∏±‡∏î 25 ‡∏Ñ‡∏≥‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏°‡∏Ç‡∏≠‡∏á Rerank
            if q_norm and q_norm not in seen:
                final_queries.append(q_norm)
                seen.add(q_norm)
        
        logger.info(f"üöÄ [Query Gen] {sub_id} L{level} | Generated {len(final_queries[:5])} refined queries.")
        return final_queries[:5]
    
    def _get_semantic_tag(self, text: str, sub_id: str, level: int, filename: str = "") -> str:
        """
        [ULTIMATE REVISE v2026.12] 
        - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤ "‡∏î‡∏π‡∏á‡∏≤‡∏ô/‡∏≠‡∏ö‡∏£‡∏°" ‡∏´‡∏•‡∏∏‡∏î‡πÑ‡∏õ‡πÄ‡∏õ‡πá‡∏ô P ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Filename + Linguistic Analysis
        - ‡πÉ‡∏ä‡πâ Strict Zero-Tolerance ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å‡πÅ‡∏¢‡∏∞ Plan ‡πÅ‡∏•‡∏∞ Do
        """
        # 1. ‡∏ô‡∏¥‡∏¢‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏ô‡πâ‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á '‡πÄ‡∏à‡∏ï‡∏ô‡∏≤' (P) ‡πÅ‡∏•‡∏∞ '‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏õ‡∏£‡∏∞‡∏à‡∏±‡∏Å‡∏©‡πå' (D)
        system_prompt = """
        You are a KM Audit Specialist for PEA. Classify the text into P, D, C, or A.
        
        STRICT RULES:
        - P (Plan): "‡∏™‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏ï‡πâ‡∏ô" - ‡πÅ‡∏ú‡∏ô‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£, ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÅ‡∏ï‡πà‡∏á‡∏ï‡∏±‡πâ‡∏á, ‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå, ‡∏á‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì, ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ó‡∏≥.
        - D (Do): "‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏à‡∏£‡∏¥‡∏á" - **‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ '‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•', '‡∏†‡∏≤‡∏û‡∏ñ‡πà‡∏≤‡∏¢', '‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°', '‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏π‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡∏£‡πà‡∏ß‡∏°', '‡∏î‡∏π‡∏á‡∏≤‡∏ô' ‡∏´‡∏£‡∏∑‡∏≠ '‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô‡πÅ‡∏•‡πâ‡∏ß' ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö D ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô**
        - C (Check): "‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô" - ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô KPI, ‡∏ú‡∏•‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏∂‡∏á‡∏û‡∏≠‡πÉ‡∏à, ‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏∑‡∏ö‡∏´‡∏ô‡πâ‡∏≤.
        - A (Act): "‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á" - ‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö (AAR), ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞.
        """
        
        user_prompt = f"""
        Analyze this KM Evidence:
        ---
        Source Filename: "{filename}"
        Text: "{text[:800]}"
        ---
        CRITICAL CHECK:
        - ‡∏´‡∏≤‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ "‡∏î‡∏π‡∏á‡∏≤‡∏ô", "‡∏†‡∏≤‡∏û‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°", "‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£..." ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏´‡πá‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û (.png, .jpg) -> **‡∏ï‡∏≠‡∏ö "D" ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ**
        - ‡∏´‡∏≤‡∏Å‡πÄ‡∏õ‡πá‡∏ô "‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á" ‡∏´‡∏£‡∏∑‡∏≠ "‡πÅ‡∏ú‡∏ô" -> **‡∏ï‡∏≠‡∏ö "P"**
        
        Return ONLY JSON: {{"tag": "P/D/C/A/Other", "reason": "thai_reason"}}
        """
            
        try:
            # ‡πÉ‡∏ä‡πâ temperature=0 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
            response_json_str = _fetch_llm_response(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                llm_executor=self.llm,
                max_retries=2
            )
            
            import json
            data = json.loads(response_json_str)
            tag = data.get('tag', 'Other').strip().upper()
            
            # Validation logic
            valid_tags = ['P', 'D', 'C', 'A']
            return tag if tag in valid_tags else 'Other'
            
        except Exception as e:
            self.logger.error(f"[SEMANTIC-TAG-ERROR] {sub_id} L{level}: {str(e)}")
            return 'Other'
        
    def _build_pdca_context(self, blocks: Dict[str, str]) -> str:
        """
        [REVISE] ‡∏£‡∏ß‡∏° PDCA Blocks ‡∏à‡∏≤‡∏Å Dictionary ‡πÄ‡∏õ‡πá‡∏ô XML Context
        ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Key: Plan, Do, Check, Act, Other
        """
        tags = ["Plan", "Do", "Check", "Act", "Other"]
        xml_parts = []
        for t in tags:
            content = blocks.get(t, "N/A")
            xml_parts.append(f"<{t}>\n{content}\n</{t}>")
        return "\n".join(xml_parts)

    def _log_pdca_status(self, sub_id, name, level, blocks, req_phases, sources_count, score, conf_level, **kwargs):
        """
        [ULTIMATE LOG v2026.1.14] 
        - ‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤‡πÅ‡∏™‡∏î‡∏á Level Name (‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏∏‡πà‡∏á‡∏°‡∏±‡πà‡∏ô...) ‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô
        - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Argument ‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡∏î‡πâ‡∏ß‡∏¢ **kwargs (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Crash)
        - ‡∏à‡∏±‡∏î Format ‡πÑ‡∏≠‡∏Ñ‡∏≠‡∏ô PDCA ‡πÉ‡∏´‡πâ‡∏™‡πÅ‡∏Å‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏™‡∏≤‡∏¢‡∏ï‡∏≤‡∏á‡πà‡∏≤‡∏¢
        """
        try:
            # 1. üõ°Ô∏è Guard & Format Level Name
            # ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤ name (‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏£‡∏≤‡∏™‡πà‡∏á stmt ‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å)
            raw_name = str(name) if name else "No Level Statement Defined"
            # ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏ñ‡πâ‡∏≤‡πÄ‡∏Å‡∏¥‡∏ô 60 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Log ‡πÑ‡∏°‡πà‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠
            display_name = (raw_name[:57] + "...") if len(raw_name) > 60 else raw_name

            # 2. üõ°Ô∏è Guard Blocks (PDCA Results)
            if not isinstance(blocks, dict):
                blocks = {}

            # 3. üõ°Ô∏è Guard & Format Required Phases
            if not isinstance(req_phases, list):
                req_phases = [str(req_phases)]
            req_str = f"[{','.join(map(str, req_phases))}]"

            # 4. ‚öôÔ∏è Build PDCA Icons (P D C A)
            mapping = [("Plan", "P"), ("Do", "D"), ("Check", "C"), ("Act", "A")]
            icons_list = []
            for full_phase, short_phase in mapping:
                content = blocks.get(full_phase, "")
                # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà "N/A"
                is_valid = content and str(content).strip().upper() != "N/A"
                status_icon = "‚úÖ" if is_valid else "‚ùå"
                icons_list.append(f"{short_phase}:{status_icon}")
            
            icons_str = " ".join(icons_list)

            # 5. üè∑Ô∏è Extra Metadata (‡πÄ‡∏ä‡πà‡∏ô ‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏´‡∏•‡∏±‡∏Å 1.2)
            rubric_title = kwargs.get('rubric_name', '')
            extra_info = f" | {rubric_title}" if rubric_title else ""

            # üìä [FINAL OUTPUT] ‡∏û‡πà‡∏ô Log ‡∏™‡∏£‡∏∏‡∏õ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏±‡∏î‡πÄ‡∏õ‡πä‡∏∞‡πÜ
            self.logger.info(
                f"üìä [PDCA-STATUS] {sub_id} L{level} | {display_name}{extra_info} | "
                f"Req:{req_str} | Res:{icons_str} | "
                f"Docs:{sources_count} | Score:{score:.4f} | Conf:{conf_level}"
            )

        except Exception as e:
            # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏Å‡∏≤‡∏£ Log ‡∏ó‡∏≥‡∏£‡∏∞‡∏ö‡∏ö‡∏û‡∏±‡∏á ‡πÅ‡∏ï‡πà‡πÉ‡∏´‡πâ‡πÅ‡∏à‡πâ‡∏á Error ‡πÑ‡∏ß‡πâ
            self.logger.error(f"‚ùå Critical Error in _log_pdca_status: {e}")

    def _perform_adaptive_retrieval(self, sub_id, level, stmt, vectorstore_manager):
        """ 
        [STRATEGIC REVISE v2026.Expert] 
        - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Priority Documents (User-Specified) ‡πÅ‡∏ö‡∏ö 100%
        - ‡∏ú‡∏™‡∏≤‡∏ô‡∏£‡∏∞‡∏ö‡∏ö Discovery ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏°‡∏≤‡πÄ‡∏ï‡∏¥‡∏°‡πÄ‡∏ï‡πá‡∏° PDCA ‡∏ó‡∏µ‡πà‡∏Ç‡∏≤‡∏î‡∏´‡∏≤‡∏¢
        - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏∞‡∏ö‡∏ö Early Exit ‡πÅ‡∏•‡∏∞ Metadata Reinforcement
        """
        # 1. ‡∏î‡∏∂‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà User ‡∏£‡∏∞‡∏ö‡∏∏‡∏°‡∏≤ (Priority) ‡∏´‡∏£‡∏∑‡∏≠‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£ Mapping ‡πÑ‡∏ß‡πâ‡∏•‡πà‡∏ß‡∏á‡∏´‡∏ô‡πâ‡∏≤
        # mapped_ids: ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡∏Å‡∏£‡∏≠‡∏á‡πÉ‡∏ô VectorStore, priority_docs: ‡∏Å‡πâ‡∏≠‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô
        mapped_ids, priority_docs = self._get_mapped_uuids_and_priority_chunks(
            sub_id, level, stmt, vectorstore_manager
        )
        
        candidates = []
        final_max_rerank = 0.0
        
        # ‡πÉ‡∏™‡πà‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡πÅ‡∏™‡∏î‡∏á‡πÉ‡∏ô Log ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡∏°‡∏µ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ "‡∏™‡∏±‡πà‡∏á‡∏ï‡∏£‡∏ß‡∏à" ‡∏°‡∏≤
        if priority_docs:
            self.logger.info(f"üìå [TARGETED-AUDIT] Found {len(priority_docs)} priority chunks for {sub_id} L{level}")
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤ Rerank ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏à‡∏≤‡∏Å Priority Docs ‡∏Å‡πà‡∏≠‡∏ô
            if any(p.get('rerank_score') for p in priority_docs):
                final_max_rerank = max((float(p.get('rerank_score', 0)) for p in priority_docs), default=0.0)

        # 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á Queries ‡πÄ‡∏û‡∏∑‡πà‡∏≠ "‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏û‡∏¥‡πà‡∏°" (Discovery Mode)
        # ‡πÅ‡∏°‡πâ User ‡∏à‡∏∞‡πÉ‡∏´‡πâ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß ‡πÅ‡∏ï‡πà‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡∏´‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡∏ó‡∏µ‡πà "‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤" ‡∏´‡∏£‡∏∑‡∏≠ "‡∏°‡∏≤‡πÄ‡∏ï‡∏¥‡∏°‡πÄ‡∏ï‡πá‡∏°" ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        queries = self.enhance_query_for_statement(stmt, sub_id, f"{sub_id}.L{level}", level)
        
        # 3. Retrieval Loop (Adaptive 3-Loop)
        for i, q in enumerate(queries[:3]):
            # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏î‡∏¢‡πÄ‡∏õ‡∏¥‡∏î‡∏Å‡∏ß‡πâ‡∏≤‡∏á (‡πÅ‡∏ï‡πà‡∏ñ‡πâ‡∏≤‡∏°‡∏µ mapped_ids ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡∏™‡∏ô‡πÉ‡∏à‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©)
            res = self.rag_retriever(
                query=q, 
                doc_type=self.doc_type, 
                sub_id=sub_id, 
                level=level,
                vectorstore_manager=vectorstore_manager, 
                stable_doc_ids=mapped_ids 
            )
            
            loop_docs = res.get("top_evidences", [])
            
            if loop_docs:
                # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
                current_max = max((float(c.get('rerank_score', 0)) for c in loop_docs), default=0.0)
                final_max_rerank = max(final_max_rerank, current_max)
                
                # ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ö Priority ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß
                new_docs = [
                    d for d in loop_docs 
                    if d.get('chunk_uuid') not in [p.get('chunk_uuid') for p in priority_docs]
                ]
                candidates.extend(new_docs)
            
            # ‚ú® Early Exit Logic: ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á‡πÄ‡∏Å‡∏¥‡∏ô 0.88 ‡πÅ‡∏•‡∏∞‡πÑ‡∏î‡πâ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏°‡∏≤‡∏Å‡∏û‡∏≠‡πÅ‡∏•‡πâ‡∏ß ‡πÉ‡∏´‡πâ‡∏´‡∏¢‡∏∏‡∏î‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏°
            if final_max_rerank >= 0.88 and len(candidates) >= 10:
                self.logger.info(f"üéØ High relevance found ({final_max_rerank:.4f}). Optimizing speed by stopping loop.")
                break

        # 4. Final Integration & Scoring Reinforcement
        # ‡∏ô‡∏≥‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà User ‡∏™‡∏±‡πà‡∏á (Priority) ‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ö‡∏ó‡∏µ‡πà AI ‡∏´‡∏≤‡∏°‡∏≤‡πÑ‡∏î‡πâ (Discovery)
        all_retrieved = priority_docs + candidates
        
        # ‡πÄ‡∏™‡∏£‡∏¥‡∏° Metadata ‡πÉ‡∏´‡πâ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏ö‡∏ô UI (‡πÄ‡∏ä‡πà‡∏ô ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå, ‡∏´‡∏ô‡πâ‡∏≤)
        self._normalize_evidence_metadata(all_retrieved)
        
        # 5. Safety Net: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Priority Docs ‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏¢‡∏π‡πà‡∏Ñ‡∏£‡∏ö (‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å Diversify ‡∏ó‡∏¥‡πâ‡∏á‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ñ‡∏±‡∏î‡πÑ‡∏õ)
        for p in priority_docs:
            p['is_priority'] = True # ‡∏ó‡∏≥ Tag ‡∏û‡∏¥‡πÄ‡∏®‡∏©‡πÑ‡∏ß‡πâ
            p['rerank_score'] = max(p.get('rerank_score', 0), 0.70) # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö Floor Score ‡πÉ‡∏´‡πâ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ï‡∏±‡πâ‡∏á‡πÉ‡∏à‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏°‡∏≤

        self.logger.info(f"üèÅ Retrieval Finished: Total {len(all_retrieved)} units (Priority: {len(priority_docs)}, Discovered: {len(candidates)})")

        return all_retrieved, final_max_rerank

    def _run_single_assessment(
        self, 
        sub_criteria: Dict[str, Any], 
        statement_data: Dict[str, Any], 
        vectorstore_manager: Optional['VectorStoreManager'], 
        **kwargs
    ) -> Dict[str, Any]:
        """ [ULTIMATE FINISH - v2026.1.14] """
        start_time = time.time()
        
        # --- üõ°Ô∏è 1. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ñ‡πà‡∏≤‡πÅ‡∏•‡∏∞‡∏î‡∏∂‡∏á‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡πÄ‡∏Å‡∏ì‡∏ë‡πå ---
        sub_id = str(sub_criteria.get('sub_id', 'Unknown'))
        level = statement_data.get('level', 1)
        level_idx = str(level)
        name = str(sub_criteria.get('name', sub_criteria.get('sub_criteria_name', 'No Title')))

        levels_map = sub_criteria.get('levels', {})
        target_val = levels_map.get(level_idx, "") if isinstance(levels_map, dict) else ""
        if isinstance(target_val, dict):
            stmt = str(target_val.get('statement', ''))
        else:
            stmt = str(target_val) or f"‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏£‡∏∞‡∏î‡∏±‡∏ö {level}"

        self.logger.info(f"üöÄ [AUDIT START] {sub_id} L{level} | {name}")
        self.logger.info(f"üìå ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡∏µ‡πâ: {stmt}") 

        # --- üõ°Ô∏è 2. Retrieval & Rules ---
        try:
            all_candidates, raw_max_score = self._perform_adaptive_retrieval(sub_id, level, stmt, vectorstore_manager)
        except:
            all_candidates, raw_max_score = [], 0.0

        rules_map = getattr(self, 'contextual_rules_map', {})
        current_rules = rules_map.get(sub_id, {}).get(level_idx, {}) if isinstance(rules_map, dict) else {}

        # --- üõ°Ô∏è 3. ‡∏Å‡∏£‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏° Blocks (‡∏à‡∏∏‡∏î‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ Res: ‚úÖ) ---
        diverse_docs = self._apply_diversity_filter(all_candidates, level)
        
        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏´‡∏°‡πà (‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô Dict)
        blocks = self._get_pdca_blocks_from_evidences(diverse_docs, None, level, sub_id, rules_map)

        # --- üõ°Ô∏è 4. ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏• Log ---
        req_phases = current_rules.get('require_phase') or (['P','D'] if level <= 2 else ['P','D','C'])
        display_score = raw_max_score if raw_max_score > 0 else (0.85 if diverse_docs else 0.0)

        if hasattr(self, '_log_pdca_status'):
            self._log_pdca_status(
                sub_id=sub_id, name=stmt, level=level, blocks=blocks, 
                req_phases=req_phases, sources_count=len(diverse_docs), 
                score=display_score, conf_level="High", rubric_name=name
            )

        # --- üõ°Ô∏è 5. LLM Evaluation ---
        ctx = self._build_pdca_context(blocks) 
        eval_fn = evaluate_with_llm_low_level if level <= 2 else evaluate_with_llm
        res = eval_fn(
            context=f"{ctx}\n\n{self._get_level_constraint_prompt(sub_id, level)}", 
            sub_criteria_name=name, level=level, statement_text=stmt, 
            sub_id=sub_id, llm_executor=self.llm, require_phase=req_phases
        )
        
        # Final Guard
        res = self.post_process_llm_result(res, level, sub_id=sub_id)
        if not hasattr(self, 'level_details_map'): self.level_details_map = {}
        self.level_details_map[str(level)] = res

        return {
            "sub_criteria_id": sub_id, "level": level, "score": res.get('score', display_score),
            "is_passed": res.get('is_passed', False), "reason": res.get('reason', ""),
            "duration": round(time.time() - start_time, 2)
        }