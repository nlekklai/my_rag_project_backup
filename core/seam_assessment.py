# -*- coding: utf-8 -*-
# core/seam_assessment.py

import sys
import json
import logging
import time
from datetime import datetime, date
import os
from typing import List, Dict, Any, Optional, Union, Tuple, Set, Final, Literal
from collections import defaultdict, OrderedDict
from dataclasses import dataclass, field
import multiprocessing 
from functools import partial
import pathlib, uuid
from copy import deepcopy
import tempfile
import shutil
import re
import hashlib
import unicodedata 
import random

from core.json_extractor import _robust_extract_json, _robust_extract_json_list


# -------------------- 5. LOGGER SETUP --------------------
logger = logging.getLogger(__name__)
if not logger.handlers:
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")


# -------------------- 1. PROTECTIVE IMPORTS --------------------
# ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á FileLock ‡πÅ‡∏•‡∏∞ Database ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏™‡πà‡∏ß‡∏ô‡∏≠‡∏∑‡πà‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏ä‡∏±‡∏ß‡∏£‡πå‡πÜ
try:
    from filelock import FileLock
except ImportError:
    class FileLock:
        def __init__(self, *args, **kwargs): pass
        def __enter__(self): return self
        def __exit__(self, *args): pass
    print("‚ö†Ô∏è WARNING: 'filelock' not installed.")

try:
    from database import init_db, db_update_task_status
    update_db_core = db_update_task_status
except ImportError:
    def init_db(): pass
    def update_db_core(*args, **kwargs): pass

# -------------------- 2. PATH SETUP --------------------
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

# -------------------- 3. CORE LOGIC & CONFIG IMPORTS --------------------
try:
    # --- Configs ---
    from config.global_vars import (
        MAX_LEVEL, EVIDENCE_DOC_TYPES, RERANK_THRESHOLD, MAX_EVI_STR_CAP,
        DEFAULT_LLM_MODEL_NAME, LLM_TEMPERATURE, MIN_RETRY_SCORE,
        MAX_PARALLEL_WORKERS, PDCA_PRIORITY_ORDER, TARGET_DEVICE,
        PDCA_PHASE_MAP, INITIAL_TOP_K, FINAL_K_RERANKED,
        MAX_CHUNKS_PER_FILE, MAX_CHUNKS_PER_BLOCK, MATURITY_LEVEL_GOALS,
        SEAM_ENABLER_FULL_NAME_TH, SEAM_ENABLER_FULL_NAME_EN,
        SCORING_MODE, MAX_CHUNKS_LOW, MAX_CHUNKS_HIGH,
        CRITICAL_CA_THRESHOLD, EVIDENCE_SELECTION_STRATEGY, EVIDENCE_CUMULATIVE_CAP,
        GLOBAL_EVIDENCE_INSTRUCTION, DEFAULT_ENABLER, PDCA_CONFIG_MAP, PDCA_PHASE_DESCRIPTIONS,
        SEAM_ENABLER_FULL_NAME_TH, ANALYSIS_FINAL_K, RETRIEVAL_RERANK_FLOOR, 
        RETRIEVAL_EARLY_EXIT_COUNT, RETRIEVAL_HIGH_RERANK_THRESHOLD,
        RETRIEVAL_EARLY_EXIT_SCORE_THRESHOLD, RETRIEVAL_RELEVANCE_THRESHOLD,
        DEFAULT_TENANT
    )
    

    # --- Utilities ---
    from core.llm_data_utils import ( 
        evaluate_with_llm, retrieve_context_with_filter, 
        action_plan_normalize_keys, evaluate_with_llm_low_level, 
        LOW_LEVEL_K, create_context_summary_llm, _fetch_llm_response,
        _check_and_handle_empty_context, set_mock_control_mode as set_llm_data_mock_mode
    )
    from core.vectorstore import VectorStoreManager, load_all_vectorstores
    from core.retry_policy import RetryPolicy, RetryResult  # <-- ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å
    from core.json_extractor import _robust_extract_json

    # --- Path Utils ---
    from utils.path_utils import (
        get_mapping_file_path, get_evidence_mapping_file_path, 
        get_contextual_rules_file_path, get_assessment_export_file_path,
        get_export_dir, get_rubric_file_path, _n, get_doc_type_collection_key,
        get_tenant_info_file_path
    )

    # --- Prompts ---
    try:
        from core.seam_prompts import (
            ATOMIC_ACTION_PROMPT, SUB_ROADMAP_PROMPT,
            SYSTEM_ATOMIC_ACTION_PROMPT, SYSTEM_SUB_ROADMAP_PROMPT,
            STRATEGIC_OVERALL_PROMPT, SYSTEM_OVERALL_STRATEGIC_PROMPT
        )
    except ImportError:
        ATOMIC_ACTION_PROMPT = "Recommendation: {coaching_insight} Level: {level}"
        SUB_ROADMAP_PROMPT = "Roadmap: {aggregated_insights}"
        STRATEGIC_OVERALL_PROMPT = "Overall: {aggregated_context}"
        SYSTEM_ATOMIC_ACTION_PROMPT = "Assistant mode."
        SYSTEM_SUB_ROADMAP_PROMPT = "Strategy mode."

except ImportError as e:
    # üö® EMERGENCY FALLBACK: ‡∏î‡πà‡∏≤‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ üö®
    print(f"‚ö†Ô∏è EMERGENCY: Core Module missing, initializing safety fallbacks: {e}")
    
    # Constants
    MAX_LEVEL = 5
    EVIDENCE_DOC_TYPES = "evidence"
    LOW_LEVEL_K = 5  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ï‡∏±‡∏ß‡∏ô‡∏µ‡πâ
    
    # [FIX] Class/Policy
    class RetryResult:
        def __init__(self, data=None): self.data = data or {}
        def get(self, k, d=None): return self.data.get(k, d)
    class RetryPolicy:
        def __init__(self, *args, **kwargs): pass
        def execute(self, func, *args, **kwargs): return RetryResult(func(*args, **kwargs))

    # [FIX] Path Functions
    def _n(s): return str(s).lower().strip()
    def get_rubric_file_path(tenant, enabler, **kwargs): 
        return f"data_store/{_n(tenant)}/config/{_n(tenant)}_{_n(enabler)}_rubric.json"
    def get_contextual_rules_file_path(tenant, enabler, **kwargs):
        return f"data_store/{_n(tenant)}/config/{_n(tenant)}_{_n(enabler)}_contextual_rules.json"
    def get_evidence_mapping_file_path(tenant, year, enabler, **kwargs):
        return f"data_store/{_n(tenant)}/mapping/{year}/{_n(tenant)}_{year}_{_n(enabler)}_evidence_mapping.json"
    def get_mapping_file_path(doc_type, tenant, year=None, enabler=None, **kwargs):
        return f"data_store/{_n(tenant)}/mapping/{year}/{_n(tenant)}_{year}_{_n(enabler)}_doc_id_mapping.json"
    def get_assessment_export_file_path(*args, **kwargs): return "exports/temp_report.json"
    def get_export_dir(*args, **kwargs): return "exports"

    # [FIX] Logic Functions (‡∏î‡πà‡∏≤‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏ö‡∏ñ‡∏≤‡∏°‡∏´‡∏≤)
    def evaluate_with_llm_low_level(*args, **kwargs): 
        return {"score": 0.0, "reason": "Fallback mode active", "is_passed": False}
    
    def evaluate_with_llm(*args, **kwargs): 
        return {"score": 0.0, "reason": "Fallback mode active", "is_passed": False}

    def retrieve_context_with_filter(*args, **kwargs): 
        return {"top_evidences": [], "aggregated_context": ""}


    # Placeholders
    def _fetch_llm_response(*args, **kwargs): return "{}"
    def _robust_extract_json(t): return {}
    def set_llm_data_mock_mode(m): pass
    def action_plan_normalize_keys(d): return d
    def create_context_summary_llm(*args, **kwargs): return {"summary": "N/A", "coaching": "N/A"}
    def _check_and_handle_empty_context(*args, **kwargs): return None, False

    ATOMIC_ACTION_PROMPT = "Level {level}: {coaching_insight}"
    SUB_ROADMAP_PROMPT = "Roadmap: {aggregated_insights}"
    SYSTEM_ATOMIC_ACTION_PROMPT = "Assistant"
    SYSTEM_SUB_ROADMAP_PROMPT = "Strategist"

# try:
#     from core.seam_prompts import SUB_ROADMAP_TEMPLATE
#     logger.info("[PROMPT] Loaded SUB_ROADMAP_TEMPLATE from core.seam_prompts (real version)")
# except ImportError as e:
#     logger.warning(f"[PROMPT] Import failed ({str(e)}), using fallback SUB_ROADMAP_TEMPLATE")
#     SUB_ROADMAP_TEMPLATE = """
# ### [Strategic Context]
# - ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠: {sub_criteria_name} ({sub_id}) | Enabler: {enabler}
# - ‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á‡πÄ‡∏ä‡∏¥‡∏á‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå: {strategic_focus}

# ### [Input Data: Assets & Gaps - ‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏µ‡πâ ‡∏´‡πâ‡∏≤‡∏°‡∏°‡πÇ‡∏ô‡πÄ‡∏û‡∏¥‡πà‡∏°]
# {aggregated_insights}

# ---
# ‡∏™‡∏£‡πâ‡∏≤‡∏á Master Roadmap ‡∏ï‡∏≤‡∏°‡∏Å‡∏é‡πÄ‡∏Ç‡πâ‡∏°‡∏á‡∏ß‡∏î‡∏Ç‡πâ‡∏≤‡∏á‡∏ï‡πâ‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏Ñ‡∏£‡πà‡∏á‡∏Ñ‡∏£‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î:
# - ‡∏ó‡∏∏‡∏Å action ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á + ‡∏≠‡πâ‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏£‡∏¥‡∏á + ‡∏´‡∏ô‡πâ‡∏≤/‡∏™‡πà‡∏ß‡∏ô (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ) + verb ‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡πÑ‡∏î‡πâ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
# - ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏ä‡πâ verb ‡∏ï‡πâ‡∏≠‡∏á‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≤‡∏î (‡∏£‡∏ß‡∏°‡πÉ‡∏ô goal ‡πÅ‡∏•‡∏∞ overall_strategy)
# - ‡∏´‡∏≤‡∏Å‡∏ú‡πà‡∏≤‡∏ô L5 ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏µ gap ‡πÉ‡∏´‡πâ Phase 1 = "Reinforce & Sustain" ‡πÅ‡∏•‡∏∞ Phase 2 = Standardization / Automation / ‡∏Ç‡∏¢‡∏≤‡∏¢‡∏ú‡∏•‡∏ï‡πâ‡∏ô‡πÅ‡∏ö‡∏ö
# - ‡∏´‡πâ‡∏≤‡∏°‡∏°‡∏µ Phase ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô L5

# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á action ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà copy ‡∏ï‡∏£‡∏á ‡πÜ):
# - "‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡πÉ‡∏ä‡πâ KMS Policy ‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏•‡∏á‡∏ô‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤ 12 ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå KM6.1L301 KM_6_3_PEA_Assessment Report.pdf ‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£ ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Å‡∏≤‡∏£‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™‡∏•‡∏∞ 1 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ú‡πà‡∏≤‡∏ô KM-Si"
# - "‡∏™‡∏ñ‡∏≤‡∏õ‡∏ô‡∏≤ dashboard ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô KM ‡∏à‡∏≤‡∏Å‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤ 7 ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå KM2.1L405 PEA KM Master Plan_...13Dec24_edit.pdf ‡πÇ‡∏î‡∏¢‡∏ö‡∏π‡∏£‡∏ì‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏ö KM-Survey"
# - "‡∏Ç‡∏¢‡∏≤‡∏¢‡∏ú‡∏•‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡πÄ‡∏£‡πà‡∏á‡∏î‡πà‡∏ß‡∏ô 12 ‡∏î‡πâ‡∏≤‡∏ô‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤ 48 ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå KM1.2L301 ‡πÅ‡∏ú‡∏ô‡πÅ‡∏°‡πà‡∏ö‡∏ó ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà 4 ‡∏¢‡πà‡∏≠.pdf ‡∏°‡∏≤‡∏à‡∏±‡∏î‡∏ó‡∏≥‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏≠‡∏ö‡∏£‡∏°‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏ó‡∏∏‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ç‡∏±‡∏ö‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô KM"

# {{
#   "status": "SUCCESS",
#   "overall_strategy": "‡πÉ‡∏ä‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå A ‡∏´‡∏ô‡πâ‡∏≤ X ‡∏°‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏∞‡∏ö‡∏ö‡∏¢‡∏±‡πà‡∏á‡∏¢‡∏∑‡∏ô‡πÅ‡∏•‡∏∞‡∏Ç‡∏¢‡∏≤‡∏¢‡∏ú‡∏•‡∏Ç‡πâ‡∏≤‡∏°‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô (‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏≤‡∏Å input)",
#   "phases": [
#     {{
#       "phase": "Phase 1: Quick Win (Reinforce & Sustain ‡∏´‡∏£‡∏∑‡∏≠ Remediation)",
#       "goal": "‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏Ç‡πá‡∏á‡πÅ‡∏Å‡∏£‡πà‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏õ‡∏¥‡∏î‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡πÇ‡∏î‡∏¢‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á",
#       "key_actions": [
#         {{
#           "action": "‡∏£‡∏∞‡∏ö‡∏∏ action ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á + ‡∏≠‡πâ‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå + ‡∏´‡∏ô‡πâ‡∏≤/‡∏™‡πà‡∏ß‡∏ô",
#           "priority": "High"
#         }}
#       ]
#     }},
#     {{
#       "phase": "Phase 2: Level-Up Excellence",
#       "goal": "‡∏¢‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏î‡πâ‡∏ß‡∏¢ standardization, automation ‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡∏¢‡∏≤‡∏¢‡∏ú‡∏•‡∏ï‡πâ‡∏ô‡πÅ‡∏ö‡∏ö",
#       "key_actions": [
#         {{
#           "action": "‡∏£‡∏∞‡∏ö‡∏∏‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô‡πÄ‡∏ä‡∏¥‡∏á‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏° + ‡∏≠‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á",
#           "priority": "Medium"
#         }}
#       ]
#     }}
#   ],
#   "strategic_focus_applied": "{strategic_focus}"
# }}
# """

def get_enabler_full_name(enabler: str, lang: str = "th") -> str:
    """
    ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡πá‡∏°‡∏Ç‡∏≠‡∏á Enabler ‡∏ï‡∏≤‡∏°‡∏£‡∏´‡∏±‡∏™ (‡πÄ‡∏ä‡πà‡∏ô "KM" ‚Üí "‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ")
    
    Args:
        enabler_code (str): ‡∏£‡∏´‡∏±‡∏™ enabler ‡πÄ‡∏ä‡πà‡∏ô "KM", "CG", "SP"
        lang (str): "th" ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢, "en" ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏© (default: "th")
    
    Returns:
        str: ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡πá‡∏° ‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏∑‡∏ô‡∏£‡∏´‡∏±‡∏™‡πÄ‡∏î‡∏¥‡∏°‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏ö
    
    Example:
        get_enabler_full_name("KM") ‚Üí "‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ"
        get_enabler_full_name("CG", "en") ‚Üí "Corporate Governance"
    """
    code = str(enabler).upper().strip()
    if lang.lower() == "th":
        return SEAM_ENABLER_FULL_NAME_TH.get(code, code)
    return SEAM_ENABLER_FULL_NAME_EN.get(code, code)


def get_pdca_goal_for_level(level: int) -> str:
    """
    ‡∏î‡∏∂‡∏á‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á Maturity Level ‡∏ô‡∏±‡πâ‡∏ô ‡πÜ
    
    Args:
        level (int): ‡∏£‡∏∞‡∏î‡∏±‡∏ö 1-5
    
    Returns:
        str: ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ ‡∏´‡∏£‡∏∑‡∏≠ "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢" ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏ö
    
    Example:
        get_pdca_goal_for_level(5) ‚Üí "‡πÄ‡∏ô‡πâ‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏±‡πà‡∏á‡∏¢‡∏∑‡∏ô ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÄ‡∏ä‡∏¥‡∏á‡∏£‡∏∏‡∏Å...‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏ô‡πÅ‡∏ö‡∏ö (Role Model)"
    """
    return MATURITY_LEVEL_GOALS.get(int(level), "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢")
    
def _static_worker_process(worker_input_tuple: Tuple) -> Any:
    """
    [ULTIMATE WORKER v2026.1.26 - SELF-HEALING & VSM-READY]
    ---------------------------------------------------------------------
    - üõ°Ô∏è Isolated Execution: ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏ö Spawn ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Server 8-GPU ‡πÄ‡∏ï‡πá‡∏°‡∏™‡∏π‡∏ö
    - üî´ Pre-loaded VSM: ‡πÇ‡∏´‡∏•‡∏î VectorStore ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏£‡πà‡∏≤‡∏á‡πÅ‡∏¢‡∏Å ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô AttributeError
    - üß¨ Evidence Streaming: ‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô Memory ‡∏Å‡∏•‡∏±‡∏ö‡∏™‡∏π‡πà Main Process ‡πÅ‡∏ö‡∏ö Real-time
    """
    # 1. üìÇ PATH & ENVIRONMENT SETUP
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
    if project_root not in sys.path:
        sys.path.append(project_root)
        
    worker_logger = logging.getLogger(f"Worker_{os.getpid()}")

    # 2. üì¶ UNPACKING ARGS
    try:
        (
            sub_criteria_data, enabler, target_level, mock_mode, 
            evidence_map_path, model_name, temperature,
            min_retry_score, max_retrieval_attempts, document_map, 
            action_plan_model, year, tenant
        ) = worker_input_tuple
        
        sub_id = sub_criteria_data.get('sub_id', 'UNKNOWN')
    except Exception as e:
        return {"error": f"Worker unpacking failed: {str(e)}", "status": "failure"}, {}

    # 3. üèóÔ∏è RECONSTRUCT ISOLATED ENGINE (With Self-Healing LLM & VSM)
    try:
        # üéØ CRITICAL: ‡∏ï‡πâ‡∏≠‡∏á‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏´‡∏°‡πà‡∏†‡∏≤‡∏¢‡πÉ‡∏ô Worker ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏°‡∏≠‡∏á‡πÄ‡∏´‡πá‡∏ô Module ‡πÉ‡∏ô‡πÇ‡∏´‡∏°‡∏î Spawn
        from models.llm import create_llm_instance
        from core.seam_assessment import SEAMPDCAEngine, AssessmentConfig
        from core.vectorstore import load_all_vectorstores
        from config.global_vars import EVIDENCE_DOC_TYPES

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Config ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Å‡∏¥‡∏à‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Worker ‡∏ô‡∏µ‡πâ
        worker_config = AssessmentConfig(
            enabler=enabler,
            tenant=tenant,
            year=int(year) if year else None,     
            target_level=target_level,
            mock_mode=mock_mode,
            model_name=model_name, 
            temperature=temperature,
            min_retry_score=min_retry_score,            
            max_retrieval_attempts=max_retrieval_attempts 
        )

        # üöÄ 3.1 ‡∏™‡∏£‡πâ‡∏≤‡∏á LLM Instance (‡∏£‡πà‡∏≤‡∏á‡πÅ‡∏¢‡∏Å)
        worker_llm = None
        if mock_mode == "none":
            worker_llm = create_llm_instance(
                model_name=model_name, 
                temperature=temperature
            )

        # üî´ 3.2 ‡πÇ‡∏´‡∏•‡∏î VectorStoreManager (VSM) ‡∏•‡πà‡∏ß‡∏á‡∏´‡∏ô‡πâ‡∏≤
        # ‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤: 'SEAMPDCAEngine' object has no attribute 'vectorstore_manager'
        worker_vsm = None
        try:
            worker_vsm = load_all_vectorstores(
                doc_types=[EVIDENCE_DOC_TYPES], 
                enabler_filter=enabler, 
                tenant=tenant, 
                year=str(year)
            )
        except Exception as v_err:
            worker_logger.warning(f"‚ö†Ô∏è VSM Load Warning for {sub_id}: {v_err}")

        # üõ†Ô∏è 3.3 ‡∏Ñ‡∏∑‡∏ô‡∏ä‡∏µ‡∏û Engine ‡∏û‡∏£‡πâ‡∏≠‡∏° "‡∏≠‡∏≤‡∏ß‡∏∏‡∏ò" ‡∏Ñ‡∏£‡∏ö‡∏°‡∏∑‡∏≠
        worker_instance = SEAMPDCAEngine(
            config=worker_config, 
            evidence_map_path=evidence_map_path, 
            llm_instance=worker_llm,
            vectorstore_manager=worker_vsm,  # ‚úÖ ‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏´‡∏π‡∏Ç‡∏ß‡∏≤‡πÑ‡∏õ‡πÄ‡∏•‡∏¢!
            logger_instance=worker_logger,
            document_map=document_map,      
            ActionPlanActions=action_plan_model
        )
    except Exception as e:
        worker_logger.error(f"‚ùå Worker initialization failed for {sub_id}: {e}")
        return {
            "sub_id": sub_id, 
            "error": f"Init Error: {str(e)}",
            "status": "failed"
        }, {}

    # 4. ‚ö° EXECUTE & STREAM BACK RESULTS
    try:
        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏Å‡πà‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏û‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏ß‡πâ v2026.01.25
        result, worker_evidence_mem = worker_instance._run_sub_criteria_assessment_worker(sub_criteria_data)
        
        if isinstance(result, dict):
            if 'sub_id' not in result: result['sub_id'] = sub_id
            result['status'] = "success"

        return result, worker_evidence_mem
        
    except Exception as e:
        worker_logger.error(f"‚ùå Execution error for {sub_id}: {str(e)}")
        return {
            "sub_id": sub_id,
            "error": str(e),
            "status": "failed",
            "is_passed": False,
            "score": 0.0,
            "reason": f"Worker Runtime Exception: {str(e)}"
        }, {}
        
# =================================================================
# Configuration Class
# =================================================================
@dataclass
class AssessmentConfig:
    """Configuration for the SEAM PDCA Assessment Run."""
    
    # ------------------ 1. Assessment Context ------------------
    enabler: str = None
    tenant: str = None
    year: int = None  # üëà ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å DEFAULT_YEAR ‡πÄ‡∏õ‡πá‡∏ô None
    target_level: int = MAX_LEVEL
    mock_mode: str = "none" # 'none', 'random', 'control'
    force_sequential: bool = field(default=False) # Flag ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏£‡∏±‡∏ô‡πÅ‡∏ö‡∏ö Sequential

    # ------------------ 2. LLM Configuration (Configurable) ------------------
    # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Default ‡∏à‡∏≤‡∏Å global_vars.py
    model_name: str = DEFAULT_LLM_MODEL_NAME 
    temperature: float = LLM_TEMPERATURE

    # ------------------ 3. Adaptive RAG Retrieval Configuration ------------------
    # üü¢ NEW: ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô Rerank ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Adaptive Loop (MIN_RETRY_SCORE)
    # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Default 0.65 ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏û‡∏ö‡πÉ‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î Logic
    min_retry_score: float = MIN_RETRY_SCORE
    # üü¢ NEW: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≠‡∏ö‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á Adaptive RAG Loop (MAX_RETRIEVAL_ATTEMPTS)
    # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Default 3
    max_retrieval_attempts: int = 3
    
    # ------------------ 4. Export Configuration ------------------
    export_output: bool = field(default=False) # Flag ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏¥‡∏î/‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£ Export ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    export_path: str = "" # Path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå Export (‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å)


# =================================================================
# SEAM Assessment Engine (PDCA Focused) - Full Revise v2026
# =================================================================
class SEAMPDCAEngine:
    
    def __init__(
        self, 
        config: AssessmentConfig,
        llm_instance: Any = None, 
        logger_instance: logging.Logger = None,
        rag_retriever_instance: Any = None,
        doc_type: str = None, 
        vectorstore_manager: Optional['VectorStoreManager'] = None,
        evidence_map_path: Optional[str] = None,
        document_map: Optional[Dict[str, str]] = None,
        is_parallel_all_mode: bool = False,
        sub_id: str = 'all',
        record_id: Optional[str] = None,
        **kwargs  
    ):
        """
        [ULTIMATE REVISE v2026.3.1] 
        - FIXED: flattened_rubric logic corruption (No more overwrite)
        - FIXED: Robust doc_type comparison (is_evidence_mode)
        - FIXED: Safe VSM/LLM initialization order
        """
        # -------------------------------------------------------
        # 1. Basic Config & Logger Setup
        # -------------------------------------------------------
        self.config = config
        self.doc_type = doc_type or getattr(config, 'doc_type', EVIDENCE_DOC_TYPES)
        
        # [REVISED] Robust Comparison Pattern ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥
        self.is_evidence_mode = str(self.doc_type).strip().lower() in (
            t.lower() for t in (EVIDENCE_DOC_TYPES if isinstance(EVIDENCE_DOC_TYPES, (list, tuple)) else [EVIDENCE_DOC_TYPES])
        )
        
        log_year = config.year if self.is_evidence_mode else "general"

        if logger_instance is not None:
            self.logger = logger_instance
        else:
            self.logger = logging.getLogger(__name__).getChild(
                f"Engine|{config.enabler}|{config.tenant}/{log_year}"
            )

        self.logger.info(f"üöÄ Initializing SEAMPDCAEngine: {config.enabler} ({config.tenant}/{log_year})")

        # -------------------------------------------------------
        # 2. Core Configuration & Sanity Check
        # -------------------------------------------------------
        if not self.config.enabler or not self.config.tenant:
            self.logger.critical("‚ùå Mandatory Config Missing: enabler and tenant must be provided!")
            raise ValueError("Enabler and Tenant are required for SEAMPDCAEngine.")

        self.enabler = config.enabler
        self.tenant_id = config.tenant
        self.year = config.year
        self.target_level = config.target_level
        self.sub_id = sub_id
        self.record_id = record_id
        
        # State Management
        self.is_parallel_all_mode = is_parallel_all_mode
        self.is_sequential = getattr(config, 'force_sequential', True)
        
        # [REVISED] results vs assessment_results_map
        # results ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠ legacy compatibility, assessment_results_map ‡∏Ñ‡∏∑‡∏≠ source of truth
        self.results = {} 
        self.assessment_results_map = {} 

        # -------------------------------------------------------
        # 3. System Warm-up
        # -------------------------------------------------------
        try:
            init_db()
        except Exception as e:
            self.logger.error(f"‚ö†Ô∏è DB Init Warning: {e}")

        # -------------------------------------------------------
        # 4. Data Loading (Rubric & Rules)
        # -------------------------------------------------------
        self.rubric = self._load_rubric()

        # [REVISED] Flatten Rubric - ‡∏ó‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà Overwrite ‡∏ã‡πâ‡∏≥‡∏ï‡∏≠‡∏ô‡∏ó‡πâ‡∏≤‡∏¢
        try:
            self.flattened_rubric = self._flatten_rubric_to_statements(self.rubric)
            self.logger.info(f"‚úÖ Rubric Meta-Data Flattened: {len(self.flattened_rubric)} levels ready.")
        except Exception as e:
            self.logger.error(f"‚ö†Ô∏è Rubric Flattening Failed: {e}")
            self.flattened_rubric = []
        
        self.contextual_rules_map = self._load_contextual_rules_map()
        self.retry_policy = RetryPolicy(max_attempts=3, base_delay=2.0)

        # -------------------------------------------------------
        # 5. Mapping & Evidence Setup
        # -------------------------------------------------------
        self.evidence_map = {}
        if self.is_evidence_mode:
            self.evidence_map_path = evidence_map_path or get_evidence_mapping_file_path(
                tenant=self.config.tenant, year=self.config.year, enabler=self.enabler
            )
            self.evidence_map = self._load_evidence_map()

        # Document Mapping
        loaded_map = document_map or {}
        if not loaded_map:
            mapping_path = get_mapping_file_path(
                self.doc_type, 
                tenant=self.config.tenant, 
                year=self.config.year if self.is_evidence_mode else None,
                enabler=self.enabler if self.is_evidence_mode else None
            )
            if os.path.exists(mapping_path):
                try:
                    with open(mapping_path, 'r', encoding='utf-8') as f:
                        raw_data = json.load(f)
                    loaded_map = {k: v.get("file_name", k) for k, v in raw_data.items()}
                except Exception as e:
                    self.logger.error(f"‚ùå Error parsing mapping file: {e}")

        self.doc_id_to_filename_map = loaded_map
        self.document_map = loaded_map

        # -------------------------------------------------------
        # 6. Lazy Engine Initialization (VSM & LLM)
        # -------------------------------------------------------
        # [REVISED] Safe initialization order
        if llm_instance is None: 
            self._initialize_llm_if_none()
        else:
            self.llm = llm_instance

        if vectorstore_manager is None: 
            # ‡∏™‡πà‡∏á LLM ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡∏ñ‡πâ‡∏≤ VSM ‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ embedding ‡∏à‡∏≤‡∏Å LLM
            self._initialize_vsm_if_none() 
        else:
            self.vectorstore_manager = vectorstore_manager

        if self.vectorstore_manager:
            try:
                self.vectorstore_manager._load_doc_id_mapping()
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è VSM mapping not loaded: {e}")

       
        # -------------------------------------------------------
        # 7. Function Registry & Final States (CLEAN)
        # -------------------------------------------------------
        # [REGISTER AGENTS] - ‡∏ï‡∏±‡∏ß‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏´‡∏•‡∏±‡∏Å‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏î‡πâ‡∏ß‡∏¢ LLM
        self.standard_audit_agent = evaluate_with_llm              # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö L3‚ÄìL5 (‡πÄ‡∏ô‡πâ‡∏ô Audit)
        self.foundation_coaching_agent = evaluate_with_llm_low_level  # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö L1‚ÄìL2 (‡πÄ‡∏ô‡πâ‡∏ô Coaching)
        
        # [ROUTING & RETRIEVAL]
        self.assessment_router = self.evaluate_pdca
        self.rag_retriever = retrieve_context_with_filter

        # [STATE: CORE RESULTS] - ‡∏ï‡∏∞‡∏Å‡∏£‡πâ‡∏≤‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏´‡∏•‡∏±‡∏Å
        self.final_subcriteria_results = []      # ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏£‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠ (‡∏û‡∏£‡πâ‡∏≠‡∏° sub_roadmap)
        self.total_stats = self._get_empty_stats_template()  # ‡∏™‡∏£‡∏∏‡∏õ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°
        
        # [STATE: STRATEGIC ROADMAPS] - ‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå
        self.enabler_roadmap_data = {}           # ‡πÅ‡∏ú‡∏ô‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏° Enabler (Tier-3 Synthesis)
        self.sub_roadmap_data = {}               # ‡πÅ‡∏ú‡∏ô‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏£‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠ (‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß)
        
        # [STATE: EVIDENCE & CACHE] - ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û
        self.level_details_map = {} 
        self.level_evidence_cache = {}
        self.previous_levels_evidence = [] 
        self._cumulative_rules_cache = {}
        
        # [STATE: LOGGING & DEBUG]
        self.raw_llm_results = []

        # üéØ [CRITICAL] ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏™‡πà self.flattened_rubric = [] ‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≤‡∏î 
        # ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏à‡∏∞‡πÑ‡∏õ Overwrite ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 4
        
        self.logger.info(f"‚úÖ Engine Initialized: Ready for Assessment (Sub-ID: {self.sub_id})")

    # =================================================================
    # DB Proxy Methods (Enhanced v2026)
    # =================================================================
    def db_update_task_status(self, message: str, progress: Optional[int] = None, status: str = "RUNNING"):
        """
        Enhanced Wrapper ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞
        - ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏™‡πà‡∏á record_id ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á (‡πÉ‡∏ä‡πâ self.current_record_id ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥)
        - ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏™‡πà‡∏á progress ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÄ‡∏â‡∏û‡∏≤‡∏∞ message (‡∏Ñ‡∏á‡∏Ñ‡πà‡∏≤ % ‡πÄ‡∏î‡∏¥‡∏°‡πÑ‡∏ß‡πâ)
        """
        # 1. ‡∏î‡∏∂‡∏á record_id ‡∏à‡∏≤‡∏Å instance ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≤‡∏°
        rid = getattr(self, 'current_record_id', None) or getattr(self, 'record_id', None)
        if not rid: 
            return

        try:
            # 2. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï
            # ‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà progress ‡πÄ‡∏õ‡πá‡∏ô None ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å memory ‡∏´‡∏£‡∏∑‡∏≠ database 
            # ‡πÅ‡∏ï‡πà‡πÇ‡∏î‡∏¢‡∏õ‡∏Å‡∏ï‡∏¥ database.db_update_task_status ‡∏Ñ‡∏ß‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≤‡∏°‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô None ‡πÉ‡∏´‡πâ‡πÄ‡∏≠
            
            # 3. ‡∏™‡πà‡∏á‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï
            update_db_core(
                record_id=rid, 
                progress=progress, 
                message=message, 
                status=status
            )
            
            self.logger.debug(f"[DB-PROGRESS] {rid}: {progress if progress is not None else 'KEEP'}% - {message}")
            
        except Exception as e:
            self.logger.error(f"‚ùå DB Update Error for {rid}: {e}")

    def _initialize_llm_if_none(self):
        """Initializes LLM instance if self.llm is None."""
        if self.llm is None:
            self.logger.warning("‚ö†Ô∏è Initializing LLM: model=%s, temperature=%s", 
                                self.config.model_name, self.config.temperature)
            try:
                # üü¢ FIX: Import ‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ create_llm_instance
                from models.llm import create_llm_instance 
                self.llm = create_llm_instance( 
                    model_name=self.config.model_name,
                    temperature=self.config.temperature
                )
                self.logger.info("‚úÖ LLM Instance created successfully: %s (Temp: %s)", 
                                 self.config.model_name, self.config.temperature)
            except Exception as e:
                self.logger.error(f"FATAL: Could not initialize LLM: {e}")
                raise

    def _initialize_vsm_if_none(self):
        """
        Initializes VectorStoreManager with Smart Year Selection.
        - If Evidence: Priority 1 = Specific Year, Priority 2 = Root Fallback.
        - If Document: Priority 1 = Root (General), Priority 2 = Specific Year Fallback.
        """
        if self.vectorstore_manager is not None:
            return

        # 1. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ç‡∏≠‡∏á DocType
        clean_dt = str(self.doc_type or getattr(self.config, 'doc_type', EVIDENCE_DOC_TYPES)).strip().lower()
        is_evidence = (clean_dt == EVIDENCE_DOC_TYPES.lower())
        
        self.logger.info(f"üöÄ Loading vectorstore(s) for DocType: '{clean_dt}' (Mode: {'Evidence' if is_evidence else 'General'})")

        try:
            target_enabler = str(self.enabler).lower() if self.enabler else None
            
            # üéØ [SMART SELECTION] ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å
            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô evidence ‡πÉ‡∏´‡πâ‡∏´‡∏≤‡∏ï‡∏≤‡∏°‡∏õ‡∏µ (2568) ‡∏Å‡πà‡∏≠‡∏ô ‡πÅ‡∏ï‡πà‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô document ‡πÉ‡∏´‡πâ‡∏´‡∏≤‡∏ó‡∏µ‡πà Root (None) ‡∏Å‡πà‡∏≠‡∏ô
            primary_year = self.config.year if is_evidence else None
            secondary_year = None if is_evidence else self.config.year

            # 2. First Attempt: ‡πÇ‡∏´‡∏•‡∏î‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
            self.vectorstore_manager = load_all_vectorstores(
                doc_types=[clean_dt], 
                enabler_filter=target_enabler, 
                tenant=self.config.tenant, 
                year=primary_year       
            )
            
            def count_retrievers(vsm):
                if vsm and hasattr(vsm, '_multi_doc_retriever') and vsm._multi_doc_retriever:
                    return len(vsm._multi_doc_retriever._all_retrievers)
                return 0

            len_retrievers = count_retrievers(self.vectorstore_manager)
            
            # 3. Second Attempt (Fallback): ‡∏ñ‡πâ‡∏≤‡∏´‡∏≤‡πÅ‡∏ö‡∏ö‡πÅ‡∏£‡∏Å‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ ‡πÉ‡∏´‡πâ‡∏™‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏´‡∏≤‡∏≠‡∏µ‡∏Å‡πÅ‡∏ö‡∏ö
            if len_retrievers == 0:
                lookup_label = f"year {secondary_year}" if secondary_year else "tenant root"
                self.logger.info(f"‚ö†Ô∏è No collections found in primary path, searching in {lookup_label}...")
                
                self.vectorstore_manager = load_all_vectorstores(
                    doc_types=[clean_dt], 
                    enabler_filter=target_enabler, 
                    tenant=self.config.tenant, 
                    year=secondary_year       
                )
                len_retrievers = count_retrievers(self.vectorstore_manager)

            # 4. Post-Load Process
            if self.vectorstore_manager and len_retrievers > 0:
                self.vectorstore_manager._load_doc_id_mapping() 
                self.logger.info(f"‚úÖ MultiDocRetriever loaded with {len_retrievers} collections.") 
            else:
                # 5. Final Error Handling
                expected_p = f"data_store/{self.config.tenant}/vectorstore/{primary_year or 'root'}"
                self.logger.error(f"‚ùå FATAL: 0 vector store collections loaded. Please check folder: {expected_p}")
                raise ValueError(f"No vector collections found for '{target_enabler}' in {self.config.tenant}")

        except Exception as e:
            self.logger.error(f"‚ùå FATAL: Could not initialize VectorStoreManager: {str(e)}")
            raise

    # -------------------- Contextual Rules Handlers (FIXED) --------------------
    def _load_contextual_rules_map(self) -> Dict[str, Any]:
        """
        [FINAL REVISED v2026.5] ‡πÇ‡∏´‡∏•‡∏î Contextual Rules ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
        - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö multi-tenant ‡πÅ‡∏•‡∏∞ multi-enabler ‡πÄ‡∏ï‡πá‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö
        - ‡∏°‡∏µ fallback ‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
        - ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Maturity (L1-L5) ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏Ç‡πâ‡∏°‡∏á‡∏ß‡∏î
        - Logging ‡∏ó‡∏µ‡πà‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏ä‡πà‡∏ß‡∏¢ debug
        - ‡πÑ‡∏°‡πà raise exception (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ engine ‡∏•‡πâ‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏£‡∏∞‡∏ö‡∏ö)
        """
        # 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á path ‡∏ï‡∏≤‡∏° tenant + enabler
        try:
            filepath = get_contextual_rules_file_path(
                tenant=self.config.tenant,
                enabler=self.enabler
            )
            self.logger.debug(f"üîç Attempting to load contextual rules from: {filepath}")
        except Exception as e:
            self.logger.error(f"‚ùå FATAL: Failed to generate rules file path: {e}")
            return {"_enabler_defaults": {}}

        # 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if not os.path.exists(filepath):
            self.logger.warning(
                f"‚ö†Ô∏è Contextual Rules file not found: {filepath}\n"
                f"   ‚Üí Using only global defaults (if available from fallback)."
            )
            return {"_enabler_defaults": {}}

        # 3. ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞ parse JSON
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
        except json.JSONDecodeError as e:
            self.logger.error(f"‚ùå JSON Decode Error in {filepath}: {e} (line {e.lineno}, col {e.colno})")
            return {"_enabler_defaults": {}}
        except Exception as e:
            self.logger.error(f"‚ùå Unexpected error reading {filepath}: {e}")
            return {"_enabler_defaults": {}}

        # 4. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        if not isinstance(data, dict):
            self.logger.error(f"‚ùå Invalid rules format in {filepath}: Expected dict, got {type(data)}")
            return {"_enabler_defaults": {}}

        # 5. ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô criteria ‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Maturity Structure
        sub_criteria_keys = [k for k in data.keys() if not k.startswith("_")]
        num_criteria = len(sub_criteria_keys)

        if num_criteria == 0:
            self.logger.warning(f"‚ö†Ô∏è No sub-criteria found in {filepath} (only defaults).")
        else:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ Maturity Levels (L1, L2, ...) ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            sample_sub = sub_criteria_keys[0]
            sub_data = data[sample_sub]
            level_keys = [k for k in sub_data.keys() if k.startswith("L") and len(k) >= 2]
            
            if level_keys:
                detected_levels = sorted(level_keys)
                self.logger.info(
                    f"‚úÖ Maturity-Based Rules loaded successfully!\n"
                    f"   File: {filepath}\n"
                    f"   Criteria: {num_criteria} (e.g., {sample_sub})\n"
                    f"   Levels detected: {', '.join(detected_levels)}"
                )
            else:
                self.logger.warning(
                    f"‚ö†Ô∏è Rules for '{sample_sub}' in {filepath} do not contain Maturity Levels (L1-L5).\n"
                    f"   Falling back to flat structure or defaults."
                )

        # 6. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö _enabler_defaults
        if "_enabler_defaults" in data:
            defaults = data["_enabler_defaults"]
            if isinstance(defaults, dict) and any(k.endswith("_keywords") for k in defaults.keys()):
                self.logger.info("‚úÖ Global PDCA Keywords (_enabler_defaults) loaded.")
            else:
                self.logger.warning("‚ö†Ô∏è _enabler_defaults exists but has invalid structure.")
        else:
            self.logger.info("‚ÑπÔ∏è No _enabler_defaults section found. Using empty defaults.")

        # 7. ‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô data ‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
        self.logger.info(f"‚úÖ Contextual Rules fully loaded from {filepath} ({num_criteria} criteria).")
        return data
    
    def get_rule_content(self, sub_id: str, level: int, key_type: str):
        """
        ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏à‡∏≤‡∏Å Contextual Rules ‡πÅ‡∏ö‡∏ö‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ä‡∏±‡πâ‡∏ô
        Priority: Specific Level > Sub-ID Root > Global Defaults > Fallback
        """
        rule = self.contextual_rules_map.get(sub_id, {})
        level_key = f"L{level}"

        # 1. Specific Level (e.g., 1.2 -> L1 -> query_synonyms)
        if key_type in rule.get(level_key, {}):
            return rule[level_key][key_type]

        # 2. Sub-ID Root (e.g., 1.2 -> query_synonyms)
        if key_type in rule:
            return rule[key_type]

        # 3. Global Defaults (e.g., _enabler_defaults -> plan_keywords)
        defaults = self.contextual_rules_map.get("_enabler_defaults", {})
        if key_type in defaults:
            return defaults[key_type]

        # 4. Fallback (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Error ‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ Key)
        fallbacks = {
            "require_phase": ["P", "D"], # Default ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
            "must_include_keywords": [],
            "plan_keywords": [],
            "do_keywords": [],
            "check_keywords": [],
            "act_keywords": [],
            "query_synonyms": ""
        }
        return fallbacks.get(key_type, "")

    def get_cumulative_rules_cached(self, sub_id: str, level: int) -> Dict[str, Any]:
        """
        Return cumulative rules with per-engine caching.
        Cache key: (sub_id, level)
        """
        key = (sub_id, level)

        if key not in self._cumulative_rules_cache:
            self._cumulative_rules_cache[key] = self.get_cumulative_rules(
                sub_id=sub_id,
                current_level=level
            )

        return self._cumulative_rules_cache[key]


    def get_cumulative_rules(self, sub_id: str, current_level: int) -> Dict[str, Any]:
        """
        [FINAL REVISED v2026.1.20] - SMART ACCUMULATION & ROBUST MAPPING
        ------------------------------------------------------------------
        - ‡∏™‡∏∞‡∏™‡∏° Keywords (PDCA) ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏à‡∏≤‡∏Å L1 ‡∏à‡∏ô‡∏ñ‡∏∂‡∏á‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
        - **Robust Synonym Split**: ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥‡∏î‡πâ‡∏ß‡∏¢ ‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á, ‡∏Ñ‡∏≠‡∏°‡∏°‡πà‡∏≤, ‡πÅ‡∏•‡∏∞‡πÄ‡∏ã‡∏°‡∏¥‡πÇ‡∏Ñ‡∏•‡∏≠‡∏ô
        - **Auto-Fallback Phases**: ‡πÄ‡∏ï‡∏¥‡∏° Required Phases ‡πÉ‡∏´‡πâ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡∏´‡∏≤‡∏Å Config ‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏
        - **Case-Insensitive**: ‡∏õ‡∏£‡∏±‡∏ö Keywords ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏û‡∏¥‡∏°‡∏û‡πå‡πÄ‡∏•‡πá‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Match
        """
        defaults = self.contextual_rules_map.get('_enabler_defaults', {})
        sub_rules = self.contextual_rules_map.get(sub_id, {})
        
        # 1. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° OrderedDict ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÅ‡∏•‡∏∞‡∏Å‡∏≥‡∏à‡∏±‡∏î‡∏Ñ‡∏≥‡∏ã‡πâ‡∏≥
        cum_keywords = {
            "plan": OrderedDict((k.lower(), None) for k in defaults.get('plan_keywords', [])),
            "do":   OrderedDict((k.lower(), None) for k in defaults.get('do_keywords', [])),
            "check": OrderedDict((k.lower(), None) for k in defaults.get('check_keywords', [])),
            "act":  OrderedDict((k.lower(), None) for k in defaults.get('act_keywords', []))
        }
        
        cum_must_include = OrderedDict()
        required_phases = set()
        level_specific_instructions = {}
        source_levels = []

        # 2. ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏™‡∏∞‡∏™‡∏°‡∏Å‡∏é‡∏à‡∏≤‡∏Å L1 ‡∏à‡∏ô‡∏ñ‡∏∂‡∏á‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
        for lv in range(1, current_level + 1):
            lv_key = f"L{lv}"
            level_rule = sub_rules.get(lv_key, {})
            
            if not level_rule:
                continue
            
            source_levels.append(lv)

            # A) ‡∏î‡∏∂‡∏á query_synonyms ‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô must_include (Robust Split)
            synonyms_str = level_rule.get('query_synonyms', "")
            if synonyms_str:
                # ‡πÉ‡∏ä‡πâ Regex ‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á "‡∏Ñ‡∏≥1 ‡∏Ñ‡∏≥2" ‡∏´‡∏£‡∏∑‡∏≠ "‡∏Ñ‡∏≥1,‡∏Ñ‡∏≥2" ‡∏´‡∏£‡∏∑‡∏≠ "‡∏Ñ‡∏≥1;‡∏Ñ‡∏≥2"
                words = re.split(r'[,\s;|]+', synonyms_str)
                for word in words:
                    clean_word = word.strip().lower()
                    if clean_word:
                        cum_must_include[clean_word] = None

            # B) ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï PDCA Keywords (‡∏™‡∏∞‡∏™‡∏°‡∏ï‡πà‡∏≠‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤)
            for phase, key_name in [("plan", "plan_keywords"), ("do", "do_keywords"),
                                ("check", "check_keywords"), ("act", "act_keywords")]:
                new_kws = level_rule.get(key_name, [])
                for kw in new_kws:
                    cum_keywords[phase][kw.lower()] = None
            
            # C) ‡∏™‡∏∞‡∏™‡∏° Required Phases
            if 'require_phase' in level_rule:
                # ‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ‡∏ó‡∏±‡πâ‡∏á List ["P", "D"] ‡∏´‡∏£‡∏∑‡∏≠ String "P,D"
                phases = level_rule['require_phase']
                if isinstance(phases, str):
                    phases = re.split(r'[,\s]+', phases)
                required_phases.update([p.upper() for p in phases if p])
            
            # D) ‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ (Specific Instructions)
            specific = level_rule.get('specific_contextual_rule')
            if specific:
                level_specific_instructions[lv] = specific.strip()

        # 3. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å (Finalize & Fallback)
        result_keywords = {phase: list(cum_keywords[phase].keys()) for phase in cum_keywords}
        
        # Smart Fallback ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Required Phases (‡∏ñ‡πâ‡∏≤‡πÉ‡∏ô Config ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏∞‡∏ö‡∏∏‡πÄ‡∏•‡∏¢)
        final_phases = sorted(list(required_phases))
        if not final_phases:
            if current_level <= 3: final_phases = ["P", "D"]
            elif current_level == 4: final_phases = ["P", "D", "C"]
            else: final_phases = ["P", "D", "C", "A"]

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Instructions String ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Prompt
        instructions_lines = [f"‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {sub_id} ‡∏£‡∏∞‡∏î‡∏±‡∏ö Maturity L{current_level}:"]
        for lv in sorted(level_specific_instructions.keys()):
            icon = "üéØ" if lv == current_level else "‚úÖ"
            instructions_lines.append(f"{icon} [Level {lv}]: {level_specific_instructions[lv]}")

        # 4. Logging & Return
        self.logger.info(
            f"üöÄ [RULE_CUMULATIVE] {sub_id} L{current_level} | "
            f"Must-Include: {len(cum_must_include)} words | "
            f"Phases: {final_phases}"
        )

        return {
            "plan_keywords": result_keywords["plan"],
            "do_keywords": result_keywords["do"],
            "check_keywords": result_keywords["check"],
            "act_keywords": result_keywords["act"],
            "required_phases": final_phases,
            "must_include_keywords": list(cum_must_include.keys()),
            "level_specific_instructions": level_specific_instructions,
            "all_instructions": "\n".join(instructions_lines),
            "source_summary": f"Accumulated from levels: {source_levels}"
        }

    def _check_contextual_rule_condition(
        self, 
        condition: Dict[str, Any], 
        sub_id: str, 
        level: int, 
        top_evidences: List[Dict[str, Any]]
    ) -> bool:
        """
        [SIMPLIFIED v2026] ‡πÅ‡∏Ñ‡πà log ‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô continuity + min evidence
        ‡πÑ‡∏°‡πà block ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô (return True ‡πÄ‡∏™‡∏°‡∏≠)
        """
        if level > 1:
            prev_level = level - 1
            is_prev_passed = False
            if hasattr(self, 'level_details_map') and str(prev_level) in self.level_details_map:
                is_prev_passed = self.level_details_map[str(prev_level)].get('is_passed', False)
            
            if not is_prev_passed:
                self.logger.warning(f"‚ö†Ô∏è [GAP DETECTED] L{prev_level} not passed for {sub_id} L{level} - may affect validity")

        min_docs = condition.get('min_evidences', 1)
        if len(top_evidences) < min_docs:
            self.logger.warning(f"‚ö†Ô∏è [LOW EVIDENCE] {sub_id} L{level}: {len(top_evidences)} docs (required: {min_docs})")

        return True  # ‡πÑ‡∏°‡πà block


    def post_process_llm_result(
        self,
        llm_output: Any,
        level: int,
        sub_id: str = None,
        contextual_config: Optional[Dict] = None,
        top_evidences: Optional[List[Dict[str, Any]]] = None
    ) -> Dict[str, Any]:
        """
        [ULTIMATE REVISED v2026.02.05-final]
        - SYNC: ‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ UI Matrix ‡∏Å‡∏£‡∏≠‡∏ö‡∏õ‡∏£‡∏∞‡πÇ‡∏î‡∏¢‡∏™‡πà‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏Å‡∏±‡∏ö Tag
        - VALIDATION: ‡πÄ‡∏û‡∏¥‡πà‡∏° Keyword Cross-check ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ó‡∏µ‡πà AI ‡∏•‡∏∑‡∏°‡πÉ‡∏´‡πâ
        - GOVERNANCE: ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡∏ú‡πà‡∏≤‡∏ô (Gatekeeper) ‡∏ï‡∏≤‡∏° SE-AM Standard
        """
        contextual_config = contextual_config or {}
        top_evidences = top_evidences or []
        
        # 1. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Metadata ‡πÅ‡∏•‡∏∞ Enabler Context
        meta = contextual_config.get("_metadata", {})
        tenant = meta.get("tenant", "PEA").upper()
        enabler = meta.get("enabler", "KM").upper()
        replacement_term = contextual_config.get("replacement_term", "‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£")
        log_prefix = f"[{tenant}-{enabler}] {sub_id} L{level}"

        # 2. Robust JSON Repair
        if isinstance(llm_output, str):
            try:
                cleaned = re.sub(r'```json\s*|\s*```|\n+', ' ', llm_output.strip())
                llm_output = json.loads(cleaned)
            except:
                return self._get_fallback_result(log_prefix)

        # 3. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Keyword Lookup ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á (Consistency Check)
        enabler_defaults = contextual_config.get("_enabler_defaults", {})
        global_enabler_map = PDCA_CONFIG_MAP.get(enabler, PDCA_CONFIG_MAP["DEFAULT"])
        
        phase_kws_lookup = {
            "P": contextual_config.get(sub_id, {}).get(f"L{level}", {}).get("query_synonyms", "").split() or 
                enabler_defaults.get("plan_keywords", []) or global_enabler_map["P"],
            "D": enabler_defaults.get("do_keywords", []) or global_enabler_map["D"],
            "C": enabler_defaults.get("check_keywords", []) or global_enabler_map["C"],
            "A": enabler_defaults.get("act_keywords", []) or global_enabler_map["A"]
        }

        # 4. Extract PDCA Scores (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Multiple Key Formats)
        pdca_raw = {"P": 0.0, "D": 0.0, "C": 0.0, "A": 0.0}
        norm_out = {str(k).lower().strip(): v for k, v in llm_output.items() if isinstance(k, str)}
        reason_text = str(norm_out.get("reason", "")).lower()

        search_keys = {
            "P": ["p_score", "p_plan_score", "score_p", "plan_score"],
            "D": ["d_score", "d_do_score", "score_d", "do_score"],
            "C": ["c_score", "c_check_score", "score_c", "check_score"],
            "A": ["a_score", "a_act_score", "score_a", "act_score"]
        }

        for phase, keys in search_keys.items():
            score = 0.0
            for k in keys:
                if k in norm_out:
                    try: score = float(norm_out[k]); break
                    except: continue
            
            # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô AI "‡∏ï‡∏≤‡∏ñ‡∏±‡πà‡∏ß": ‡∏ñ‡πâ‡∏≤‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏à‡∏≠‡πÉ‡∏ô Reason ‡πÅ‡∏ï‡πà‡∏•‡∏∑‡∏°‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
            if score < 0.4 and any(kw.lower() in reason_text for kw in phase_kws_lookup.get(phase, [])):
                score = 0.75
                
            pdca_raw[phase] = min(max(score, 0.0), 2.0)

        # 5. Scoring Logic & Floor Injection (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ UI Matrix ‡∏Ç‡∏∂‡πâ‡∏ô‡∏™‡∏µ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß Solid)
        current_cfg = contextual_config.get(sub_id, {}).get(f"L{level}", {})
        required_phases = current_cfg.get("require_phase") or current_cfg.get("required_phases") or ["P"]
        
        pdca_scored = pdca_raw.copy()
        # Floor Score: ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏£‡πà‡∏≠‡∏á‡∏£‡∏≠‡∏¢ (Score > 0) ‡πÉ‡∏´‡πâ‡∏î‡∏±‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏ï‡∏≤‡∏° Level
        floor = 1.0 if level == 1 else 0.8 if level <= 3 else 0.5
        for p in required_phases:
            if pdca_raw[p] > 0.1: # ‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ö‡πâ‡∏≤‡∏á
                pdca_scored[p] = max(pdca_raw[p], floor)

        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏∏‡∏ó‡∏ò‡∏¥ (0.0 - 2.0)
        sum_req = sum(pdca_scored[p] for p in required_phases)
        max_req = len(required_phases) * 2.0
        normalized_score = round((sum_req / max_req) * 2.0, 2) if max_req else 0.0

        # 6. Gatekeeper Decision (‡∏Å‡∏é‡∏Å‡∏≤‡∏£‡∏ú‡πà‡∏≤‡∏ô)
        max_rr = max([float(ev.get("rerank_score", ev.get("score", 0))) for ev in top_evidences] or [0.0])
        is_passed = bool(norm_out.get("is_passed", False))
        
        # Auto-Pass based on score
        if normalized_score >= 1.2: is_passed = True
        
        # ‡∏Å‡∏é‡πÄ‡∏´‡∏•‡πá‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á (L4-L5)
        if level >= 4:
            if max_rr < 0.65: is_passed = False # Retrieval ‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏°‡πà‡∏ô‡∏à‡∏£‡∏¥‡∏á
            # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ Check/Act ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡∏ò‡∏£‡∏£‡∏°
            for critical in [p for p in ["C", "A"] if p in required_phases]:
                if pdca_raw[critical] < 0.4: is_passed = False

        # 7. Coaching Insight & Anti-IT Ghost Pattern
        coaching = str(norm_out.get("coaching_insight", norm_out.get("reason", "")))
        it_patterns = r"(‡∏£‡∏∞‡∏ö‡∏ö‡∏™‡∏≤‡∏£‡∏™‡∏ô‡πÄ‡∏ó‡∏®‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥|KMS|Software|‡πÅ‡∏≠‡∏õ‡∏û‡∏•‡∏¥‡πÄ‡∏Ñ‡∏ä‡∏±‡∏ô|IT System|‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏£‡∏∞‡∏ö‡∏ö|Digital Platform)"
        cleaned_coaching = re.sub(it_patterns, replacement_term, coaching, flags=re.IGNORECASE)

        # 8. ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡πÑ‡∏õ‡∏¢‡∏±‡∏á UI
        return {
            "score": normalized_score if is_passed else min(normalized_score, 0.9),
            "is_passed": is_passed,
            "pdca_breakdown": pdca_scored, # ‡∏ï‡∏±‡∏ß‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ó‡∏≥‡πÉ‡∏´‡πâ Matrix ‡∏Ç‡∏∂‡πâ‡∏ô‡∏™‡∏µ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß
            "pdca_raw": pdca_raw,
            "required_phases": required_phases,
            "reason": norm_out.get("reason", "‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô SE-AM"),
            "coaching_insight": f"[{'STRENGTH' if is_passed else 'GAP'}] {cleaned_coaching}",
            "max_rerank": max_rr,
            "metadata": {"tenant": tenant, "enabler": enabler}
        }
        
    def _get_fallback_result(self, prefix: str) -> Dict[str, Any]:
        """Fallback ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á JSON ‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢"""
        return {
            "score": 0.0,
            "is_passed": False,
            "pdca_breakdown": {"P": 0.0, "D": 0.0, "C": 0.0, "A": 0.0},
            "reason": "AI Output Parse Error - ‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏ó‡∏≥‡∏á‡∏≤‡∏ô",
            "coaching_insight": "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏™‡∏£‡∏∏‡∏õ‡πÑ‡∏î‡πâ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏ô‡∏ö‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á",
            "required_phases": []
        }

    def _expand_context_with_neighbor_pages(self, top_evidences: List[Any], collection_name: str) -> List[Dict[str, Any]]:
        """
        [ULTIMATE REVISE v2026.1.25]
        - Log Transparency: ‡πÅ‡∏™‡∏î‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏£‡∏¥‡∏á‡πÅ‡∏ó‡∏ô UUID ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ Neighbor Fetch
        - Smart Offsets: ‡∏õ‡∏£‡∏±‡∏ö‡∏ä‡πà‡∏ß‡∏á‡∏Å‡∏≤‡∏£‡∏Å‡∏ß‡∏≤‡∏î‡∏´‡∏ô‡πâ‡∏≤‡∏Å‡∏£‡∏∞‡∏î‡∏≤‡∏©‡∏ï‡∏≤‡∏°‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
        - Metadata Enrichment: ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡πÇ‡∏¢‡∏á ID ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏£‡∏¥‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô
        """
        if not self.vectorstore_manager or not top_evidences:
            return top_evidences

        standardized_evidences = []
        for d in top_evidences:
            # 1. Normalize ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á Dict ‡πÅ‡∏•‡∏∞ Langchain Document)
            orig_score = d.get('score', d.get('rerank_score', 0.5)) if isinstance(d, dict) else getattr(d, 'metadata', {}).get('score', 0.5)
            
            if hasattr(d, 'page_content'):
                standardized_evidences.append({
                    "text": d.page_content, 
                    "metadata": getattr(d, 'metadata', {}), 
                    "score": orig_score
                })
            elif isinstance(d, dict):
                d['score'] = orig_score
                standardized_evidences.append(deepcopy(d))

        expanded_evidences = list(standardized_evidences)
        # ‡πÉ‡∏ä‡πâ stable_doc_uuid ‡∏´‡∏£‡∏∑‡∏≠ source_id ‡πÄ‡∏õ‡πá‡∏ô key ‡∏´‡∏•‡∏±‡∏Å‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Å‡∏±‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ã‡πâ‡∏≥
        seen_keys = {
            f"{ev.get('metadata', {}).get('stable_doc_uuid', ev.get('metadata', {}).get('doc_id'))}_{ev.get('metadata', {}).get('page_label')}" 
            for ev in standardized_evidences
        }
        
        added_count = 0
        max_neighbors = 15  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏µ‡∏î‡∏à‡∏≥‡∏Å‡∏±‡∏î‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°
        
        for doc in standardized_evidences:
            if added_count >= max_neighbors: break
            
            meta = doc.get('metadata', {})
            doc_uuid = meta.get("stable_doc_uuid") or meta.get("doc_id") or meta.get("source_id")
            if not doc_uuid: continue

            try:
                curr_page = int(str(meta.get("page_label", "1")).strip())
            except (ValueError, TypeError): 
                continue

            # üéØ ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å Map (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏û‡πà‡∏ô Log ‡πÅ‡∏•‡∏∞‡πÉ‡∏™‡πà‡πÉ‡∏ô Metadata)
            display_filename = self.doc_id_to_filename_map.get(doc_uuid, f"DOC-{str(doc_uuid)[:8]}")

            # üß† Smart Offsets Logic: ‡∏õ‡∏£‡∏±‡∏ö‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ï‡∏≤‡∏°‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
            text_lower = doc.get('text', '').lower()
            offsets = [1] # Default ‡∏Ñ‡∏∑‡∏≠‡∏î‡∏π‡∏´‡∏ô‡πâ‡∏≤‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
            
            # ‡∏Å‡∏£‡∏ì‡∏µ‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏ß‡∏Å‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢/‡πÅ‡∏ú‡∏ô (‡∏°‡∏±‡∏Å‡∏à‡∏∞‡∏¢‡∏≤‡∏ß‡πÑ‡∏õ‡∏Ç‡πâ‡∏≤‡∏á‡∏´‡∏ô‡πâ‡∏≤)
            if any(k in text_lower for k in ["‡∏ß‡∏¥‡∏™‡∏±‡∏¢‡∏ó‡∏±‡∏®‡∏ô‡πå", "‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢", "‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå", "‡πÅ‡∏ú‡∏ô‡πÅ‡∏°‡πà‡∏ö‡∏ó"]):
                offsets = [-1, 1, 2]
            # ‡∏Å‡∏£‡∏ì‡∏µ‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏ß‡∏Å‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô/‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏• (‡∏°‡∏±‡∏Å‡∏à‡∏∞‡∏°‡∏µ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏ô‡πâ‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤)
            elif any(k in text_lower for k in ["‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•", "‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô", "‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô", "‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô", "lesson learned"]):
                offsets = [-2, -1, 1]

            for off in sorted(list(set(offsets))):
                if off == 0: continue
                target_page = curr_page + off
                
                # ‡∏Ç‡πâ‡∏≤‡∏°‡∏ñ‡πâ‡∏≤‡∏´‡∏ô‡πâ‡∏≤ < 1 ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏Ñ‡∏¢‡∏î‡∏∂‡∏á‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß
                if target_page < 1 or f"{doc_uuid}_{target_page}" in seen_keys: 
                    continue
                
                # ‡∏î‡∏∂‡∏á Chunks ‡∏à‡∏≤‡∏Å VectorStoreManager
                neighbor_chunks = self.vectorstore_manager.get_chunks_by_page(
                    collection_name, 
                    doc_uuid, 
                    str(target_page)
                )
                
                if neighbor_chunks:
                    # ‚ûï ‡∏û‡πà‡∏ô Log ‡∏ó‡∏µ‡πà‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå‡∏≠‡πà‡∏≤‡∏ô‡∏≠‡∏≠‡∏Å (‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏£‡∏¥‡∏á)
                    self.logger.info(
                        f"‚ûï Neighbor Fetch: ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏ô‡πâ‡∏≤ {target_page} "
                        f"‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå {os.path.basename(display_filename)} ({len(neighbor_chunks)} chunks)"
                    )

                    for nc in neighbor_chunks:
                        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Metadata ‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏Ç‡∏∂‡πâ‡∏ô
                        new_meta = {**nc.metadata}
                        new_meta["is_supplemental"] = True
                        new_meta["pdca_tag"] = "Support"
                        new_meta["filename"] = display_filename  # ‡πÉ‡∏™‡πà‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏£‡∏¥‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÄ‡∏•‡∏¢
                        
                        new_ev = {
                            "text": nc.page_content,
                            "metadata": new_meta,
                            "score": doc.get('score', 0.5) * 0.85, # ‡∏•‡∏î‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏•‡∏á‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢
                            "is_supplemental": True
                        }
                        expanded_evidences.append(new_ev)
                        seen_keys.add(f"{doc_uuid}_{target_page}")
                    
                    added_count += 1

        return expanded_evidences
    
    def _resolve_evidence_filenames(self, evidence_entries: List[Any]) -> List[Dict[str, Any]]:
        """
        [ULTIMATE SAFE v2026.3] - ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Error 'str' has no attribute 'get'
        - ‡∏Å‡∏≤‡∏£‡∏±‡∏ô‡∏ï‡∏µ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ List of Dictionary ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
        - ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô String ‡∏´‡∏£‡∏∑‡∏≠ None
        """
        resolved_entries = []
        if not evidence_entries:
            return []

        for entry in evidence_entries:
            # üõ°Ô∏è GUARD 1: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô Dictionary ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            if not isinstance(entry, dict):
                # ‡∏ñ‡πâ‡∏≤‡∏´‡∏•‡∏∏‡∏î‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô String ‡∏´‡∏£‡∏∑‡∏≠ Langchain Document ‡πÉ‡∏´‡πâ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÅ‡∏õ‡∏•‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πâ‡∏≤‡∏°
                if hasattr(entry, 'page_content'): # ‡∏Å‡∏£‡∏ì‡∏µ‡πÄ‡∏õ‡πá‡∏ô Document Object
                    entry = {
                        "content": entry.page_content,
                        "metadata": getattr(entry, 'metadata', {}),
                        "doc_id": getattr(entry, 'metadata', {}).get('doc_id', str(hash(entry.page_content)))
                    }
                else:
                    self.logger.warning(f"‚ö†Ô∏è Skipping non-dict evidence: {type(entry)}")
                    continue

            resolved_entry = deepcopy(entry)
            doc_id = resolved_entry.get("doc_id", "")
            
            # üõ°Ô∏è GUARD 2: Metadata Check
            meta = resolved_entry.get("metadata", {})
            if not isinstance(meta, dict): meta = {}
            
            # ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏´‡∏•‡πà‡∏á
            meta_filename = meta.get("source") or meta.get("source_filename") or meta.get("filename")
            content_raw = resolved_entry.get('content') or resolved_entry.get('text', '')
            page_label = str(meta.get("page_label") or meta.get("page") or meta.get("page_number") or "N/A")

            # 1. AI Generated / Missing Content Case
            if not content_raw:
                # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ ‡πÅ‡∏ï‡πà‡∏°‡∏µ ID ‡πÉ‡∏´‡πâ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô Reference ‡∏ß‡πà‡∏≤‡∏á
                resolved_entry["filename"] = "MISSING-CONTENT"
                resolved_entry["display_source"] = f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ (ID: {doc_id})"
            
            # 2. Match ‡πÉ‡∏ô Map
            elif doc_id in self.doc_id_to_filename_map:
                mapped_name = self.doc_id_to_filename_map[doc_id]
                resolved_entry["filename"] = mapped_name
                resolved_entry["display_source"] = f"{os.path.basename(mapped_name)} (‡∏´‡∏ô‡πâ‡∏≤ {page_label})"
            
            # 3. Match ‡πÉ‡∏ô Metadata
            elif meta_filename:
                clean_name = os.path.basename(str(meta_filename))
                resolved_entry["filename"] = clean_name
                resolved_entry["display_source"] = f"{clean_name} (‡∏´‡∏ô‡πâ‡∏≤ {page_label})"

            # 4. Fallback ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
            else:
                short_id = str(doc_id)[:8] if doc_id else "UNKNOWN"
                resolved_entry["filename"] = f"DOC-{short_id}"
                resolved_entry["display_source"] = f"‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á {short_id} (‡∏´‡∏ô‡πâ‡∏≤ {page_label})"

            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ Key ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ñ‡∏£‡∏ö‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏û‡∏¥‡πà‡∏°
            if 'content' not in resolved_entry and content_raw:
                resolved_entry['content'] = content_raw

            resolved_entries.append(resolved_entry)
            
        return resolved_entries

    # ----------------------------------------------------------------------
    # üéØ FINAL FIX 2.3: Manual Map Reload Function (inside SEAMPDCAEngine)
    # ----------------------------------------------------------------------
    def _collect_previous_level_evidences(self, sub_id: str, current_level: int) -> Dict[str, List[Dict]]:
        """
        [REVISED v2026.1.25] - Nested Key & Context Hydration
        - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Format ‡∏Ñ‡∏µ‡∏¢‡πå‡πÉ‡∏´‡∏°‡πà "1.1_L1"
        - ‡∏î‡∏∂‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏ï‡πá‡∏°‡∏à‡∏≤‡∏Å VectorStore ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡πá‡∏ô Baseline ‡πÉ‡∏´‡πâ‡πÄ‡∏•‡πÄ‡∏ß‡∏•‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
        """
        if getattr(self, 'is_parallel_all_mode', False):
            return {}

        collected = {}
        # üîÑ ‡∏õ‡∏£‡∏±‡∏ö Logic ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á Key ‡πÉ‡∏´‡πâ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á 1.1.L1 ‡πÅ‡∏•‡∏∞ 1.1_L1
        for key, bucket in self.evidence_map.items():
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏µ‡∏¢‡πå (‡πÄ‡∏ä‡πà‡∏ô 1.1_L1 ‡∏´‡∏£‡∏∑‡∏≠ 1.1.L1)
            if key.startswith(f"{sub_id}_L") or key.startswith(f"{sub_id}.L"):
                try:
                    # ‡πÅ‡∏¢‡∏Å‡πÄ‡∏≠‡∏≤‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç Level ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤ (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á "_" ‡πÅ‡∏•‡∏∞ ".")
                    level_part = key.replace(f"{sub_id}_L", "").replace(f"{sub_id}.L", "")
                    level_num = int(level_part)
                    
                    if level_num < current_level:
                        # ‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ List ‡∏Ç‡∏≠‡∏á evidences ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤
                        ev_list = bucket.get("evidences", []) if isinstance(bucket, dict) else bucket
                        collected[key] = ev_list
                except: continue

        if not collected: return {}

        # 1. ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏° Unique IDs (Stable ID logic)
        stable_ids = set()
        for ev_list in collected.values():
            for ev in ev_list:
                sid = ev.get("stable_doc_uuid") or ev.get("doc_id")
                if sid and str(sid).lower() not in ["n/a", "none", ""]:
                    stable_ids.add(str(sid))

        if not stable_ids: return collected

        # 2. Bulk Hydration (Query ‡∏à‡∏≤‡∏Å VectorStore ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏≠‡∏≤ Text ‡πÄ‡∏ï‡πá‡∏°)
        vsm = self.vectorstore_manager
        chunk_map = {}
        try:
            # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å VectorStore ‡∏°‡∏≤‡∏Ñ‡∏∑‡∏ô‡∏ä‡∏µ‡∏û (Restore) ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
            full_chunks = vsm.get_documents_by_id(list(stable_ids), self.doc_type, self.enabler)
            for chunk in full_chunks:
                m = chunk.metadata
                keys = [str(m.get(k)) for k in ["stable_doc_uuid", "doc_id", "chunk_uuid"] if m.get(k)]
                for k in keys:
                    chunk_map[k] = {"text": chunk.page_content, "metadata": m}
                    chunk_map[k.replace("-", "")] = {"text": chunk.page_content, "metadata": m}
        except Exception as e:
            self.logger.error(f"‚ùå Hydration VSM Error: {e}")
            return collected

        # 3. Restoration Loop (‡∏¢‡∏±‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏ï‡πá‡∏°‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤ Evidence List)
        restored_count = 0
        for key, ev_list in collected.items():
            for ev in ev_list:
                sid = str(ev.get("stable_doc_uuid") or ev.get("doc_id") or "")
                data = chunk_map.get(sid) or chunk_map.get(sid.replace("-", ""))

                if data:
                    ev.update({
                        "text": data["text"],
                        "metadata": data.get("metadata", {}),
                        "is_baseline": True  # üö© Mark ‡πÑ‡∏ß‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ AI ‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏á‡πÄ‡∏Å‡πà‡∏≤
                    })
                    restored_count += 1
                
        self.logger.info(f"‚úÖ Hydrated {restored_count} baseline chunks for {sub_id} L{current_level}")
        return collected

    def _get_contextual_rules_prompt(self, sub_id: str, level: int) -> str:
        """
        [REVISED v2026.1.25]
        - ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö (Specific Rules)
        - ‡∏â‡∏µ‡∏î L5 Special Rule ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£ "Reset ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô" ‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
        """
        sub_id_rules = self.contextual_rules_map.get(sub_id, {})
        rule_text = ""
        
        # 1. ‡∏Å‡∏é‡∏£‡∏≤‡∏¢ Level (‡∏î‡∏∂‡∏á‡∏°‡∏≤‡∏à‡∏≤‡∏Å config)
        level_key = f"L{level}"
        specific_rule = sub_id_rules.get(level_key)
        if specific_rule:
            rule_text += f"\n[CRITICAL RULE L{level}]\n{specific_rule}\n"
        
        # 2. üéñÔ∏è L5 SPECIAL RULE: ‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏•‡∏¥‡∏®
        if level == 5:
            rule_text += """
            \n--- [JUDICIAL GUIDELINE FOR LEVEL 5] ---
            * ‡∏ó‡πà‡∏≤‡∏ô‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö 'Excellence' (L5)
            * **Score Continuity:** ‡∏´‡∏≤‡∏Å‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô P-D-C-A ‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÄ‡∏î‡∏¥‡∏° (Baseline) ‡∏°‡∏±‡πà‡∏ô‡∏Ñ‡∏á‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß ‡∏´‡πâ‡∏≤‡∏°‡∏•‡∏î‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤ 7.0
            * **Excellence Bonus (+2.0):** ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô '‡∏Ñ‡∏ß‡∏≤‡∏°‡πÇ‡∏î‡∏î‡πÄ‡∏î‡πà‡∏ô' ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 1 ‡∏≠‡∏¢‡πà‡∏≤‡∏á ‡πÄ‡∏ä‡πà‡∏ô:
                - ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏ô‡πÅ‡∏ö‡∏ö (Best Practice/Role Model)
                - ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ä‡∏≤‡∏ï‡∏¥/‡∏ô‡∏≤‡∏ô‡∏≤‡∏ä‡∏≤‡∏ï‡∏¥
                - ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç ROI ‡∏´‡∏£‡∏∑‡∏≠ Business Impact ‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
            * **Final Decision:** ‡∏´‡∏≤‡∏Å‡∏û‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÇ‡∏î‡∏î‡πÄ‡∏î‡πà‡∏ô ‡πÉ‡∏´‡πâ‡∏õ‡∏£‡∏±‡∏ö Score ‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏õ‡πá‡∏ô 9.0+ ‡πÅ‡∏•‡∏∞‡πÄ‡∏ã‡∏ï is_passed = true ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
            """
            
        return rule_text

    def _load_rubric(self) -> Dict[str, Any]:
        """ Loads the SEAM rubric JSON file using path_utils. """
        
        # üéØ FIX: ‡πÉ‡∏ä‡πâ get_rubric_file_path ‡∏à‡∏≤‡∏Å path_utils ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Path ‡πÄ‡∏≠‡∏á
        filepath = None # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô UnboundLocalError
        
        try:
            # 1. ‡∏£‡∏±‡∏ö Path ‡∏à‡∏≤‡∏Å path_utils ‡∏ã‡∏∂‡πà‡∏á‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏ä‡∏µ‡πâ‡πÑ‡∏õ‡∏ó‡∏µ‡πà 'config/' ‡πÅ‡∏•‡πâ‡∏ß
            filepath = get_rubric_file_path(
                tenant=self.config.tenant,
                enabler=self.enabler
            )
        except Exception as e:
            # ‡∏î‡∏±‡∏Å‡∏à‡∏±‡∏ö Exception ‡∏´‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏î‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô path_utils
            self.logger.error(f"‚ùå FATAL: Error calling get_rubric_file_path: {e}")
            return {} 

        # 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if filepath is None or not os.path.exists(filepath):
            self.logger.error(f"‚ö†Ô∏è Rubric file not found at expected path: {filepath}")
            return {}

        # 3. ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå JSON
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
            self.logger.info(f"‚úÖ Rubric loaded successfully from: {filepath}")
            return data
        except json.JSONDecodeError:
            self.logger.error(f"‚ùå Error decoding Rubric JSON. File might be corrupted: {filepath}")
            return {}
        except Exception as e:
            self.logger.error(f"‚ùå Error loading Rubric file from {filepath}: {e}")
            return {}
    
    # -------------------- Helper Function for Map Processing --------------------
    def _get_level_order_value(self, level_str: str) -> int:
        """Converts Level string ('L1', 'L5') to an integer for comparison."""
        try:
            return int(level_str.upper().replace('L', ''))
        except:
            return 0

    
    def _clean_temp_entries(self, evidence_map: Dict[str, List[Any]]) -> Dict[str, List[Dict]]:
        """
        [ULTIMATE SANITIZER v2026.1.23]
        ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î Evidence Map ‡πÅ‡∏ö‡∏ö‡πÄ‡∏ö‡πá‡∏î‡πÄ‡∏™‡∏£‡πá‡∏à:
        1. ‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡∏¢‡∏∞ (TEMP-, HASH-, Unknown)
        2. ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ã‡πâ‡∏≥ (Deduplication) ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô Level ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
        3. ‡∏ã‡πà‡∏≠‡∏°‡πÅ‡∏ã‡∏° Metadata (Filename, Page)
        4. ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Type Error (Type-Safe Processing)
        """
        if not evidence_map or not isinstance(evidence_map, dict):
            return {}

        cleaned_map = {}
        stats = {"removed": 0, "fixed": 0, "dupes": 0}

        for key, entries in evidence_map.items():
            if not isinstance(entries, list):
                continue
                
            valid_entries = []
            seen_doc_ids = set() # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Deduplication ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô Key (Level) ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô

            for entry in entries:
                # --- üõ°Ô∏è ‡∏ä‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà 1: Type Validation ---
                if not isinstance(entry, dict):
                    if isinstance(entry, str) and len(entry.strip()) > 5:
                        # ‡∏Å‡∏£‡∏ì‡∏µ‡∏´‡∏•‡∏∏‡∏î‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô String (‡πÄ‡∏ä‡πà‡∏ô ID ‡∏´‡∏£‡∏∑‡∏≠ Content) ‡πÉ‡∏´‡πâ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á Dict ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
                        entry = {
                            "doc_id": entry.strip(), 
                            "filename": "Unknown_Reference.pdf", 
                            "relevance_score": 0.1,
                            "page": "N/A"
                        }
                    else:
                        stats["removed"] += 1
                        continue

                # --- üõ°Ô∏è ‡∏ä‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà 2: Garbage Filtering ---
                doc_id = entry.get("doc_id") or entry.get("chunk_uuid")
                if not doc_id:
                    stats["removed"] += 1
                    continue
                
                doc_id_str = str(doc_id).strip()

                # ‡∏Å‡∏£‡∏≠‡∏á‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á ‡∏´‡∏£‡∏∑‡∏≠ Keyword ‡∏Ç‡∏¢‡∏∞
                if doc_id_str.lower() in ["none", "unknown", "n/a", "", "null"]:
                    stats["removed"] += 1
                    continue

                # ‡∏Å‡∏£‡∏≠‡∏á Prefix ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß
                if doc_id_str.startswith(("TEMP-", "HASH-", "REF-")):
                    stats["removed"] += 1
                    continue

                # --- üõ°Ô∏è ‡∏ä‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà 3: Deduplication (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡∏ã‡πâ‡∏≥) ---
                if doc_id_str in seen_doc_ids:
                    stats["dupes"] += 1
                    continue
                seen_doc_ids.add(doc_id_str)

                # --- üõ°Ô∏è ‡∏ä‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà 4: Metadata Repair & Normalization ---
                # 1. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå (Filename)
                filename = str(entry.get("filename") or entry.get("source", "")).strip()
                if not filename or filename.lower() in ["unknown", "none", "n/a", "unknown_file.pdf"]:
                    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ ID ‡∏¢‡πà‡∏≠‡∏°‡∏≤‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏ó‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ User ‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÑ‡∏î‡πâ
                    short_id = doc_id_str[:8]
                    entry["filename"] = f"Reference_{short_id}.pdf"
                    stats["fixed"] += 1
                else:
                    # Clean path ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÅ‡∏Ñ‡πà‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå
                    try:
                        entry["filename"] = os.path.basename(filename)
                    except:
                        entry["filename"] = filename

                # 2. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô (Relevance Score)
                try:
                    score = float(entry.get("relevance_score", 0.0))
                    entry["relevance_score"] = max(0.0, min(1.0, score)) # ‡∏ö‡∏µ‡∏ö‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á 0-1
                except (ValueError, TypeError):
                    entry["relevance_score"] = 0.0

                # 3. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤ (Page)
                if "page" not in entry or entry["page"] is None:
                    entry["page"] = entry.get("page_label") or "N/A"

                valid_entries.append(entry)

            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Key ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û
            if valid_entries:
                # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á (‡∏à‡∏≤‡∏Å‡∏°‡∏≤‡∏Å‡πÑ‡∏õ‡∏ô‡πâ‡∏≠‡∏¢)
                valid_entries.sort(key=lambda x: x.get("relevance_score", 0.0), reverse=True)
                cleaned_map[key] = valid_entries

        self.logger.info(
            f"üßπ [CLEAN-MAP] Stats: Removed={stats['removed']}, Fixed={stats['fixed']}, Dupes={stats['dupes']}"
        )
        return cleaned_map

    def _clean_map_for_json(self, data_map: Dict[str, Any]) -> Dict[str, Any]:
        """
        [v2026.1.23] ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô JSON-Compatible
        - ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô TypeError ‡∏à‡∏≤‡∏Å Object ‡∏ó‡∏µ‡πà JSON ‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å
        - ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô AttributeError ‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ï‡πà‡∏≠
        """
        if not isinstance(data_map, dict):
            return {}
        
        clean_data = {}
        for k, v in data_map.items():
            str_key = str(k) # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö Key ‡πÄ‡∏õ‡πá‡∏ô String
            
            if isinstance(v, dict):
                clean_data[str_key] = self._clean_map_for_json(v)
            elif isinstance(v, list):
                clean_data[str_key] = [
                    (self._clean_map_for_json(item) if isinstance(item, dict) 
                     else (str(item) if not isinstance(item, (str, int, float, bool)) and item is not None else item))
                    for item in v
                ]
            elif isinstance(v, (str, int, float, bool)) or v is None:
                clean_data[str_key] = v
            else:
                # ‡πÅ‡∏õ‡∏•‡∏á Object ‡∏≠‡∏∑‡πà‡∏ô‡πÜ (‡πÄ‡∏ä‡πà‡∏ô Datetime, UUID) ‡πÄ‡∏õ‡πá‡∏ô String ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
                clean_data[str_key] = str(v)
                
        return clean_data
    

    def _group_statements_by_sub_criteria(
        self,
        flat_statements: List[Dict[str, Any]]
    ) -> Dict[str, Dict[str, Any]]:
        """
        Group flattened rubric statements into Sub-Criteria bundles.

        Expected input item structure (PEA compatible):
        {
            "sub_id": "1.1",
            "sub_criteria_name": "...",
            "weight": 4,
            "levels": [
                { "level": 1, "statement": "..." },
                ...
            ]
        }
        """
        grouped: Dict[str, Dict[str, Any]] = {}

        for item in flat_statements:
            sub_id = item.get("sub_id")

            # üîí Minimal validation only (‡∏≠‡∏¢‡πà‡∏≤ strict ‡πÄ‡∏Å‡∏¥‡∏ô)
            if not sub_id:
                self.logger.warning(f"‚ö†Ô∏è Skip item without sub_id: {item}")
                continue

            levels = item.get("levels")

            if not isinstance(levels, list) or not levels:
                self.logger.warning(
                    f"‚ö†Ô∏è Skip sub_id {sub_id}: invalid or empty levels"
                )
                continue

            sub_id = str(sub_id)

            # ‚úÖ init group
            if sub_id not in grouped:
                grouped[sub_id] = {
                    "sub_id": sub_id,
                    "sub_criteria_name": item.get("sub_criteria_name", ""),
                    "weight": float(item.get("weight", 0.0)),
                    "levels": []
                }

            # ‚úÖ normalize each level
            for lv in levels:
                level_no = lv.get("level")
                statement = lv.get("statement")

                if level_no is None or not statement:
                    self.logger.warning(
                        f"‚ö†Ô∏è Skip invalid level in {sub_id}: {lv}"
                    )
                    continue

                grouped[sub_id]["levels"].append({
                    "level": int(level_no),
                    "statement": statement,
                    "keywords": lv.get("keywords", []),
                    "score_threshold": lv.get("score_threshold"),
                    "raw": lv
                })

        # üîí sort L1 ‚Üí L5 and drop empty subs
        cleaned_grouped = {}
        for sub_id, sub in grouped.items():
            if not sub["levels"]:
                self.logger.warning(
                    f"‚ö†Ô∏è Drop sub_id {sub_id}: no valid levels after normalization"
                )
                continue

            sub["levels"] = sorted(
                sub["levels"], key=lambda x: x["level"]
            )
            cleaned_grouped[sub_id] = sub

        self.logger.info(
            f"üì¶ Grouped rubric into {len(cleaned_grouped)} sub-criteria bundles"
        )

        return cleaned_grouped

    # -------------------- Statement Preparation & Filtering Helpers --------------------
    def _flatten_rubric_to_statements(self, rubric_data: Optional[Dict] = None) -> List[Dict[str, Any]]:
        """
        [ULTIMATE REVISED v2026.1.24 - PROMPT-READY VERSION]
        ‡πÅ‡∏õ‡∏•‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Rubric ‡∏ó‡∏µ‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô Flat List ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡∏Å‡∏±‡∏î Focus Points 
        ‡πÅ‡∏•‡∏∞ Evidence Guidelines ‡∏£‡∏≤‡∏¢ Level ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ LLM Agent ‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥
        """
        # 1. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Source ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: Argument > self.rubric)
        source_rubric = rubric_data if rubric_data is not None else getattr(self, 'rubric', None)
        
        if not source_rubric:
            self.logger.warning("‚ö†Ô∏è [FLATTEN] Cannot proceed: Source rubric is empty or None.")
            return []
            
        try:
            # ‡πÉ‡∏ä‡πâ deepcopy ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö (Thread-safety)
            data = deepcopy(source_rubric)
            criteria_map = data.get('criteria', {}) if isinstance(data, dict) else {}
            
            if not criteria_map:
                 self.logger.error("‚ùå [FLATTEN] Invalid structure: 'criteria' key not found.")
                 return []
                 
            extracted_list = []
            
            # 2. Loop ‡πÄ‡∏à‡∏≤‡∏∞‡∏•‡∏∂‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö Criteria ‡πÅ‡∏•‡∏∞ Sub-Criteria
            for criteria_id, criteria_data in criteria_map.items():
                if not isinstance(criteria_data, dict): continue
                    
                sub_criteria_map = criteria_data.get('subcriteria', {})
                criteria_name = criteria_data.get('name', 'Unknown Criteria')
                
                for sub_id, sub_data in sub_criteria_map.items():
                    if not isinstance(sub_data, dict): continue
                    
                    # --- [CORE EXTRACTION] ---
                    # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Focus Points: ‡πÅ‡∏õ‡∏•‡∏á List ‡πÄ‡∏õ‡πá‡∏ô String ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏™‡πà‡πÉ‡∏ô Prompt ‡∏á‡πà‡∏≤‡∏¢‡πÜ
                    fps = sub_data.get('focus_points', [])
                    focus_points_str = " | ".join(fps) if isinstance(fps, list) else str(fps or "-")
                    
                    # ‡πÄ‡∏Å‡πá‡∏ö Evidence Guidelines ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÑ‡∏ß‡πâ‡∏Å‡πà‡∏≠‡∏ô (Dictionary)
                    all_guidelines = sub_data.get('evidence_guidelines', {})

                    # ‡∏™‡∏£‡πâ‡∏≤‡∏á Base Object ‡∏Ç‡∏≠‡∏á Sub-Criteria ‡∏ô‡∏µ‡πâ
                    item = {
                        'criteria_id': criteria_id,
                        'criteria_name': criteria_name,
                        'sub_id': sub_id,
                        'sub_criteria_name': sub_data.get('name', f"{criteria_name} - {sub_id}"),
                        'weight': sub_data.get('weight', criteria_data.get('weight', 0)),
                        'focus_points': focus_points_str,            # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Prompt {focus_points}
                        'evidence_guidelines_all': all_guidelines,    # ‡πÅ‡∏´‡∏•‡πà‡∏á‡πÄ‡∏Å‡πá‡∏ö‡∏ï‡πâ‡∏ô‡∏ó‡∏≤‡∏á
                        'raw_levels': sub_data.get('levels', {})      # ‡∏£‡∏≠‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ï‡πà‡∏≠
                    }
                    extracted_list.append(item)

            # 3. [LEVEL PROCESSING] ‡πÅ‡∏ï‡∏Å‡∏¢‡πà‡∏≠‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏≤‡∏¢ Level ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ú‡∏π‡∏Å Guideline ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ï‡∏±‡∏ß
            final_list = []
            for sub_item in extracted_list:
                raw_levels = sub_item.pop('raw_levels') 
                processed_levels = []
                
                if isinstance(raw_levels, dict):
                    for level_str, statement in raw_levels.items():
                        try:
                            level_int = int(level_str)
                            # üéØ ‡∏î‡∏∂‡∏á Guideline ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡∏≠‡∏á Level ‡∏ô‡∏±‡πâ‡∏ô‡πÜ (e.g., 'level_1', 'level_2')
                            # ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Default ‡πÄ‡∏õ‡πá‡∏ô "-"
                            current_guideline = sub_item['evidence_guidelines_all'].get(f"level_{level_int}", "-")
                            
                            processed_levels.append({
                                "level": level_int, 
                                "statement": statement,
                                "level_specific_guideline": current_guideline # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Prompt {evidence_guidelines}
                            })
                        except (ValueError, TypeError):
                            self.logger.error(f"‚ùå [FLATTEN] Invalid level key '{level_str}' in Sub-ID: {sub_item['sub_id']}")
                            continue
                
                if processed_levels:
                    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö Level 1 -> 5 ‡πÄ‡∏™‡∏°‡∏≠
                    processed_levels.sort(key=lambda x: x.get("level", 0))
                    sub_item["levels"] = processed_levels
                    final_list.append(sub_item)
                else:
                    self.logger.warning(f"‚ö†Ô∏è [FLATTEN] Sub-criteria {sub_item['sub_id']} has no valid levels.")

            self.logger.info(f"‚úÖ [FLATTEN] Rubric transformation complete. Processed {len(final_list)} sub-criteria.")
            return final_list

        except Exception as e:
            self.logger.error(f"üõë [FLATTEN-ERROR] Failure during flattening: {str(e)}", exc_info=True)
            return []
    
    # -------------------- Evidence Classification Helper (Robust 2026) --------------------
    def _get_mapped_uuids_and_priority_chunks(
        self,
        sub_id: str,
        level: int,
        statement_text: str = "",
        level_constraint: Optional[Any] = None, # üü¢ ‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô Optional ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Error Missing Argument
        vectorstore_manager: Optional['VectorStoreManager'] = None,
        evidence_map: Optional[Dict] = None 
    ) -> Tuple[List[str], List[Dict]]:
        """
        [DYNAMIC CONTINUITY v2026.6.2] - ROBUST SIGNATURE
        ----------------------------------------------
        - Fix: ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÅ‡∏ö‡∏ö‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Error 'missing 1 required positional argument'
        - Logic: ‡∏î‡∏∂‡∏á Baseline ‡∏à‡∏≤‡∏Å Memory/Map ‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á (Inheritance)
        """
        priority_chunks = []
        mapped_stable_ids = []

        # 1. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ä‡πâ Evidence Map (‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: 1. Argument ‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏°‡∏≤ -> 2. Class Attribute -> 3. Empty Dict)
        target_map = evidence_map if evidence_map is not None else getattr(self, 'evidence_map', {})

        # 2. üß† [AUTO-HISTORY & INHERITANCE]
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡∏ú‡πà‡∏≤‡∏ô‡πÉ‡∏ô Level ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏≥‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡πÉ‡∏ô Level ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
        for key, evidences in target_map.items():
            if key.startswith(f"{sub_id}.L") and isinstance(evidences, list):
                try:
                    lvl_in_key = int(key.split(".L")[-1])
                    # ‡∏Å‡∏é Inheritance: ‡πÉ‡∏ä‡πâ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏à‡∏≤‡∏Å Level ‡∏ó‡∏µ‡πà <= ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
                    if lvl_in_key <= level:
                        history_items = deepcopy(evidences)
                        for item in history_items:
                            item["is_baseline"] = True
                            # ‡∏ö‡∏π‡∏™‡∏ï‡πå‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡∏≠‡∏á RAG
                            item["rerank_score"] = max(item.get("rerank_score", 0.0), 0.85)
                        priority_chunks.extend(history_items)
                except (ValueError, IndexError):
                    continue

        # 3. üîç [SEMANTIC HINTING] ‡∏Å‡∏£‡∏ì‡∏µ L1 ‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡πÉ‡∏ô Map
        if not priority_chunks and level == 1:
            rule_config = getattr(self, 'contextual_rules_map', {}).get(sub_id, {}).get(str(level), {})
            hints = rule_config.get("plan_keywords", [])[:2]
            if hints and vectorstore_manager:
                self.logger.info(f"üîé L1 Discovery: Searching using hints: {hints}")
                try:
                    # ‡πÅ‡∏Å‡πâ‡∏à‡∏∏‡∏î 1: ‡πÉ‡∏ä‡πâ helper ‡∏à‡∏£‡∏¥‡∏á + ‡∏î‡∏∂‡∏á enabler ‡∏à‡∏≤‡∏Å self
                    enabler = getattr(self, 'enabler', 'KM').upper()  # fallback KM ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ
                    collection_name = get_doc_type_collection_key("evidence", enabler)
                    
                    discovery_docs = vectorstore_manager.retrieve(
                        query=f"{sub_id} {' '.join(hints)}",
                        collection_name=collection_name,
                        top_k=5
                    )
                    
                    for doc in discovery_docs:
                        # ‡πÅ‡∏Å‡πâ‡∏à‡∏∏‡∏î 2: fallback chunk_uuid ‡πÅ‡∏•‡∏∞ source ‡πÉ‡∏´‡πâ‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
                        chunk_uuid = (
                            doc.metadata.get("chunk_uuid") or
                            doc.metadata.get("id") or
                            doc.metadata.get("chunk_id") or
                            hashlib.sha256(doc.page_content.encode()).hexdigest()[:32]  # last resort
                        )
                        
                        source = (
                            doc.metadata.get("source") or
                            doc.metadata.get("source_filename") or
                            doc.metadata.get("file_path") or
                            "unknown_source"
                        )
                        
                        chunk = {
                            "page_content": doc.page_content,
                            "metadata": doc.metadata or {},
                            "rerank_score": 0.85,
                            "chunk_uuid": chunk_uuid,
                            "source": source
                        }
                        priority_chunks.append(chunk)
                        
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Discovery retrieve failed for {collection_name}: {e}")
                    

        if not priority_chunks:
            return [], []

        # 4. üíß [ROBUST HYDRATION] ‡πÄ‡∏ï‡∏¥‡∏°‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ Full Text ‡∏à‡∏≤‡∏Å Vector DB
        try:
            priority_chunks = self._robust_hydrate_documents_for_priority_chunks(
                chunks_to_hydrate=priority_chunks,
                vsm=vectorstore_manager
            )
        except Exception as e:
            self.logger.error(f"‚ùå Hydration failed in priority module: {e}")

        # 5. üéØ [ID SYNC] ‡∏™‡∏Å‡∏±‡∏î UUID ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ Filter ‡∏Ç‡∏≠‡∏á Main Retriever
        seen_ids = set()
        for chunk in priority_chunks:
            sid = chunk.get("stable_doc_uuid") or chunk.get("doc_id")
            if sid and isinstance(sid, str):
                if sid not in seen_ids and len(sid) >= 32:
                    mapped_stable_ids.append(sid)
                    seen_ids.add(sid)

        self.logger.info(f"‚úÖ Continuity Ready: {len(priority_chunks)} priority chunks | Sub:{sub_id} L{level}")
        return mapped_stable_ids, priority_chunks

    
    def get_actual_score(self, ev: Any) -> float:
        """
        [v2026.1 - ROBUST SCORING EXTRACTOR]
        ‡∏î‡∏∂‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô ‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö Dict ‡∏´‡∏£‡∏∑‡∏≠ Object
        ‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô 0.0 ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ã‡πà‡∏≠‡∏ô‡∏Ñ‡∏µ‡∏¢‡πå‡πÉ‡∏ô Metadata
        """
        if not ev:
            return 0.0

        # 1. ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ñ‡∏µ‡∏¢‡πå‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏ö‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö (‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç)
        score_keys = ["rerank_score", "score", "relevance_score"]
        
        # 2. ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö Top-level ‡∏Å‡πà‡∏≠‡∏ô
        for key in score_keys:
            # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á dict.get() ‡πÅ‡∏•‡∏∞ getattr() ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Document Object
            val = ev.get(key) if isinstance(ev, dict) else getattr(ev, key, None)
            if val is not None:
                try:
                    return float(val)
                except (ValueError, TypeError):
                    continue

        # 3. Fallback: ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏†‡∏≤‡∏¢‡πÉ‡∏ô Metadata
        meta = ev.get("metadata", {}) if isinstance(ev, dict) else getattr(ev, "metadata", {})
        if isinstance(meta, dict):
            for key in score_keys:
                val = meta.get(key)
                if val is not None:
                    try:
                        return float(val)
                    except (ValueError, TypeError):
                        continue

        return 0.0
    
    def _calculate_evidence_strength_cap(
        self,
        top_evidences: List[Any],
        level: int,
        highest_rerank_score: Optional[float] = None
    ) -> Dict[str, Any]:
        """
        [PROTECTED v2026.1.17 - REVISED]
        - ‡πÉ‡∏ä‡πâ get_actual_score ‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô
        - ‡πÅ‡∏¢‡∏Å Logic ‡∏Å‡∏≤‡∏£‡∏´‡∏≤ '‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•' ‡πÉ‡∏´‡πâ‡∏â‡∏•‡∏≤‡∏î‡∏Ç‡∏∂‡πâ‡∏ô
        - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏∞‡∏ö‡∏ö Logging ‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ Audit
        """
        try:
            # ‚öôÔ∏è Load Configuration
            threshold = getattr(self, "RERANK_THRESHOLD", 0.35)
            cap_value = getattr(self, "MAX_EVI_STR_CAP", 5.0)
            
            # 1. ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏°‡∏≤ (Baseline)
            max_score_found = 0.0
            try:
                if highest_rerank_score is not None:
                    max_score_found = float(highest_rerank_score)
            except (ValueError, TypeError):
                max_score_found = 0.0

            max_score_source = "Adaptive_RAG_Loop"
            
            if not isinstance(top_evidences, list):
                top_evidences = []

            # 2. Iterate ‡∏´‡∏≤‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ
            for idx, doc in enumerate(top_evidences, 1):
                # ‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡∏î‡∏∂‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏ö
                current_score = self.get_actual_score(doc)

                # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏à‡∏≠‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤ ‡πÉ‡∏´‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ß‡πâ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏≤
                if current_score > max_score_found:
                    max_score_found = current_score
                    
                    # --- ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö Robust ---
                    meta = doc.get("metadata", {}) if isinstance(doc, dict) else getattr(doc, "metadata", {})
                    if not isinstance(meta, dict): meta = {}
                    
                    max_score_source = (
                        meta.get("source_filename") or 
                        meta.get("file_name") or 
                        meta.get("source") or 
                        f"Doc_{idx}"
                    )

            # 3. Decision Logic (Gated Check)
            # ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏¢‡∏±‡∏á‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤ Threshold (0.35) -> ‡∏™‡∏±‡πà‡∏á Capped
            is_capped = max_score_found < threshold
            # ‡∏Ñ‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤ Prompt: 5.0 (‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô) ‡∏´‡∏£‡∏∑‡∏≠ 10.0 (‡∏°‡∏µ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏ú‡πà‡∏≤‡∏ô)
            max_evi_str_for_prompt = float(cap_value) if is_capped else 10.0

            # üìä Internal Audit Log
            status_icon = "üö® [CAPPED]" if is_capped else "‚úÖ [FULL-STRENGTH]"
            self.logger.info(
                f"{status_icon} L{level} | Best Score: {max_score_found:.4f} "
                f"from: '{os.path.basename(str(max_score_source))}' | Threshold: {threshold}"
            )

            return {
                "is_capped": bool(is_capped),
                "max_evi_str_for_prompt": float(max_evi_str_for_prompt),
                "top_score": round(float(max_score_found), 4),
                "max_score_source": str(max_score_source),
                "threshold_used": threshold
            }

        except Exception as e:
            self.logger.error(f"‚ùå [CRITICAL-CAP-ERROR] {e}", exc_info=True)
            # ‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡πÑ‡∏ß‡πâ‡∏Å‡πà‡∏≠‡∏ô: ‡∏ñ‡πâ‡∏≤ Error ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á Cap ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ LLM ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏î‡πâ
            return {
                "is_capped": False, 
                "max_evi_str_for_prompt": 10.0, 
                "top_score": 0.0, 
                "max_score_source": "Fallback-Error"
            }
       
    
    def _extract_strategic_gaps(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        [HELPER] ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏´‡∏≤‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á (Gaps) ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á‡∏Å‡∏±‡∏ö‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢
        ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡πÄ‡∏™‡∏ô‡∏≠ Coaching Insight ‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°
        """
        gaps = []
        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á‡∏Ç‡∏≠‡∏á Gap (‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ô‡πâ‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡∏Å‡πà‡∏≠‡∏ô)
        sorted_results = sorted(results, key=lambda x: x.get('score', 0.0))
        
        for res in sorted_results:
            sub_id = res.get('sub_id', 'Unknown')
            current_score = res.get('score', 0.0)
            passed = res.get('is_passed', False)
            
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô ‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤ 0.8 ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô Gap
            if not passed or current_score < 0.8:
                gap_info = {
                    "sub_id": sub_id,
                    "level": res.get('level'),
                    "current_score": current_score,
                    "impact": "High" if current_score < 0.5 else "Medium",
                    "reason": res.get('reason', '‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡∏µ‡πâ'),
                    "suggestion": res.get('coaching_insight', '‡∏Ñ‡∏ß‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏Ç‡∏≤‡∏î‡∏´‡∏≤‡∏¢‡∏ï‡∏≤‡∏° PDCA')
                }
                gaps.append(gap_info)
        
        # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ Gap ‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î 5 ‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å
        return gaps[:5]

    def calculate_audit_confidence(
        self,
        matched_chunks: List[Any],
        sub_id: str = "unknown",
        level: int = 1
    ) -> Dict[str, Any]:
        """
        [ULTIMATE AUDIT CONFIDENCE v2026.3.10 ‚Äì Real-Count Enabled]
        - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏•‡∏Ç (1) ‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á Raw Tags ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏ô‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà (Frequency Counting)
        - ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö PDCA ‡∏ú‡πà‡∏≤‡∏ô 3 ‡∏ä‡∏±‡πâ‡∏ô: Metadata -> Tagging -> Keyword Fallback
        - ‡∏õ‡∏£‡∏±‡∏ö Decision Matrix ‡πÉ‡∏´‡πâ‡∏™‡∏°‡∏î‡∏∏‡∏•‡∏Å‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á
        """
        if not matched_chunks:
            return {
                "level": "NONE", "reason": "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö", "source_count": 0,
                "coverage_ratio": 0.0, "pdca_found": [], "valid_chunks_count": 0,
                "traceability_score": 0.0, "recency_bonus": 0.0
            }

        # 1. Quality Gate: ‡∏Ñ‡∏±‡∏î‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Chunk ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏ô‡πâ‡∏ô‡πÜ (Relevance >= 0.40)
        valid_chunks = [doc for doc in matched_chunks if self.get_actual_score(doc) >= 0.40]
        valid_count = len(valid_chunks)

        if valid_count == 0:
            return {
                "level": "LOW", "reason": "‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û",
                "source_count": 0, "coverage_ratio": 0.0, "pdca_found": [], "valid_chunks_count": 0
            }

        # 2. Independence Check: ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        unique_sources = set()
        for doc in valid_chunks:
            meta = getattr(doc, 'metadata', {}) if hasattr(doc, 'metadata') else doc.get('metadata', {})
            src = next((meta.get(k) for k in ['source_filename', 'filename', 'source'] if meta.get(k)), None)
            if src:
                unique_sources.add(os.path.basename(str(src).strip()))
        
        independence_score = len(unique_sources)

        # 3. PDCA Detection (Revised: Multi-Tag Frequency Collection)
        all_detected_tags = [] # ‡πÄ‡∏Å‡πá‡∏ö List ‡∏Ç‡∏≠‡∏á Tag ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠‡πÉ‡∏ô‡∏ó‡∏∏‡∏Å Chunks (‡∏´‡πâ‡∏≤‡∏°‡∏ó‡∏≥ Unique)
        
        # ‡∏î‡∏∂‡∏á Keyword rules ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Fallback
        cum_rules = {}
        if hasattr(self, "get_cumulative_rules_cached"):
            try:
                loaded = self.get_cumulative_rules_cached(sub_id, level)
                if isinstance(loaded, dict):
                    cum_rules = loaded
                else:
                    self.logger.warning(
                        f"[PDCA] cumulative rules invalid type for {sub_id}-{level}"
                    )
            except Exception as e:
                self.logger.error(
                    f"[PDCA] failed to load cumulative rules for {sub_id}-{level}: {e}"
                )
                
        kw_map = {
            "P": [k.lower() for k in cum_rules.get('plan_keywords', [])],
            "D": [k.lower() for k in cum_rules.get('do_keywords', [])],
            "C": [k.lower() for k in cum_rules.get('check_keywords', [])],
            "A": [k.lower() for k in cum_rules.get('act_keywords', [])]
        }

        for doc in valid_chunks:
            meta = getattr(doc, 'metadata', {}) if hasattr(doc, 'metadata') else doc.get('metadata', {})
            tag = (getattr(doc, 'pdca_tag', None) or meta.get('pdca_tag') or meta.get('tag') or "").strip().upper()
            
            chunk_detected_phases = []
            
            # ‡∏ä‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà 1: ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏≤‡∏Å Tag ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
            if tag in ["P", "D", "C", "A"]:
                chunk_detected_phases.append(tag)
            
            # ‡∏ä‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà 2: Fallback Keyword Detection (‡∏ñ‡πâ‡∏≤ Tag ‡∏ß‡πà‡∏≤‡∏á ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á)
            text = (doc.get('text') or doc.get('page_content') or '').lower()
            for phase, kws in kw_map.items():
                if any(k in text for k in kws):
                    if phase not in chunk_detected_phases: # 1 chunk ‡∏ô‡∏±‡∏ö 1 ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå‡∏ï‡πà‡∏≠ 1 ‡πÄ‡∏ü‡∏™
                        chunk_detected_phases.append(phase)
            
            # ‡∏ô‡∏≥ Tags ‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠‡πÉ‡∏ô Chunk ‡∏ô‡∏µ‡πâ‡πÑ‡∏õ‡∏™‡∏∞‡∏™‡∏°‡∏£‡∏ß‡∏° (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÅ‡∏™‡∏î‡∏á‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô Log)
            all_detected_tags.extend(chunk_detected_phases)

        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Coverage Ratio (‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏°‡∏ß‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠ / 4)
        unique_found_phases = set(all_detected_tags)
        coverage_ratio = len(unique_found_phases) / 4.0

        # 4. Traceability & Recency Check
        traceable_count = 0
        recent_count = 0
        # ‚úÖ ‡πÉ‡∏ä‡πâ self.year ‡∏à‡∏≤‡∏Å argument ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏´‡πâ‡∏ñ‡∏≠‡∏¢‡πÑ‡∏õ‡πÉ‡∏ä‡πâ DEFAULT_YEAR ‡∏à‡∏≤‡∏Å global_vars
        from config.global_vars import DEFAULT_YEAR
        try:
            current_year = int(self.year) if self.year else DEFAULT_YEAR
        except (ValueError, TypeError):
            current_year = DEFAULT_YEAR
            
        self.logger.debug(f"üìä [CONFIDENCE] Evaluating recency using year baseline: {current_year}")

        
        for doc in valid_chunks:
            meta = getattr(doc, 'metadata', {}) if hasattr(doc, 'metadata') else doc.get('metadata', {})
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏Ç‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏≤ (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏ó‡∏±‡πâ‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡∏∞‡∏´‡∏ô‡πâ‡∏≤)
            if any(meta.get(k) for k in ['page', 'page_label']) and any(meta.get(k) for k in ['source', 'filename']):
                traceable_count += 1
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏´‡∏°‡πà (Bonus 2-3 ‡∏õ‡∏µ‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á)
            year_val = str(meta.get('year') or meta.get('doc_year') or "")
            if not year_val: # Fallback search in filename
                year_match = re.search(r'(25[67]\d)', str(meta.get('source', '')))
                if year_match: year_val = year_match.group(1)
            
            if year_val.isdigit() and int(year_val) >= current_year - 2:
                recent_count += 1

        trace_score = traceable_count / valid_count if valid_count > 0 else 0
        recency_score = recent_count / valid_count if valid_count > 0 else 0

        # 5. Decision Matrix: ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏±‡πà‡∏ô (Confidence Level)
        if independence_score >= 8 and coverage_ratio >= 0.75:
            conf_level = "HIGH"
            reason = "‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏£‡∏ö‡∏ß‡∏á‡∏à‡∏£ PDCA ‡∏à‡∏≤‡∏Å‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏£‡∏∞‡∏ö‡∏∏‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô"
        elif independence_score >= 4 and coverage_ratio >= 0.50:
            conf_level = "MEDIUM"
            reason = "‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏°‡∏¥‡∏ï‡∏¥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô"
        else:
            conf_level = "LOW"
            reason = "‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏Ñ‡∏£‡∏ö‡∏ß‡∏á‡∏à‡∏£ PDCA ‡∏´‡∏£‡∏∑‡∏≠‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏û‡∏≠"

        # Penalty: ‡∏•‡∏î‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏´‡∏≤‡∏Å‡∏£‡∏∞‡∏ö‡∏∏‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö
        if trace_score < 0.6 and conf_level != "LOW":
            conf_level = "MEDIUM" if conf_level == "HIGH" else "LOW"
            reason += " (‡∏•‡∏î‡∏£‡∏∞‡∏î‡∏±‡∏ö‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå)"

        return {
            "level": conf_level,
            "reason": reason,
            "source_count": independence_score,
            "coverage_ratio": round(coverage_ratio, 3),
            "traceability_score": round(trace_score, 3),
            "recency_bonus": round(recency_score, 3),
            "valid_chunks_count": valid_count,
            "pdca_found": all_detected_tags  # <--- ‡∏™‡πà‡∏á LIST ‡∏î‡∏¥‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏à‡∏£‡∏¥‡∏á‡πÑ‡∏î‡πâ
        }

    def _calculate_weighted_score(
        self, 
        highest_full_level: int, 
        weight: float, 
        level_details: Dict[str, Any] = None
    ) -> float:
        """
        [ULTIMATE REVISED v2026.01.24 - MATURITY-DRIVEN SCORING]
        - üß© Logic: Continuous Base Level + Partial PDCA from the FIRST GAP level.
        - üõ°Ô∏è Governance: ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô Partial ‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏Ç‡πâ‡∏≤‡∏°‡πÄ‡∏•‡πÄ‡∏ß‡∏• (‡∏ô‡∏±‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏•‡πÄ‡∏ß‡∏•‡∏ñ‡∏±‡∏î‡∏à‡∏≤‡∏Å‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏ï‡∏Å‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á)
        - üéØ Precision: ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Scaling ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏∞‡∏ó‡πâ‡∏≠‡∏ô Maturity ‡∏à‡∏£‡∏¥‡∏á‡∏ï‡∏≤‡∏°‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ SE-AM
        """
        # 1. Configuration Setup
        max_lv = getattr(self.config, 'max_level', 5) or 5
        safe_weight = float(weight) if weight else 4.0
        # ‡πÉ‡∏ä‡πâ‡πÇ‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏à‡∏≤‡∏Å Global Vars (PARTIAL_PDCA)
        mode = getattr(self, 'scoring_mode', SCORING_MODE)
        
        # 2. Base Maturity Calculation
        # highest_full_level ‡∏Ñ‡∏∑‡∏≠‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á (‡πÄ‡∏ä‡πà‡∏ô ‡∏ñ‡πâ‡∏≤‡∏ú‡πà‡∏≤‡∏ô L1, L2 ‡πÅ‡∏•‡πâ‡∏ß L3 ‡∏ï‡∏Å ‡∏Ñ‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô 2)
        base_level = float(max(0, min(highest_full_level, max_lv)))
        partial_contribution = 0.0

        # 3. Partial Score Logic (‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏Å‡πá‡∏ö‡∏ï‡∏Å‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà "‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô")
        if mode == 'PARTIAL_PDCA' and level_details:
            # ‡πÄ‡∏•‡πÄ‡∏ß‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏≠‡∏≤‡∏°‡∏≤‡∏Ñ‡∏¥‡∏î‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏®‡∏©‡∏™‡πà‡∏ß‡∏ô ‡∏Ñ‡∏∑‡∏≠‡πÄ‡∏•‡πÄ‡∏ß‡∏•‡∏ñ‡∏±‡∏î‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á
            # ‡πÄ‡∏ä‡πà‡∏ô ‡∏ñ‡πâ‡∏≤‡∏ú‡πà‡∏≤‡∏ô L2 ‡πÄ‡∏ï‡πá‡∏°‡∏ï‡∏±‡∏ß ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÑ‡∏õ‡∏î‡∏π PDCA ‡∏Ç‡∏≠‡∏á L3 ‡∏ß‡πà‡∏≤‡∏ó‡∏≥‡πÑ‡∏î‡πâ‡∏Å‡∏µ‡πà %
            next_lv_idx = int(base_level + 1)
            
            if next_lv_idx <= max_lv:
                lv_data = level_details.get(str(next_lv_idx), {})
                pdca = lv_data.get('pdca_breakdown', {})
                
                if isinstance(pdca, dict) and pdca:
                    # ‡∏î‡∏∂‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏≤‡∏¢ Phase (P, D, C, A) ‡∏°‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢
                    scores = [float(v) for v in pdca.values() if v is not None]
                    if scores:
                        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ç‡∏≠‡∏á PDCA (0.0 - 1.0)
                        raw_partial = sum(scores) / len(scores)
                        # ‡∏ñ‡πà‡∏ß‡∏á‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å: ‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ó‡∏®‡∏ô‡∏¥‡∏¢‡∏° (Max 0.99)
                        partial_contribution = round(raw_partial, 4)
                        
                        self.logger.info(f"‚ûï [PARTIAL-BOOST] Found Gap at L{next_lv_idx}: +{partial_contribution:.2f} PDCA progress")

        # 4. Final Maturity Level Assembly
        # Formula: ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á + ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏∑‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
        # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: ‡∏ú‡πà‡∏≤‡∏ô L2 (Base 2.0) + ‡∏ó‡∏≥ L3 ‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏∂‡πà‡∏á‡∏´‡∏ô‡∏∂‡πà‡∏á (Partial 0.5) = 2.50
        effective_level = min(base_level + partial_contribution, float(max_lv))
        
        # 5. Scaling to Weighted Score
        # ‡πÅ‡∏õ‡∏•‡∏á‡∏£‡∏∞‡∏î‡∏±‡∏ö Maturity ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ñ‡πà‡∏ß‡∏á‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏ï‡∏≤‡∏°‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô
        # Formula: (Effective Level / Max Level) * Weight
        # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: (2.5 / 5.0) * 4.0 = 2.0 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô
        base_ratio = effective_level / max_lv
        final_score = base_ratio * safe_weight

        # 6. Step-Ladder Final Touch
        # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏´‡∏°‡∏î STEP_LADDER ‡πÅ‡∏ó‡πâ‡πÜ ‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏ô‡∏±‡∏ö‡πÄ‡∏®‡∏©‡∏™‡πà‡∏ß‡∏ô (‡πÉ‡∏ä‡πâ base_level ‡πÄ‡∏û‡∏µ‡∏¢‡∏ß‡πÜ)
        if mode == 'STEP_LADDER':
            final_score = (base_level / max_lv) * safe_weight

        final_score = round(final_score, 4)
        
        # 7. Detailed Audit Logging (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÉ‡∏ô Dashboard)
        self.logger.info(
            f"üìä [SCORING-SUMMARY] Sub-ID Weighting:\n"
            f"   > Scoring Mode: {mode}\n"
            f"   > Maturity Level: {base_level} (Continuous Full)\n"
            f"   > Partial Progress: +{partial_contribution} (from Level {int(base_level+1)})\n"
            f"   > Weighted Score: {final_score} / {safe_weight} ({base_ratio:.2%} of target)"
        )
        
        return final_score
    
    def _calculate_overall_stats(self, target_sub_id: str):
        """
        [ULTIMATE REVISED v2026.1.25 - SMART AGGREGATOR]
        - üõ°Ô∏è Data Resilience: ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô 0
        - ‚öñÔ∏è Step-Ladder Logic: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏î‡∏±‡∏ö Maturity (1->5)
        - üß¨ Analytics: ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ï‡∏≤‡∏° Rubric Weight (Normalized Scoring)
        """
        from datetime import datetime
        
        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Memory ‡∏´‡∏•‡∏±‡∏Å
        results = getattr(self, 'final_subcriteria_results', [])
        
        if not results:
            self.logger.critical(f"‚ùå [STATS-FAIL] No results found for {target_sub_id}. Assessment might have crashed.")
            self.total_stats = self._get_empty_stats_template()
            return

        passed_levels_pool = []
        sub_details = []
        total_weighted_sum = 0.0
        total_weight_sum = 0.0
        
        for r in results:
            if not isinstance(r, dict): continue
            
            sub_id = r.get('sub_id', 'Unknown')
            # ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤ Weight ‡∏à‡∏≤‡∏Å Rubric (‡πÄ‡∏ä‡πà‡∏ô 4.0)
            weight = float(r.get('weight', 4.0))
            
            # 1. SMART DETECTION: ‡∏î‡∏∂‡∏á level_details
            details_map = r.get('level_details', {})
            if not details_map:
                possible_wrapper = r.get(sub_id) or r.get('results')
                if isinstance(possible_wrapper, dict):
                    details_map = possible_wrapper.get('level_details', {})

            # 2. STEP-LADDER MATURITY CALCULATION (1 -> 5)
            # ‡∏ï‡πâ‡∏≠‡∏á‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏•‡πÄ‡∏ß‡∏•‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
            current_maturity_lvl = 0
            for l_idx in range(1, 6):
                lv_data = details_map.get(str(l_idx))
                if not lv_data: break
                
                # ‡πÄ‡∏ä‡πá‡∏Ñ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡∏ú‡πà‡∏≤‡∏ô (Score >= 0.7 ‡∏´‡∏£‡∏∑‡∏≠ is_passed ‡πÄ‡∏õ‡πá‡∏ô True)
                is_passed = lv_data.get('is_passed') is True or float(lv_data.get('score', 0)) >= 0.7
                
                if is_passed:
                    current_maturity_lvl = l_idx
                else:
                    break # ‡∏ï‡∏Å‡πÄ‡∏•‡πÄ‡∏ß‡∏•‡πÑ‡∏´‡∏ô ‡∏´‡∏¢‡∏∏‡∏î‡∏ô‡∏±‡∏ö‡∏ó‡∏±‡∏ô‡∏ó‡∏µ

            # 3. NORMALIZED SCORE CALCULATION (‡πÅ‡∏Å‡πâ‡∏à‡∏≤‡∏Å‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô 20 ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ï‡∏≤‡∏° Rubric)
            # üéØ ‡∏™‡∏π‡∏ï‡∏£: (Maturity Level / 5) * Weight
            # ‡πÄ‡∏ä‡πà‡∏ô (L5 / 5) * 4.0 = 4.00 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô (‡πÄ‡∏ï‡πá‡∏° Rubric)
            # ‡πÄ‡∏ä‡πà‡∏ô (L4 / 5) * 4.0 = 3.20 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô
            sub_weighted_score = (float(current_maturity_lvl) / 5.0) * weight
            
            total_weighted_sum += sub_weighted_score
            total_weight_sum += weight

            # 4. PREPARE ANALYTICS DATA
            passed_levels_pool.append(current_maturity_lvl)
            sub_details.append({
                "sub_id": sub_id,
                "sub_name": r.get('sub_criteria_name', 'N/A'),
                "maturity": current_maturity_lvl,
                "score": round(sub_weighted_score, 2), # ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ï‡∏≤‡∏°‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô weight
                "weight": weight,
                "evidence_count": len(details_map)
            })

        # 5. FINAL CALCULATION (OVERALL)
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏° (Overall Level) ‡∏°‡∏±‡∏Å‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏ê‡∏≤‡∏ô (Min) ‡∏Ç‡∏≠‡∏á‡∏ó‡∏∏‡∏Å‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠
        overall_min = min(passed_levels_pool) if passed_levels_pool else 0
        overall_max = max(passed_levels_pool) if passed_levels_pool else 0
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Average Score (0.0 - 5.0) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ Radar Chart
        # ‡∏™‡∏π‡∏ï‡∏£: (‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ / ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏£‡∏ß‡∏°) * 5
        avg_maturity_score = (total_weighted_sum / total_weight_sum * 5.0) if total_weight_sum > 0 else 0.0

        self.total_stats = {
            "overall_max_level": int(overall_max),
            "overall_min_level": int(overall_min),
            "overall_level_label": f"L{int(overall_min)}", 
            "overall_avg_score": round(avg_maturity_score, 2), # ‡∏™‡πÄ‡∏Å‡∏• 0-5
            "total_weighted_score": round(total_weighted_sum, 2), # ‡∏™‡πÄ‡∏Å‡∏•‡∏ï‡∏≤‡∏°‡∏ú‡∏•‡∏£‡∏ß‡∏° weight
            "total_weight": round(total_weight_sum, 2),
            "evaluated_at": datetime.now().isoformat(),
            "status": "SUCCESS",
            "analytics": {
                "sub_details": sub_details,
                "total_sub_items": len(results)
            }
        }

        self.logger.info(
            f"‚úÖ [AGGREGATION SUCCESS] Maturity: {self.total_stats['overall_level_label']} | "
            f"Final Score: {self.total_stats['total_weighted_score']}/{total_weight_sum}"
        )

    def _get_empty_stats_template(self):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á Template ‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Error ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Export"""
        return {
            "overall_max_level": 0,
            "overall_min_level": 0,
            "overall_level_label": "L0",
            "overall_avg_score": 0.0,
            "total_weighted_score": 0.0,
            "total_weight": 0.0,
            "status": "NO_DATA"
        }

    def _export_results(self, results_data: Any, sub_criteria_id: str, record_id: str = None, **kwargs) -> str:
        """
        [FINAL EXPORTER v2026.01.28 ‚Äî SEMANTIC SAFE & POSITION-AWARE]
        - ‚úÖ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç TypeError ‡πÇ‡∏î‡∏¢‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö record_id ‡πÄ‡∏õ‡πá‡∏ô positional arg ‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà 3
        - ‚úÖ ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Ñ‡πà‡∏≤ null ‡∏´‡∏£‡∏∑‡∏≠ Empty List ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏´‡∏≤ Max Level
        - ‚úÖ ‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏Ñ‡πà‡∏≤ PDCA ‡πÅ‡∏•‡∏∞ Confidence ‡∏î‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏¥‡∏° (‡πÑ‡∏°‡πà Force/‡πÑ‡∏°‡πà Fabricate)
        """

        try:
            # --------------------------------------------------
            # 0. Metadata (Priority: Arg > Kwargs > Self)
            # --------------------------------------------------
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏à‡∏∏‡∏î‡∏ô‡∏µ‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏°‡∏≤‡∏à‡∏≤‡∏Å run_assessment
            final_record_id = (
                record_id 
                or kwargs.get("record_id") 
                or getattr(self, "current_record_id", f"auto_{timestamp}")
            )

            tenant = getattr(self.config, "tenant", "unknown")
            year = getattr(self.config, "year", "unknown")
            enabler = getattr(self, "enabler", "unknown").upper()

            # --------------------------------------------------
            # 1. Result data normalization
            # --------------------------------------------------
            if results_data is None:
                results_data = getattr(self, "final_subcriteria_results", [])

            if isinstance(results_data, dict):
                results_data = [results_data]

            if not results_data:
                self.logger.warning(f"‚ö†Ô∏è [EXPORT] No result data for {sub_criteria_id}")
                return ""

            # --------------------------------------------------
            # 2. Summary stats (Safety Logic)
            # --------------------------------------------------
            stats = getattr(self, "total_stats", {}) or {}

            # ‡πÉ‡∏ä‡πâ default=0 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô ValueError ‡∏Å‡∏£‡∏ì‡∏µ list ‡∏ß‡πà‡∏≤‡∏á
            highest_lvl = stats.get(
                "overall_max_level",
                max((int(r.get("highest_full_level", 0)) for r in results_data), default=0)
            )

            total_weighted = stats.get(
                "total_weighted_score",
                sum(float(r.get("weighted_score", 0.0)) for r in results_data)
            )

            # --------------------------------------------------
            # 3. Evidence Audit Trail (Semantic Safe)
            # --------------------------------------------------
            master_map = getattr(self, "evidence_map", {}) or {}
            processed_evidence = {}

            for level_key, bucket in master_map.items():
                if not bucket: continue

                # Normalization
                ev_list = bucket.get("evidences", []) if isinstance(bucket, dict) else bucket
                if not isinstance(ev_list, list) or not ev_list: continue

                # Sort by highest confidence (rerank_score)
                valid_evs = [ev for ev in ev_list if isinstance(ev, dict)]
                if not valid_evs: continue

                sorted_ev = sorted(
                    valid_evs,
                    key=lambda x: float(x.get("rerank_score") or -1.0),
                    reverse=True
                )
                top_ev = sorted_ev[0]

                # Resolve Filename
                doc_id = top_ev.get("doc_id") or top_ev.get("stable_doc_uuid")
                filename = (
                    top_ev.get("filename")
                    or (self.document_map.get(doc_id) if hasattr(self, "document_map") else None)
                    or top_ev.get("source")
                    or "Unknown_Source"
                )

                # PDCA (No Guesses)
                pdca = top_ev.get("pdca_tag") or top_ev.get("pdca_phase")
                pdca = pdca.upper() if isinstance(pdca, str) else None

                # Confidence (No Force 0)
                confidence = top_ev.get("rerank_score")
                if not isinstance(confidence, (int, float)) or confidence < 0:
                    confidence = None

                processed_evidence[str(level_key)] = {
                    "file": filename,
                    "page": top_ev.get("page") or top_ev.get("page_label", "N/A"),
                    "pdca": pdca,
                    "confidence": confidence,
                    "snippet": (top_ev.get("content") or top_ev.get("text") or "")[:160].strip()
                }

            # --------------------------------------------------
            # 4. Final payload (STABLE SCHEMA)
            # --------------------------------------------------
            payload = {
                "metadata": {
                    "record_id": final_record_id,
                    "tenant": tenant,
                    "year": year,
                    "enabler": enabler,
                    "engine_version": "SEAM-ENGINE-v2026.01.28",
                    "exported_at": datetime.now().isoformat()
                },
                "result_summary": {
                    "maturity_level": f"L{highest_lvl}",
                    "total_weighted_score": round(total_weighted, 4),
                    "status": "COMPLETED"
                },
                "sub_criteria_details": results_data,
                "evidence_audit_trail": processed_evidence,
                "enabler_roadmap": getattr(self, "enabler_roadmap_data", {})
            }

            # --------------------------------------------------
            # 5. Export
            # --------------------------------------------------
            export_path = get_assessment_export_file_path(
                tenant=tenant, year=year, enabler=enabler.lower(),
                suffix=f"{sub_criteria_id}_{timestamp}", ext="json"
            )

            os.makedirs(os.path.dirname(export_path), exist_ok=True)
            with open(export_path, "w", encoding="utf-8") as f:
                json.dump(payload, f, indent=2, ensure_ascii=False)

            self.logger.info(f"‚úÖ [EXPORT OK] {export_path}")
            return export_path

        except Exception as e:
            self.logger.error(f"üõë [EXPORT FAILED] {str(e)}", exc_info=True)
            return ""

    def _guarantee_text_key(
        self,
        chunks: List[Dict],
        total_count: int = 0,
        restored_count: int = 0
    ) -> List[Dict]:
        """
        Guarantee 'text' key exists in every chunk
        """
        final_chunks = []

        for chunk in chunks:
            if "text" not in chunk or not isinstance(chunk["text"], str):
                chunk["text"] = ""
                cid = str(chunk.get("chunk_uuid", "N/A"))
                self.logger.debug(f"Guaranteed 'text' key for chunk (ID: {cid[:8]})")
            final_chunks.append(chunk)

        if total_count > 0:
            baseline_count = sum(1 for c in final_chunks if c.get("is_baseline"))
            self.logger.info(
                f"HYDRATION FINAL: Restored {restored_count}/{total_count} "
                f"(Baseline={baseline_count}, Total final={len(final_chunks)})"
            )

        return final_chunks

    
    def _normalize_meta(self, c: Dict) -> Tuple[str, str]:
        """
        [REVISED] ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡∏∞‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏£‡∏∞‡∏ö‡∏ö Priority Fallback
        - ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤ 0 ‡∏´‡∏≤‡∏¢ (Index-based)
        - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏à‡∏≤‡∏Å Ingest ‡πÉ‡∏´‡∏°‡πà ‡πÅ‡∏•‡∏∞ Reranker Output
        - ‡πÄ‡∏ô‡πâ‡∏ô page_label ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏ö‡∏ô UI
        """
        if not isinstance(c, dict):
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡πá‡∏ô LangChain Document ‡πÉ‡∏´‡πâ‡∏î‡∏∂‡∏á metadata ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤
            if hasattr(c, "metadata"):
                meta = c.metadata
                # ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á dict ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÇ‡∏Ñ‡πâ‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏•‡πà‡∏≤‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏î‡πâ
                c = {"metadata": meta}
            else:
                return "Unknown Source", "N/A"

        # ‡∏î‡∏∂‡∏á metadata ‡∏°‡∏≤‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÑ‡∏ß‡πâ (‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏Å‡∏£‡∏ì‡∏µ None ‡∏´‡∏£‡∏∑‡∏≠ Dict)
        meta = c.get("metadata") or {}

        # 1. üîç ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå (Source Name Priority)
        source_priority = [
            c.get("source_filename"),
            meta.get("source_filename"),
            c.get("filename"),
            meta.get("source"),
            meta.get("file_name"),
            c.get("id") 
        ]
        
        source = "Unknown"
        for s in source_priority:
            if s and str(s).strip():
                source = str(s).strip()
                break

        # 2. üîç ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤ (Page Label Priority)
        page_keys = ["page_label", "page", "page_number"]
        found_page = None
        for key in page_keys:
            val_root = c.get(key)
            if val_root is not None and str(val_root).strip().lower() != "n/a":
                found_page = val_root
                break
            val_meta = meta.get(key)
            if val_meta is not None and str(val_meta).strip().lower() != "n/a":
                found_page = val_meta
                break

        # 3. ‚ú® Final Cleaning & UI Formatting
        clean_source = os.path.basename(source) if "/" in source or "\\" in source else source
        if found_page is not None:
            page_str = str(found_page).strip()
            clean_page = page_str if page_str.lower() != "n/a" else "N/A"
        else:
            clean_page = "N/A"

        return clean_source, clean_page
    
    def audit_agent_router(
        self,
        *,
        context: str,
        sub_criteria_name: str,
        level: int,
        statement_text: str,
        sub_id: str,
        llm_executor,
        confidence_reason: str = "N/A", # ‚úÖ ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏à‡∏≤‡∏Å evaluate_pdca
        **kwargs
    ):
        """
        [AUDIT AGENT ROUTER ‚Äì v2026.01.27]
        - FIXED: ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠ confidence_reason ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡∏ö‡∏ö
        - ROUTE: L1‚ÄìL2 ‚Üí foundation_coaching_agent (Low Level)
        - ROUTE: L3‚ÄìL5 ‚Üí standard_audit_agent (High Level)
        """

        if llm_executor is None:
            raise RuntimeError(f"üõë [ROUTER ERROR] LLM executor missing for {sub_id}")

        # 1. üîç ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Agent ‡∏ï‡∏≤‡∏°‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏Å‡∏Ç‡∏≠‡∏á‡πÄ‡∏Å‡∏ì‡∏ë‡πå
        # L1-L2 ‡πÄ‡∏ô‡πâ‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö/‡πÅ‡∏ú‡∏ô (Low level logic)
        # L3-L5 ‡πÄ‡∏ô‡πâ‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ö‡∏π‡∏£‡∏ì‡∏≤‡∏Å‡∏≤‡∏£/‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå (High level logic)
        if level <= 2:
            agent = self.foundation_coaching_agent
        else:
            agent = self.standard_audit_agent

        # 2. üì°‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠ Payload ‡πÑ‡∏õ‡∏¢‡∏±‡∏á Agent ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
        # ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: **kwargs ‡∏à‡∏∞‡∏£‡∏ß‡∏°‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÄ‡∏™‡∏£‡∏¥‡∏° ‡πÄ‡∏ä‡πà‡∏ô enabler_name_th, ai_confidence, focus_points
        return agent(
            context=context,
            sub_criteria_name=sub_criteria_name,
            level=level,
            statement_text=statement_text,
            sub_id=sub_id,
            llm_executor=llm_executor,
            confidence_reason=confidence_reason, # ‚úÖ ‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
            **kwargs
        )

    def evaluate_pdca(
        self,
        pdca_blocks: Union[Dict[str, Any], str],
        sub_id: str,
        level: int,
        audit_confidence: Any,
        audit_instruction: str = ""
    ) -> Dict[str, Any]:
        """
        [ULTIMATE REVISED v2026.01.27] - Optimized for SE-AM v36.9
        - FIXED: ‡∏™‡πà‡∏á enabler_full_name ‡πÅ‡∏•‡∏∞ enabler_code ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Result Builder
        - FIXED: ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Multiple Values ‡πÉ‡∏ô Router ‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ payload ‡πÉ‡∏´‡πâ‡∏Ñ‡∏•‡∏µ‡∏ô
        - STABLE: ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ Audit Trail ‡∏î‡πâ‡∏ß‡∏¢ Metadata ‡∏ó‡∏µ‡πà‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô
        """
        log_prefix = f"üß† [{sub_id}-L{level}]"

        # 1. [PREPARE CONTEXT] ‡∏•‡πâ‡∏≤‡∏á‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏û‡∏¥‡πÄ‡∏®‡∏©‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏Ç‡∏≠‡∏á JSON
        pdca_summary = []
        if isinstance(pdca_blocks, dict):
            for tag in ["P", "D", "C", "A"]:
                val = pdca_blocks.get(tag)
                if val:
                    clean_val = str(val).replace('"', "'")
                    pdca_summary.append(f"### {tag} PHASE EVIDENCE ###\n{clean_val}")
        else:
            final_context_str = str(pdca_blocks).replace('"', "'")
        
        final_context_str = "\n\n".join(pdca_summary) if pdca_summary else str(pdca_blocks)

        # 2. [LOOKUP RUBRIC]
        sub_item = next((i for i in self.flattened_rubric if i.get("sub_id") == sub_id), {})
        sub_name = sub_item.get("sub_criteria_name", sub_id)
        
        level_info = next((lv for lv in sub_item.get("levels", []) if lv.get("level") == level), {})
        statement = level_info.get("statement", "")

        # 3. [NORMALIZE CONFIDENCE]
        try:
            if isinstance(audit_confidence, dict):
                conf_val = float(
                    audit_confidence.get("coverage_ratio") or 
                    audit_confidence.get("rerank_score") or 
                    audit_confidence.get("score") or 0.0
                )
            else:
                conf_val = float(audit_confidence or 0.0)
        except:
            conf_val = 0.5

        # 4. [ENABLER RESOLUTION]
        if self.llm is None: self._initialize_llm_if_none()
        
        e_code = str(getattr(self, 'enabler', 'UNKNOWN')).upper()
        # ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡πá‡∏°‡∏à‡∏≤‡∏Å‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£ Global ‡∏´‡∏£‡∏∑‡∏≠ Mapping
        e_name_th = SEAM_ENABLER_FULL_NAME_TH.get(e_code, f"‡∏î‡πâ‡∏≤‡∏ô {e_code}")

        # 5. [BUILD AGENT PAYLOAD] 
        # ‡∏£‡∏ß‡∏°‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà Prompt ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ ‡πÅ‡∏•‡∏∞‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà Result Builder ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
        agent_payload = {
            "context": final_context_str,
            "pdca_context": final_context_str,
            "sub_id": sub_id,
            "sub_criteria_name": sub_name,
            "level": level,
            "statement_text": statement,
            "llm_executor": self.llm,
            
            # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Prompt Mapping
            "confidence_reason": f"System Confidence (Rerank/Coverage): {conf_val:.4f}",
            "ai_confidence": "HIGH" if conf_val >= 0.7 else "MEDIUM",
            "enabler_name_th": e_name_th,
            
            # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Result Builder (_build_audit_result_object)
            "enabler_full_name": e_name_th,
            "enabler_code": e_code,
            
            # Extra Guidelines
            "focus_points": sub_item.get("focus_points", "-"),
            "evidence_guidelines": level_info.get("level_specific_guideline", "-"),
            "specific_contextual_rule": audit_instruction,
        }

        # 6. [EXECUTE ROUTER]
        try:
            # ‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢ payload ‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà router
            return self.audit_agent_router(**agent_payload)

        except Exception as e:
            self.logger.error(f"üõë [EVAL-ERROR] {log_prefix}: {str(e)}", exc_info=True)
            # Fallback ‡∏ó‡∏µ‡πà‡∏°‡∏µ Enabler Info ‡∏Ñ‡∏£‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ UI ‡πÑ‡∏°‡πà‡πÅ‡∏™‡∏î‡∏á Error ‡πÄ‡∏õ‡∏•‡πà‡∏≤‡πÜ
            return {
                "sub_id": sub_id,
                "level": level,
                "score": 0.0,
                "is_passed": False,
                "reason": f"Evaluation Failure: {str(e)}",
                "enabler_full_name": e_name_th,
                "enabler_code": e_code
            }
            
    def _prepare_worker_tuple(self, sub_criteria_data: Dict, document_map: Optional[Dict]) -> Tuple:
        """‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤ Process ‡πÉ‡∏´‡∏°‡πà (Pickle-friendly)"""
        return (
            sub_criteria_data,                 # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Rubric ‡∏£‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠
            self.config.enabler,               # Enabler Code
            self.config.target_level,          # ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢
            self.config.mock_mode,             # ‡πÇ‡∏´‡∏°‡∏î Mock
            getattr(self, 'evidence_map_path', None), 
            self.config.model_name,            # ‡∏ä‡∏∑‡πà‡∏≠‡∏£‡∏∏‡πà‡∏ô LLM
            self.config.temperature,           # ‡∏Ñ‡πà‡∏≤ Temp
            self.config.min_retry_score,       # ‡πÄ‡∏Å‡∏ì‡∏ë‡πå Rerank
            self.config.max_retrieval_attempts,
            document_map or self.document_map, # ID Mapping
            None,                              # Action Plan Model (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
            self.config.year,
            self.config.tenant
        )
    
    def _create_failed_result(self, record_id: str, message: str, start_ts: float) -> Dict[str, Any]:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô Response ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏Å‡∏¥‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô Orchestrator"""
        self.logger.error(f"‚ùå Assessment Failed: {message}")
        return {
            "record_id": record_id,
            "status": "FAILED",
            "error": message,
            "run_time_seconds": round(time.time() - start_ts, 2),
            "summary": {},
            "sub_criteria_results": {}
        }
    
    def _normalize_evidence_metadata(self, evidence_list: List[Dict[str, Any]]):
        """
        [ULTIMATE REVISED v2026.01.27] - FIXED CONFIDENCE 0.0 ISSUE
        - üõ°Ô∏è Score Sync: ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï confidence ‡πÉ‡∏´‡πâ‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö relevance_score
        - üîç Deep Resolve: ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏µ‡∏¢‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ (score, rerank, relevance)
        - üèóÔ∏è UUID Alignment: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á ID ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ Merge Mapping
        """
        import uuid
        import os

        for ev in evidence_list:
            if not isinstance(ev, dict): continue
            
            meta = ev.get("metadata", {})
            if not isinstance(meta, dict): meta = {}
            
            # 1. Resolve ID & UUID (‡∏´‡∏±‡∏ß‡πÉ‡∏à‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ Mapping)
            doc_id = (
                ev.get("doc_id") or 
                ev.get("stable_doc_uuid") or 
                meta.get("stable_doc_uuid") or 
                meta.get("doc_id") or
                f"gen_{uuid.uuid4().hex[:8]}"
            )
            ev["doc_id"] = doc_id
            ev["stable_doc_uuid"] = doc_id

            # 2. Resolve Filename (‡∏•‡πâ‡∏≤‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏Ñ‡∏≥‡∏û‡∏π‡∏î‡πÅ‡∏•‡∏∞ Metadata ‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô)
            raw_source = (
                meta.get("source_filename") or 
                meta.get("file_name") or 
                ev.get("filename") or 
                ev.get("source") or 
                meta.get("source")
            )
            # ‡∏•‡πâ‡∏≤‡∏á‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå (‡∏ï‡∏±‡∏î‡∏û‡∏ß‡∏Å |SCORE:0.xxxx ‡∏≠‡∏≠‡∏Å‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
            filename_raw = str(raw_source).split('|')[0] if raw_source else "Unknown_File"
            filename = os.path.basename(filename_raw)
            
            # Cross-check ‡∏Å‡∏±‡∏ö‡∏Ñ‡∏•‡∏±‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏Å‡∏•‡∏≤‡∏á
            if (filename == "Unknown_File" or not filename) and hasattr(self, 'document_map'):
                filename = self.document_map.get(doc_id, "Unknown_File")
                
            ev["filename"] = filename
            ev["source_filename"] = filename
            ev["source"] = filename

            # 3. Resolve Page Label
            raw_page = meta.get("page_label") or meta.get("page") or meta.get("page_number") or ev.get("page") or "0"
            ev["page"] = str(raw_page)

            # 4. üéØ [FIXED] Resolve Scoring & Confidence (‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤)
            actual_score = 0.0
            if hasattr(self, 'get_actual_score'):
                actual_score = self.get_actual_score(ev)
            else:
                # ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡∏µ‡πà JSON ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏≠‡∏≤‡∏à‡∏û‡πà‡∏ô‡∏≠‡∏≠‡∏Å‡∏°‡∏≤
                actual_score = float(
                    ev.get("relevance_score") or 
                    ev.get("score") or 
                    meta.get("rerank_score") or 
                    meta.get("score") or 0.0
                )
            
            # Sync ‡∏ó‡∏∏‡∏Å‡∏ü‡∏¥‡∏•‡∏î‡πå‡∏ó‡∏µ‡πà UI ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô
            ev["relevance_score"] = actual_score
            ev["score"] = actual_score
            ev["confidence"] = actual_score # ‚úÖ ‡∏õ‡∏•‡∏î‡∏•‡πá‡∏≠‡∏Ñ‡∏Ñ‡πà‡∏≤ 0.0 ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏à‡∏£‡∏¥‡∏á

            # 5. UI Fields Consistency
            ev["source_type"] = ev.get("source_type") or meta.get("source_type") or "system_gen"
            ev["is_selected"] = ev.get("is_selected") if ev.get("is_selected") is not None else True
            
            # ‡∏î‡∏∂‡∏á PDCA Tag ‡∏à‡∏≤‡∏Å Metadata (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
            ev["pdca_tag"] = ev.get("pdca_tag") or meta.get("pdca_tag") or "Other"
            ev["note"] = ev.get("note") or ""

        return evidence_list
        
    # ------------------------------------------------------------------------------------------
    # [ULTIMATE REVISE v2026.01.30] üß† LAYER 1: Decision Engine (The Brain ‚Äì Final Hardened)
    # ------------------------------------------------------------------------------------------
    def _get_semantic_tag(
        self,
        text: str,
        sub_id: str,
        level: int,
        filename: str = ""
    ) -> str:
        """
        [FINAL STABLE v2026.02]
        Decision Engine ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö PDCA Tag
        Priority:
        1) Keyword Heuristic (Enabler-based)
        2) Behavioral Heuristic (_get_heuristic_pdca_tag)
        3) LLM Semantic Classification
        4) Contextual Fallback (LIMITED)
        5) Ultimate Maturity Fallback
        """

        # ------------------ Preparation ------------------
        text_clean = (text or "").strip()
        text_lower = text_clean.lower()

        # Enabler name + keywords (safe fallback)
        enabler_key = getattr(self.config, "enabler", "DEFAULT").upper()
        enabler_keywords = PDCA_CONFIG_MAP.get(enabler_key, PDCA_CONFIG_MAP.get("DEFAULT", {}))

        # Required phases from contextual rules
        require_phases = self.get_rule_content(sub_id, level, "require_phase") or []

        self.logger.debug(
            f"[TAG-PREP] {sub_id} L{level} | "
            f"ReqPhases={require_phases} | File={filename[:40]}"
        )

        # ------------------ Short Text Guard ------------------
        if len(text_clean) < 20:
            fallback = (
                require_phases[0]
                if require_phases and level <= 2
                else ("P" if level == 1 else "D")
            )
            self.logger.debug(f"[TAG-SHORT] ‚Üí {fallback}")
            return fallback

        # ------------------ LAYER 1: Enabler Keyword Heuristic ------------------
        for tag, keywords in enabler_keywords.items():
            if any(k.lower() in text_lower for k in keywords):
                self.logger.debug(
                    f"‚ö° [KEYWORD-HIT] {tag} | {filename[:30]}"
                )
                return tag

        # ------------------ LAYER 1.5: Behavioral Heuristic (CRITICAL FIX) ------------------
        heuristic_pdca = self._get_heuristic_pdca_tag(text_clean, level)
        if heuristic_pdca:
            self.logger.debug(
                f"üß† [BEHAVIOR-HIT] {heuristic_pdca} | {filename[:30]}"
            )
            return heuristic_pdca

        # ------------------ LAYER 2: LLM Semantic Classification ------------------
        require_str = ", ".join(require_phases) if require_phases else "P, D, C, A"
        desc_bullets = "\n".join(
            f"- {v}" for v in PDCA_PHASE_DESCRIPTIONS.values()
        )

        system_prompt = (
            "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ï‡∏≤‡∏°‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô SE-AM\n"
            f"‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà: ‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡∏´‡∏°‡∏ß‡∏î PDCA\n\n"
            f"{desc_bullets}\n\n"
            f"‡∏ö‡∏£‡∏¥‡∏ö‡∏ó Level {level}: ‡πÉ‡∏´‡πâ‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤ {require_str} ‡∏Å‡πà‡∏≠‡∏ô\n"
            "‡∏ñ‡πâ‡∏≤‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á phase ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô ‡πÉ‡∏´‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å phase ‡∏ô‡∏±‡πâ‡∏ô\n"
            "‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏à‡∏£‡∏¥‡∏á ‡πÜ ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ Other\n\n"
            "‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô:\n"
            "{'tag':'P|D|C|A|Other','reason':'‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏™‡∏±‡πâ‡∏ô'}"
        )

        user_prompt = (
            f"‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå: {filename}\n"
            f"‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤:\n{text_clean[:700]}\n\n"
            "‡∏£‡∏∞‡∏ö‡∏∏ tag"
        )

        try:
            raw = _fetch_llm_response(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                llm_executor=self.llm,
                max_retries=3
            )

            self.logger.debug(f"[LLM-TAG-RAW] {raw[:200]}")

            raw = re.sub(r"```json|```", "", raw).strip()
            data = json.loads(raw)
            if isinstance(data, list) and data:
                data = data[0]

            tag = str(data.get("tag", "Other")).upper().strip()
            if tag in {"P", "D", "C", "A"}:
                self.logger.info(
                    f"üéØ [AI-TAG] {tag} | {filename[:30]}"
                )
                return tag

        except Exception as e:
            self.logger.warning(f"[AI-TAG-ERROR] {filename[:30]} | {e}")

        # ------------------ LAYER 3: Contextual Fallback (LIMITED) ------------------
        # ‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ L1‚ÄìL2 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏±‡∏ô PDCA ‡∏õ‡∏•‡∏≠‡∏°‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á
        if require_phases and level <= 2:
            fallback = require_phases[0]
            self.logger.debug(
                f"[CTX-FALLBACK-LIMITED] {fallback}"
            )
            return fallback

        # ------------------ Ultimate Fallback by Maturity ------------------
        if level == 1:
            fallback = "P"
        elif level <= 3:
            fallback = "D"
        elif level == 4:
            fallback = "C"
        else:
            fallback = "A"

        self.logger.debug(
            f"[ULTIMATE-FALLBACK] {fallback} for L{level}"
        )
        return fallback


    def _get_heuristic_pdca_tag(self, text: str, level: int) -> Optional[str]:
        t = text.lower()
        
        # Do-specific ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö L1 (‡πÄ‡∏ô‡πâ‡∏ô‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏ó‡∏≥‡∏à‡∏£‡∏¥‡∏á)
        do_keywords = [
            "‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£", "‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥", "‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°", "‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°", "‡∏≠‡∏ö‡∏£‡∏°", "‡∏à‡∏±‡∏î‡∏ó‡∏≥", "‡∏•‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà", 
            "‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏ú‡∏•", "‡∏†‡∏≤‡∏û‡∏ñ‡πà‡∏≤‡∏¢", "‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£", "‡∏°‡∏∏‡πà‡∏á‡∏°‡∏±‡πà‡∏ô", "‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á", "‡∏™‡∏ô‡∏±‡∏ö‡∏™‡∏ô‡∏∏‡∏ô", 
            "‡∏Ç‡∏±‡∏ö‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô", "‡∏ô‡∏≥‡∏£‡πà‡∏≠‡∏á", "‡∏•‡∏á‡∏°‡∏∑‡∏≠‡∏ó‡∏≥", "‡∏ô‡∏≥‡πÑ‡∏õ‡πÉ‡∏ä‡πâ"
        ]
        if level <= 2 and any(k in t for k in do_keywords):
            return "D"

        # Check keywords (‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å log)
        check_keywords = [
            "‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏•", "‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô", "‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°", "‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î", "kpi", "‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå", "‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•", 
            "‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥", "‡∏™‡∏≥‡∏£‡∏ß‡∏à", "‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö", "‡∏ß‡∏±‡∏î‡∏ú‡∏•"
        ]
        if any(k in t for k in check_keywords):
            return "C"

        # Plan & Act (‡∏Ñ‡∏á‡πÄ‡∏î‡∏¥‡∏° ‡πÅ‡∏ï‡πà‡∏•‡∏î priority)
        if any(k in t for k in ["‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢", "‡πÅ‡∏ú‡∏ô", "‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå", "‡∏°‡∏ï‡∏¥", "‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á", "‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢", "‡πÄ‡∏à‡∏ï‡∏ô‡∏≤‡∏£‡∏°‡∏ì‡πå"]):
            return "P"
        if any(k in t for k in ["‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á", "‡∏û‡∏±‡∏í‡∏ô‡∏≤", "‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç", "‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô", "lesson learned", "‡∏ï‡πà‡∏≠‡∏¢‡∏≠‡∏î", "‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°"]):
            return "A"

        return None
    
    # ------------------------------------------------------------------------------------------
    # [ULTIMATE REVISE v2026.01.28] üìä LAYER 2: Contextual Blocker (The Focus)
    # ------------------------------------------------------------------------------------------
    def _get_pdca_blocks_from_evidences(
        self,
        evidences: List[Dict[str, Any]],
        baseline_evidences: List[Dict[str, Any]],
        level: int,
        sub_id: str,
        contextual_rules_map: Dict[str, Any],
        record_id: str = None
    ) -> Dict[str, Any]:
        """
        [FINAL LOCKED v2026.01.28.1]
        - üîí Forced PDCA ‚â† Real PDCA
        - üîí ‡∏™‡πà‡∏á pdca_confidence downstream
        """

        pdca_groups = defaultdict(list)
        seen_texts = set()
        all_candidate = (evidences or []) + (baseline_evidences or [])

        require_phases = self.get_rule_content(sub_id, level, "require_phase") or ["P", "D"]

        for idx, chunk in enumerate(all_candidate, start=1):
            txt = (chunk.get("text") or chunk.get("page_content") or "").strip()
            if not txt or len(txt) < 10:
                continue

            txt_hash = hashlib.sha256(txt.encode()).hexdigest()
            if txt_hash in seen_texts:
                continue
            seen_texts.add(txt_hash)

            meta = chunk.get("metadata", {}) or {}
            fname = chunk.get("source_filename") or meta.get("source_filename") or "Unknown"
            page = meta.get("page_label") or meta.get("page") or "N/A"
            is_baseline = chunk.get("is_baseline", False)

            source_display = f"{'[BASELINE] ' if is_baseline else ''}{fname} (P.{page})"

            # ---------- MULTI-LAYER TAGGING ----------
            is_forced = False
            tag_source = "Semantic-Engine"
            final_tag = self._get_semantic_tag(txt, sub_id, level, fname)

            if final_tag not in {"P", "D", "C", "A"}:
                heuristic = self._get_heuristic_pdca_tag(txt, level)
                if heuristic in {"P", "D", "C", "A"}:
                    final_tag = heuristic
                    tag_source = "Heuristic-Rule-Base"

            if final_tag not in {"P", "D", "C", "A"}:
                if level >= 4:
                    continue  # STRICT MODE
                is_forced = True
                final_tag = require_phases[(idx - 1) % len(require_phases)]
                tag_source = f"Forced-L{level}"

            pdca_confidence = (
                0.9 if tag_source == "Semantic-Engine"
                else 0.7 if tag_source == "Heuristic-Rule-Base"
                else 0.4
            )

            pdca_groups[final_tag].append({
                "text": txt,
                "filename": fname,
                "page": page,
                "source_display": source_display,
                "pdca_tag": final_tag,
                "pdca_confidence": pdca_confidence,
                "is_forced": is_forced,
                "is_baseline": is_baseline,
                "relevance": float(chunk.get("rerank_score") or chunk.get("score") or 0.5),
                "tag_source": tag_source
            })

        # ---------- BLOCK OUTPUT ----------
        max_ch = getattr(self.config, "MAX_CHUNKS_PER_BLOCK", 5)
        blocks = {
            "sources": {},
            "actual_counts": {},
            "all_evidences_with_tags": []
        }

        for tag in ["P", "D", "C", "A"]:
            real_only = [c for c in pdca_groups.get(tag, []) if not c["is_forced"]]

            ranked = sorted(
                real_only,
                key=lambda x: (-x["pdca_confidence"], -x["relevance"])
            )[:max_ch]

            blocks["actual_counts"][tag] = len(real_only)
            blocks["sources"][tag] = [c["source_display"] for c in ranked]

            if ranked:
                blocks[tag] = "\n\n".join(
                    f"[{c['source_display']} | {c['tag_source']}]\n{c['text'][:1000]}"
                    for c in ranked
                )
                blocks["all_evidences_with_tags"].extend(ranked)
            else:
                blocks[tag] = f"[‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô PDCA ‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô‡∏´‡∏°‡∏ß‡∏î {tag}]"

        return blocks

    # ------------------------------------------------------------------------------------------
    # [ULTIMATE REVISE v2026.01.28] üíæ LAYER 3: Persistence & Retroactive Sync
    # ------------------------------------------------------------------------------------------
    def _save_level_evidences_and_calculate_strength(
        self,
        level_temp_map: List[Dict[str, Any]],
        sub_id: str,
        level: int,
        llm_result: Dict[str, Any],
        highest_rerank_score: float = 0.0
    ) -> float:
        """
        [FINAL STRENGTH v2026.01.28.1]
        - üîí Forced PDCA NEVER counted
        """

        map_key = f"{sub_id}_L{level}"
        new_evidence_list = []
        seen = set()

        for ev in level_temp_map:
            if ev.get("is_forced"):
                continue  # üî• DROP FORCED

            tag = ev.get("pdca_tag")
            if tag not in {"P", "D", "C", "A"}:
                continue

            uid = f"{ev.get('doc_id')}:{ev.get('page')}:{tag}"
            if uid in seen:
                continue
            seen.add(uid)

            new_evidence_list.append({
                "sub_id": sub_id,
                "level": level,
                "pdca_tag": tag,
                "doc_id": ev.get("doc_id"),
                "filename": ev.get("filename"),
                "page": ev.get("page"),
                "relevance_score": ev.get("relevance", 0.0),
                "pdca_confidence": ev.get("pdca_confidence", 0.5),
                "timestamp": datetime.now().isoformat()
            })

        if not new_evidence_list:
            return 0.0

        self.evidence_map.setdefault(map_key, []).extend(new_evidence_list)

        # ---------- REAL COVERAGE ONLY ----------
        found = {e["pdca_tag"] for e in new_evidence_list}
        coverage = len(found) / 4.0

        # ---------- STRENGTH ----------
        strength = round(
            (highest_rerank_score * 0.6) +
            (coverage * 0.4),
            2
        )

        self.assessment_results_map[map_key] = {
            "is_passed": llm_result.get("is_passed", False),
            "strength": strength,
            "coverage": coverage
        }

        return strength

    def _robust_hydrate_documents_for_priority_chunks(
        self,
        chunks_to_hydrate: List[Dict],
        vsm: Optional['VectorStoreManager'],
        current_sub_id: Optional[str] = None,
        level: Optional[int] = None
    ) -> List[Dict]:
        """
        [ULTIMATE HYDRATION ‚Äì FINAL LOCKED VERSION]
        - Hydrate full text from stable_doc_uuid
        - Preserve ingest metadata (page / page_label / chunk_uuid)
        - Correct PDCA tagging (no OTHER leakage)
        - Full-text hash dedup
        """

        active_sub_id = current_sub_id or getattr(self, "sub_id", "unknown")
        level = level or 1

        if not chunks_to_hydrate:
            self.logger.debug(f"‚ÑπÔ∏è [HYDRATION] No chunks for {active_sub_id} L{level}")
            return []

        # --------------------------------------------------
        # 1. Safe PDCA classification
        # --------------------------------------------------
        def _safe_classify(text: str, filename: str = "") -> str:
            try:
                tag = self._get_semantic_tag(text, active_sub_id, level, filename)

                if tag in {"P", "D", "C", "A"}:
                    return tag

                # fallback using rule-based required phase
                reqs = self.get_rule_content(active_sub_id, level, "require_phase") or []
                return reqs[0] if reqs else ("P" if level <= 1 else "D")

            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è [PDCA-CLASSIFY-ERR] {e} | file={filename}")
                reqs = self.get_rule_content(active_sub_id, level, "require_phase") or []
                return reqs[0] if reqs else ("P" if level <= 1 else "D")

        # --------------------------------------------------
        # 2. Standardize chunk without destroying ingest metadata
        # --------------------------------------------------
        def _standardize_chunk(chunk: Dict, boost: float) -> Dict:
            c = chunk.copy()

            text = (c.get("text") or "").strip()
            meta = c.get("metadata") or {}

            filename = meta.get("source") or meta.get("source_filename") or "unknown"

            # PDCA fix: re-classify ONLY if invalid
            if c.get("pdca_tag") in [None, "", "OTHER", "Other"]:
                if text:
                    c["pdca_tag"] = _safe_classify(text, filename)

            # Preserve is_baseline
            c["is_baseline"] = bool(c.get("is_baseline", False))

            # Score boost (non-destructive)
            c["score"] = max(float(c.get("score", 0.0)), boost)
            c["rerank_score"] = max(float(c.get("rerank_score", 0.0)), boost)

            return c

        # --------------------------------------------------
        # 3. Collect stable_doc_uuid
        # --------------------------------------------------
        stable_ids = {
            c.get("stable_doc_uuid") or c.get("doc_id")
            for c in chunks_to_hydrate
            if c.get("stable_doc_uuid") or c.get("doc_id")
        }

        if not stable_ids or not vsm:
            self.logger.warning("‚ö†Ô∏è [HYDRATION] Missing stable IDs or VSM ‚Üí fallback mode")
            return [
                _standardize_chunk(c, 0.85)
                for c in chunks_to_hydrate
            ]

        # --------------------------------------------------
        # 4. Fetch full documents from VSM
        # --------------------------------------------------
        stable_doc_map = defaultdict(list)

        try:
            docs = vsm.get_documents_by_id(
                list(stable_ids),
                doc_type=self.doc_type,
                enabler=self.config.enabler
            )
            for d in docs:
                sid = d.metadata.get("stable_doc_uuid") or d.metadata.get("doc_id")
                if sid:
                    stable_doc_map[sid].append({
                        "text": d.page_content,
                        "metadata": d.metadata
                    })
        except Exception as e:
            self.logger.error(f"‚ùå [HYDRATION] VSM fetch failed: {e}")
            return [
                _standardize_chunk(c, 0.85)
                for c in chunks_to_hydrate
            ]

        # --------------------------------------------------
        # 5. Hydrate + Dedup
        # --------------------------------------------------
        hydrated_docs = []
        seen_hashes = set()
        hydrated_count = 0

        SAFE_META_KEYS = {
            "source",
            "source_filename",
            "page",
            "page_label",
            "page_number",
            "chunk_uuid",
            "chunk_index",
            "doc_id",
            "stable_doc_uuid",
            "version",
            "year",
            "enabler",
            "subject",
            "sub_topic",
        }

        for chunk in chunks_to_hydrate:
            new_chunk = chunk.copy()
            meta = new_chunk.get("metadata") or {}

            sid = meta.get("stable_doc_uuid") or meta.get("doc_id")
            hydrated = False

            if sid and sid in stable_doc_map:
                # choose the most complete text
                full_doc = max(
                    stable_doc_map[sid],
                    key=lambda d: len(d.get("text", ""))
                )

                new_chunk["text"] = full_doc["text"]

                # merge metadata carefully (ingest is source of truth)
                merged_meta = meta.copy()
                for k in SAFE_META_KEYS:
                    if k in full_doc["metadata"] and k not in merged_meta:
                        merged_meta[k] = full_doc["metadata"][k]

                new_chunk["metadata"] = merged_meta
                hydrated = True
                hydrated_count += 1

            boost = 1.0 if hydrated else 0.85
            new_chunk = _standardize_chunk(new_chunk, boost)

            raw_text = (new_chunk.get("text") or "").strip()
            if not raw_text:
                hydrated_docs.append(new_chunk)
                continue

            text_hash = hashlib.sha256(raw_text.encode()).hexdigest()
            if text_hash not in seen_hashes:
                seen_hashes.add(text_hash)
                hydrated_docs.append(new_chunk)

        self.logger.info(
            f"‚úÖ [HYDRATION] Done: {len(hydrated_docs)} chunks | "
            f"hydrated={hydrated_count}/{len(chunks_to_hydrate)} | "
            f"dedup={len(chunks_to_hydrate) - len(hydrated_docs)}"
        )

        return (
            self._guarantee_text_key(hydrated_docs)
            if hasattr(self, "_guarantee_text_key")
            else hydrated_docs
        )
    

    def _is_previous_level_passed(self, sub_id: str, level: int) -> bool:
        """
        [STRICT REVISE v2026.01.18.1] - ‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏ö‡∏ö‡πÄ‡∏Ç‡πâ‡∏°‡∏á‡∏ß‡∏î
        """
        if level <= 1: 
            return True
            
        prev_level = level - 1
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Key ‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏ö‡∏≠‡∏≤‡∏à‡πÉ‡∏ä‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å
        possible_keys = [f"{sub_id}.L{prev_level}", f"{sub_id}_L{prev_level}"]
        
        for key in possible_keys:
            # ‡πÉ‡∏ä‡πâ get() ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô KeyError ‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
            result = getattr(self, 'assessment_results_map', {}).get(key)
            if result:
                if result.get('is_passed') is True:
                    self.logger.info(f"‚úÖ [LEVEL-GATE] Level {prev_level} passed for {sub_id}")
                    return True
                else:
                    self.logger.warning(f"‚ö†Ô∏è [LEVEL-GATE] Level {prev_level} found but status is FAIL")
                    return False

        # Safe Guard: ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≤‡∏° Level ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å
        self.logger.warning(f"üö´ [LEVEL-GATE] No assessment record for L{prev_level}. Blocking L{level}.")
        return False

    def _normalize_thai_text(self, text: str) -> str:
        """
        [ULTIMATE THAI NORMALIZE v2026.1.31 ‚Äì FULL REVISED]
        - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤ '‡∏™‡∏£‡∏∞‡∏´‡∏≤‡∏¢' (‡πÄ‡∏ä‡πà‡∏ô ‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£ -> ‡∏ú‡∏ö‡∏£‡∏´‡∏≤‡∏£) ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ñ‡∏≤‡∏ß‡∏£
        - High Performance: ‡∏ß‡∏ô‡∏•‡∏π‡∏õ Filter + Lowercase ‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏£‡∏≠‡∏ö‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏à‡∏ö
        - Unicode NFC: ‡∏£‡∏ß‡∏° Combining Characters (‡∏™‡∏£‡∏∞/‡∏ß‡∏£‡∏£‡∏ì‡∏¢‡∏∏‡∏Å‡∏ï‡πå) ‡∏Å‡πà‡∏≠‡∏ô‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
        - Strict Thai Range: ‡∏£‡∏±‡∏Å‡∏©‡∏≤ ‡∏Å-‡∏Æ, ‡∏™‡∏£‡∏∞, ‡∏ß‡∏£‡∏£‡∏ì‡∏¢‡∏∏‡∏Å‡∏ï‡πå ‡πÅ‡∏•‡∏∞‡πÄ‡∏•‡∏Ç‡πÑ‡∏ó‡∏¢ (\u0E00-\u0E7F) ‡∏Ñ‡∏£‡∏ö 100%
        """
        if not text or not isinstance(text, str):
            return ""

        # 1. Unicode NFC normalization 
        # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡πÑ‡∏ó‡∏¢ (‡πÄ‡∏ä‡πà‡∏ô ‡∏™‡∏£‡∏∞‡∏≠‡∏≥ ‡∏´‡∏£‡∏∑‡∏≠ ‡∏™‡∏£‡∏∞‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≠‡∏ô‡∏Å‡∏±‡∏ô)
        text = unicodedata.normalize('NFC', text)

        # 2. Compile Regex Pattern ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏ó‡∏µ‡πà‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï
        # ‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏© (a-zA-Z), ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (\u0E00-\u0E7F), ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç (0-9), ‡πÅ‡∏•‡∏∞‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á (\s)
        allowed_pattern = re.compile(r"[a-zA-Z0-9\u0E00-\u0E7F\s]")

        # 3. Single-pass Filter & Lowercase
        # ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£
        res = []
        for char in text:
            if allowed_pattern.match(char):
                # ‡∏ó‡∏≥ Lowercase ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏© (isascii ‡∏ä‡πà‡∏ß‡∏¢‡πÅ‡∏¢‡∏Å‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏≠‡∏≠‡∏Å‡∏ó‡∏±‡∏ô‡∏ó‡∏µ)
                if char.isascii() and char.isalpha():
                    res.append(char.lower())
                else:
                    res.append(char)
        
        # ‡∏£‡∏ß‡∏° List ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô String
        text = "".join(res)

        # 4. Collapse whitespace (‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ã‡πâ‡∏≥‡πÄ‡∏õ‡πá‡∏ô 1 ‡∏ä‡πà‡∏≠‡∏á) ‡πÅ‡∏•‡∏∞ Trim
        text = re.sub(r"\s+", " ", text).strip()

        return text
    
    
    def enhance_query_for_statement(
        self,
        statement_text: str,
        sub_id: str,
        statement_id: str,
        level: int,
        focus_hint: str = "",
    ) -> List[str]:
        """
        [STRATEGIC QUERY GEN v2026.1.25 ‚Äì HYBRID OPTIMIZED]
        - Robust Safety: ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ input ‡∏ß‡πà‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á fallback ‡πÄ‡∏™‡∏°‡∏≠
        - Level-Aware Negatives: ‡∏õ‡∏•‡∏î‡∏•‡πá‡∏≠‡∏Å '-MasterPlan' ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö L1-L2 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏à‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢
        - Phase-Targeted: ‡∏î‡∏∂‡∏á keywords ‡∏ï‡∏≤‡∏° PDCA phases ‡∏à‡∏≤‡∏Å JSON Config
        - Precision Shuffle: ‡∏à‡∏≥‡∏Å‡∏±‡∏î 8 queries ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
        """
        logger = logging.getLogger(__name__)
        log_prefix = f"[QUERY-GEN] {sub_id} L{level}"

        # 0. Safety guard
        if not statement_text or not isinstance(statement_text, str):
            logger.warning(f"{log_prefix} Empty/invalid statement_text ‚Üí fallback basic")
            fallback_q = f"{sub_id} {focus_hint or '‡∏ß‡∏¥‡∏™‡∏±‡∏¢‡∏ó‡∏±‡∏®‡∏ô‡πå ‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢'} ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ"
            return [fallback_q, f"{sub_id} ‡πÅ‡∏ú‡∏ô‡πÅ‡∏°‡πà‡∏ö‡∏ó KM"]

        # Anchors
        enabler_id = getattr(self.config, 'enabler', 'KM').upper()
        tenant_name = getattr(self.config, 'tenant', 'PEA').upper()
        id_anchor = f"{enabler_id} {sub_id}"

        # --- üõ°Ô∏è 1. Dynamic Negative Keywords (The Core Fix) ---
        if level <= 2:
            # ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢/‡∏ß‡∏≤‡∏á‡∏£‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô ‡∏´‡πâ‡∏≤‡∏°‡∏ö‡∏•‡πá‡∏≠‡∏Å‡πÅ‡∏ú‡∏ô‡πÅ‡∏°‡πà‡∏ö‡∏ó‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≤‡∏î
            neg_strict = "-‡∏†‡∏≤‡∏Ñ‡∏ú‡∏ô‡∏ß‡∏Å -‡∏†‡∏≤‡∏û‡∏ñ‡πà‡∏≤‡∏¢‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°"
        else:
            # ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥/‡∏¢‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö ‡∏ö‡∏•‡πá‡∏≠‡∏Å‡πÅ‡∏ú‡∏ô‡πÅ‡∏°‡πà‡∏ö‡∏ó‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏à‡∏£‡∏¥‡∏á
            neg_strict = "-‡πÅ‡∏ú‡∏ô‡πÅ‡∏°‡πà‡∏ö‡∏ó -‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏ä‡∏≤‡∏ï‡∏¥ -MasterPlan -‡∏†‡∏≤‡∏Ñ‡∏ú‡∏ô‡∏ß‡∏Å"

        # 2. Keywords Preparation
        require_phases = self.get_rule_content(sub_id, level, "require_phase") or ['P', 'D']
        query_syn = self.get_rule_content(sub_id, level, "query_synonyms") or ""
        
        raw_kws = self.get_rule_content(sub_id, level, "must_include_keywords") or []
        phase_map = {"P": "plan_keywords", "D": "do_keywords", "C": "check_keywords", "A": "act_keywords"}
        
        # ‡∏î‡∏∂‡∏á Default Keywords ‡∏à‡∏≤‡∏Å Enabler (KM)
        defaults = getattr(self, 'contextual_rules_map', {}).get("_enabler_defaults", {})
        for phase in require_phases:
            kw_key = phase_map.get(phase)
            if kw_key:
                raw_kws.extend(defaults.get(kw_key, []))

        clean_kws = " ".join(sorted(set(str(k).strip() for k in raw_kws if k))[:3])
        
        # Clean Statement (‡∏ï‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô '‡πÄ‡∏ä‡πà‡∏ô' ‡∏≠‡∏≠‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏°‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Search)
        clean_stmt = statement_text.split("‡πÄ‡∏ä‡πà‡∏ô", 1)[0].strip()
        clean_stmt = re.sub(r'[^\w\s]', '', clean_stmt)[:60]

        queries: List[str] = []

        # 3. Strategy: Multi-Angle Retrieval
        # A: Core Strategy (Synonyms + Statement)
        queries.append(f"{id_anchor} {query_syn} {clean_stmt}")

        # B: Evidence Type Strategy (‡∏ï‡∏≤‡∏°‡∏£‡∏∞‡∏î‡∏±‡∏ö Maturity)
        if level <= 2:
            queries.append(f"{tenant_name} ‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏® ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á ‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° ‡∏°‡∏ï‡∏¥‡∏ö‡∏≠‡∏£‡πå‡∏î ‡∏•‡∏á‡∏ô‡∏≤‡∏° {id_anchor} {query_syn}")
        else:
            queries.append(f"{tenant_name} ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏• KPI ‡∏ú‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏ú‡∏• {id_anchor} {query_syn} {neg_strict}")

        # C: Phase-Specific Strategy
        for phase in require_phases:
            queries.append(f"{id_anchor} {query_syn} {phase} {clean_kws} {neg_strict}")

        # D: Fallback Core
        if len(queries) < 4:
            queries.append(f"{tenant_name} {id_anchor} {clean_stmt} {clean_kws}")

        # 4. Post-process: Dedup, Truncate, and Shuffle
        final_queries = []
        seen = set()
        import random
        
        for q in queries:
            # Normalize ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡πÉ‡∏´‡πâ‡∏û‡∏≠‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Vector Search (18-24 ‡∏Ñ‡∏≥)
            q_clean = self._normalize_thai_text(q)
            words = q_clean.split()
            q_trunc = " ".join(words[:22])
            
            # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Query ‡∏ã‡πâ‡∏≥
            q_key = " ".join(words[:15])
            if q_trunc and q_key not in seen:
                final_queries.append(q_trunc)
                seen.add(q_key)

        random.shuffle(final_queries)
        
        logger.info(f"üöÄ [Query Gen v2026.1.25] {sub_id} L{level} | Final Queries: {len(final_queries[:8])} | Neg: {neg_strict}")
        return final_queries[:8]
    

    def _get_level_aware_queries(self, criteria_id: str, level_key: str) -> List[str]:
        """
        [REVISED v2026.1.25 - PRECISION EVIDENCE]
        """
        criteria_rules = self.contextual_rules_map.get(criteria_id, {})
        level_rule = criteria_rules.get(level_key, {})
        synonyms = level_rule.get("query_synonyms", "")
        
        tenant = getattr(self.config, 'tenant', 'PEA').upper()
        prefix = f"{tenant} {self.enabler} {criteria_id}"
        
        # ‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå‡∏ï‡∏≤‡∏° Synonyms ‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å JSON
        generated_queries = [
            f"{prefix} {synonyms}", # ‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á‡∏´‡∏•‡∏±‡∏Å
            f"{prefix} {synonyms} ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏° ‡∏°‡∏ï‡∏¥‡∏≠‡∏ô‡∏∏‡∏°‡∏±‡∏ï‡∏¥ ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÅ‡∏ï‡πà‡∏á‡∏ï‡∏±‡πâ‡∏á", # ‡∏á‡∏≤‡∏ô‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£
            f"{prefix} {synonyms} ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° ‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡πÉ‡∏ä‡πâ ‡∏•‡∏á‡∏ô‡∏≤‡∏°‡∏≠‡∏ô‡∏∏‡∏°‡∏±‡∏ï‡∏¥", # ‡∏á‡∏≤‡∏ô‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢
            f"{prefix} {synonyms} ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô ‡∏™‡∏£‡∏∏‡∏õ‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£ ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏•" # ‡∏á‡∏≤‡∏ô‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥
        ]
        
        return [self._normalize_thai_text(q) for q in generated_queries]

    
    def relevance_score_fn(self, evidence: Dict[str, Any], sub_id: str, level: int) -> float:
        """
        [ULTIMATE PRECISION v2026.02.01]
        - 40% Rerank (Base Confidence)
        - 30% Level-Aware Keywords (Semantic & Partial Match)
        - 20% Temporal & Source Alignment (Fiscal Year & Doc Type)
        - 10% Structural Quality (Density & Penalty)
        - Contextual Bonuses: Neighbor / Specific Rule
        """
        if not evidence or not isinstance(evidence, dict):
            return 0.0

        # --- 1. PREPARE DATA & METADATA ---
        text = str(evidence.get("text") or evidence.get("page_content") or "").lower().strip()
        meta = evidence.get("metadata") or {}
        
        # ‡∏î‡∏∂‡∏á‡∏õ‡∏µ‡∏á‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏à‡∏≤‡∏Å Config (‡∏™‡∏°‡∏°‡∏ï‡∏¥‡πÄ‡∏õ‡πá‡∏ô 2569 ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏‡πÉ‡∏ô Header)
        target_year = str(getattr(self.config, 'year', '2569'))
        
        # --- 2. BASE RERANK SCORE (40%) ---
        raw_val = evidence.get("rerank_score") or evidence.get("score") or 0.0
        normalized_rerank = min(max(float(raw_val), 0.0), 1.0)

        # --- 3. KEYWORD MATCHING (30%) - PARTIAL MATCH LOGIC ---
        cum_rules = self.get_cumulative_rules_cached(sub_id, level)
        target_kws = set()
        if level <= 2:
            target_kws.update(cum_rules.get("plan_keywords", []) + cum_rules.get("do_keywords", []))
        else:
            target_kws.update(cum_rules.get("check_keywords", []) + cum_rules.get("act_keywords", []))

        keyword_score = 0.0
        if target_kws and text:
            # ‡πÉ‡∏ä‡πâ Partial Match (‡πÑ‡∏ó‡∏¢): ‡∏ñ‡πâ‡∏≤ Keyword ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß > 4 ‡πÉ‡∏´‡πâ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô Text ‡πÑ‡∏´‡∏°
            matches = [kw for kw in target_kws if str(kw).lower() in text]
            if matches:
                # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢ (Diversity) ‡∏Ç‡∏≠‡∏á Keyword ‡∏ó‡∏µ‡πà‡∏û‡∏ö
                match_ratio = len(matches) / max(1, int(len(target_kws) * 0.4))
                keyword_score = min(match_ratio ** 0.7, 1.0)
                keyword_score = max(keyword_score, 0.20) # Floor 0.20 ‡∏ñ‡πâ‡∏≤‡πÄ‡∏à‡∏≠‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 1 ‡∏Ñ‡∏≥

        # --- 4. TEMPORAL & SOURCE ALIGNMENT (20%) ---
        alignment_bonus = 0.0
        
        # üìÖ Year Check: ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ñ‡πâ‡∏≤‡πÄ‡∏à‡∏≠‡∏õ‡∏µ‡∏á‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô / ‡∏´‡∏±‡∏Å‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ñ‡πâ‡∏≤‡πÄ‡∏Å‡πà‡∏≤‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
        if target_year in text:
            alignment_bonus += 0.10
        elif any(old_yr in text for old_yr in ["2566", "2567"]):
            alignment_bonus -= 0.10 # Penalty ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏•‡πâ‡∏≤‡∏™‡∏°‡∏±‡∏¢

        # üìÑ Source Check: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠‡∏Ç‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
        filename = str(meta.get("source") or evidence.get("source") or "").lower()
        primary_docs = ["‡∏°‡∏ï‡∏¥", "‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å", "‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á", "‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®", "‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢", "‡πÅ‡∏ú‡∏ô‡πÅ‡∏°‡πà‡∏ö‡∏ó", "‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå"]
        if any(p in filename for p in primary_docs):
            alignment_bonus += 0.10

        # --- 5. STRUCTURAL QUALITY & PENALTY (10%) ---
        quality_score = 0.0
        # ‚ùå Penalty ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Chunks ‡∏ó‡∏µ‡πà‡∏™‡∏±‡πâ‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏Ñ‡πà‡∏™‡∏≤‡∏£‡∏ö‡∏±‡∏ç
        if len(text) < 100:
            quality_score -= 0.15
        if "‡∏™‡∏≤‡∏£‡∏ö‡∏±‡∏ç" in text or "‡∏†‡∏≤‡∏Ñ‡∏ú‡∏ô‡∏ß‡∏Å" in text:
            quality_score -= 0.20
        
        # ‚úÖ Bonus ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Chunk ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏ô‡∏≤‡πÅ‡∏ô‡πà‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏π‡∏á (‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏¢‡∏≤‡∏ß‡∏û‡∏≠‡πÄ‡∏´‡∏°‡∏≤‡∏∞)
        if 500 < len(text) < 2000:
            quality_score += 0.05

        # --- 6. CONTEXTUAL BONUSES ---
        neighbor_bonus = 0.15 if (evidence.get("is_neighbor") or meta.get("is_neighbor")) else 0.0
        
        rule_bonus = 0.0
        specific_rule = str(cum_rules.get("specific_contextual_rule", "")).lower()
        if specific_rule and any(w in text for w in specific_rule.split()[:5]):
            rule_bonus = 0.15

        # --- 7. FINAL AGGREGATION ---
        # (0.40 * Rerank) + (0.30 * Keyword) + Alignment + Quality + Bonuses
        final_score = (
            (0.40 * normalized_rerank) + 
            (0.30 * keyword_score) + 
            alignment_bonus + 
            quality_score + 
            neighbor_bonus + 
            rule_bonus
        )

        # 8. HIGH-CONFIDENCE OVERRIDE
        # ‡∏ñ‡πâ‡∏≤ Rerank ‡∏°‡∏≤‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å (0.85+) ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏´‡πâ‡∏ú‡πà‡∏≤‡∏ô Threshold ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡πÄ‡∏™‡∏°‡∏≠
        if normalized_rerank >= 0.85:
            final_score = max(final_score, 0.50)

        final_score = min(max(final_score, 0.0), 1.0)

        # 9. LOGGING (PDCA-FREE DEBUG)
        try:
            if final_score > 0.30: # Log ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à
                self.logger.info(
                    f"üîé [REL] {sub_id} L{level} | Score:{final_score:.3f} | "
                    f"R:{normalized_rerank:.2f} KW:{keyword_score:.2f} "
                    f"Align:{alignment_bonus:+.2f} Q:{quality_score:+.2f} "
                    f"Src:{os.path.basename(filename)[:20]}"
                )
        except: pass

        return float(final_score)
    
    def _perform_adaptive_retrieval(
        self,
        sub_id: str,
        level: int,
        stmt: str,
        vectorstore_manager: Any,
    ) -> Tuple[List[Dict], float]:
        """
        [ULTIMATE REVISED v2026.02.01]
        - Adaptive Exit: ‡πÉ‡∏ä‡πâ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ó‡∏µ‡πà‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô‡∏ï‡∏≤‡∏°‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏Å (L4-L5 ‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö 0.35)
        - Forced Injection: ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Relevance Score ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Top 5 ‡πÄ‡∏™‡∏°‡∏≠
        - Clean Recovery: ‡∏•‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡∏¥‡∏î Recovery Sweep ‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡πÄ‡∏ß‡∏•‡∏≤
        """
        start_time = time.time()
        if not stmt or not isinstance(stmt, str):
            return [], 0.0

        candidates: List[Dict] = []
        used_uuids = set()
        final_max_score = 0.0
        level_key = f"L{level}"
        tenant = getattr(self.config, "tenant", "PEA").upper()

        # üéØ STRATEGY 1: Dynamic Threshold (‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ L4 ‡∏ó‡∏µ‡πà‡∏°‡∏±‡∏Å‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô Rerank ‡∏ï‡πà‡∏≥)
        # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö 4-5 ‡∏ó‡∏µ‡πà‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡πÇ‡∏¢‡∏á‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô 0.35 ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏ô‡∏±‡∏¢‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÅ‡∏•‡πâ‡∏ß
        effective_threshold = RETRIEVAL_RELEVANCE_THRESHOLD if level < 4 else 0.35

        def safe_relevance_score(evidence: Dict) -> float:
            try:
                return self.relevance_score_fn(evidence, sub_id, level)
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è [SAFE-SCORE] {sub_id} L{level}: {e}")
                return float(evidence.get('rerank_score') or evidence.get('score') or 0.0)

        # --- STEP 1: PRIORITY MAPPING (‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà Mapping ‡πÑ‡∏ß‡πâ) ---
        try:
            _, priority_docs = self._get_mapped_uuids_and_priority_chunks(
                sub_id=sub_id, level=level, statement_text=stmt, vectorstore_manager=vectorstore_manager
            ) or (set(), [])
            
            for p in priority_docs:
                uid = p.get("chunk_uuid")
                if not uid or uid in used_uuids: continue
                p["source"] = os.path.basename(p.get("source") or "Unknown")
                p["score"] = max(safe_relevance_score(p), 0.90) # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö High Score ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Priority
                used_uuids.add(uid)
                candidates.append(p)
                final_max_score = max(final_max_score, p["score"])
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Priority mapping skip: {e}")

        # --- STEP 2: HYBRID QUERY GENERATION ---
        json_queries = self._get_level_aware_queries(sub_id, level_key)
        legacy_queries = self.enhance_query_for_statement(stmt, sub_id, f"{sub_id}.L{level}", level)
        active_queries = list(dict.fromkeys(json_queries + legacy_queries))[:10]

        # --- STEP 3: ITERATIVE RETRIEVAL (‡∏û‡∏£‡πâ‡∏≠‡∏° Early Exit ‡πÉ‡∏´‡∏°‡πà) ---
        for i, q in enumerate(active_queries):
            # üéØ [EARLY EXIT] ‡πÉ‡∏ä‡πâ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏° Level ‡πÅ‡∏•‡πâ‡∏ß
            if len(candidates) >= RETRIEVAL_EARLY_EXIT_COUNT and final_max_score >= effective_threshold:
                self.logger.info(f"üéØ [EARLY-EXIT] {sub_id} L{level} | Found {len(candidates)} | Score {final_max_score:.4f} >= {effective_threshold}")
                break
            
            try:
                res = self.rag_retriever(
                    self._normalize_thai_text(q), self.doc_type, sub_id=sub_id, level=level,
                    vectorstore_manager=vectorstore_manager
                ) or {}
                
                for d in (res.get("top_evidences") or []):
                    uid = d.get("chunk_uuid")
                    score = float(d.get("score", 0.0))
                    
                    if uid and uid not in used_uuids and score >= RETRIEVAL_RERANK_FLOOR:
                        d["source"] = os.path.basename(d.get("source") or "Unknown")
                        
                        # üéØ [THE CORE FIX]: ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏â‡∏µ‡∏î‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö 5 ‡∏ä‡∏¥‡πâ‡∏ô‡πÅ‡∏£‡∏Å‡πÄ‡∏™‡∏°‡∏≠ 
                        # ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô Rerank ‡∏ó‡∏µ‡πà‡∏ï‡πà‡∏≥ (0.36) ‡∏°‡∏≤‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á AI
                        if len(candidates) < 5 or score > RETRIEVAL_HIGH_RERANK_THRESHOLD:
                            d["score"] = max(score, safe_relevance_score(d))
                        else:
                            d["score"] = score  
                        
                        used_uuids.add(uid)
                        candidates.append(d)
                        final_max_score = max(final_max_score, d["score"])
            except Exception as e:
                self.logger.error(f"‚ùå Query Loop {i+1} failed: {e}")

        # --- STEP 4: RECOVERY SWEEP (‡∏£‡∏±‡∏ô‡∏ï‡πà‡∏≠‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ô‡πâ‡∏≠‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏û‡πâ‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå) ---
        if final_max_score < effective_threshold or len(candidates) < 3:
            self.logger.info(f"üö® [RECOVERY] L{level} Max:{final_max_score:.4f} < {effective_threshold}. Sweep triggered.")
            self._execute_recovery_sweep(sub_id, level, stmt, tenant, used_uuids, candidates, vectorstore_manager)
            
            if candidates:
                for c in candidates:
                    if c.get("is_recovery"):
                        # ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ñ‡πà‡∏ß‡∏á‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏Å‡∏ß‡∏≤‡∏î‡∏°‡∏≤‡πÉ‡∏´‡∏°‡πà
                        c["score"] = max(c.get("score", 0.0), 
                                         0.7 * safe_relevance_score(c) + 0.3 * float(c.get("rerank_score", 0.0)))
                final_max_score = max([float(c.get("score", 0.0)) for c in candidates])

        # --- STEP 5: FINAL SORT & TRIM ---
        candidates.sort(key=lambda x: x.get("score", 0.0), reverse=True)
        final_docs = candidates[:ANALYSIS_FINAL_K]
        
        elapsed = time.time() - start_time
        self.logger.info(f"üèÅ [COMPLETE] {sub_id} L{level} | Docs: {len(final_docs)} | Max Score: {final_max_score:.4f} | Time: {elapsed:.2f}s")
        
        return final_docs, float(final_max_score)

    def _execute_recovery_sweep(self, sub_id, level, stmt, tenant, used_uuids, candidates, vectorstore_manager):
        """ 
        [REVISED v2026.02.01] 
        - ‡πÄ‡∏û‡∏¥‡πà‡∏° Multi-Query Fallback 
        - ‡∏õ‡∏£‡∏±‡∏ö Floor ‡∏ï‡∏≤‡∏°‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏Å
        """
        try:
            # üéØ 1. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Query ‡πÅ‡∏ö‡∏ö‡∏Å‡∏ß‡πâ‡∏≤‡∏á (‡∏ï‡∏±‡∏î Noise)
            base_stmt = stmt.split('(')[0].split('‡πÄ‡∏ä‡πà‡∏ô')[0].strip()
            
            # üéØ 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á Backup Query ‡∏à‡∏≤‡∏Å Keywords (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
            cum_rules = self.get_cumulative_rules_cached(sub_id, level)
            important_kws = " ".join((cum_rules.get("check_keywords") or [])[:3])
            
            # ‡∏ú‡∏™‡∏° Query: Tenant + ID + Statement + Keywords ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
            recovery_query = f"{tenant} {sub_id} {base_stmt} {important_kws}".strip()
            
            self.logger.info(f"üîç [RECOVERY-START] Query: {recovery_query[:60]}...")

            res_fb = self.rag_retriever(
                self._normalize_thai_text(recovery_query), 
                self.doc_type, 
                sub_id=sub_id, 
                level=level,
                vectorstore_manager=vectorstore_manager,
                enable_neighbor=False # ‡πÄ‡∏ô‡πâ‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß
            ) or {}
            
            new_found = 0
            # üéØ 3. ‡∏õ‡∏£‡∏±‡∏ö Recovery Floor ‡∏ï‡∏≤‡∏° Level (L4-L5 ‡∏¢‡∏≠‡∏°‡πÉ‡∏´‡πâ‡∏ï‡πà‡∏≥‡∏•‡∏á‡∏≠‡∏µ‡∏Å)
            recovery_floor = RETRIEVAL_RERANK_FLOOR * (0.7 if level >= 4 else 0.8)
            
            for d in (res_fb.get("top_evidences") or []):
                uid = d.get("chunk_uuid")
                score = float(d.get("score", 0.0))
                
                if uid and uid not in used_uuids and score >= recovery_floor:
                    d["source"] = os.path.basename(d.get("source") or "Unknown")
                    d["is_recovery"] = True
                    
                    # üéØ 4. Re-calculate score ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏∞‡∏ó‡πâ‡∏≠‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏à‡∏£‡∏¥‡∏á
                    # ‡πÑ‡∏°‡πà‡∏£‡∏≠‡πÉ‡∏´‡πâ loop ‡∏ô‡∏≠‡∏Å‡∏ó‡∏≥ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏Ç‡∏≠‡∏á‡∏î‡∏µ‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ó‡∏¥‡πâ‡∏á
                    d["score"] = self.relevance_score_fn(d, sub_id, level)
                    
                    used_uuids.add(uid)
                    candidates.append(d)
                    new_found += 1
            
            if new_found > 0:
                self.logger.info(f"‚úÖ [RECOVERY-SUCCESS] Found {new_found} chunks with floor {recovery_floor:.3f}")
                
        except Exception as e:
            self.logger.error(f"‚ùå Recovery sweep failed: {e}")
            
    def _log_pdca_status(self, sub_id, name, level, blocks, req_phases, sources_count, score, conf_level, **kwargs):
        """ [FULL REVISED] ‡∏û‡πà‡∏ô Dashboard ‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ PDCA ‡πÅ‡∏ö‡∏ö Real-time """
        try:
            actual_counts = kwargs.get('pdca_breakdown', {}) 
            is_safety_pass = kwargs.get('is_safety_pass', False)
            status_parts = []
            source_errors = 0
            
            # Mapping Key ‡∏à‡∏≤‡∏Å LLM Output ‡∏Å‡∏±‡∏ö PDCA Phase
            mapping = [("Extraction_P", "P"), ("Extraction_D", "D"), ("Extraction_C", "C"), ("Extraction_A", "A")]

            for full_key, short in mapping:
                count = float(actual_counts.get(short, 0.0))
                content = str(blocks.get(full_key, "")).strip()
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ AI ‡πÄ‡∏à‡∏≠‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏£‡∏¥‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                ai_found = bool(content and content.lower() not in ["-", "n/a", "none", "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"])
                has_source = "[Source:" in content and "]" in content
                
                if ai_found and not has_source:
                    source_errors += 1

                # Logic ‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Icon ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏™‡∏î‡∏á Maturity Gap
                if count >= 1.0:
                    icon = "‚úÖ" if has_source else "‚ö†Ô∏è" # ‡∏ú‡πà‡∏≤‡∏ô/‡∏ú‡πà‡∏≤‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∑‡∏°‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á
                elif is_safety_pass and short in req_phases:
                    icon = "üõ°Ô∏è" # ‡∏ú‡πà‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏£‡∏∞‡∏ö‡∏ö Safety Pass
                elif ai_found: 
                    icon = "üî∑" # ‡∏û‡∏ö‡∏£‡πà‡∏≠‡∏á‡∏£‡∏≠‡∏¢‡πÅ‡∏ï‡πà‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÑ‡∏°‡πà‡πÅ‡∏ô‡πà‡∏ô‡∏û‡∏≠
                elif short not in req_phases: 
                    icon = "‚ûñ" # ‡πÄ‡∏ü‡∏™‡∏ô‡∏µ‡πâ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡∏µ‡πâ
                else: 
                    icon = "‚ùå" # ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•

                status_parts.append(f"{short}:{icon}({count:.1f})")

            display_score = float(score or 0.0)
            alert_msg = f" üö®[REF-ERR:{source_errors}]" if source_errors > 0 else ""
            pass_label = " üõ°Ô∏è[SAFETY-PASS]" if is_safety_pass else ""
            
            # ‡∏û‡πà‡∏ô Dashboard ‡∏≠‡∏≠‡∏Å Console
            self.logger.info(
                f"üìä [PDCA-DASHBOARD] {sub_id} L{level} | {str(name)[:60]}...\n"
                f"   Maturity Status: {' '.join(status_parts)}{pass_label}{alert_msg}\n"
                f"   Summary: Score={display_score:.2f} | Evidence={sources_count} chunks | AI-Conf={str(conf_level).upper()}"
            )
        except Exception as e:
            self.logger.error(f"‚ùå Dashboard Logging Failed: {str(e)}")

    
    def _summarize_evidence_list_short(self, evidences: list, max_sentences: int = 3) -> str:
        """ [REVISED] ‡πÅ‡∏õ‡∏•‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ AI ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏á‡πà‡∏≤‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô """
        if not evidences: 
            return "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÄ‡∏ä‡∏¥‡∏á‡∏õ‡∏£‡∏∞‡∏à‡∏±‡∏Å‡∏©‡πå‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö"
        
        parts = []
        # ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏£‡∏¥‡∏á
        valid_evidences = [ev for ev in evidences if isinstance(ev, dict) and (ev.get("text") or ev.get("content", "")).strip()]
        
        for ev in valid_evidences[:max_sentences]:
            # ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå (‡πÉ‡∏ä‡πâ os.path.basename ‡∏à‡∏≤‡∏Å Header)
            filename = os.path.basename(ev.get("file_name") or ev.get("source") or "Unknown_Document")
            page = ev.get("page", "N/A")
            
            # ‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏•‡∏∞‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î Token
            raw_text = ev.get("text") or ev.get("content") or ""
            clean_text = " ".join(raw_text[:150].split()).strip()
            
            parts.append(f"‚Ä¢ [{filename}, ‡∏´‡∏ô‡πâ‡∏≤ {page}]: \"{clean_text}...\"")

        return "\n".join(parts)
    
    def _run_expert_re_evaluation(
        self,
        sub_id: str,
        level: int,
        statement_text: str,
        context: str,
        first_attempt_reason: str,
        missing_tags: Union[List[str], Set[str]],
        highest_rerank_score: float,
        sub_criteria_name: str,
        llm_evaluator_to_use: Any,
        base_kwargs: Dict[str, Any],
        audit_instruction: str = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        [JUDICIAL REVIEW - ENHANCED v2026.02.05]
        - ‡πÄ‡∏û‡∏¥‡πà‡∏° log ‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î + appeal hint ‡∏ó‡∏µ‡πà‡∏Ñ‡∏°‡∏ä‡∏±‡∏î‡∏Ç‡∏∂‡πâ‡∏ô
        - ‡πÉ‡∏ä‡πâ highest_rerank_score ‡πÄ‡∏õ‡πá‡∏ô weight ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ override
        - ‡πÄ‡∏û‡∏¥‡πà‡∏° fallback JSON structure ‡∏ñ‡πâ‡∏≤ LLM ‡∏ï‡∏≠‡∏ö‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö
        - ‡πÑ‡∏°‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô signature ‡∏´‡∏£‡∏∑‡∏≠ input ‡πÄ‡∏î‡∏¥‡∏°‡πÄ‡∏•‡∏¢
        """
        log_prefix = f"‚öñÔ∏è [EXPERT-APPEAL] {sub_id} L{level}"
        self.logger.info(f"{log_prefix} | Starting rescue (Max Rerank: {highest_rerank_score:.4f} | Missing: {missing_tags})")

        # 1. [PREPARE APPEAL HINT] ‚Äì ‡∏î‡∏∏‡∏î‡∏±‡∏ô + ‡πÄ‡∏ô‡πâ‡∏ô substance over form
        missing_str = ", ".join(sorted(set(missing_tags))) if missing_tags else "PDCA Core Criteria (P/D/C/A)"
        
        appeal_instruction = f"""
    ### üö® EXPERT JUDICIAL REVIEW - SECOND CHANCE (OVERRIDE MODE) üö®
    [CONTEXT]: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å (Rerank {highest_rerank_score:.4f}) ‡πÅ‡∏ï‡πà‡∏£‡∏≠‡∏ö‡πÅ‡∏£‡∏Å‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏û‡∏£‡∏≤‡∏∞ "{first_attempt_reason[:150]}..."
    [CRITICAL FOCUS]: ‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏ä‡∏¥‡∏á‡∏™‡∏≤‡∏£‡∏∞ (Substance over Form) ‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô‡∏ó‡∏µ‡πà‡∏Ç‡∏≤‡∏î: {missing_str}
    [MANDATORY RULE]: 
    - ‡∏´‡∏≤‡∏Å‡∏û‡∏ö‡∏£‡πà‡∏≠‡∏á‡∏£‡∏≠‡∏¢‡πÅ‡∏°‡πâ‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ "‡∏î‡∏∏‡∏•‡∏¢‡∏û‡∏¥‡∏ô‡∏¥‡∏à‡πÄ‡∏ä‡∏¥‡∏á‡∏ö‡∏ß‡∏Å" (Expert Positive Override)
    - ‡∏≠‡∏¢‡πà‡∏≤‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö ‡πÅ‡∏ï‡πà‡∏î‡∏π‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏à‡∏£‡∏¥‡∏á‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô
    - ‡∏ñ‡πâ‡∏≤‡∏ú‡πà‡∏≤‡∏ô ‡πÉ‡∏´‡πâ‡πÄ‡∏û‡∏¥‡πà‡∏° reason ‡∏ß‡πà‡∏≤ "‡∏ú‡πà‡∏≤‡∏ô‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏≠‡∏∏‡∏ó‡∏ò‡∏£‡∏ì‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç"
    """

        # 2. [PDCA BLOCKS INJECTION] ‚Äì ‡∏â‡∏µ‡∏î hint ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô‡∏™‡∏∏‡∏î
        original_blocks = base_kwargs.get("pdca_blocks", [])
        expert_pdca_blocks = []
        if isinstance(original_blocks, list):
            expert_pdca_blocks = [{"tag": "SYSTEM-APPEAL", "content": appeal_instruction}] + list(original_blocks)
        else:
            expert_pdca_blocks = f"{appeal_instruction}\n\n{str(original_blocks)}"

        # 3. [SAFE KWARGS CONSTRUCTION] ‚Äì ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà llm_evaluator_to_use ‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ
        safe_kwargs = {
            "pdca_blocks": expert_pdca_blocks,
            "sub_id": sub_id,
            "level": level,
            "audit_confidence": highest_rerank_score,  # ‡πÉ‡∏ä‡πâ rerank ‡πÄ‡∏õ‡πá‡∏ô confidence ‡∏´‡∏•‡∏±‡∏Å
        }
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏° audit_instruction ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ (fallback ‡∏ñ‡πâ‡∏≤ base_kwargs ‡πÑ‡∏°‡πà‡∏°‡∏µ)
        if audit_instruction:
            safe_kwargs["audit_instruction"] = audit_instruction
        elif "audit_instruction" in base_kwargs:
            safe_kwargs["audit_instruction"] = base_kwargs["audit_instruction"]

        # 4. [EXECUTE RE-EVALUATION]
        try:
            self.logger.debug(f"{log_prefix} | Calling evaluator with safe_kwargs: {safe_kwargs.keys()}")
            re_eval_result = llm_evaluator_to_use(**safe_kwargs)
            
            if not isinstance(re_eval_result, dict):
                self.logger.warning(f"{log_prefix} | Evaluator returned non-dict ‚Üí fallback")
                re_eval_result = {"is_passed": False, "score": 0.0, "reason": "Evaluator returned invalid format"}

            # 5. [EVALUATE & OVERRIDE]
            is_passed_now = bool(re_eval_result.get("is_passed", False))
            
            if is_passed_now:
                self.logger.info(f"üõ°Ô∏è [OVERRIDE-SUCCESS] {log_prefix} | ‡∏≠‡∏∏‡∏ó‡∏ò‡∏£‡∏ì‡πå‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à! (New Score: {re_eval_result.get('score', 'N/A')})")
                re_eval_result.update({
                    "is_safety_pass": True,
                    "appeal_status": "GRANTED",
                    "reason": f"üåü [EXPERT OVERRIDE]: {re_eval_result.get('reason', '‡∏ú‡πà‡∏≤‡∏ô‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏≠‡∏∏‡∏ó‡∏ò‡∏£‡∏ì‡πå')}"
                })
            else:
                self.logger.info(f"‚ùå [APPEAL-DENIED] {log_prefix} | ‡∏ú‡∏•‡∏≠‡∏∏‡∏ó‡∏ò‡∏£‡∏ì‡πå: ‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô")
                re_eval_result["appeal_status"] = "DENIED"

            # Fallback ‡∏ñ‡πâ‡∏≤ JSON ‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö field
            required_keys = ["is_passed", "score", "reason"]
            for key in required_keys:
                if key not in re_eval_result:
                    re_eval_result[key] = False if key == "is_passed" else 0.0 if key == "score" else "Missing key after appeal"

            return re_eval_result

        except Exception as e:
            self.logger.error(f"üõë [APPEAL-CRASH] {log_prefix} failed: {str(e)}", exc_info=True)
            return {
                "is_passed": False,
                "score": 0.0,
                "appeal_status": "FATAL_ERROR",
                "reason": f"Appeal system error: {str(e)[:200]}",
                "is_safety_pass": False
            }

    def _build_multichannel_context_for_level( # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô Private Method
        self, # ‡πÄ‡∏û‡∏¥‡πà‡∏° self
        level: int,
        top_evidences: List[Dict[str, Any]],
        previous_levels_map: Optional[Dict[str, Any]] = None,
        previous_levels_evidence: Optional[List[Dict[str, Any]]] = None,
        max_main_context_tokens: int = 3000, 
        max_summary_sentences: int = 4,
        max_context_length: Optional[int] = None, 
        **kwargs
    ) -> Dict[str, Any]:
        """
        [ULTIMATE OPTIMIZED v2026.8 - REFACTORED AS CLASS METHOD]
        """
        K_MAIN = 5
        # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å Config ‡πÉ‡∏ô Class ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢‡∏ñ‡πâ‡∏≤‡∏°‡∏µ ‡πÄ‡∏ä‡πà‡∏ô self.config.l1_threshold
        MIN_RELEVANCE_FOR_AUX = 0.15 if level == 1 else 0.4 

        # 1. Baseline Summary
        baseline_evidence = previous_levels_evidence or []
        if previous_levels_map:
            for lvl_ev in previous_levels_map.values():
                baseline_evidence.extend(lvl_ev)

        # Normalize list (‡∏ï‡∏≤‡∏° Logic ‡πÄ‡∏î‡∏¥‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡∏ö‡∏±‡πä‡∏Å Slice ‡πÅ‡∏•‡πâ‡∏ß)
        baseline_evidence_list = []
        if isinstance(baseline_evidence, list):
            baseline_evidence_list = baseline_evidence
        elif isinstance(baseline_evidence, dict):
            baseline_evidence_list = list(baseline_evidence.values())
        
        summarizable_baseline = [
            item for item in baseline_evidence_list[:40] 
            if isinstance(item, dict) and (item.get("text") or item.get("content", "")).strip()
        ]

        if not summarizable_baseline:
            baseline_summary = "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ (‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏ó‡∏µ‡πà Level 1)"
        else:
            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ú‡πà‡∏≤‡∏ô self
            baseline_summary = self._summarize_evidence_list_short(
                summarizable_baseline,
                max_sentences=max_summary_sentences
            )

        # 2. Direct + Aux Separation
        direct, aux_candidates = [], []

        for idx, ev in enumerate(top_evidences[:40], 1):
            if not isinstance(ev, dict): continue

            tag = (ev.get("pdca_tag") or ev.get("PDCA") or "Other").upper()
            relevance = ev.get("rerank_score") or ev.get("score", 0.0)
            text_preview = (ev.get('text', '')[:80] + "...") if ev.get('text') else "[No text]"

            # ‡πÉ‡∏ä‡πâ self.logger ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢
            self.logger.debug(f"[TAG-CHECK L{level} #{idx}] Rel: {relevance:.3f} | Tag: {tag} | Preview: {text_preview}")

            if tag in {"P", "PLAN", "D", "DO", "C", "CHECK", "A", "ACT"}:
                direct.append(ev)
            elif relevance >= MIN_RELEVANCE_FOR_AUX:
                aux_candidates.append(ev)

        # 3. Logic ‡∏Å‡∏≤‡∏£‡∏¢‡πâ‡∏≤‡∏¢ Aux ‡πÅ‡∏•‡∏∞ Fallback (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏° ‡πÅ‡∏ï‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô self.logger)
        if len(direct) < K_MAIN:
            need = K_MAIN - len(direct)
            moved = aux_candidates[:need]
            direct.extend(moved)
            aux_candidates = aux_candidates[need:]
            self.logger.info(f"[DIRECT-FILL] Moved {need} aux chunks to direct (total direct: {len(direct)})")

        if level == 1 and len(direct) == 0 and top_evidences:
            need = min(K_MAIN, len(top_evidences))
            forced_chunks = sorted(top_evidences, key=lambda e: e.get("rerank_score", 0) or e.get("score", 0), reverse=True)[:need]
            direct.extend(forced_chunks)
            self.logger.warning(f"[L1-ULTRA-FALLBACK] No PDCA tag at all ‚Üí Forced top {need} chunks to direct")

        # 5. ‡∏™‡∏£‡πâ‡∏≤‡∏á aux_summary (‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ú‡πà‡∏≤‡∏ô self)
        aux_summary = self._summarize_evidence_list_short(aux_candidates, max_sentences=3) if aux_candidates else \
            "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏£‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠"

        # 6. Return ‡∏û‡∏£‡πâ‡∏≠‡∏° Debug Meta
        self.logger.info(f"Context L{level} ‚Üí Direct:{len(direct)} | Aux:{len(aux_candidates)} | Baseline:{len(summarizable_baseline)}")

        return {
            "baseline_summary": baseline_summary,
            "direct_context": "", 
            "aux_summary": aux_summary,
            "debug_meta": {
                "level": level,
                "direct_count": len(direct),
                "aux_count": len(aux_candidates),
                "top_relevance": max((ev.get("rerank_score", 0) for ev in top_evidences), default=0)
            },
        }

    def _apply_diversity_filter(
        self,
        evidences: List[Dict[str, Any]],
        max_per_source: int = 3,
        max_total: int = 40,
    ):
        """
        Diversity & dedup filter for evidence chunks

        Rules:
        - Preserve priority chunks
        - Deduplicate by chunk_uuid
        - Limit chunks per source file
        - Sort by (priority, rerank_score)
        - Stable / deterministic
        """

        if not evidences:
            return []

        # --------------------------------------------------
        # 1Ô∏è‚É£ Deduplicate by chunk_uuid (or content hash)
        # --------------------------------------------------
        unique = {}
        for d in evidences:
            if not isinstance(d, dict):
                continue

            uid = d.get("chunk_uuid")
            if not uid:
                uid = hashlib.sha256(
                    str(d.get("page_content", "")).encode()
                ).hexdigest()

            if uid not in unique:
                unique[uid] = d

        docs = list(unique.values())

        # --------------------------------------------------
        # 2Ô∏è‚É£ Sort: priority first, then score
        # --------------------------------------------------
        docs = sorted(
            docs,
            key=lambda x: (
                bool(x.get("is_priority", False)),
                self.get_actual_score(x),
            ),
            reverse=True,
        )

        # --------------------------------------------------
        # 3Ô∏è‚É£ Enforce per-source diversity
        # --------------------------------------------------
        source_counter = {}
        diversified = []

        for d in docs:
            src = (
                d.get("source")
                or d.get("metadata", {}).get("source")
                or "unknown"
            )

            count = source_counter.get(src, 0)
            if count >= max_per_source:
                continue

            diversified.append(d)
            source_counter[src] = count + 1

            if len(diversified) >= max_total:
                break

        return diversified

    
    def _load_evidence_map(self, is_for_merge: bool = False) -> Dict[str, Any]:
        """
        [REVISED v2026.1.24]
        - ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏Å‡πà‡∏≤ (List) ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà (UI-Ready)
        - ‡∏î‡∏∂‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà User ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å (is_selected) ‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å
        """
        if hasattr(self, '_evidence_cache') and self._evidence_cache is not None:
            return deepcopy(self._evidence_cache)

        try:
            path = get_evidence_mapping_file_path(
                tenant=self.config.tenant, year=self.config.year, enabler=self.enabler
            )
        except: return {}

        if not os.path.exists(path):
            return {}

        try:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)

            processed_map = {}
            for key, content in data.items():
                # üîÑ Auto-Convert: ‡∏ñ‡πâ‡∏≤‡∏Ç‡∏≠‡∏á‡πÄ‡∏Å‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô List ‡πÉ‡∏´‡πâ‡∏¢‡∏±‡∏î‡πÉ‡∏™‡πà‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà
                if isinstance(content, list):
                    evidences = content
                    status = "pending"
                else:
                    evidences = content.get("evidences", [])
                    status = content.get("status", "pending")
                
                cleaned = []
                for e in evidences:
                    if not isinstance(e, dict): continue
                    # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏ü‡∏¥‡∏•‡∏î‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö UI
                    e["is_selected"] = e.get("is_selected", True)
                    e["source_type"] = e.get("source_type", "ai_found")
                    cleaned.append(e)
                
                processed_map[key] = {"status": status, "evidences": cleaned}

            self._evidence_cache = deepcopy(processed_map)
            return processed_map
        except Exception as e:
            self.logger.error(f"‚ùå Load failed: {e}")
            return {}


    def _update_internal_evidence_map(self, merged_evidence: Dict[str, Any]):
        """
        [FINAL REVISED v2026.01.27 - THE PERSISTENCE GUARD]
        - üõ°Ô∏è Atomic Sync: ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö Normalize Metadata ‡∏Å‡πà‡∏≠‡∏ô‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏Ç‡πâ‡∏≤ State ‡πÄ‡∏™‡∏°‡∏≠
        - üß† Hash Intelligence: ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ã‡πâ‡∏≥ (Deduplication)
        - ‚ö° Data Alignment: ‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏ß‡πà‡∏≤‡∏ü‡∏¥‡∏•‡∏î‡πå 'content' ‡πÅ‡∏•‡∏∞ 'confidence' ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å Sync ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô
        """
        if not hasattr(self, 'evidence_map') or self.evidence_map is None:
            self.evidence_map = {}
            
        if not isinstance(merged_evidence, dict): 
            return

        def get_stable_hash(text: str) -> str:
            """‡∏™‡∏£‡πâ‡∏≤‡∏á Fingerprint ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡πâ‡∏≥"""
            if not text: return ""
            # ‡πÉ‡∏ä‡πâ‡∏´‡∏ô‡πâ‡∏≤-‡∏´‡∏•‡∏±‡∏á‡∏Ç‡∏≠‡∏á Content ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡πÅ‡∏•‡∏∞‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥
            target = f"{text[:250]}...{text[-250:]}" if len(text) > 500 else text
            return hashlib.md5(target.encode('utf-8')).hexdigest()

        for key, incoming_data in merged_evidence.items():
            # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á {"evidences": []} ‡πÅ‡∏•‡∏∞ list ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
            new_ev_list = incoming_data.get("evidences", []) if isinstance(incoming_data, dict) else incoming_data
            if not isinstance(new_ev_list, list): 
                continue
                
            # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Bucket ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Level Key ‡∏ô‡∏±‡πâ‡∏ô‡πÜ (‡πÄ‡∏ä‡πà‡∏ô 1.1_L1)
            if key not in self.evidence_map or not isinstance(self.evidence_map[key], dict):
                self.evidence_map[key] = {"status": "completed", "evidences": []}
            
            target_bucket = self.evidence_map[key]
            
            # ‡∏î‡∏∂‡∏á Hash ‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß‡∏°‡∏≤‡πÄ‡∏ä‡πá‡∏Ñ
            existing_hashes = {
                get_stable_hash(str(e.get('content') or e.get('text', ''))) 
                for e in target_bucket["evidences"]
            }
            
            for ev in new_ev_list:
                if not isinstance(ev, dict): continue
                
                # 1. ‡∏™‡∏Å‡∏±‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å
                content_str = str(ev.get('content') or ev.get('text') or "").strip()
                if not content_str: 
                    continue 
                
                # 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô (Deduplication)
                ev_hash = get_stable_hash(content_str)
                if ev_hash not in existing_hashes:
                    
                    # üéØ [THE CORE FIX]: ‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤ Normalize ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Confidence 0.0
                    # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤ Metadata ‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ñ‡∏π‡∏Å‡∏™‡∏Å‡∏±‡∏î‡∏°‡∏≤‡∏•‡∏á‡∏ó‡∏µ‡πà Root Object
                    normalized_batch = self._normalize_evidence_metadata([ev])
                    
                    if normalized_batch:
                        clean_ev = normalized_batch[0]
                        
                        # 3. Final Content Restoration
                        # ‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏ß‡πà‡∏≤ Content ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏´‡∏≤‡∏¢‡πÑ‡∏õ‡πÅ‡∏•‡∏∞‡∏ñ‡∏π‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏ä‡∏∑‡πà‡∏≠ 'content' ‡πÄ‡∏™‡∏°‡∏≠
                        clean_ev["content"] = content_str
                        
                        # 4. Persistence
                        target_bucket["evidences"].append(clean_ev)
                        existing_hashes.add(ev_hash)

        self.logger.info(f"‚úÖ State Sync complete. Level Groups: {len(self.evidence_map)}")

    # evidence map structure (for ai understanding)
    # {
    # ¬† "1.1_L1": {
    # ¬† ¬† "status": "reviewed",
    # ¬† ¬† "evidences": [
    # ¬† ¬† ¬† {
    # ¬† ¬† ¬† ¬† "doc_id": "15f0060f-674d-551e-b855-3b7e335450a8",
    # ¬† ¬† ¬† ¬† "filename": "KM1.1L502 Learning Form ‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£.pdf",
    # ¬† ¬† ¬† ¬† "page": "11",
    # ¬† ¬† ¬† ¬† "source_type": "human_map",
    # ¬† ¬† ¬† ¬† "is_selected": true,
    # ¬† ¬† ¬† ¬† "relevance_score": 0.95,
    # ¬† ¬† ¬† ¬† "note": "‡∏´‡πâ‡∏≤‡∏°‡∏•‡∏ö! ‡πÉ‡∏ä‡πâ‡∏¢‡∏±‡∏ô‡∏Ç‡πâ‡∏≠ 1.1 ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞"
    # ¬† ¬† ¬† },
    # ¬† ¬† ¬† {
    # ¬† ¬† ¬† ¬† "doc_id": "ai-file-999",
    # ¬† ¬† ¬† ¬† "filename": "KM_Policy_2567_Final.pdf",
    # ¬† ¬† ¬† ¬† "page": "1",
    # ¬† ¬† ¬† ¬† "source_type": "system_gen",
    # ¬† ¬† ¬† ¬† "relevance_score": 0.98
    # ¬† ¬† ¬† }
    # ¬† ¬† ]
    # ¬† }
    # }

    def _save_evidence_map(
        self,
        map_to_save: Optional[Dict[str, Any]] = None,
        clear_existing: bool = False
    ):
        """
        [ULTIMATE HARDENED BUILD v2026.01.27.1700]
        - ‚úÖ ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô NoneType Crash 100% ‡∏î‡πâ‡∏ß‡∏¢ _safe_float ‡πÅ‡∏•‡∏∞ _safe_str
        - ‚úÖ ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏∏‡∏Å‡πÇ‡∏´‡∏ô‡∏î‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£ Normalize ‡∏Å‡πà‡∏≠‡∏ô‡∏Å‡∏≤‡∏£ Sort
        - ‚úÖ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤ 'float() argument must be a string or a real number, not NoneType'
        - üõ°Ô∏è Atomic write (tempfile + move) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏û‡∏±‡∏á‡∏´‡∏≤‡∏Å‡∏£‡∏∞‡∏ö‡∏ö‡∏î‡∏±‡∏ö
        """

        # --- INTERNAL HELPERS ---
        def _safe_float(val, default=0.0) -> float:
            try:
                if val is None: return float(default)
                return float(val)
            except (TypeError, ValueError):
                return float(default)

        def _safe_str(val, default="0") -> str:
            if val is None: return str(default)
            return str(val).strip()

        def _normalize_evidence_node(e: Dict[str, Any]) -> Dict[str, Any]:
            """‡πÅ‡∏õ‡∏•‡∏á Raw Data ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏•‡∏µ‡∏ô‡πÇ‡∏´‡∏ô‡∏î‡∏ó‡∏µ‡πà‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö UI ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì"""
            return {
                "doc_id": _safe_str(e.get("doc_id") or e.get("chunk_uuid"), "unknown"),
                "filename": _safe_str(e.get("filename"), "Unknown File"),
                "page": _safe_str(e.get("page") or e.get("page_label"), "0"),
                "source_type": _safe_str(e.get("source_type"), "system_gen"),
                "is_selected": bool(e.get("is_selected", True)),
                "relevance_score": _safe_float(
                    e.get("relevance_score"), 
                    _safe_float(e.get("rerank_score", 0.0))
                ),
                "note": _safe_str(e.get("note"), "")
            }

        try:
            # 1. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Path ‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ç‡∏≠‡∏á Folder
            map_file_path = get_evidence_mapping_file_path(
                tenant=self.config.tenant,
                year=self.config.year,
                enabler=self.enabler
            )
            os.makedirs(os.path.dirname(map_file_path), exist_ok=True)

            # 2. ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏°‡∏û‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏°‡∏≤‡∏ï‡∏±‡πâ‡∏á‡∏ï‡πâ‡∏ô (Merge Logic)
            final_map = {} if clear_existing else self._load_evidence_map(is_for_merge=True)

            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Incoming Data
            incoming = map_to_save if map_to_save is not None else getattr(self, "evidence_map", {})
            if not isinstance(incoming, dict):
                self.logger.warning("‚ö†Ô∏è [EVIDENCE-MAP] Incoming map is not dict, skip save")
                return

            # 3. ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£ Merge ‡πÅ‡∏•‡∏∞ Normalize
            for key, evidence_data in incoming.items():
                if not key or not isinstance(key, str): continue

                # ‡∏™‡∏£‡πâ‡∏≤‡∏á Bucket ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô (e.g., 1.1_L1)
                bucket = final_map.setdefault(
                    key, {"status": "pending", "evidences": []}
                )
                existing_evs = bucket["evidences"]

                # ‡∏î‡∏∂‡∏á List ‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏≠‡∏≠‡∏Å‡∏°‡∏≤
                new_evs = (
                    evidence_data.get("evidences", [])
                    if isinstance(evidence_data, dict)
                    else (evidence_data if isinstance(evidence_data, list) else [])
                )

                for raw_e in new_evs:
                    if not isinstance(raw_e, dict): continue

                    # Normalize ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô‡∏ô‡∏≥‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
                    normalized = _normalize_evidence_node(raw_e)
                    
                    # ‡∏™‡∏£‡πâ‡∏≤‡∏á Key ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ä‡πá‡∏Ñ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡πâ‡∏≥ (doc_id + ‡∏´‡∏ô‡πâ‡∏≤)
                    idx_key = f"{normalized['doc_id']}_{normalized['page']}"

                    match = next(
                        (e for e in existing_evs if f"{e.get('doc_id')}_{e.get('page')}" == idx_key),
                        None
                    )

                    if match:
                        # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà (Update)
                        match.update(normalized)
                        # ‡∏ñ‡πâ‡∏≤‡∏Ç‡∏≠‡∏á‡πÄ‡∏î‡∏¥‡∏°‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡πÄ‡∏õ‡πá‡∏ô human_map ‡πÉ‡∏´‡πâ‡∏Ñ‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡πÑ‡∏ß‡πâ
                        if normalized.get("source_type") == "human_map":
                            match["source_type"] = "human_map"
                    else:
                        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà (Insert)
                        existing_evs.append(normalized)

            # 4. Post-processing (‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á ‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞)
            for bucket in final_map.values():
                evs = bucket.get("evidences", [])

                # ‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏ä‡∏±‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏Å‡πà‡∏≠‡∏ô‡∏Å‡∏≤‡∏£ Sort (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô NoneType ‡∏£‡∏±‡πà‡∏ß‡πÑ‡∏´‡∏•)
                for e in evs:
                    e["relevance_score"] = _safe_float(e.get("relevance_score"))
                    e["page"] = _safe_str(e.get("page"))

                # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á (‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡πÑ‡∏õ‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î)
                evs.sort(key=lambda x: x["relevance_score"], reverse=True)

                # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏ä‡∏¥‡πâ‡∏ô‡πÑ‡∏´‡∏ô User ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏≠‡∏á ‡πÉ‡∏´‡πâ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô reviewed)
                bucket["status"] = (
                    "reviewed"
                    if any(e.get("source_type") == "human_map" for e in evs)
                    else "ai_generated"
                )

            # 5. ‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏ö‡∏ö Atomic (‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß‡∏Å‡πà‡∏≠‡∏ô‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏≠‡∏¢‡∏¢‡πâ‡∏≤‡∏¢)
            temp_path = None
            temp_dir = os.path.dirname(map_file_path)
            try:
                with tempfile.NamedTemporaryFile(
                    mode="w", delete=False, dir=temp_dir, suffix=".tmp", encoding="utf-8"
                ) as tmp:
                    json.dump(final_map, tmp, indent=4, ensure_ascii=False)
                    temp_path = tmp.name

                shutil.move(temp_path, map_file_path)
                
                # Sync ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà Instance Memory
                self.evidence_map = final_map
                self.logger.info(f"‚úÖ [EVIDENCE-MAP] Save Successful: {map_file_path}")

            except Exception as io_err:
                self.logger.error(f"‚ùå [EVIDENCE-MAP] File System Error: {str(io_err)}")
                if temp_path and os.path.exists(temp_path):
                    os.remove(temp_path)

        except Exception as e:
            # üßØ ‡∏Å‡∏é‡πÄ‡∏´‡∏•‡πá‡∏Å: ‡∏´‡πâ‡∏≤‡∏°‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏´‡∏•‡∏±‡∏Å‡πÅ‡∏°‡πâ‡∏Å‡∏≤‡∏£‡πÄ‡∏ã‡∏ü‡∏à‡∏∞‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß
            self.logger.error(f"‚ùå [EVIDENCE-MAP] Fatal Internal Error: {str(e)}")
            self.logger.warning("üßØ [EVIDENCE-MAP] Skipped saving this round, assessment continues")

    def merge_evidence_mappings(self, results_list: List[Any]) -> Dict[str, Any]:
        """
        [FIXED v2026.1.25] - ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Nested Level Key
        """
        merged_mapping = {}
        
        for item in results_list:
            if not item: continue
            
            # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏à‡∏≤‡∏Å tuple (res, worker_mem)
            res_data = item[0] if isinstance(item, tuple) else item
            worker_ev_map = item[1] if isinstance(item, tuple) and len(item) > 1 else {}

            # ‡∏Å‡∏£‡∏ì‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏≤‡∏à‡∏≤‡∏Å _run_single_assessment ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á (‡∏ú‡πà‡∏≤‡∏ô Parallel)
            if 'evidence_sources' in res_data:
                sub_id = res_data.get('sub_id')
                level = res_data.get('level')
                level_key = f"{sub_id}_L{level}"
                
                if level_key not in merged_mapping:
                    merged_mapping[level_key] = {"status": "pending", "evidences": []}
                
                # ‡∏ô‡∏≥‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà list
                merged_mapping[level_key]["evidences"].extend(res_data['evidence_sources'])
            
            # ‡∏£‡∏ß‡∏°‡∏à‡∏≤‡∏Å temp_map (worker_mem) ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
            if isinstance(worker_ev_map, dict):
                for l_key, ev_list in worker_ev_map.items():
                    if l_key not in merged_mapping:
                        merged_mapping[l_key] = {"status": "pending", "evidences": []}
                    
                    # ‡∏ñ‡πâ‡∏≤ l_key ‡πÄ‡∏õ‡πá‡∏ô "1.1" ‡πÄ‡∏â‡∏¢‡πÜ ‡πÉ‡∏´‡πâ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô "1.1_L?" (‡∏ñ‡πâ‡∏≤‡∏ó‡∏≥‡πÑ‡∏î‡πâ) ‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏õ
                    # ‡πÅ‡∏ï‡πà‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö‡πÉ‡∏´‡∏°‡πà Worker ‡∏Ñ‡∏ß‡∏£‡∏™‡πà‡∏á 1.1_L1 ‡∏°‡∏≤‡πÄ‡∏•‡∏¢
                    target_list = merged_mapping[l_key]["evidences"]
                    target_list.extend(ev_list if isinstance(ev_list, list) else [])

        # Deduplicate ‡πÅ‡∏ï‡πà‡∏•‡∏∞ Level Key ‡∏ó‡∏¥‡πâ‡∏á‡∏ó‡πâ‡∏≤‡∏¢
        for k in merged_mapping:
            merged_mapping[k]["evidences"] = self._deduplicate_list(merged_mapping[k]["evidences"])

        return merged_mapping
    
    def _deduplicate_list(self, items: List[Dict]) -> List[Dict]:
        """
        [ULTIMATE REVISED v2026.1.25 - SMART DEDUPE]
        - ‚öñÔ∏è ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ä‡∏¥‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
        - üß¨ ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö ID ‡∏ó‡∏∏‡∏Å‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏à‡∏≤‡∏Å VectorStore
        """
        if not items: return []
        unique_map = {}
        for item in items:
            if not isinstance(item, dict): continue
            raw_id = str(item.get('doc_id') or item.get('stable_doc_uuid') or item.get('chunk_uuid') or "unknown")
            doc_id = raw_id.replace("-", "").lower().strip()
            page = str(item.get('page') or item.get('page_label', '0')).strip()
            uid = f"{doc_id}_pg{page}"
            
            score = float(item.get('relevance_score') or item.get('rerank_score') or 0.0)
            if uid not in unique_map or score > float(unique_map[uid].get('relevance_score') or 0.0):
                unique_map[uid] = item
        
        res = list(unique_map.values())
        res.sort(key=lambda x: float(x.get('relevance_score') or x.get('rerank_score') or 0.0), reverse=True)
        return res
    
    def _merge_worker_results(self, sub_result: Dict[str, Any], temp_map: Dict[str, Any]):
        """
        [ULTIMATE REVISED MERGE v2026.01.31-final-stable]
        - üõ°Ô∏è Metadata Sync: ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏° Key ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏ö Ingestion ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î (source_filename, filename, source, metadata.source)
        - üß¨ Traceability Guard: ‡πÉ‡∏ä‡πâ UID (doc_id + page) ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏ô‡∏±‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ã‡πâ‡∏≥
        - ‚öñÔ∏è Maturity Lock: ‡∏´‡∏¢‡∏∏‡∏î‡∏ô‡∏±‡∏ö‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡∏ñ‡πâ‡∏≤ level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏´‡∏£‡∏∑‡∏≠ capped
        - ‚öñÔ∏è Weighted Score: highest_level * weight / 5 (‡∏ï‡∏≤‡∏° SE-AM logic)
        - Human-in-the-loop Guard: ‡πÑ‡∏°‡πà‡∏ó‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• source_type = "human_map"
        - Logging ‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠ debug metadata sync ‡πÅ‡∏•‡∏∞ maturity lock
        """
        if not sub_result:
            return None

        sub_id = str(sub_result.get("sub_id", "Unknown"))
        incoming_levels = sub_result.get("level_details", {}) or {}
        
        # --------------------------------------------------
        # 1. Evidence Merge & Audit Trail Sync
        # --------------------------------------------------
        if temp_map and isinstance(temp_map, dict):
            if not hasattr(self, "evidence_map"):
                self.evidence_map = {}

            for level_key, ev_list in temp_map.items():
                if not isinstance(ev_list, list) or not ev_list:
                    continue
                if "_L" not in level_key:
                    continue

                lv_num = level_key.split("_L")[-1]
                lv_data = incoming_levels.get(lv_num, {})
                
                node = self.evidence_map.setdefault(level_key, {
                    "status": "pending",
                    "evidences": [],
                    "pdca": None,
                    "confidence": None,
                    "snippet": "",
                    "file": "Unknown File",
                    "page": "N/A"
                })
                
                existing_evs = node["evidences"]
                # ‡πÉ‡∏ä‡πâ UID ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô duplicate (doc_id + page)
                seen_uids = {f"{e.get('doc_id')}_{e.get('page')}": i for i, e in enumerate(existing_evs)}

                for ev in ev_list:
                    doc_id = ev.get("doc_id") or ev.get("stable_doc_uuid") or "unknown_id"
                    page = str(ev.get("page_label") or ev.get("page") or "0")
                    uid = f"{doc_id}_{page}"

                    # Guard: ‡πÑ‡∏°‡πà‡∏ó‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå‡πÅ‡∏Å‡πâ (human_map)
                    if uid in seen_uids:
                        idx = seen_uids[uid]
                        if existing_evs[idx].get("source_type") == "human_map":
                            self.logger.debug(f"[MERGE-GUARD] Skip overwrite for human_map UID: {uid}")
                            continue
                        existing_evs[idx].update(ev)
                    else:
                        existing_evs.append(ev)
                        seen_uids[uid] = len(existing_evs) - 1

                # Sync Metadata ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö UI/Export (‡πÄ‡∏•‡∏∑‡∏≠‡∏Å top evidence)
                if existing_evs:
                    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (rerank_score ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î)
                    existing_evs.sort(key=lambda x: float(x.get("rerank_score") or x.get("relevance_score") or 0.0), reverse=True)
                    top_ev = existing_evs[0]
                    
                    # ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å key ‡∏ó‡∏µ‡πà ingestion ‡∏™‡∏£‡πâ‡∏≤‡∏á (priority order)
                    file_keys = ["source_filename", "filename", "source", "metadata.source"]
                    res_file = next((top_ev.get(k) for k in file_keys if top_ev.get(k)), "Unknown File")
                    
                    raw_text = top_ev.get("text") or top_ev.get("content") or ""
                    clean_snippet = raw_text[:350].replace("\n", " ").strip() + "..." if raw_text else ""
                    
                    node.update({
                        "pdca": top_ev.get("pdca_tag") or lv_data.get("pdca_breakdown", {}).get("top_phase") or "P",
                        "confidence": round(float(top_ev.get("rerank_score") or top_ev.get("relevance_score") or 0.0), 2),
                        "snippet": clean_snippet,
                        "file": res_file,
                        "page": str(top_ev.get("page_label") or top_ev.get("page") or "N/A"),
                        "status": "completed"
                    })

                    self.logger.debug(f"[MERGE-SYNC] {sub_id}_L{lv_num} | Top File: {res_file} | Confidence: {node['confidence']}")

        # --------------------------------------------------
        # 2. Results Integration (The Bridge)
        # --------------------------------------------------
        if not hasattr(self, "final_subcriteria_results"):
            self.final_subcriteria_results = []

        target = next((r for r in self.final_subcriteria_results if str(r.get("sub_id")) == sub_id), None)

        if not target:
            target = {
                "sub_id": sub_id,
                "sub_criteria_name": sub_result.get("sub_criteria_name", f"Criteria {sub_id}"),
                "weight": float(sub_result.get("weight", 3.0)),
                "level_details": {},
                "highest_full_level": 0,
                "weighted_score": 0.0,
                "pdca_overall": {"P": 0.0, "D": 0.0, "C": 0.0, "A": 0.0},
                "sub_roadmap": sub_result.get("sub_roadmap") or {},
                "strategic_focus": sub_result.get("strategic_focus") or ""
            }
            self.final_subcriteria_results.append(target)

        if isinstance(incoming_levels, dict):
            for lv, lv_data in incoming_levels.items():
                target["level_details"][str(lv)] = lv_data

        # --------------------------------------------------
        # 3. Maturity Step-Ladder & Score Calculation
        # --------------------------------------------------
        highest = 0
        pdca_sum = {"P": 0.0, "D": 0.0, "C": 0.0, "A": 0.0}
        passed_count = 0

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏£‡∏∞‡∏î‡∏±‡∏ö 1-5 (‡∏´‡∏¢‡∏∏‡∏î‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡∏ñ‡πâ‡∏≤‡πÄ‡∏à‡∏≠‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏´‡∏£‡∏∑‡∏≠ capped)
        for l in range(1, 6):
            data = target["level_details"].get(str(l))
            if not data or not data.get("is_passed") or data.get("is_maturity_capped"):
                self.logger.debug(f"[MATURITY-STOP] {sub_id} ‡∏´‡∏¢‡∏∏‡∏î‡∏ó‡∏µ‡πà L{l} (‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏´‡∏£‡∏∑‡∏≠ capped)")
                break
            
            highest = l
            bd = data.get("pdca_breakdown", {}) or {}
            for k in pdca_sum:
                pdca_sum[k] += float(bd.get(k, 0.0))
            passed_count += 1

        target["highest_full_level"] = highest
        
        # Weighted Score ‡∏ï‡∏≤‡∏° SE-AM (‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á * ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å / 5)
        target["weighted_score"] = round(highest * (float(target["weight"]) / 5), 2)
        
        if passed_count > 0:
            target["pdca_overall"] = {k: round(v / passed_count, 2) for k, v in pdca_sum.items()}

        self.logger.info(f"[MERGE-FINAL] {sub_id} | Highest: L{highest} | Weighted Score: {target['weighted_score']}")

        return target
    
    def run_assessment(
        self,
        target_sub_id: str = "all",
        export: bool = False,
        vectorstore_manager: Optional['VectorStoreManager'] = None,
        sequential: bool = False,
        document_map: Optional[Dict[str, str]] = None,
        record_id: str = None,
    ) -> Dict[str, Any]:
        """
        [FINAL SHIELDED BUILD v2026.01.28 - PDCA SYNC ENABLED]
        - üõ°Ô∏è Shield Pattern: Score ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Å‡πà‡∏≠‡∏ô IO
        - üîÑ Evidence Sync: ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Worker ‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà Audit Trail ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
        - ‚úÖ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Ñ‡πà‡∏≤ null ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå JSON
        """

        start_ts = time.time()
        self.is_sequential = sequential
        self.current_record_id = record_id or self.record_id

        # -------------------------------
        # 0. Init State (CRITICAL)
        # -------------------------------
        self.final_subcriteria_results = []
        self.sub_roadmap_data = None
        self.enabler_roadmap_data = None

        if document_map:
            self.document_map.update(document_map)

        if not getattr(self, "evidence_map", None):
            self.evidence_map = {}

        # -------------------------------
        # 1. Load Rubric
        # -------------------------------
        self.flattened_rubric = self._flatten_rubric_to_statements()
        grouped_sub_criteria = self._group_statements_by_sub_criteria(self.flattened_rubric)

        is_all = str(target_sub_id).lower() == "all"
        sub_criteria_list = (
            list(grouped_sub_criteria.values())
            if is_all
            else [grouped_sub_criteria.get(target_sub_id)]
        )

        if not all(sub_criteria_list):
            return self._create_failed_result(
                self.current_record_id,
                f"Criteria '{target_sub_id}' not found",
                start_ts
            )

        total_subs = len(sub_criteria_list)
        results_list = []

        # -------------------------------
        # 2. Core Assessment
        # -------------------------------
        if is_all and not sequential:
            # MODE A: PARALLEL
            max_workers = int(os.environ.get("MAX_PARALLEL_WORKERS", 4))
            worker_args = [
                self._prepare_worker_tuple(sub, self.document_map)
                for sub in sub_criteria_list
            ]

            ctx = multiprocessing.get_context("spawn")
            with ctx.Pool(processes=max_workers) as pool:
                for idx, (res, worker_mem) in enumerate(
                    pool.imap_unordered(_static_worker_process, worker_args)
                ):
                    results_list.append((res, worker_mem))
                    self._merge_worker_results(res, worker_mem)

                    self.db_update_task_status(
                        progress=15 + int(((idx + 1) / total_subs) * 65),
                        message=f"üß† ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô {res.get('sub_id', '?')} ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à"
                    )
        else:
            # MODE B: SEQUENTIAL
            if not vectorstore_manager:
                self._initialize_vsm_if_none()
            vsm = vectorstore_manager or self.vectorstore_manager

            for idx, sub_criteria in enumerate(sub_criteria_list):
                sub_id = str(sub_criteria.get("sub_id", "Unknown"))
                res, worker_mem = self._run_sub_criteria_assessment_worker(
                    sub_criteria, vsm, []
                )

                results_list.append((res, worker_mem))
                self._merge_worker_results(res, worker_mem)

                self.db_update_task_status(
                    progress=15 + int(((idx + 1) / total_subs) * 65),
                    message=f"üß† ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô {sub_id} ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (Sequential)"
                )

        # -------------------------------
        # 3. Evidence Guard (REVISED & PATCHED)
        # -------------------------------
        self.db_update_task_status(progress=85, message="üß© ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏à‡∏±‡∏î‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå PDCA")

        full_raw_mapping = self.merge_evidence_mappings(results_list)
        self._update_internal_evidence_map(full_raw_mapping)

        total_evidence_found = 0
        for key in list(self.evidence_map.keys()):
            bucket = self.evidence_map[key]
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á bucket ‡πÉ‡∏´‡πâ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
            if not isinstance(bucket, dict):
                bucket = {"status": "ai_generated", "evidences": bucket}
                self.evidence_map[key] = bucket

            # üéØ [FIX START]: ‡πÄ‡∏ï‡∏¥‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏Ç‡∏≤‡∏î‡∏´‡∏≤‡∏¢ (PDCA, Confidence, Snippet) ‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
            ev_list = self._deduplicate_list(bucket.get("evidences", []))
            if ev_list:
                # 1. ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÄ‡∏≠‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà AI ‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÅ‡∏ó‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö Level
                ev_list.sort(key=lambda x: float(x.get("rerank_score") or x.get("relevance_score") or 0.0), reverse=True)
                top_ev = ev_list[0]
                
                # 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á Snippet ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡πâ‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏™‡∏î‡∏á‡∏ö‡∏ô Dashboard
                raw_content = top_ev.get("text") or top_ev.get("content") or top_ev.get("page_content") or ""
                clean_snippet = raw_content[:350].replace("\n", " ").strip() + "..." if raw_content else ""
                
                # 3. Sync ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Top Evidence ‡∏Ç‡∏∂‡πâ‡∏ô‡∏™‡∏π‡πà‡∏£‡∏∞‡∏î‡∏±‡∏ö Node (Audit Trail)
                bucket.update({
                    "pdca": top_ev.get("pdca_tag") or "P",
                    "confidence": round(float(top_ev.get("rerank_score") or top_ev.get("relevance_score") or 0.0), 2),
                    "snippet": clean_snippet,
                    "file": top_ev.get("filename") or top_ev.get("source_filename") or top_ev.get("source"),
                    "page": str(top_ev.get("page") or top_ev.get("page_label") or "N/A")
                })
                
                bucket["evidences"] = ev_list
                total_evidence_found += len(ev_list)
                bucket["status"] = "completed"
            # üéØ [FIX END]

        # -------------------------------
        # 4. üõ°Ô∏è SCORE FREEZE (CRITICAL)
        # -------------------------------
        overall_stats = self._calculate_overall_stats(target_sub_id) or {}
        overall_stats["evidence_used_count"] = total_evidence_found

        # -------------------------------
        # 5. IO RISK ZONE (SAFE NOW)
        # -------------------------------
        try:
            self._save_evidence_map(self.evidence_map)
        except Exception as e:
            self.logger.warning(f"üßØ [EVIDENCE-MAP] Save failed: {e}")

        # -------------------------------
        # 6. Enabler Roadmap
        # -------------------------------
        if self.final_subcriteria_results:
            self.enabler_roadmap_data = self.synthesize_enabler_roadmap(
                sub_criteria_results=self.final_subcriteria_results,
                enabler_name=self.enabler,
                llm_executor=self.llm
            )

        # -------------------------------
        # 7. Final Response
        # -------------------------------
        final_response = {
            "record_id": self.current_record_id,
            "status": "COMPLETED",
            "enabler": self.enabler,
            "summary": overall_stats,
            "sub_criteria_results": self.final_subcriteria_results,
            "evidence_audit_trail": self.evidence_map,
            "enabler_roadmap": self.enabler_roadmap_data,
            "run_time_seconds": round(time.time() - start_ts, 2)
        }

        if export:
            self._export_results(self.final_subcriteria_results, target_sub_id, self.current_record_id)

        self.db_update_task_status(progress=100, message="‚úÖ ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå", status="COMPLETED")
        return final_response
    

    # ------------------------------------------------------------------
    # üèõÔ∏è [ULTIMATE REVISED] generate_sub_roadmap - USING UB_ROADMAP_PROMPT
    # ------------------------------------------------------------------
    def generate_sub_roadmap(
        self,
        sub_id: str,
        sub_criteria_name: str,
        enabler: str,
        aggregated_insights: List[Dict[str, Any]],
        strategic_focus: str = ""
    ) -> Dict[str, Any]:
        """
        - ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏°‡∏≤‡πÉ‡∏ä‡πâ UB_ROADMAP_PROMPT ‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏° System Rules ‡πÅ‡∏•‡∏∞ Template ‡πÑ‡∏ß‡πâ‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô
        - ‡πÉ‡∏ä‡πâ .format() ‡∏´‡∏£‡∏∑‡∏≠ .invoke() ‡∏ï‡∏≤‡∏°‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô PromptTemplate
        """
        if not aggregated_insights:
            return self._get_emergency_fallback_plan(sub_id, sub_criteria_name, "No insights provided")

        self.logger.info(f"üöÄ [ROADMAP] Generating Professional Coach Plan via UB_PROMPT for {sub_id}")

        # --- [STEP 1: PREPARE DATA - ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°‡πÅ‡∏ï‡πà‡πÄ‡∏ô‡πâ‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î] ---
        best_practice_assets = []
        top_asset_name = None
        highest_continuous = 0
        has_gap = False
        noise_filenames = ["Unknown File", "‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á", "N/A", "Reference Document", "SCORE:", ""]
        evidence_map = getattr(self, "evidence_map", {})

        for item in aggregated_insights:
            lv = int(item.get("level", 0))
            passed = (item.get("status") == "PASSED")
            ev_key = f"{sub_id}_L{lv}"
            ev_node = evidence_map.get(ev_key, {})
            
            raw_filename = ev_node.get("file", "Unknown File")
            clean_filename = raw_filename if raw_filename not in noise_filenames else None
            
            if passed:
                if not has_gap: highest_continuous = lv
                if not top_asset_name and clean_filename: top_asset_name = clean_filename
                best_practice_assets.append(f"- Level {lv}: {clean_filename if clean_filename else '‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡πÄ‡∏î‡∏¥‡∏°'}")
            else:
                has_gap = True

        # --- [STEP 2: PREPARE ENRICHED CONTEXT] ---
        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£ aggregated_insights ‡πÉ‡∏ô Prompt
        enriched_context = (
            f"üíé EXISTING STRATEGIC ASSETS:\n" +
            ("\n".join(best_practice_assets) if best_practice_assets else "- ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏´‡∏•‡∏±‡∏Å") +
            "\n\nüö® GAP ANALYSIS:\n" +
            "\n".join([f"- L{i.get('level')}: {i.get('insight_summary')}" for i in aggregated_insights])
        )

        if not strategic_focus:
            strategic_focus = f"‡∏¢‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏à‡∏≤‡∏Å L{highest_continuous} ‡∏°‡∏∏‡πà‡∏á‡∏™‡∏π‡πà‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô Excellence"

        # --- [STEP 3: EXECUTE UB_ROADMAP_PROMPT] ---
        try:
            # ‡πÉ‡∏ä‡πâ UB_ROADMAP_PROMPT ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Prompt String
            # ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ LangChain ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ .format() ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢
            final_prompt_string = SUB_ROADMAP_PROMPT.format(
                sub_id=sub_id,
                sub_criteria_name=sub_criteria_name,
                enabler=enabler,
                aggregated_insights=enriched_context,
                strategic_focus=strategic_focus
            )

            # ‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤ LLM (‡∏™‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô User Prompt ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ System Rules ‡∏ñ‡∏π‡∏Å‡∏£‡∏ß‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô String ‡πÅ‡∏•‡πâ‡∏ß)
            # ‡∏´‡∏£‡∏∑‡∏≠‡∏ñ‡πâ‡∏≤ _fetch_llm_response ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÅ‡∏¢‡∏Å System ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏±‡∏î‡πÅ‡∏ö‡πà‡∏á‡πÑ‡∏î‡πâ
            raw = _fetch_llm_response(
                system_prompt="‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡πÄ‡∏ä‡∏¥‡∏á‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏î‡πâ‡∏≤‡∏ô SE-AM", 
                user_prompt=final_prompt_string, 
                llm_executor=self.llm
            )

            data = _robust_extract_json(raw) or {}
            raw_phases = data.get("phases") or []

            # --- [STEP 4: NORMALIZE & INJECT EVIDENCE] ---
            final_phases = []
            for i, p in enumerate(raw_phases, 1):
                actions = p.get("key_actions") or []
                
                # Injection Logic: ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô Action ‡πÉ‡∏´‡πâ‡πÅ‡∏ó‡∏£‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏£‡∏¥‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ
                if top_asset_name and not any(top_asset_name in str(a) for a in actions):
                    actions.insert(0, {
                        "action": f"‡∏ï‡πà‡∏≠‡∏¢‡∏≠‡∏î‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏à‡∏≤‡∏Å {top_asset_name} ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥ Standardization",
                        "priority": "High"
                    })

                final_phases.append({
                    "phase": p.get("phase", f"Phase {i}"),
                    "target_levels": p.get("target_levels") or [min(highest_continuous + i, 5)],
                    "main_objective": p.get("main_objective") or "‡∏¢‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏ö‡∏á‡∏≤‡∏ô",
                    "key_actions": actions,
                    "expected_outcome": p.get("expected_outcome") or "‡πÄ‡∏û‡∏¥‡πà‡∏° Traceability Score > 85%"
                })

            # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö L5 Sustainability ‡πÄ‡∏™‡∏°‡∏≠
            if highest_continuous == 5 and len(final_phases) < 2:
                final_phases.append({
                    "phase": "Phase 2: Sustainability & Learning Culture",
                    "main_objective": "‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô Excellence ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏∞‡∏ö‡∏ö Knowledge Governance",
                    "key_actions": [
                        {"action": f"‡∏ñ‡∏≠‡∏î‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏à‡∏≤‡∏Å {top_asset_name} ‡πÄ‡∏õ‡πá‡∏ô Best Practice ‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£", "priority": "High"}
                    ],
                    "expected_outcome": "Traceability 100%"
                })

            return {
                "scope": "SUB_CRITERIA",
                "sub_id": sub_id,
                "highest_maturity_level": highest_continuous,
                "overall_strategy": data.get("overall_strategy", strategic_focus),
                "phases": final_phases,
                "is_gap_detected": has_gap,
                "status": "SUCCESS"
            }

        except Exception as e:
            self.logger.error(f"üõë UB_PROMPT Error: {str(e)}")
            return self._get_emergency_fallback_plan(sub_id, sub_criteria_name, str(e))
        
    def synthesize_enabler_roadmap(
        self,
        sub_criteria_results: List[Dict[str, Any]],
        enabler_name: str,
        llm_executor: Any
    ) -> Dict[str, Any]:
        """
        [TIER-3 STRATEGIC ORCHESTRATOR - FULL REVISE v2026.01.28]
        - üß© Macro-Synthesis: ‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏ú‡∏ô‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å Sub-id
        - üõ°Ô∏è KeyError Shield: ‡πÉ‡∏ä‡πâ STRATEGIC_OVERALL_PROMPT ‡∏ó‡∏µ‡πà‡πÅ‡∏¢‡∏Å‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
        - üöÄ Performance: ‡∏õ‡∏£‡∏±‡∏ö‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏Ç‡∏≠‡∏á Context ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ L40S ‡∏ï‡∏≠‡∏ö‡∏™‡∏ô‡∏≠‡∏á‡πÑ‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô
        """

        self.logger.info(f"üåê [TIER-3] Synthesizing Strategic Master Plan for {enabler_name}")

        if not sub_criteria_results:
            return {
                "status": "INCOMPLETE",
                "overall_strategy": "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏ú‡∏ô‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°",
                "phases": []
            }

        # --- [STEP 1: DETERMINING GLOBAL MATURITY & FOCUS] ---
        # ‡∏´‡∏≤‡πÄ‡∏•‡πÄ‡∏ß‡∏•‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏ó‡∏∏‡∏Å‡∏Ç‡πâ‡∏≠‡∏ú‡πà‡∏≤‡∏ô (Baseline)
        global_maturity = min(
            [int(r.get("highest_full_level", 0)) for r in sub_criteria_results if r],
            default=0
        )

        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Strategic Focus ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£
        if global_maturity < 3:
            global_focus = f"Focus: Foundational Integrity (‡∏Å‡∏≤‡∏£‡∏™‡∏ñ‡∏≤‡∏õ‡∏ô‡∏≤‡∏£‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏£‡∏∞‡∏ö‡∏ö {enabler_name} ‡πÅ‡∏•‡∏∞‡∏õ‡∏¥‡∏î‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô)"
        elif 3 <= global_maturity < 5:
            global_focus = f"Focus: Strategic Integration (‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡πÇ‡∏¢‡∏á‡∏£‡∏∞‡∏ö‡∏ö {enabler_name} ‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ö‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£)"
        else:
            global_focus = f"Focus: Excellence & Innovation (‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏ô‡πÅ‡∏ö‡∏ö‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏™‡∏≤‡∏Å‡∏•)"

        # --- [STEP 2: AGGREGATING MULTI-SUB GAPS] ---
        blocking_gaps = []
        key_strengths = []

        for res in sub_criteria_results:
            sid = res.get("sub_id", "N/A")
            sname = res.get("sub_criteria_name", "N/A")
            
            # ‡πÄ‡∏Å‡πá‡∏ö‡∏à‡∏∏‡∏î‡πÅ‡∏Ç‡πá‡∏á (‡πÄ‡∏•‡πÄ‡∏ß‡∏•‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô)
            if res.get("highest_full_level", 0) > 0:
                key_strengths.append(f"- [{sid}] ‡∏ú‡πà‡∏≤‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö L{res.get('highest_full_level')}: {sname}")

            # ‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Coaching Insight ‡∏Ç‡∏≠‡∏á‡πÄ‡∏•‡πÄ‡∏ß‡∏•‡∏ó‡∏µ‡πà‡∏ï‡∏¥‡∏î‡∏Ç‡∏±‡∏î (Next Target Level)
            next_lv = str(res.get("highest_full_level", 0) + 1)
            details = res.get("level_details", {}).get(next_lv)
            
            if details and not details.get("is_passed"):
                insight = details.get("coaching_insight", "").strip()
                if insight:
                    blocking_gaps.append(f"üî¥ [{sid} L{next_lv}]: {insight[:200]}")

        # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ LLM ‡∏™‡∏±‡∏ö‡∏™‡∏ô
        aggregated_context = (
            "üíé KEY STRENGTHS (‡∏à‡∏∏‡∏î‡πÅ‡∏Ç‡πá‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏ô‡∏ó‡∏∏‡∏ô):\n" +
            ("\n".join(key_strengths[:5]) if key_strengths else "- ‡∏≠‡∏¢‡∏π‡πà‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô") +
            "\n\nüö® CRITICAL BLOCKING GAPS (‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏£‡∏ß‡∏°‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏£‡πà‡∏á‡∏õ‡∏¥‡∏î):\n" +
            ("\n".join(blocking_gaps[:10]) if blocking_gaps else "- ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ß‡∏¥‡∏Å‡∏§‡∏ï")
        )

        # --- [STEP 3: PROMPT ORCHESTRATION (THE FIX)] ---
        # üõ°Ô∏è ‡πÉ‡∏ä‡πâ Prompt ‡∏ï‡∏±‡∏ß‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ö 3 ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£ (‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö seam_prompts.py)
        final_prompt = STRATEGIC_OVERALL_PROMPT.format(
            enabler_name=enabler_name,
            aggregated_context=aggregated_context,
            strategic_focus=global_focus
        )

        try:
            # ‡πÉ‡∏ä‡πâ helper _fetch_llm_response ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏£‡∏µ‡∏¢‡∏Å llm_executor ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
            raw = _fetch_llm_response(
                system_prompt=SYSTEM_OVERALL_STRATEGIC_PROMPT,
                user_prompt=final_prompt,
                llm_executor=llm_executor
            )

            data = _robust_extract_json(raw) or {}
            raw_phases = data.get("phases") or data.get("roadmap") or []

            # --- [STEP 4: NORMALIZE PHASES] ---
            final_phases = []
            for i, p in enumerate(raw_phases, 1):
                if isinstance(p, dict):
                    final_phases.append({
                        "phase": p.get("phase") or f"Phase {i}: ‡∏Å‡∏≤‡∏£‡∏¢‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°",
                        "target_levels": p.get("target_levels") or f"L{global_maturity + 1}",
                        "main_objective": p.get("main_objective") or p.get("target_objectives") or "‡∏õ‡∏¥‡∏î‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏ä‡∏¥‡∏á‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå",
                        "key_actions": p.get("key_actions") or p.get("strategic_actions") or [],
                        "expected_outcome": p.get("expected_outcome") or p.get("key_performance_indicator") or "‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô"
                    })

            if not final_phases:
                final_phases = self._get_emergency_fallback_plan("OVERALL", enabler_name).get("phases", [])

            return {
                "status": "SUCCESS",
                "overall_strategy": data.get("overall_strategy") or global_focus,
                "phases": final_phases,
                "metadata": {
                    "global_maturity_baseline": global_maturity,
                    "applied_strategic_focus": global_focus,
                    "total_sub_evaluated": len(sub_criteria_results),
                    "generated_at": datetime.now().isoformat()
                }
            }

        except Exception as e:
            self.logger.error(f"üõë [TIER-3-CRITICAL] Global Roadmap Error: {e}", exc_info=True)
            return {
                "status": "ERROR",
                "overall_strategy": "‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏ú‡∏ô‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå",
                "reason": str(e),
                "phases": []
            }

    def create_atomic_action_plan(
        self, 
        insight: str, 
        level: int, 
        level_criteria: str = "", 
        **kwargs
    ) -> List[Dict[str, Any]]:
        """
        [ULTIMATE REVISED v2026.01.25 - FINAL STABLE]
        - FIXED: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô system_msg ‡πÄ‡∏õ‡πá‡∏ô system_prompt ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö llm_data_utils.py
        - Clean Code: ‡πÉ‡∏ä‡πâ‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏à‡∏≤‡∏Å Header (re, SEAM_ENABLER_FULL_NAME_TH, ‡∏Ø‡∏•‡∏Ø)
        - Anti-IT Ghost: ‡∏Å‡∏£‡∏≠‡∏á‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏î‡πâ‡∏≤‡∏ô IT ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö L1-L3 ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£
        """
        try:
            # 1. Validation & Data Sanitization
            clean_insight = str(insight or "").strip()
            if not clean_insight or clean_insight.lower() in ["-", "n/a", "none", "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", ""]:
                return []

            # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏Å‡∏ì‡∏ë‡πå (Criteria)
            actual_criteria = level_criteria or kwargs.get('level_statement') or "‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô SE-AM"
            
            # 2. ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô f-string formatting error (Double Braces Escape)
            safe_insight = clean_insight.replace('"', "'").replace('{', '{{').replace('}', '}}')
            safe_criteria = str(actual_criteria).replace('"', "'").replace('{', '{{').replace('}', '}}')

            # 3. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£ Enabler (Mapping Name & Code)
            enabler_code = str(getattr(self, 'enabler', 'UNKNOWN')).upper()
            enabler_name_th = SEAM_ENABLER_FULL_NAME_TH.get(enabler_code, f"‡∏î‡πâ‡∏≤‡∏ô {enabler_code}")

            # 4. Packaging ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Prompt
            prompt_payload = {
                "coaching_insight": safe_insight,
                "level": level,
                "enabler": enabler_code,
                "enabler_name_th": enabler_name_th,
                "level_criteria": safe_criteria,
                "focus_points": kwargs.get('focus_points', '‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô') # üëà ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ
            }

            try:
                # ‡πÉ‡∏ä‡πâ Template ‡∏ó‡∏µ‡πà Import ‡∏°‡∏≤‡∏à‡∏≤‡∏Å Header
                human_prompt = ATOMIC_ACTION_PROMPT.format(**prompt_payload)
                system_prompt_content = SYSTEM_ATOMIC_ACTION_PROMPT.format(
                    enabler_name_th=enabler_name_th, 
                    enabler=enabler_code
                )
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è [FORMAT-ERROR] {e} -> Use raw backup format")
                system_prompt_content = f"Expert Action Plan Generator for {enabler_name_th}"
                human_prompt = f"Insight: {safe_insight}\nLevel: {level}"

            # 5. LLM Execution (FIXED Parameter Name: system_prompt)
            # 
            raw_response = _fetch_llm_response(
                system_prompt=system_prompt_content, # ‚úÖ ‡πÅ‡∏Å‡πâ‡∏à‡∏≤‡∏Å system_msg ‡πÄ‡∏õ‡πá‡∏ô system_prompt ‡πÅ‡∏•‡πâ‡∏ß
                user_prompt=human_prompt,
                llm_executor=self.llm
            )

            # 6. Robust Extraction (JSON -> Regex Fallback)
            actions = []
            try:
                # ‡πÉ‡∏ä‡πâ helper _robust_extract_json ‡∏à‡∏≤‡∏Å Header
                parsed = _robust_extract_json(raw_response)
                if isinstance(parsed, list):
                    actions = parsed
                elif isinstance(parsed, dict):
                    actions = parsed.get("actions", [parsed])
            except:
                pass

            # Regex Scavenger (‡∏ñ‡πâ‡∏≤ JSON ‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤) - ‡πÉ‡∏ä‡πâ re ‡∏à‡∏≤‡∏Å Header
            if not actions:
                matches = re.findall(r'["\']action["\']\s*:\s*["\']([^"\']+)["\']', raw_response)
                for m in matches:
                    actions.append({"action": m, "target_evidence": "‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô"})

            # 7. Post-Processing & Anti-IT Ghost (L1-L3 Safety)
            final_actions = []
            it_ghost_terms = r"(‡∏£‡∏∞‡∏ö‡∏ö‡∏™‡∏≤‡∏£‡∏™‡∏ô‡πÄ‡∏ó‡∏®|KMS|Software|Automation|‡πÅ‡∏≠‡∏õ‡∏û‡∏•‡∏¥‡πÄ‡∏Ñ‡∏ä‡∏±‡∏ô|IT System|‡πÅ‡∏û‡∏•‡∏ï‡∏ü‡∏≠‡∏£‡πå‡∏°‡∏î‡∏¥‡∏à‡∏¥‡∏ó‡∏±‡∏•|‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå)"
            
            for item in actions:
                if not isinstance(item, dict): continue
                
                act_text = (item.get("action") or "").strip()
                if len(act_text) < 5: continue
                
                # ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ IT ‡∏≠‡∏≠‡∏Å‡∏ñ‡πâ‡∏≤‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏£‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô L1-L3 (‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏á‡∏≤‡∏ô‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£/‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°)
                if level <= 3:
                    act_text = re.sub(it_ghost_terms, "‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥/‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô/‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏°‡∏µ‡∏™‡πà‡∏ß‡∏ô‡∏£‡πà‡∏ß‡∏°", act_text, flags=re.IGNORECASE)
                
                final_actions.append({
                    "action": act_text,
                    "target_evidence": item.get("target_evidence", "‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö/‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°"),
                    "level": level
                })

            # 8. Emergency Fallback
            if not final_actions:
                # ‡∏î‡∏∂‡∏á‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏Ç‡∏≠‡∏á insight ‡∏°‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á action ‡πÅ‡∏ö‡∏ö‡∏â‡∏∏‡∏Å‡πÄ‡∏â‡∏¥‡∏ô
                short_insight = clean_insight[:50] + "..."
                final_actions = [{
                    "action": f"‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏¥‡∏î‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ï‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞: {short_insight}",
                    "target_evidence": "‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£/‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á",
                    "level": level
                }]

            self.logger.info(f"‚úÖ [ATOMIC-PLAN] {enabler_code} L{level} Success (Output: {len(final_actions[:2])})")
            return final_actions[:2]

        except Exception as e:
            self.logger.error(f"üõë [ATOMIC-PLAN-CRITICAL] {str(e)}", exc_info=True)
            return [{"action": "‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏á‡∏≤‡∏ô‡∏ï‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞‡∏Ç‡∏≠‡∏á‡πÄ‡∏Å‡∏ì‡∏ë‡πå", "target_evidence": "‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÄ‡∏ä‡∏¥‡∏á‡∏õ‡∏£‡∏∞‡∏à‡∏±‡∏Å‡∏©‡πå", "level": level}]
        
    
    def _get_emergency_fallback_plan(self, sub_id, name, error_msg=""):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡∏™‡∏≥‡∏£‡∏≠‡∏á‡∏Å‡∏£‡∏ì‡∏µ LLM ‡∏û‡∏±‡∏á ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡∏´‡∏¢‡∏∏‡∏î‡∏ó‡∏≥‡∏á‡∏≤‡∏ô"""
        return {
            "overall_strategy": "‡πÅ‡∏ú‡∏ô‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô (Fallback Mode)",
            "phases": [
                {
                    "phase": "Quick Win",
                    "goal": f"‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ç‡πâ‡∏≠‡∏ö‡∏Å‡∏û‡∏£‡πà‡∏≠‡∏á‡πÉ‡∏ô {name}",
                    "actions": [{"action": "‡∏™‡∏≠‡∏ö‡∏ó‡∏≤‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Gap ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô", "priority": "High"}]
                }
            ],
            "status": "FALLBACK",
            "error_context": error_msg[:100]
        }
    

    def _apply_evidence_cap(self, evidence_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        [REFINED] ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡πá‡∏Ñ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô Sort
        """
        if not evidence_list:
            return []

        # ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô dict ‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
        valid_evidences = [ev for ev in evidence_list if isinstance(ev, dict) and (ev.get('text') or ev.get('content'))]
        
        # 1. Deduplicate ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Hash ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ (‡∏ñ‡πâ‡∏≤‡∏ó‡∏≥‡πÑ‡∏î‡πâ) ‡∏´‡∏£‡∏∑‡∏≠ Source+Page
        unique_evidences = self._deduplicate_list(valid_evidences)

        # 2. Strategy-based Selection
        if EVIDENCE_SELECTION_STRATEGY == "score":
            # ‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Å‡∏±‡∏ö rerank_score ‡∏Å‡πà‡∏≠‡∏ô ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ relevance_score
            sorted_list = sorted(
                unique_evidences, 
                key=lambda x: x.get('rerank_score') or x.get('relevance_score') or 0, 
                reverse=True
            )
        else:
            sorted_list = unique_evidences

        # 3. Apply Cap
        return sorted_list[:EVIDENCE_CUMULATIVE_CAP]
    
    def _run_sub_criteria_assessment_worker(
        self,
        sub_criteria: Dict[str, Any],
        vectorstore_manager: Optional[Any] = None,
        initial_baseline: Optional[List[Dict[str, Any]]] = None,
    ) -> Tuple[Dict[str, Any], Dict[str, List[Dict[str, Any]]]]:
        """
        [ULTIMATE REVISED v2026.02.01 - FULL UI SYNC & PDCA STATUS]
        - ‡πÄ‡∏û‡∏¥‡πà‡∏° rubric_statement ‡πÄ‡∏Ç‡πâ‡∏≤ level_details ‡πÄ‡∏û‡∏∑‡πà‡∏≠ tooltip ‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏à‡∏£‡∏¥‡∏á
        - ‡πÄ‡∏û‡∏¥‡πà‡∏° pdca_status (actual/missing phases, coverage %, is_full_coverage, status_label)
        - Logic PDCA ‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Ç‡∏∂‡πâ‡∏ô (‡πÉ‡∏ä‡πâ set + upper case + ‡∏Å‡∏£‡∏≠‡∏á tag ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á)
        - Logging ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠ debug statement + pdca_status
        - ‡∏£‡∏±‡∏Å‡∏©‡∏≤ Maturity Ladder Lock + Gap Traceability ‡πÄ‡∏î‡∏¥‡∏°
        """
        sub_id = str(sub_criteria.get("sub_id", "Unknown"))
        sub_name = sub_criteria.get("sub_criteria_name", "No Name")
        sub_weight = float(sub_criteria.get("weight", 0.0))
        target_limit = getattr(self.config, "target_level", 5)

        vsm = vectorstore_manager or self.vectorstore_manager
        level_details: Dict[str, Any] = {}
        roadmap_input_bundle = []

        highest_continuous_level = 0
        is_still_continuous = True  # ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏Ñ‡∏∏‡∏°‡∏ö‡∏±‡∏ô‡πÑ‡∏î Maturity (‡∏ñ‡πâ‡∏≤‡∏ï‡∏¥‡∏î gap ‚Üí False ‡∏ï‡∏•‡∏≠‡∏î)
        cumulative_baseline = list(initial_baseline or [])
        evidence_delta: Dict[str, List[Dict[str, Any]]] = {}

        levels = sorted(sub_criteria.get("levels", []), key=lambda x: x.get("level", 0))

        for stmt in levels:
            level = int(stmt.get("level", 0))
            if level == 0 or level > target_limit:
                continue

            level_key = f"{sub_id}_L{level}"
            required_pdca = self.get_rule_content(sub_id, level, "require_phase") or ["P"]

            # --- [STEP 1: EVIDENCE HYDRATION] ---
            saved = self.evidence_map.get(level_key, {})
            saved_evs = saved.get("evidences", []) if isinstance(saved, dict) else []
            priority_items = [e for e in saved_evs if e.get("is_selected", True)]
            current_baseline = self._deduplicate_list(cumulative_baseline + priority_items)

            # --- [STEP 2: CORE ASSESSMENT] ---
            res = self._run_single_assessment(
                sub_id=sub_id,
                level=level,
                criteria={"name": sub_name, "statement": stmt.get("statement", "")},
                keyword_guide=stmt.get("keywords", []),
                baseline_evidences=current_baseline,
                vectorstore_manager=vsm,
            )

            # --- [STEP 3: MATURITY STEP-LADDER LOGIC] ---
            llm_passed = bool(res.get("is_passed", False))
            actual_passed = llm_passed and is_still_continuous
            is_capped = llm_passed and not is_still_continuous

            if is_capped:
                self.logger.warning(f"[MATURITY-LOCK] {sub_id}_L{level} CAPPED: Previous level has unresolved GAP")

            if actual_passed:
                highest_continuous_level = level
                top_chunks = res.get("top_chunks_data", []) or []
                cumulative_baseline.extend(top_chunks)
                cumulative_baseline = self._apply_evidence_cap(cumulative_baseline)
            else:
                is_still_continuous = False

            # --- [STEP 4: SCORE & DATA ENRICHMENT] ---
            effective_score = float(res.get("score", 0.0)) if actual_passed else 0.0

            enriched_chunks = res.get("top_chunks_data", []) or []
            pdca_results = res.get("pdca_breakdown", {})

            try:
                atomic_actions = self.create_atomic_action_plan(
                    insight=res.get("coaching_insight", ""),
                    level=level,
                    level_criteria=stmt.get("statement", ""),
                    focus_points=sub_criteria.get("focus_points", "-")
                )
            except Exception as e:
                self.logger.warning(f"Atomic action failed for {sub_id}_L{level}: {e}")
                atomic_actions = []

            # --- [STEP 5: RESULT COMPILATION - REVISED FOR UI SYNC] ---
            # 1. ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏° PDCA Phases ‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏≤‡∏Å evidence (‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î)
            actual_found_phases = set(
                chunk.get("pdca_tag", "").upper()
                for chunk in enriched_chunks
                if chunk.get("pdca_tag", "").upper() in ["P", "D", "C", "A"]
            )

            # 2. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì missing + coverage + status
            missing_phases = [p for p in required_pdca if p not in actual_found_phases]
            coverage_percentage = round(
                (len(actual_found_phases) / len(required_pdca)) * 100 if required_pdca else 0, 1
            )
            is_full_coverage = len(missing_phases) == 0

            level_details[str(level)] = {
                "level": level,
                "is_passed": actual_passed,
                "is_maturity_capped": is_capped,
                "score": round(effective_score, 2),
                "reason": res.get("reason", "‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏£‡∏∞‡∏ö‡∏∏"),
                "coaching_insight": res.get("coaching_insight", ""),
                "atomic_action_plan": atomic_actions,
                "evidence_sources": enriched_chunks,
                "pdca_breakdown": pdca_results,
                "required_pdca_phases": required_pdca,
                "rubric_statement": stmt.get("statement", "").strip() or "‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡∏µ‡πâ",

                # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• PDCA Status ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö UI (PDCA Matrix + Tooltip)
                "pdca_status": {
                    "actual_phases": list(actual_found_phases),
                    "missing_phases": missing_phases,
                    "coverage_percentage": coverage_percentage,
                    "is_full_coverage": is_full_coverage,
                    "status_label": "PASS" if is_full_coverage else "GAP"
                }
            }

            # Log ‡πÄ‡∏û‡∏∑‡πà‡∏≠ debug PDCA + statement (‡∏ä‡πà‡∏ß‡∏¢‡∏´‡∏≤‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÑ‡∏î‡πâ‡πÄ‡∏£‡πá‡∏ß)
            self.logger.debug(
                f"[PDCA-STATUS] {sub_id}_L{level} | "
                f"Required: {required_pdca} | "
                f"Found: {actual_found_phases} | "
                f"Missing: {missing_phases} | "
                f"Coverage: {coverage_percentage}% | "
                f"Statement: {level_details[str(level)]['rubric_statement'][:80]}..."
            )

            # --- [STEP 5.1: ROADMAP BUNDLE - SIMPLE & TRACEABLE] ---
            if enriched_chunks:
                top_ev = max(enriched_chunks, key=lambda x: float(x.get("rerank_score") or x.get("relevance_score") or 0.0))
                top_file = top_ev.get("source_filename") or top_ev.get("filename") or "N/A"
                top_page = top_ev.get("page_label") or top_ev.get("page", "N/A")
                top_score = f"{float(top_ev.get('rerank_score') or top_ev.get('relevance_score') or 0.0):.4f}"
                top_pdca = top_ev.get("pdca_tag", "N/A")
                snippet = (top_ev.get("content") or top_ev.get("text") or "")[:150].replace("\n", " ").strip() + "..."

                insight_summary = (
                    f"L{level}: {'PASSED' if actual_passed else 'FAILED'} | "
                    f"Top: {top_file} (p.{top_page}) | Score: {top_score} | PDCA: {top_pdca} | "
                    f"Insight: {res.get('coaching_insight', 'N/A')[:200]}... | Snippet: {snippet}"
                )
            else:
                insight_summary = (
                    f"L{level}: {'PASSED' if actual_passed else 'FAILED'} | "
                    f"No evidence | Statement: {stmt.get('statement', '')[:200]} | "
                    f"Insight: {res.get('coaching_insight', 'N/A')[:150]}..."
                )

            roadmap_input_bundle.append({
                "level": level,
                "status": "PASSED" if actual_passed else ("CAPPED" if is_capped else "FAILED"),
                "statement": stmt.get("statement", "")[:400],
                "insight_summary": insight_summary
            })

            self.logger.debug(f"[ROADMAP-BUNDLE] {sub_id}_L{level} | {insight_summary[:400]}...")

        # --- [STEP 6: STRATEGIC FOCUS SELECTION] ---
        has_gap = any(not ld["is_passed"] for ld in level_details.values())

        if highest_continuous_level < 3:
            strategic_focus = "Focus: Stabilization (‡πÄ‡∏ô‡πâ‡∏ô‡∏™‡∏ñ‡∏≤‡∏õ‡∏ô‡∏≤‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏õ‡∏¥‡∏î Gap ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ê‡∏≤‡∏ô)"
        elif highest_continuous_level < 5:
            strategic_focus = "Focus: Scaling & Integration (‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡πÇ‡∏¢‡∏á‡πÅ‡∏•‡∏∞‡∏Ç‡∏¢‡∏≤‡∏¢‡∏ú‡∏•)"
        else:
            strategic_focus = (
                "Focus: Strategic Excellence & Sustainability (‡πÄ‡∏ô‡πâ‡∏ô‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏±‡πà‡∏á‡∏¢‡∏∑‡∏ô)"
                if not has_gap else
                "Focus: Strategic Excellence (‡πÄ‡∏ô‡πâ‡∏ô‡∏õ‡∏¥‡∏î gap ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÅ‡∏•‡∏∞‡∏¢‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏™‡∏π‡πà‡∏ï‡πâ‡∏ô‡πÅ‡∏ö‡∏ö)"
            )

        self.logger.info(f"[STRATEGIC-FOCUS] {sub_id} | Highest: L{highest_continuous_level} | Gap: {has_gap} | {strategic_focus}")

        sub_roadmap = self.generate_sub_roadmap(
            sub_id=sub_id,
            sub_criteria_name=sub_name,
            enabler=getattr(self, "enabler", "KM"),
            aggregated_insights=roadmap_input_bundle,
            strategic_focus=strategic_focus
        )

        return {
            "sub_id": sub_id,
            "sub_criteria_name": sub_name,
            "weight": sub_weight,
            "highest_full_level": highest_continuous_level,
            "weighted_score": round(highest_continuous_level * (sub_weight / 5), 2),
            "is_passed": highest_continuous_level >= 1,
            "level_details": level_details,
            "sub_roadmap": sub_roadmap,
            "strategic_focus": strategic_focus
        }, evidence_delta
            
    def _get_level_constraint_prompt(
        self,
        sub_id: str,
        level: int,
        req_phases: list | None = None,
        spec_rule: str | None = None
    ) -> str:
        """
        [FINAL LOCKED VERSION v2026.01.27]
        - PDCA Scope Guard (‡πÑ‡∏°‡πà over-require maturity)
        - Align ‡∏Å‡∏±‡∏ö relevance_score_fn / cumulative rules
        - ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô LLM hallucinate SUPPORT / OTHER
        """

        enabler = getattr(self, "enabler", "KM").upper()
        enabler_name = "‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ (KM)"

        # -------------------------------
        # 1. Level Goal & Focus
        # -------------------------------
        level_goal = get_pdca_goal_for_level(level)
        level_focus = PDCA_PHASE_MAP.get(level, "‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏ö")

        # -------------------------------
        # 2. Mandatory PDCA (Truth-based)
        # -------------------------------
        if req_phases:
            required_phases = list(dict.fromkeys(req_phases))
        else:
            if level >= 4:
                required_phases = ["P", "D", "C"]
            elif level >= 2:
                required_phases = ["P", "D"]
            else:
                required_phases = ["P"]

        req_str = " + ".join(required_phases)

        # -------------------------------
        # 3. Guardrails (Hard Rules)
        # -------------------------------
        strict_rules = [
            f"1. [PDCA Scope Lock] ‡πÉ‡∏´‡πâ‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞ PDCA = {req_str} ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡∏´‡πâ‡∏≤‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏°‡∏ß‡∏î‡∏≠‡∏∑‡πà‡∏ô ‡πÄ‡∏ä‡πà‡∏ô SUPPORT, OTHER, CONTEXT",
            f"2. [Maturity Guard] ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤ Level {level} (‡πÄ‡∏ä‡πà‡∏ô Automation, AI, Benchmarking) ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏´‡∏ï‡∏∏‡πÉ‡∏´‡πâ '‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô' ‡∏´‡∏≤‡∏Å‡∏£‡∏π‡∏ö‡∏£‡∏¥‡∏Å‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏∞‡∏ö‡∏∏",
            "3. [Evidence Integrity] ‡∏´‡πâ‡∏≤‡∏°‡∏™‡∏£‡∏∏‡∏õ PDCA Phase ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô ‡∏´‡∏£‡∏∑‡∏≠‡∏´‡∏ô‡πâ‡∏≤ (page) ‡πÑ‡∏î‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô",
            "4. [Policy Shortcut] ‡∏´‡∏≤‡∏Å‡∏û‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÄ‡∏ä‡∏¥‡∏á‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢/‡πÅ‡∏ú‡∏ô‡πÅ‡∏°‡πà‡∏ö‡∏ó‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏≠‡∏ô‡∏∏‡∏°‡∏±‡∏ï‡∏¥ ‡πÉ‡∏´‡πâ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡∏ú‡πà‡∏≤‡∏ô Phase 'P' (‡πÅ‡∏•‡∏∞ 'D' ‡∏´‡∏≤‡∏Å‡∏°‡∏µ‡πÅ‡∏ú‡∏ô‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥)",
            f"5. {GLOBAL_EVIDENCE_INSTRUCTION}",
            "6. [Substance First] ‡πÉ‡∏´‡πâ‡∏î‡∏π‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ß‡πà‡∏≤‡∏ï‡∏≠‡∏ö‡πÇ‡∏à‡∏ó‡∏¢‡πå‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡∏µ‡πâ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå",
            "7. [Coaching Output] ‡∏´‡∏≤‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ß‡πà‡∏≤ '‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô' ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏∏‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏Ç‡∏≤‡∏î‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡∏ò‡∏£‡∏£‡∏° ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÑ‡∏î‡πâ"
        ]

        # -------------------------------
        # 4. Prompt Assembly
        # -------------------------------
        lines = [
            f"\n### üõ°Ô∏è AUDIT GUIDELINE : {enabler} | LEVEL {level} ###",
            f"üéØ ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô: {enabler_name} ({sub_id})",
            f"üìà ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏£‡∏∞‡∏î‡∏±‡∏ö: {level_goal}",
            f"üîç PDCA ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏ö (Mandatory): {req_str}",
            f"üìå ‡∏à‡∏∏‡∏î‡πÄ‡∏ô‡πâ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡∏µ‡πâ: {level_focus}",
            f"üí° ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÉ‡∏ô‡∏£‡∏π‡∏ö‡∏£‡∏¥‡∏Å: {spec_rule}" if spec_rule else "",
            "\n‚ö†Ô∏è STRICT DECISION RULES:",
            *strict_rules
        ]

        return "\n".join(filter(None, lines))

    def _run_single_assessment(
        self,
        sub_id: str,
        level: int,
        criteria: Dict[str, Any],
        keyword_guide: List[str],
        baseline_evidences: List[Dict[str, Any]],
        vectorstore_manager: Optional[Any] = None,
    ) -> Dict[str, Any]:
        """
        [ULTIMATE REVISED v2026.02.05-enhanced]
        - Governance: ‡∏â‡∏µ‡∏î audit_instruction ‡∏Ñ‡∏∏‡∏°‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï
        - Retrieval: Adaptive + Neighbor Expansion + Diversity Filter
        - Resilience: Judicial Review (‡∏≠‡∏∏‡∏ó‡∏ò‡∏£‡∏ì‡πå) ‡πÄ‡∏°‡∏∑‡πà‡∏≠ Rerank ‡∏™‡∏π‡∏á‡πÅ‡∏ï‡πà LLM ‡∏°‡∏≠‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏´‡πá‡∏ô
        - Logging: ‡πÄ‡∏û‡∏¥‡πà‡∏° debug appeal trigger/success + fallback JSON
        - ‡πÑ‡∏°‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô signature ‡∏´‡∏£‡∏∑‡∏≠ flow ‡πÄ‡∏î‡∏¥‡∏°
        """
        log_prefix = f"Sub:{sub_id} L{level}"
        sub_name = criteria.get('name', 'Unknown Sub-item')
        statement_text = criteria.get('statement', 'No statement defined')
        
        self.logger.info(f"üîç [START-ASSESSMENT] {log_prefix} | {sub_name}")
        self.logger.info(f"üìã [CRITERIA] Level {level}: \"{statement_text}\"")

        # --- [STEP 1: GOVERNANCE & RULES] ---
        audit_instruction = self._get_level_constraint_prompt(sub_id, level)
        current_rules = getattr(self, 'contextual_rules_map', {}).get(sub_id, {}).get(f"L{level}", {})

        # --- [STEP 2: ADAPTIVE RETRIEVAL & EXPANSION] ---
        retrieved_chunks, max_rerank = self._perform_adaptive_retrieval(
            sub_id=sub_id,
            level=level,
            stmt=statement_text,
            vectorstore_manager=vectorstore_manager,
        )

        if retrieved_chunks:
            enabler_key = str(getattr(self, 'enabler', 'km')).lower()
            collection_name = f"evidence_{enabler_key}"
            retrieved_chunks = self._expand_context_with_neighbor_pages(
                top_evidences=retrieved_chunks, 
                collection_name=collection_name
            )

        retrieved_chunks = self._apply_diversity_filter(retrieved_chunks, level)

        # --- [STEP 3: EVIDENCE FUSION & METADATA] ---
        evidences = (baseline_evidences or []) + (retrieved_chunks or [])
        pdca_blocks = self._get_pdca_blocks_from_evidences(
            evidences=evidences,
            baseline_evidences=baseline_evidences,
            level=level,
            sub_id=sub_id,
            contextual_rules_map=getattr(self, 'contextual_rules_map', {})
        )

        audit_confidence = self.calculate_audit_confidence(
            matched_chunks=retrieved_chunks,
            sub_id=sub_id,
            level=level,
        )
        self.current_audit_meta = audit_confidence

        # --- [STEP 4: MULTICHANNEL LLM EXECUTION] ---
        llm_context = self._build_multichannel_context_for_level(
            level=level,
            top_evidences=retrieved_chunks,
            previous_levels_evidence=baseline_evidences
        )

        llm_raw = self.evaluate_pdca(
            pdca_blocks=pdca_blocks,
            sub_id=sub_id,
            level=level,
            audit_confidence=audit_confidence,
            audit_instruction=audit_instruction
        )
        if not isinstance(llm_raw, dict): 
            llm_raw = {}

        # --- [STEP 5: SMART NORMALIZATION] ---
        result = self.post_process_llm_result(
            llm_output=llm_raw,
            level=level,
            sub_id=sub_id,
            contextual_config=current_rules,
            top_evidences=retrieved_chunks
        )

        # --- [STEP 6: JUDICIAL REVIEW (RESCUE LOGIC)] ---
        is_safety_pass = False
        if not result.get("is_passed") and max_rerank >= 0.70:
            self.logger.info(f"‚öñÔ∏è [TRIGGER-APPEAL] {log_prefix} | Rerank {max_rerank:.4f} HIGH ‚Üí Re-evaluating... (Missing: {result.get('missing_phases', [])})")

            appeal_result = self._run_expert_re_evaluation(
                sub_id=sub_id,
                level=level,
                statement_text=statement_text,
                context=str(llm_context.get("full_context", "")),
                first_attempt_reason=result.get("reason", "‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÑ‡∏°‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô"),
                missing_tags=result.get("missing_phases", []),
                highest_rerank_score=max_rerank,
                sub_criteria_name=sub_name,
                llm_evaluator_to_use=self.evaluate_pdca,
                audit_instruction=audit_instruction,
                base_kwargs={
                    "pdca_blocks": pdca_blocks,
                    "audit_instruction": audit_instruction,
                    "audit_confidence": audit_confidence
                }
            )

            if appeal_result and appeal_result.get("appeal_status") == "GRANTED":
                self.logger.info(f"‚úÖ [APPEAL-GRANTED] {log_prefix} | Expert Rescue OK! (New Score: {appeal_result.get('score', 'N/A')})")
                final_appeal = self.post_process_llm_result(
                    llm_output=appeal_result,
                    level=level,
                    sub_id=sub_id,
                    contextual_config=current_rules,
                    top_evidences=retrieved_chunks
                )
                result.update(final_appeal)
                result["is_passed"] = True
                result["score"] = max(result.get("score", 0.0), 1.0)
                is_safety_pass = True
            else:
                self.logger.info(f"‚ùå [APPEAL-DENIED/ERROR] {log_prefix} | ‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏≠‡∏∏‡∏ó‡∏ò‡∏£‡∏ì‡πå")

        # --- [STEP 7: FINAL INSIGHTS & LOGGING] ---
        final_insight = (result.get("coaching_insight") or result.get("reason") or "").strip()
        final_insight = f"[{'STRENGTH' if result.get('is_passed') else 'GAP'}] {final_insight}"

        if hasattr(self, "_log_pdca_status"):
            self._log_pdca_status(
                sub_id=sub_id,
                name=sub_name,
                level=level,
                blocks=llm_raw,
                req_phases=result.get("required_phases", []),
                sources_count=len(retrieved_chunks),
                score=result.get("score", 0.0),
                conf_level=audit_confidence.get("level", "LOW"),
                pdca_breakdown=result.get("pdca_breakdown", {}),
                tagging_result=audit_confidence.get("pdca_found", []),
                is_safety_pass=is_safety_pass
            )

        return {
            "is_passed": bool(result.get("is_passed", False)),
            "score": float(result.get("score", 0.0)),
            "reason": result.get("reason", ""),
            "coaching_insight": final_insight,
            "pdca_breakdown": result.get("pdca_breakdown", {}),
            "audit_confidence": audit_confidence,
            "top_chunks_data": retrieved_chunks,
            "is_safety_pass": is_safety_pass,
            "debug_meta": {
                "max_rerank": max_rerank,
                "evidence_count": len(evidences),
                "judicial_review": is_safety_pass,
                "neighbor_expansion": True,
                "audit_instruction_applied": True
            }
        }