# core/seam_assessment.py

import sys
import json
import logging
import time
import os
from typing import List, Dict, Any, Optional, Union, Tuple, Set, Final, Literal
from collections import defaultdict
from datetime import datetime
from dataclasses import dataclass, field
import multiprocessing # NEW: Import for parallel execution
from functools import partial
from core.llm_data_utils import enhance_query_for_statement
import pathlib, uuid
from langchain_core.documents import Document as LcDocument
from core.retry_policy import RetryPolicy, RetryResult
from copy import deepcopy
import tempfile
import shutil
# from json_extractor import _robust_extract_json
from .json_extractor import _robust_extract_json
from filelock import FileLock  # ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á: pip install filelock
import re


# -------------------- PATH SETUP & IMPORTS --------------------
try:
    PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
    if PROJECT_ROOT not in sys.path:
        sys.path.append(PROJECT_ROOT)

    from config.global_vars import (
        EXPORTS_DIR, MAX_LEVEL, INITIAL_LEVEL, FINAL_K_RERANKED,
        RUBRIC_FILENAME_PATTERN, RUBRIC_CONFIG_DIR, DEFAULT_ENABLER,
        EVIDENCE_DOC_TYPES, INITIAL_TOP_K,
        EVIDENCE_MAPPING_FILENAME_SUFFIX,
        LIMIT_CHUNKS_PER_PRIORITY_DOC,
        IS_LOG_L3_CONTEXT,
        PRIORITY_CHUNK_LIMIT,
        DEFAULT_TENANT,
        DEFAULT_YEAR,
        RERANK_THRESHOLD,
        MAX_EVI_STR_CAP
    )
    
    from core.llm_data_utils import ( 
        create_structured_action_plan, evaluate_with_llm,
        retrieve_context_with_filter, retrieve_context_for_low_levels,
        evaluate_with_llm_low_level, LOW_LEVEL_K, 
        set_mock_control_mode as set_llm_data_mock_mode,
        create_context_summary_llm,
        retrieve_context_by_doc_ids,
        _fetch_llm_response,
        build_multichannel_context_for_level
    )
    from core.vectorstore import VectorStoreManager, load_all_vectorstores, get_global_reranker 
    from core.seam_prompts import PDCA_PHASE_MAP 
        
    import assessments.seam_mocking as seam_mocking 
    
except ImportError as e:
    print(f"FATAL ERROR: Failed to import required modules. Error: {e}", file=sys.stderr)
    
    # Define placeholder variables if imports fail
    EXPORTS_DIR = "exports"
    MAX_LEVEL = 5
    INITIAL_LEVEL = 1
    FINAL_K_RERANKED = 3
    RUBRIC_FILENAME_PATTERN = "{tenant}_{enabler}_rubric.json"
    RUBRIC_CONFIG_DIR = "config/rubrics"
    DEFAULT_ENABLER = "KM"
    EVIDENCE_DOC_TYPES = "evidence"
    INITIAL_TOP_K = 10
    
    def create_structured_action_plan(*args, **kwargs): return [{"Phase": "Mock Plan", "Goal": "Resolve issue"}]
    def evaluate_with_llm(*args, **kwargs): return {"score": 1, "reason": "Mock pass", "is_passed": True}
    def retrieve_context_with_filter(*args, **kwargs): return {"top_evidences": [], "aggregated_context": "Mock Context"}
    def retrieve_context_for_low_levels(*args, **kwargs): return {"top_evidences": [], "aggregated_context": "Mock Low Context"}
    def evaluate_with_llm_low_level(*args, **kwargs): return {"score": 1, "reason": "Mock pass L1/L2", "is_passed": True}
    LOW_LEVEL_K = 2
    def set_llm_data_mock_mode(mode): pass
    class VectorStoreManager: pass
    def load_all_vectorstores(*args, **kwargs): return VectorStoreManager()
    PDCA_PHASE_MAP = {1: "Plan", 2: "Do", 3: "Check", 4: "Act", 5: "Innovate"}
    class seam_mocking:
        @staticmethod
        def evaluate_with_llm_CONTROLLED_MOCK(*args, **kwargs): return {"score": 0, "reason": "Mock fail", "is_passed": False}
        @staticmethod
        def retrieve_context_with_filter_MOCK(*args, **kwargs): return {"top_evidences": [], "aggregated_context": "Mock Context"}
        @staticmethod
        def create_structured_action_plan_MOCK(*args, **kwargs): return [{"Phase": "Mock Plan", "Goal": "Resolve issue"}]
        @staticmethod
        def set_mock_control_mode(mode): pass
    
    if "FATAL ERROR" in str(e):
        pass 


logger = logging.getLogger(__name__)
if not logger.handlers:
    logging.basicConfig(level=logging.DEBUG, format="%(asctime)s - %(levelname)s - %(message)s")

# =================================================================
# üü¢ FIX: Helper Function for PDCA Calculation (Priority 1 Part 2 & Priority 2)
# NOTE: ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡πÅ‡∏õ‡∏•‡∏á llm_score (1-5) ‡πÄ‡∏õ‡πá‡∏ô PDCA Breakdown ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
# =================================================================
LEVEL_PHASE_MAP = {
    1: ['P'],
    2: ['P', 'D'],
    3: ['P', 'D', 'C'],
    4: ['P', 'D', 'C', 'A'],
    5: ['P', 'D', 'C', 'A'] # L5 ‡πÉ‡∏ä‡πâ P, D, C, A ‡πÄ‡∏ä‡πà‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö L4 ‡πÅ‡∏ï‡πà‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏ï‡πá‡∏°‡∏≠‡∏≤‡∏à‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô
}

# ----------------------------------------------------------------------
# NEW CONSTANT: ‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô PDCA ‡∏ó‡∏µ‡πà '‡∏ú‡πà‡∏≤‡∏ô' ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ Level (Achieved Score)
# ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Achieved Score (Sum of P,D,C,A) ‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö Required Score (R)
# L1 (R=1, A=1): P=1
# L2 (R=2, A=2): P=1, D=1
# L3 (R=4, A=4): P=1, D=1, C=1, A=1
# L4 (R=6, A=6): P=2, D=2, C=1, A=1
# L5 (R=8, A=8): P=2, D=2, C=2, A=2
# ----------------------------------------------------------------------
CORRECT_PDCA_SCORES_MAP: Final[Dict[int, Dict[str, int]]] = {
    1: {'P': 1, 'D': 0, 'C': 0, 'A': 0},
    2: {'P': 1, 'D': 1, 'C': 0, 'A': 0},
    3: {'P': 1, 'D': 1, 'C': 1, 'A': 1},
    4: {'P': 2, 'D': 2, 'C': 1, 'A': 1},
    5: {'P': 2, 'D': 2, 'C': 2, 'A': 2},
}

# üü¢ NOTE: ‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£ Global ‡∏´‡∏£‡∏∑‡∏≠ Config Flag ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏¥‡∏î/‡∏õ‡∏¥‡∏î‡πÇ‡∏´‡∏°‡∏î‡∏ô‡∏µ‡πâ
# ‡πÄ‡∏ä‡πà‡∏ô: IS_L3_DEBUG_TEST = True 
# ‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏™‡πà‡∏á‡∏Ñ‡πà‡∏≤‡∏ô‡∏µ‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà build_simulated_l3_evidence (‡πÄ‡∏ä‡πà‡∏ô via debug_mode argument)

def build_simulated_l3_evidence(check_blocks: list[dict]) -> str:

    if not check_blocks:
        return ""

    # --- Original Dynamic Logic ---
    source_files = ", ".join(sorted({b["file"] for b in check_blocks}))
    extracted_summary = "\n\n".join(
        f"- ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå {b['file']}:\n{b['content'][:600]}"
        for b in check_blocks
    )

    return f"""
[SIMULATED_L3_EVIDENCE]
‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö (Check Phase) ‡∏û‡∏ö‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå: {source_files}
... (‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏Ç‡∏≠‡∏á‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏î‡∏¥‡∏°)
""".strip()

def build_ordered_context(level: int,
                          plan_blocks: list[dict],
                          do_blocks: list[dict],
                          check_blocks: list[dict],
                          act_blocks: list[dict],
                          other_blocks: list[dict]) -> str:
    def fmt(blocks):
        return "\n\n".join(
            f"[{b.get('file', 'Unknown File')}]\n{b.get('content', b.get('text', ''))}" for b in blocks
        )

    if level == 3:
        # L3: Check/Act Priority 1, Plan/Do/Other ‡∏ï‡πà‡∏≠‡∏ó‡πâ‡∏≤‡∏¢
        ordered = [
            fmt(check_blocks),
            fmt(act_blocks),
            fmt(plan_blocks),
            fmt(do_blocks),
            fmt(other_blocks)
        ]
    else:
        # Default: Plan -> Do -> Check -> Act -> Other
        ordered = [
            fmt(plan_blocks),
            fmt(do_blocks),
            fmt(check_blocks),
            fmt(act_blocks),
            fmt(other_blocks)
        ]

    return "\n\n".join([o for o in ordered if o])


def calculate_pdca_breakdown_and_pass_status(llm_score: int, level: int) -> Tuple[Dict[str, int], bool, float]:
    """
    ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì PDCA breakdown, is_passed status, ‡πÅ‡∏•‡∏∞ raw_pdca_score (Achieved Score) 
    ‡πÇ‡∏î‡∏¢‡πÅ‡∏õ‡∏•‡∏á‡∏à‡∏≤‡∏Å llm_score (1-5) ‡πÅ‡∏•‡∏∞ Level ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô 
    
    ‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£:
    - L1 ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ llm_score >= 1
    - L2 ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ llm_score >= 2
    - L3 ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ llm_score >= 3
    - L4 ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ llm_score >= 4
    - L5 ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ llm_score >= 4 
    """
    pdca_map: Dict[str, int] = {'P': 0, 'D': 0, 'C': 0, 'A': 0}
    is_passed: bool = False
    raw_pdca_score: float = 0.0
    
    # 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ PASS (‡πÉ‡∏ä‡πâ Logic ‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏Å‡∏≥‡∏´‡∏ô‡∏î)
    if level == 5:
        if llm_score >= 4:
            is_passed = True
    elif level == 4:
        if llm_score >= 4:
            is_passed = True
    elif level == 3:
        if llm_score >= 3:
            is_passed = True
    elif level == 2:
        if llm_score >= 2:
            is_passed = True
    elif level == 1:
        if llm_score >= 1:
            is_passed = True

    # 2. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì PDCA Breakdown ‡πÅ‡∏•‡∏∞ raw_pdca_score (Achieved Score)
    if is_passed:
        # *** REVISED LOGIC: ‡πÉ‡∏ä‡πâ CORRECT_PDCA_SCORES_MAP ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô P, D, C, A ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ***
        correct_scores = CORRECT_PDCA_SCORES_MAP.get(level, pdca_map) 
        pdca_map.update(correct_scores)
        
        # raw_pdca_score (Achieved Score) ‡∏à‡∏∞‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö Required Score (R) ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ú‡πà‡∏≤‡∏ô
        raw_pdca_score = float(sum(pdca_map.values()))
    
    return pdca_map, is_passed, raw_pdca_score

def get_correct_pdca_required_score(level: int) -> int:
    """
    ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤ Required Score (R) ‡∏ï‡∏≤‡∏° Level ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå SE-AM:
    L1=1, L2=2, L3=4, L4=6, L5=8
    """
    # ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ô‡∏µ‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß
    if level == 1:
        return 1
    elif level == 2:
        return 2
    elif level == 3:
        return 4
    elif level == 4:
        return 6
    elif level == 5:
        return 8
    # ‡∏Å‡∏£‡∏ì‡∏µ Level ‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î
    return 8


# üìå ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç Type Hint ‡πÅ‡∏•‡∏∞ Arguments ‡∏Ç‡∏≠‡∏á Tuple ‡πÉ‡∏´‡πâ‡∏£‡∏ß‡∏° document_map (8 elements)
def _static_worker_process(worker_input_tuple: Tuple[Dict[str, Any], str, int, str, str, str, float, Optional[Dict[str, str]]]) -> Dict[str, Any]:
    """
    Static worker function for multiprocessing pool. 
    It reconstructs SeamAssessment in the new process and executes the assessment 
    for a single sub-criteria.
    
    Args:
        worker_input_tuple: (sub_criteria_data, enabler: str, target_level: int, mock_mode: str, 
                             evidence_map_path: str, model_name: str, temperature: float, 
                             document_map: Optional[Dict[str, str]]) 

    Returns:
        Dict[str, Any]: Final result of the sub-criteria assessment.
    """
    
    # üü¢ NEW FIX: PATH SETUP ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Worker Process
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
    if project_root not in sys.path:
        sys.path.append(project_root)
        
    worker_logger = logging.getLogger(__name__)

    try:
        # üü¢ FIX: Unpack ‡∏Ñ‡πà‡∏≤ Primitives ‡∏ó‡∏±‡πâ‡∏á 8 ‡∏ï‡∏±‡∏ß (‡∏£‡∏ß‡∏° document_map)
        sub_criteria_data, enabler, target_level, mock_mode, evidence_map_path, model_name, temperature, document_map = worker_input_tuple
    except ValueError as e:
        worker_logger.critical(f"Worker input tuple unpack failed (expected 8 elements): {e}")
        return {"error": f"Invalid worker input: {e}"}
        
    # 1. Reconstruct Config 
    try:
        # üü¢ FIX: ‡∏™‡∏£‡πâ‡∏≤‡∏á AssessmentConfig ‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏ô Worker Process
        worker_config = AssessmentConfig(
            enabler=enabler,
            target_level=target_level,
            mock_mode=mock_mode,
            model_name=model_name, 
            temperature=temperature
        )
    except Exception as e:
        worker_logger.critical(f"Failed to reconstruct AssessmentConfig in worker: {e}")
        return {
            "sub_criteria_id": sub_criteria_data.get('sub_id', 'UNKNOWN'),
            "error": f"Config reconstruction failed: {e}"
        }

    # 2. Re-instantiate SeamAssessment 
    try:
        # üü¢ FIX (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç): ‡∏™‡πà‡∏á document_map ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô SEAMPDCAEngine
        worker_instance = SEAMPDCAEngine(
            config=worker_config, 
            evidence_map_path=evidence_map_path, 
            llm_instance=None, 
            vectorstore_manager=None, 
            logger_instance=worker_logger,
            document_map=document_map # ‚¨ÖÔ∏è ‡∏™‡πà‡∏á document_map ‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏á Unpack ‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤
        )
    except Exception as e:
        worker_logger.critical(f"FATAL: SEAMPDCAEngine instantiation failed in worker: {e}")
        return {
            "sub_criteria_id": sub_criteria_data.get('sub_id', 'UNKNOWN'),
            "error": f"Engine initialization failed: {e}"
        }
    
    # 3. Execute the worker logic
    return worker_instance._run_sub_criteria_assessment_worker(sub_criteria_data)


def merge_evidence_mappings(results_list: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
    """
    ‡∏£‡∏ß‡∏° evidence_mapping dictionaries ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å Worker ‡∏ó‡∏∏‡∏Å‡∏ï‡∏±‡∏ß 
    """
    merged_mapping = defaultdict(list)
    for result in results_list:
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å Worker ‡∏°‡∏µ Key 'evidence_mapping' ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if 'evidence_mapping' in result and isinstance(result['evidence_mapping'], dict):
            # ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏ú‡πà‡∏≤‡∏ô Key/Value ‡∏Ç‡∏≠‡∏á Worker ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡∏±‡∏ß
            for level_key, evidence_list in result['evidence_mapping'].items():
                # ‡πÉ‡∏ä‡πâ .extend() ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ú‡∏ô‡∏ß‡∏Å‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
                if isinstance(evidence_list, list):
                    merged_mapping[level_key].extend(evidence_list)
    
    # ‡πÅ‡∏õ‡∏•‡∏á defaultdict ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô dict ‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤
    return dict(merged_mapping)

# =================================================================
# Configuration Class
# =================================================================
@dataclass
class AssessmentConfig:
    """Configuration for the SEAM PDCA Assessment Run."""
    enabler: str = DEFAULT_ENABLER
    target_level: int = MAX_LEVEL
    mock_mode: str = "none" # 'none', 'random', 'control'
    force_sequential: bool = field(default=False) # Flag to force sequential ru
    # üü¢ FIX: ‡πÄ‡∏û‡∏¥‡πà‡∏° LLM Configuration Fields ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô Dataclass
    model_name: str = "llama3.1:8b" # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ default ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÉ‡∏ä‡πâ
    temperature: float = 0.0
    # üü¢ FIX: ‡πÄ‡∏û‡∏¥‡πà‡∏° Tenant ‡πÅ‡∏•‡∏∞ Year
    tenant: str = DEFAULT_TENANT
    year: int = DEFAULT_YEAR


# =================================================================
# SEAM Assessment Engine (PDCA Focused)
# =================================================================
class SEAMPDCAEngine:
    
    # üéØ Mapping for RAG Query Augmentation at Level 1 (Plan)
    ENABLER_L1_AUGMENTATION = {
        "KM": "‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢ ‡∏ß‡∏¥‡∏™‡∏±‡∏¢‡∏ó‡∏±‡∏®‡∏ô‡πå ‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå ‡πÅ‡∏ú‡∏ô‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏∏‡πà‡∏á‡∏°‡∏±‡πà‡∏ô",
        "HCM": "‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏• ‡πÅ‡∏ú‡∏ô‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡∏ô ‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏• ‡∏Å‡∏≤‡∏£‡∏°‡∏µ‡∏™‡πà‡∏ß‡∏ô‡∏£‡πà‡∏ß‡∏°‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£",
        "DT": "‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏î‡∏¥‡∏à‡∏¥‡∏ó‡∏±‡∏• ‡πÅ‡∏ú‡∏ô‡πÅ‡∏°‡πà‡∏ö‡∏ó‡∏î‡∏¥‡∏à‡∏¥‡∏ó‡∏±‡∏• ‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á IT ‡πÅ‡∏ú‡∏ô‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ",
        "SP": "‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£ ‡πÅ‡∏ú‡∏ô‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£ ‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå ‡πÅ‡∏ú‡∏ô‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå",
        "DEFAULT": "‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢ ‡∏ß‡∏¥‡∏™‡∏±‡∏¢‡∏ó‡∏±‡∏®‡∏ô‡πå ‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå ‡πÅ‡∏ú‡∏ô‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏∏‡πà‡∏á‡∏°‡∏±‡πà‡∏ô" 
    }
    
    L1_INITIAL_TOP_K_RAG: int = 50 
    
    def __init__(
        self, 
        config: AssessmentConfig,
        llm_instance: Any = None, 
        logger_instance: logging.Logger = None,
        rag_retriever_instance: Any = None,
        # üü¢ FIX #1: ‡πÄ‡∏û‡∏¥‡πà‡∏° doc_type 
        doc_type: str = EVIDENCE_DOC_TYPES, 
        # üü¢ FIX #2: ‡πÄ‡∏û‡∏¥‡πà‡∏° vectorstore_manager
        vectorstore_manager: Optional['VectorStoreManager'] = None,
        # üìå FIX #3 (‡πÉ‡∏´‡∏°‡πà): ‡πÄ‡∏û‡∏¥‡πà‡∏° evidence_map_path ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å Worker Process
        evidence_map_path: Optional[str] = None,
        document_map: Optional[Dict[str, str]] = None 
    ):

            # =======================================================
            # üéØ FIX 1: ‡∏¢‡πâ‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤ Logger ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å
            # =======================================================
            if logger_instance is not None:
                 self.logger = logger_instance
            else:
                 # ‡∏™‡∏£‡πâ‡∏≤‡∏á Child Logger ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Log ‡∏°‡∏µ Context ‡∏Ç‡∏≠‡∏á Tenant/Year
                 self.logger = logging.getLogger(__name__).getChild(f"Engine|{config.enabler}|{config.tenant}/{config.year}")
            
            self.logger.info(f"Initializing SEAMPDCAEngine for {config.enabler} ({config.tenant}/{config.year}).")

            # =======================================================
            # üü¢ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≥‡∏´‡∏ô‡∏î Attribute ‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÄ‡∏°‡∏ò‡∏≠‡∏î‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ self.logger
            # =======================================================
            self.config = config
            self.enabler_id = config.enabler
            self.target_level = config.target_level
            self.rubric = self._load_rubric()
            
            # üü¢ ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤ VSM ‡πÅ‡∏•‡∏∞ doc_type
            self.vectorstore_manager = vectorstore_manager
            self.doc_type = doc_type

            self.FINAL_K_RERANKED = FINAL_K_RERANKED
            self.PRIORITY_CHUNK_LIMIT = PRIORITY_CHUNK_LIMIT

            # üü¢ ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤ LLM ‡πÅ‡∏•‡∏∞ Logger
            self.llm = llm_instance           
            # üéØ FIX 2: ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î logger
            if logger_instance is None:
                # ‡πÉ‡∏ä‡πâ logger ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÑ‡∏ß‡πâ‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡∏ï‡πâ‡∏ô
                self.logger.warning("Re-setting logger instance using the pre-initialized one.")
            
            # üü¢ Disable Strict Filter
            self.initial_evidence_ids: Set[str] = self._load_initial_evidence_info()
            all_statements = self._flatten_rubric_to_statements()
            initial_count = len(all_statements)

            self.logger.info(f"DEBUG: Statements found: {initial_count}. Strict Filter is **DISABLED**.")

            self.statements_to_assess = all_statements
            self.logger.info(f"DEBUG: Statements selected for assessment: {len(self.statements_to_assess)} (Skipped: {initial_count - len(self.statements_to_assess)})")

            # Assessment results storage
            self.raw_llm_results: List[Dict[str, Any]] = []
            self.final_subcriteria_results: List[Dict[str, Any]] = []
            self.total_stats: Dict[str, Any] = {}

            self.is_sequential = False  

            self.retry_policy = RetryPolicy(
                max_attempts=3,            
                base_delay=2.0,            
                jitter=True,               
                escalate_context=True,     
                shorten_prompt_on_fail=True,  
                exponential_backoff=True,  
            )

            self.RERANK_THRESHOLD: Final[float] = RERANK_THRESHOLD
            self.MAX_EVI_STR_CAP: Final[float] = MAX_EVI_STR_CAP
            # üìå Persistent Mapping Configuration
            
            # 1. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Evidence Map Path
            # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏°‡∏≤‡∏à‡∏≤‡∏Å Worker (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ) ‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤ Default
            if evidence_map_path:
                self.evidence_map_path = evidence_map_path
            else:
                base_dir = os.path.join(
                    PROJECT_ROOT, 
                    RUBRIC_CONFIG_DIR, # <-- ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠ config/gov_tenants
                    self.config.tenant, 
                    str(self.config.year)
                )

                # üü¢ FIX #2: ‡∏™‡∏£‡πâ‡∏≤‡∏á Filename: tenant_year_enabler_evidence_mapping.json
                map_filename = (
                    f"{self.config.tenant}_{self.config.year}_{self.enabler_id.lower()}"
                    f"{EVIDENCE_MAPPING_FILENAME_SUFFIX}"
                )

                # üü¢ FIX #3: ‡∏£‡∏ß‡∏° Path
                self.evidence_map_path = os.path.join(base_dir, map_filename)

            
            # 2. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Attribute ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Persistent Mapping
            self.evidence_map: Dict[str, List[str]] = {}
            self.temp_map_for_save: Dict[str, List[str]] = {}

            self.contextual_rules_map: Dict[str, Dict[str, str]] = self._load_contextual_rules_map()
            
            # 3. ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà 
            self.evidence_map = self._load_evidence_map()

            self.logger.info(f"Persistent Map Path set to: {self.evidence_map_path}")
            self.logger.info(f"Loaded {len(self.evidence_map)} existing evidence entries into self.evidence_map.")
            
            # Mock function pointers (will point to real functions by default)
            self.llm_evaluator = evaluate_with_llm
            self.rag_retriever = retrieve_context_with_filter
            self.action_plan_generator = create_structured_action_plan

            # Apply mocking if enabled
            if config.mock_mode in ["random", "control"]:
                self._set_mock_handlers(config.mock_mode)

            # Set global mock control mode for llm_data_utils if using 'control'
            if config.mock_mode == "control":
                self.logger.info("Enabling global LLM data utils mock control mode.")
                set_llm_data_mock_mode(True)
            elif config.mock_mode == "random":
                self.logger.warning("Mock mode 'random' is not fully implemented. Using 'control' logic if available.")
                if hasattr(sys.modules.get('seam_mocking'), 'set_mock_control_mode'):
                    sys.modules.get('seam_mocking').set_mock_control_mode(False)
                    set_llm_data_mock_mode(False)

            # üìå ‡πÇ‡∏´‡∏•‡∏î LLM ‡πÅ‡∏•‡∏∞ VSM ‡∏´‡∏≤‡∏Å‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ
            if self.llm is None: self._initialize_llm_if_none()
            if self.vectorstore_manager is None: self._initialize_vsm_if_none()
            
            # =======================================================
            # üéØ FIX #4: ‡πÇ‡∏´‡∏•‡∏î Document Map ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤ ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡πá‡∏ô Dictionary ‡∏ß‡πà‡∏≤‡∏á
            # (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Filename Resolution Failed)
            # =======================================================
            map_to_use: Dict[str, str] = document_map if document_map is not None else {}

            if not map_to_use:
                # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Path ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå Doc ID Mapping ‡∏ó‡∏µ‡πà VSM ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
                mapping_path = os.path.join(
                    PROJECT_ROOT, 
                    RUBRIC_CONFIG_DIR, 
                    self.config.tenant, 
                    str(self.config.year), 
                    # üü¢ FIX: ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ _doc_id_mapping.json ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ï‡∏£‡∏á‡πÜ)
                    f"{self.config.tenant}_{self.config.year}_{self.enabler_id.lower()}_doc_id_mapping.json"
                )
                
                self.logger.info(f"Attempting to load document_map from file: {mapping_path}")

                try:
                    with open(mapping_path, 'r', encoding='utf-8') as f:
                        doc_map_raw = json.load(f)
                    
                    # ‡πÅ‡∏õ‡∏•‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Doc ID -> {file_name: X, ...} ‡πÄ‡∏õ‡πá‡∏ô Doc ID -> file_name
                    map_to_use = {
                        doc_id: data.get("file_name", doc_id) # ‡πÉ‡∏ä‡πâ doc_id ‡πÄ‡∏õ‡πá‡∏ô fallback
                        for doc_id, data in doc_map_raw.items()
                    }
                    self.logger.info(f"Successfully loaded {len(map_to_use)} document mappings from file.")
                    
                except FileNotFoundError:
                    self.logger.warning(f"Document mapping file not found at: {mapping_path}. Using empty map.")
                except Exception as e:
                    self.logger.error(f"Error loading document map from file: {e}")

            # ‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö Document Map
            self.doc_id_to_filename_map: Dict[str, str] = map_to_use
            self.document_map: Dict[str, str] = self.doc_id_to_filename_map # ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ô‡πÑ‡∏î‡πâ
            
            self.logger.info(f"Loaded {len(self.doc_id_to_filename_map)} document mappings.")
            if not self.doc_id_to_filename_map:
                self.logger.warning("Document ID to Filename Map is empty. Filename resolution might be limited.")

            self.logger.info(f"Engine initialized for Enabler: {self.enabler_id}, Mock Mode: {config.mock_mode}")

    def _initialize_llm_if_none(self):
        """Initializes LLM instance if self.llm is None."""
        if self.llm is None:
            self.logger.warning("‚ö†Ô∏è Initializing LLM: model=%s, temperature=%s", 
                                self.config.model_name, self.config.temperature)
            try:
                # üü¢ FIX: Import ‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ create_llm_instance
                from models.llm import create_llm_instance 
                self.llm = create_llm_instance( 
                    model_name=self.config.model_name,
                    temperature=self.config.temperature
                )
                self.logger.info("‚úÖ LLM Instance created successfully: %s (Temp: %s)", 
                                 self.config.model_name, self.config.temperature)
            except Exception as e:
                self.logger.error(f"FATAL: Could not initialize LLM: {e}")
                raise


    def _initialize_vsm_if_none(self):
        """
        Initializes VectorStoreManager if self.vectorstore_manager is None.
        Handles multi-tenant/multi-year vector store loading.
        """
        if self.vectorstore_manager is None:
            self.logger.info("Loading central evidence vectorstore(s)...")
            try:
                # üéØ FIX: ‡∏™‡πà‡∏á tenant ‡πÅ‡∏•‡∏∞ year ‡∏à‡∏≤‡∏Å config ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô load_all_vectorstores()
                self.vectorstore_manager = load_all_vectorstores(
                    doc_types=[EVIDENCE_DOC_TYPES], 
                    evidence_enabler=self.enabler_id,
                    tenant=self.config.tenant,  # <--- NEW: ‡πÄ‡∏û‡∏¥‡πà‡∏° Argument ‡∏ô‡∏µ‡πâ
                    year=self.config.year       # <--- NEW: ‡πÄ‡∏û‡∏¥‡πà‡∏° Argument ‡∏ô‡∏µ‡πâ
                )
                
                # üìå FINAL FIX: ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á MultiDocRetriever (Private Attribute) 
                # ‡πÅ‡∏•‡∏∞‡∏ï‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ _all_retrievers (Private Attribute)
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ VSM ‡∏ñ‡∏π‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏Å‡πà‡∏≠‡∏ô
                if self.vectorstore_manager and hasattr(self.vectorstore_manager, '_multi_doc_retriever'):
                     len_retrievers = len(
                        self.vectorstore_manager._multi_doc_retriever._all_retrievers
                    )
                     self.logger.info("‚úÖ MultiDocRetriever loaded with %s collections and cached in VSM.", 
                                 len_retrievers) 
                else:
                    self.logger.warning("VectorStoreManager loaded but _multi_doc_retriever structure is missing or unexpected.")

            except Exception as e:
                self.logger.error(f"FATAL: Could not initialize VectorStoreManager: {e}")
                raise # Re-raise the exception to ‡∏´‡∏¢‡∏∏‡∏î‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°
        
    def _resolve_evidence_filenames(self, evidence_entries: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
            """
            ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á
            1. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà doc_id ‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ 'UNKNOWN-' (‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏†‡∏≤‡∏¢‡πÉ‡∏ô/‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£)
            2. ‡πÅ‡∏õ‡∏•‡∏á doc_id (‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô Hash/UUID) ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏î‡πâ ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ doc_id_to_filename_map
            """
            # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤ re ‡πÅ‡∏•‡∏∞ deepcopy ‡∏ñ‡∏π‡∏Å import ‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏±‡∏ß‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡πâ‡∏ß
            from copy import deepcopy
            import re
            
            resolved_entries = []
            for entry in evidence_entries:
                # ‡πÉ‡∏ä‡πâ deepcopy ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
                resolved_entry = deepcopy(entry)
                # ‡πÉ‡∏ä‡πâ doc_id ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏´‡∏•‡∏±‡∏Å‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå
                doc_id = resolved_entry.get("doc_id", "")
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô (‡∏à‡∏≤‡∏Å metadata ‡∏Ç‡∏≠‡∏á vectorstore ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
                current_filename = resolved_entry.get("filename", "")
                
                # --- 1. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏ì‡∏µ UNKNOWN- (AI-GENERATED or Lost Source) ---
                if doc_id.startswith("UNKNOWN-"):
                    # ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏à‡∏£‡∏¥‡∏á
                    # ‡πÄ‡∏ä‡πà‡∏ô "UNKNOWN-2fac2f11" --> "AI-GENERATED-REF-2fac2f11"
                    resolved_entry["filename"] = f"AI-GENERATED-REF-{doc_id.split('-')[-1]}"
                    resolved_entries.append(resolved_entry)
                    continue

                # --- 2. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏ì‡∏µ Doc ID (Hash/UUID) ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ---
                if doc_id:
                    # A. ‡∏•‡∏≠‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å Map
                    if doc_id in self.doc_id_to_filename_map:
                        resolved_entry["filename"] = self.doc_id_to_filename_map[doc_id]
                        # ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß
                        resolved_entries.append(resolved_entry)
                        continue

                    # B. ‡∏ñ‡πâ‡∏≤‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ (Map Fail)
                    else:
                        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° (‡πÄ‡∏ä‡πà‡∏ô "Unknown" ‡∏´‡∏£‡∏∑‡∏≠ Hash.pdf)
                        is_generic_name = (
                            current_filename.lower() == "unknown" or
                            # ‚úÖ ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Hash/UUID 64 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•
                            re.match(r"^[0-9a-f]{64}(\.pdf|\.txt)?$", current_filename, re.IGNORECASE)
                        )
                        
                        if is_generic_name:
                            # ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå Fallback ‡∏ó‡∏µ‡πà‡∏™‡∏∑‡πà‡∏≠‡∏ß‡πà‡∏≤ Map ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ‡πÅ‡∏ï‡πà‡∏°‡∏µ ID
                            resolved_entry["filename"] = f"MAPPING-FAILED-{doc_id[:8]}..."
                            self.logger.warning(f"Failed to map doc_id {doc_id[:8]}... to filename. Using fallback.")
                        # else: ‡∏´‡∏≤‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏à‡∏≤‡∏Å metadata ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà Generic Name 
                        # (‡πÄ‡∏ä‡πà‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß) ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡πÇ‡∏î‡∏¢‡∏õ‡∏£‡∏¥‡∏¢‡∏≤‡∏¢

                # --- 3. ‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏°‡πà‡∏°‡∏µ Doc ID ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ (‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÄ‡∏õ‡πá‡∏ô Unknown) ---
                elif not doc_id:
                    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ doc_id ‡πÅ‡∏•‡∏∞ filename ‡πÄ‡∏õ‡πá‡∏ô Unknown
                    if current_filename.lower() == "unknown":
                        resolved_entry["filename"] = "MISSING-SOURCE-METADATA"
                        self.logger.error("Evidence found with no doc_id and generic filename.")
                
                # ‡πÄ‡∏û‡∏¥‡πà‡∏° entry ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ (‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà)
                resolved_entries.append(resolved_entry)

            return resolved_entries

    
    # -------------------- Contextual Rules Handlers (FIXED) --------------------
    def _load_contextual_rules_map(self) -> Dict[str, Dict[str, str]]:
        """
        Loads the contextual rules JSON file from the Tenant-Only path: 
        config/mapping/{tenant}/{tenant}_{enabler}_contextual_rules.json
        """
        
        # üéØ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ï‡∏≤‡∏° Pattern
        rules_filename = f"{self.config.tenant.lower()}_{self.enabler_id.lower()}_contextual_rules.json"

        # 1. TARGET PATH (Tenant-Only): config/mapping/{tenant}/
        base_dir_tenant_only = os.path.join(
            RUBRIC_CONFIG_DIR, 
            self.config.tenant.lower() # <--- ‡πÉ‡∏ä‡πâ TENANT ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
        )
        filepath_tenant_only = os.path.join(PROJECT_ROOT, base_dir_tenant_only, rules_filename)
        
        filepath = None
        if os.path.exists(filepath_tenant_only):
            filepath = filepath_tenant_only
            self.logger.info(f"‚úÖ Contextual Rules loaded from Tenant-Only path: {filepath_tenant_only}")

        
        if not filepath:
            self.logger.warning(
                f"‚ö†Ô∏è Contextual Rules map not found at expected path. Using empty map. "
                f"(Expected File: {rules_filename} in {base_dir_tenant_only})"
            )
            return {}

        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
                self.logger.info(f"‚úÖ Loaded Contextual Rules from {filepath}. ({len(data)} sub-criteria rules)")
                return data
        except json.JSONDecodeError as e:
            self.logger.error(f"‚ùå Failed to parse Contextual Rules JSON from {filepath}: {e}")
            return {}
        except Exception as e:
            self.logger.error(f"‚ùå Failed to load Contextual Rules from {filepath}: {e}")
            return {}

    def _collect_previous_level_evidences(self, sub_id: str, current_level: int) -> Dict[str, List[Dict]]:
            """
            ‡∏î‡∏∂‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô (Metadata + Text) ‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏à‡∏≤‡∏Å Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î 
            ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô Baseline Context ‡πÉ‡∏ô Level ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô (Sequential Mode)
            
            üéØ FIX: ‡πÄ‡∏û‡∏¥‡πà‡∏° Logic ‡∏Å‡∏≤‡∏£ Mapping Stable Doc ID ‡πÄ‡∏õ‡πá‡∏ô Chunk UUIDs ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Hydration Fail
            """
            collected = {}
            # ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏á‡∏ó‡∏µ‡πà‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô (‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£ import EVIDENCE_DOC_TYPES ‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß)
            try:
                from config.global_vars import EVIDENCE_DOC_TYPES
            except ImportError:
                EVIDENCE_DOC_TYPES = "evidence"

            # 1. ‡πÉ‡∏ä‡πâ evidence_map ‡∏Ç‡∏≠‡∏á engine ‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á (shared ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á Level)
            source_map = self.evidence_map
            source_name = "evidence_map (SEQ/PAR Main)"

            # 2. ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏° Metadata ‡∏à‡∏≤‡∏Å Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤
            for key, evidence_list in source_map.items():
                if key.startswith(f"{sub_id}.L") and isinstance(evidence_list, list):
                    try:
                        level_num = int(key.split(".L")[-1])
                        if level_num < current_level:
                            collected[key] = evidence_list
                    except (ValueError, IndexError):
                        continue

            # 3. HYDRATION: ‡∏î‡∏∂‡∏á Text ‡πÄ‡∏ï‡πá‡∏°‡∏à‡∏≤‡∏Å Chroma ‡∏î‡πâ‡∏ß‡∏¢ chunk_uuid
            vectorstore_manager = self.vectorstore_manager
            is_hydration_needed = vectorstore_manager is not None and collected

            if is_hydration_needed:
                # ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏° Stable Doc/Chunk IDs ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ô Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤
                all_uuids_raw = [
                    # ‡πÉ‡∏ä‡πâ chunk_uuid ‡∏´‡∏£‡∏∑‡∏≠ doc_id ‡∏ã‡∏∂‡πà‡∏á‡πÉ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠ Stable Doc ID (64 char)
                    str(
                        ev.get('chunk_uuid') or ev.get('stable_doc_uuid') or ev.get('doc_id')
                    ).strip()
                    for ev_list in collected.values()
                    for ev in ev_list
                    # ‡∏Å‡∏£‡∏≠‡∏á ID ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô None/‡∏ß‡πà‡∏≤‡∏á ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß/Hash ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡∏ï‡πâ‡∏ô
                    if (ev.get('chunk_uuid') or ev.get('stable_doc_uuid') or ev.get('doc_id')) 
                    and not str(ev.get('chunk_uuid') or ev.get('stable_doc_uuid') or ev.get('doc_id')).startswith(("TEMP-", "HASH-", "Unknown"))
                ]
                
                # ‡πÅ‡∏ö‡πà‡∏á ID ‡πÄ‡∏õ‡πá‡∏ô Stable Doc ID (64 ‡∏ï‡∏±‡∏ß) ‡πÅ‡∏•‡∏∞ Chunk ID ‡∏≠‡∏∑‡πà‡∏ô‡πÜ (‡∏ó‡∏µ‡πà‡∏°‡∏µ _index ‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡∏∑‡πà‡∏ô‡πÜ)
                all_uuids_stable_doc_only = list(set([uid for uid in all_uuids_raw if len(uid) == 64]))
                all_uuids_non_stable_doc = list(set([uid for uid in all_uuids_raw if len(uid) > 64])) # Chunk ID ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Ñ‡∏ß‡∏£‡∏¢‡∏≤‡∏ß‡∏Å‡∏ß‡πà‡∏≤ 64

                # üéØ NEW FIX: ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á Stable Doc ID (64-char) ‡πÄ‡∏õ‡πá‡∏ô Chunk UUIDs (64-char_index)
                chunk_uuids_for_chroma = []
                mapped_count = 0
                
                if not hasattr(vectorstore_manager, 'doc_id_map') or not vectorstore_manager.doc_id_map:
                    self.logger.warning("VSM Doc ID Map is missing! Using raw IDs for ChromaDB (may fail).")
                    chunk_uuids_for_chroma = all_uuids_raw # Fallback
                else:
                    # 1. ‡πÅ‡∏õ‡∏•‡∏á Stable Doc IDs
                    for input_id in all_uuids_stable_doc_only:
                        # üéØ ‡πÅ‡∏õ‡∏•‡∏á: ‡πÉ‡∏ä‡πâ Stable Doc ID ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Chunk UUIDs ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
                        mapped_info = vectorstore_manager.doc_id_map.get(input_id, {})
                        full_chunk_list = mapped_info.get('chunk_uuids', [])
                        chunk_uuids_for_chroma.extend(full_chunk_list)
                        mapped_count += 1
                    
                    # 2. ‡πÄ‡∏û‡∏¥‡πà‡∏° Chunk IDs ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏°‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á (‡∏¢‡∏≤‡∏ß‡∏Å‡∏ß‡πà‡∏≤ 64 ‡∏ï‡∏±‡∏ß)
                    chunk_uuids_for_chroma.extend(all_uuids_non_stable_doc)

                    if mapped_count > 0:
                        self.logger.info(f"VSM: Successfully mapped {mapped_count}/{len(all_uuids_stable_doc_only)} Stable Doc IDs to {len(chunk_uuids_for_chroma)} potential Chunk UUIDs.")
                    
                    # ‡∏•‡∏ö‡∏ã‡πâ‡∏≥‡πÅ‡∏•‡∏∞‡∏Å‡∏£‡∏≠‡∏á ID ‡∏ó‡∏µ‡πà‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤/‡∏™‡∏±‡πâ‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
                    chunk_uuids_for_chroma = list(set([uid for uid in chunk_uuids_for_chroma if len(uid) > 10]))

                
                # üéØ FIX A: ‡πÄ‡∏û‡∏¥‡πà‡∏° Log ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏à‡∏≥‡∏ô‡∏ß‡∏ô ID ‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡πÑ‡∏î‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
                total_metadata_chunks = sum(len(v) for v in collected.values())
                self.logger.info(f"DEBUG HYDRATION: Total evidence entries found in metadata (evidence_map): {total_metadata_chunks} items.")
                
                # ‡πÉ‡∏ä‡πâ chunk_uuids_for_chroma ‡πÅ‡∏ó‡∏ô all_uuids
                if not chunk_uuids_for_chroma:
                    self.logger.warning(
                        f"No valid Chunk UUIDs found for hydration. Raw IDs found: {len(all_uuids_raw)}. "
                        "Skipping hydration for previous levels."
                    )
                    full_chunks = []
                else:
                    collection_name = f"{EVIDENCE_DOC_TYPES.lower()}_{self.enabler_id.lower()}"

                    try:
                        self.logger.info(
                            f"üö® DEBUG: Attempting to HYDRATE {len(chunk_uuids_for_chroma)} unique chunks from Collection: '{collection_name}'. "
                            f"First 3 IDs: {chunk_uuids_for_chroma[:3]}" # ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á ID ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö (‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏Ñ‡∏ß‡∏£‡∏°‡∏µ _index)
                        )
                        # üö® ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ ChromaDB ‡∏î‡πâ‡∏ß‡∏¢ Chunk UUIDs ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
                        retrieved_lc_docs = vectorstore_manager.retrieve_by_chunk_uuids(chunk_uuids_for_chroma, collection_name)

                        full_chunks = []
                        for doc in retrieved_lc_docs:
                            chunk_dict = doc.metadata.copy()
                            chunk_dict["text"] = doc.page_content
                            # ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏ß‡πà‡∏≤ chunk_uuid ‡∏°‡∏µ‡∏Ñ‡πà‡∏≤
                            chunk_dict["chunk_uuid"] = (
                                doc.metadata.get("chunk_uuid")
                                or doc.metadata.get("id")
                                or doc.metadata.get("_id")
                            )
                            # ‡πÄ‡∏û‡∏¥‡πà‡∏° Stable Doc ID ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
                            chunk_dict["stable_doc_uuid"] = doc.metadata.get("stable_doc_uuid") or doc.metadata.get("doc_id")
                            
                            full_chunks.append(chunk_dict)

                        self.logger.info(f"Successfully hydrated {len(full_chunks)} chunks from previous levels")
                        
                        # üéØ FIX B: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö ID ‡∏ó‡∏µ‡πà‡∏´‡∏≤‡∏¢‡πÑ‡∏õ (Critical Debug)
                        retrieved_uuids = {c.get('chunk_uuid') for c in full_chunks if c.get('chunk_uuid')}
                        missing_uuids = set(chunk_uuids_for_chroma) - retrieved_uuids
                        
                        if missing_uuids:
                            self.logger.error(
                                f"‚ùå FATAL HYDRATION ISSUE: {len(missing_uuids)} chunks were requested but NOT FOUND in ChromaDB. "
                                f"Example missing IDs (3): {list(missing_uuids)[:3]}"
                            )
                            self.logger.error(f"Please verify: 1. ChromaDB is loaded from the correct base_path. 2. These IDs were INGESTED correctly.")
                            
                    except Exception as e:
                        self.logger.error(f"Failed to retrieve full chunks for baseline: {e}")
                        full_chunks = []

                # 4. ‡∏™‡∏£‡πâ‡∏≤‡∏á map ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏ß‡∏° Text ‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ö metadata ‡πÄ‡∏î‡∏¥‡∏°
                
                # üü¢ FIX: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ map keying ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏à‡∏≤‡∏Å Stable Doc ID ‡∏´‡∏£‡∏∑‡∏≠ Chunk ID ‡πÑ‡∏î‡πâ
                # Map retrieved full chunks by their full chunk ID (Primary Key)
                full_chunk_map_by_chunk_uuid = {c.get('chunk_uuid'): c for c in full_chunks if c.get('chunk_uuid')}
                # Map retrieved full chunks by their Stable Doc ID (Hash 64)
                full_chunk_map_by_stable_doc_id = {}
                for c in full_chunks:
                    s_doc_id = c.get('stable_doc_uuid') or c.get('doc_id')
                    if s_doc_id and s_doc_id not in full_chunk_map_by_stable_doc_id:
                        # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏Å‡πá‡∏ö chunk ‡πÅ‡∏£‡∏Å (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏î‡πâ‡∏ß‡∏¢ Stable Doc ID)
                        full_chunk_map_by_stable_doc_id[s_doc_id] = c 

                hydrated_collected = {}
                for key, ev_list in collected.items():
                    hydrated_list = []
                    for ev_metadata in ev_list:
                        # 5. ‡πÉ‡∏ä‡πâ ID ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÉ‡∏ô metadata (Stable Doc ID) ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
                        uuid_key = ev_metadata.get('chunk_uuid') or ev_metadata.get('stable_doc_uuid') or ev_metadata.get('doc_id')
                        
                        full_chunk = None

                        # 5.1 ‡∏•‡∏≠‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏î‡πâ‡∏ß‡∏¢ Full Chunk UUID (‡∏´‡∏≤‡∏Å L1 ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏°‡∏≤‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á)
                        if len(uuid_key) > 64:
                            full_chunk = full_chunk_map_by_chunk_uuid.get(uuid_key)
                        
                        # 5.2 ‡∏ñ‡πâ‡∏≤‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ ‡∏´‡∏£‡∏∑‡∏≠ ID ‡∏ó‡∏µ‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô Stable Doc ID (64 ‡∏ï‡∏±‡∏ß) ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ Stable Doc ID ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
                        if full_chunk is None and len(uuid_key) == 64:
                            full_chunk = full_chunk_map_by_stable_doc_id.get(uuid_key)

                        if full_chunk and full_chunk.get('text'):
                            combined = full_chunk.copy()
                            # üü¢ FIX: ‡πÉ‡∏ä‡πâ content/text ‡∏Ç‡∏≠‡∏á chunk ‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏°‡∏≤ (Full Text) ‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ metadata ‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏µ‡πà LLM ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å (Short Text)
                            combined['text'] = full_chunk['text'] # Full Text
                            
                            # ‡∏ô‡∏≥ metadata ‡∏™‡πà‡∏ß‡∏ô‡∏≠‡∏∑‡πà‡∏ô ‡πÜ ‡∏à‡∏≤‡∏Å ev_metadata ‡∏°‡∏≤‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ï (‡πÄ‡∏ä‡πà‡∏ô score, reason ‡∏à‡∏≤‡∏Å L1)
                            # ‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô 'text' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ Text ‡∏™‡∏±‡πâ‡∏ô‡∏à‡∏≤‡∏Å L1 ‡∏ó‡∏±‡∏ö Text ‡πÄ‡∏ï‡πá‡∏°
                            metadata_to_update = {k:v for k,v in ev_metadata.items() if k != 'text'}
                            combined.update(metadata_to_update)
                            
                            hydrated_list.append(combined)
                        else:
                            # ‡∏ñ‡πâ‡∏≤‡∏î‡∏∂‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡∏¢‡∏±‡∏á‡πÉ‡∏™‡πà metadata ‡πÄ‡∏õ‡∏•‡πà‡∏≤‡πÑ‡∏ß‡πâ (‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏´‡∏≤‡∏¢)
                            hydrated_list.append(ev_metadata)

                    if hydrated_list:
                        hydrated_collected[key] = hydrated_list

                collected = hydrated_collected

            else:
                if collected:
                    self.logger.info("Hydration skipped: vectorstore_manager not ready")

            # 6. Debug Log
            total_files = sum(len(v) for v in collected.values())
            self.logger.info(
                f"BASELINE LOADED ‚Üí Mode: {'SEQ' if self.is_sequential else 'PAR'} | "
                f"Source: {source_name} | "
                f"Found {len(collected)} levels | "
                f"Keys: {sorted(collected.keys())} | "
                f"Total files: {total_files}"
            )

            return collected    


    def _get_contextual_rules_prompt(self, sub_id: str, level: int) -> str:
        """
        Retrieves the specific Contextual Rule prompt for a given Sub-Criteria and Level,
        ‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á‡∏Å‡∏≤‡∏£ Inject ‡∏Å‡∏é L5 ‡∏û‡∏¥‡πÄ‡∏®‡∏© ‡∏´‡∏≤‡∏Å Level == 5
        """
        sub_id_rules = self.contextual_rules_map.get(sub_id)
        rule_text = ""
        
        # 1. ‡∏î‡∏∂‡∏á‡∏Å‡∏é‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏¢‡πà‡∏≠‡∏¢ (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
        if sub_id_rules:
            level_key = f"L{level}"
            specific_rule = sub_id_rules.get(level_key)
            if specific_rule:
                rule_text += f"\n--- ‡∏Å‡∏é‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏¢‡πà‡∏≠‡∏¢ ({sub_id} L{level}) ---\n‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Ñ‡∏ß‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ: {specific_rule}\n"
        
        # 2. **INJECT L5 SPECIAL RULE (Safe Injection)**
        # ‡πÉ‡∏™‡πà‡∏Å‡∏é Bonus 2.0 ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô L5 ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏ö‡∏Å‡∏ß‡∏ô L3/L4
        if level == 5:
            l5_bonus_rule = """
            \n--- L5 SPECIAL RULE (Innovation & Sustainability) ---
            * **L5 PASS Condition (‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö):** ‡∏´‡∏≤‡∏Å Level ‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠ **L5** ‡∏ó‡πà‡∏≤‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏° PDCA (P+D+C+A) ‡∏ï‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô L3/L4 ‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏ô‡∏û‡∏ö‡∏Å‡πà‡∏≠‡∏ô (‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î 8.0)
            * **‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç Bonus 2.0:** ‡∏´‡∏≤‡∏Å‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏° PDCA ‡πÑ‡∏î‡πâ **‚â• 7.0** **‡πÅ‡∏•‡∏∞** ‡∏ó‡πà‡∏≤‡∏ô‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠ **‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 1 ‡∏ä‡∏¥‡πâ‡∏ô** ‡πÉ‡∏ô Context ‡∏ó‡∏µ‡πà‡πÅ‡∏™‡∏î‡∏á‡∏ñ‡∏∂‡∏á:
                * (a) ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏• KM / ‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏° (Innovation Award)
                * (b) ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡∏ó‡∏µ‡πà‡∏ß‡∏±‡∏î‡πÑ‡∏î‡πâ (ROI, Productivity, Cost Saving)
                * (c) ‡∏Å‡∏≤‡∏£‡πÄ‡∏ú‡∏¢‡πÅ‡∏û‡∏£‡πà/‡∏Å‡∏≤‡∏£‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å (External Recognition/Publication)
            * **‡πÉ‡∏´‡πâ‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡πÉ‡∏´‡πâ Bonus Score 2.0 ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ **Score ‚â• 9.0** ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡πâ‡∏á **is_passed=true** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏∞‡∏ó‡πâ‡∏≠‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏•‡∏¥‡∏® (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£ Reset ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô)
            """
            rule_text += l5_bonus_rule
            
        return rule_text
    

    def _load_rubric(self) -> Dict[str, Any]:
        """
        Loads the SEAM rubric JSON file from the Tenant-Only path: 
        config/mapping/{tenant}/{tenant}_{enabler}_rubric.json
        """
        # 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ï‡∏≤‡∏° Pattern
        try:
            rubric_filename = RUBRIC_FILENAME_PATTERN.format(
                tenant=self.config.tenant.lower(), 
                enabler=self.enabler_id.lower()
            )
        except KeyError:
             # Fallback
            rubric_filename = f"{self.enabler_id.lower()}_rubric.json"
            self.logger.warning(f"RUBRIC_FILENAME_PATTERN is malformed. Defaulting to {rubric_filename}.")

        # 2. TARGET PATH (Tenant-Only): config/mapping/{tenant}/
        base_dir_tenant_only = os.path.join(
            RUBRIC_CONFIG_DIR, 
            self.config.tenant.lower() # <--- ‡πÉ‡∏ä‡πâ TENANT ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
        )
        filepath_tenant_only = os.path.join(PROJECT_ROOT, base_dir_tenant_only, rubric_filename)
        
        # 3. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå
        filepath = None
        if os.path.exists(filepath_tenant_only):
            filepath = filepath_tenant_only
        
        if not filepath:
            self.logger.error(
                f"Rubric file not found for {self.enabler_id} ({self.config.tenant}) "
                f"at expected path: {filepath_tenant_only}"
            )
            raise FileNotFoundError(f"Rubric not found at expected path: {filepath_tenant_only}")

        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
                self.logger.info(f"‚úÖ Loaded Rubric for {self.enabler_id} ({self.config.tenant}/{self.config.year}) from {filepath}")
            return data
        except json.JSONDecodeError as e:
            self.logger.error(f"‚ùå Failed to parse Rubric JSON from {filepath}: {e}")
            raise ValueError(f"Invalid Rubric JSON format in {filepath}: {e}")
        except Exception as e:
            self.logger.error(f"‚ùå Failed to load Rubric from {filepath}: {e}")
            raise
    
    # -------------------- Helper Function for Map Processing --------------------
    def _get_level_order_value(self, level_str: str) -> int:
        """Converts Level string ('L1', 'L5') to an integer for comparison."""
        try:
            return int(level_str.upper().replace('L', ''))
        except:
            return 0

    # -------------------- Persistent Mapping Handlers (FIXED) --------------------
    def _process_temp_map_to_final_map(self, temp_map: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Converts the temporary map into the final map format for saving, 
        and filters out temporary/unresolvable evidence IDs.
        """
        working_map = temp_map or self.temp_map_for_save or {}
        final_map_for_save = {}
        total_cleaned_items = 0

        for sub_level_key, evidence_list in working_map.items():
            if isinstance(evidence_list, dict):
                evidence_list = [evidence_list]
            elif not isinstance(evidence_list, list):
                logger.warning(f"[EVIDENCE] Skipping {sub_level_key}: not a list or dict")
                continue

            clean_list = []
            seen_ids = set()
            for ev in evidence_list:
                doc_id = ev.get("doc_id")
                
                if not doc_id:
                    continue
                
                # 1. FIX: ‡∏Å‡∏£‡∏≠‡∏á ID ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß (TEMP-) ‡∏≠‡∏≠‡∏Å
                if doc_id.startswith("TEMP-"):
                    # ID ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏õ‡∏•‡∏á‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô Stable Document ID ‡πÑ‡∏î‡πâ ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÉ‡∏ä‡πâ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ
                    logger.debug(f"[EVIDENCE] Filtering out unresolvable TEMP- ID: {doc_id} for {sub_level_key}.")
                    continue 
                
                # 2. Logic ‡πÄ‡∏î‡∏¥‡∏°: ‡∏Å‡∏£‡∏≠‡∏á HASH- (Placeholder) ‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ã‡πâ‡∏≥
                if doc_id.startswith("HASH-") or doc_id in seen_ids:
                    continue
                    
                seen_ids.add(doc_id)
                clean_list.append(ev)
                total_cleaned_items += 1 

            if clean_list:
                final_map_for_save[sub_level_key] = clean_list

        logger.info(f"[EVIDENCE] Processed {len(final_map_for_save)} sub-level keys with total {total_cleaned_items} evidence items")
        return final_map_for_save

    def _clean_map_for_json(self, data: Union[Dict, List, Set, Any]) -> Union[Dict, List, Any]:
        """Recursively converts objects that cannot be serialized (like sets) into lists."""
        if isinstance(data, dict):
            return {k: self._clean_map_for_json(v) for k, v in data.items()}
        elif isinstance(data, list):
            return [self._clean_map_for_json(v) for v in data]
        elif isinstance(data, set):
            return [self._clean_map_for_json(v) for v in data]
        return data

    def _clean_temp_entries(self, evidence_map: Dict[str, List[Dict]]) -> Dict[str, List[Dict]]:
        """
        ‡∏Å‡∏£‡∏≠‡∏á TEMP-, HASH-, ‡πÅ‡∏•‡∏∞ Unknown ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å evidence map ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        ‡πÉ‡∏ä‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏ï‡∏≠‡∏ô merge ‡πÅ‡∏•‡∏∞‡∏Å‡πà‡∏≠‡∏ô save ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î 100%
        """
        if not evidence_map:
            return {}

        cleaned_map = {}
        total_removed = 0
        total_unknown_fixed = 0

        for key, entries in evidence_map.items():
            valid_entries = []
            for entry in entries:
                doc_id = entry.get("doc_id", "")

                # 1. ‡∏Å‡∏£‡∏≠‡∏á TEMP- ‡πÅ‡∏•‡∏∞ HASH- ‡∏≠‡∏≠‡∏Å‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≤‡∏î
                if str(doc_id).startswith("TEMP-") or str(doc_id).startswith("HASH-"):
                    total_removed += 1
                    continue

                # 2. ‡∏ñ‡πâ‡∏≤ doc_id ‡∏ß‡πà‡∏≤‡∏á ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏•‡∏¢ ‚Üí ‡∏ó‡∏¥‡πâ‡∏á
                if not doc_id or doc_id == "Unknown":
                    total_removed += 1
                    continue

                # 3. ‡πÅ‡∏Å‡πâ filename ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô Unknown / None / ‡∏ß‡πà‡∏≤‡∏á
                filename = entry.get("filename", "").strip()
                if not filename or filename == "Unknown" or filename.lower() == "unknown_file.pdf":
                    # ‡πÉ‡∏ä‡πâ doc_id ‡∏™‡∏±‡πâ‡∏ô ‡πÜ ‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß (‡∏î‡∏π‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤)
                    short_id = doc_id[:8]
                    entry["filename"] = f"‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á_{short_id}.pdf"
                    total_unknown_fixed += 1
                else:
                    # ‡πÄ‡∏≠‡∏≤ path ‡∏≠‡∏≠‡∏Å ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÅ‡∏Ñ‡πà‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå
                    entry["filename"] = os.path.basename(filename)

                valid_entries.append(entry)

            if valid_entries:
                cleaned_map[key] = valid_entries
            else:
                logger.debug(f"[CLEAN] Key {key} ‡∏Å‡∏•‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏ß‡πà‡∏≤‡∏á‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏£‡∏≠‡∏á ‚Üí ‡∏ñ‡∏π‡∏Å‡∏•‡∏ö‡∏≠‡∏≠‡∏Å")

        logger.info(f"[CLEANUP] ‡∏•‡∏ö TEMP-/HASH- ‡∏≠‡∏≠‡∏Å {total_removed} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | "
                    f"‡πÅ‡∏Å‡πâ Unknown filename {total_unknown_fixed} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | "
                    f"‡πÄ‡∏´‡∏•‡∏∑‡∏≠ {len(cleaned_map)} keys")

        return cleaned_map

    def _save_evidence_map(self, map_to_save: Optional[Dict[str, List[Dict[str, Any]]]] = None):
            """
            ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å evidence map ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢ 100% - Atomic + Lock + Clean + Sort + Score
            """
            map_file_path = self.evidence_map_path
            lock_path = map_file_path + ".lock"
            tmp_path = None

            logger.info(f"[EVIDENCE] Saving evidence map ‚Üí {map_file_path}")

            try:
                with FileLock(lock_path, timeout=60):
                    logger.debug("[EVIDENCE] Lock acquired.")

                    if map_to_save is not None:
                        final_map_to_write = map_to_save
                    else:
                        # 1. ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡∏≠‡∏á‡πÄ‡∏Å‡πà‡∏≤‡∏à‡∏≤‡∏Å‡∏î‡∏¥‡∏™‡∏Å‡πå
                        existing_map = self._load_evidence_map(is_for_merge=True) or {}
                        runtime_map = deepcopy(self.evidence_map)

                        # 2. Merge: ‡πÄ‡∏Å‡πà‡∏≤ + ‡πÉ‡∏´‡∏°‡πà (‡πÑ‡∏°‡πà‡∏ó‡∏±‡∏ö ‡πÅ‡∏ï‡πà‡∏£‡∏ß‡∏°)
                        final_map_to_write = existing_map
                        for key, entries in runtime_map.items():
                            if key not in final_map_to_write:
                                final_map_to_write[key] = []
                                
                            # üü¢ FIX 1: Deduplicate ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Chunk UUID (‡∏´‡∏£‡∏∑‡∏≠ doc_id ‡πÄ‡∏õ‡πá‡∏ô Fallback)
                            existing_ids = {
                                e.get("chunk_uuid", e.get("doc_id", "N/A")) 
                                for e in final_map_to_write[key]
                            }
                            
                            # üü¢ FIX 1: ‡πÉ‡∏ä‡πâ Logic ID ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö
                            new_entries = [
                                e for e in entries 
                                if e.get("chunk_uuid", e.get("doc_id", "N/A")) not in existing_ids
                            ]
                            final_map_to_write[key].extend(new_entries)

                        # 3. ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ (TEMP-, HASH-, Unknown)
                        final_map_to_write = self._clean_temp_entries(final_map_to_write)

                        # 4. ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ key ‡∏à‡∏≤‡∏Å relevance_score ‡∏™‡∏π‡∏á ‚Üí ‡∏ï‡πà‡∏≥ (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
                        for key, entries in final_map_to_write.items():
                            if entries and "relevance_score" in entries[0]:
                                entries.sort(
                                    key=lambda x: x.get("relevance_score", 0.0),
                                    reverse=True
                                )

                    if not final_map_to_write:
                        logger.warning("[EVIDENCE] Nothing to save.")
                        return

                    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå
                    os.makedirs(os.path.dirname(map_file_path), exist_ok=True)

                    # Atomic write
                    with tempfile.NamedTemporaryFile(
                        mode='w', delete=False, encoding="utf-8", dir=os.path.dirname(map_file_path)
                    ) as tmp_file:
                        cleaned_for_json = self._clean_map_for_json(final_map_to_write)
                        json.dump(cleaned_for_json, tmp_file, indent=4, ensure_ascii=False)
                        tmp_path = tmp_file.name

                    shutil.move(tmp_path, map_file_path)
                    tmp_path = None

                    # ‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ (‡∏™‡∏ß‡∏¢‡∏°‡∏≤‡∏Å)
                    total_keys = len(final_map_to_write)
                    total_items = sum(len(v) for v in final_map_to_write.values())
                    file_size_kb = os.path.getsize(map_file_path) / 1024

                    logger.info(f"[EVIDENCE] Evidence map saved successfully!")
                    logger.info(f"   Keys: {total_keys} | Items: {total_items} | Size: ~{file_size_kb:.1f} KB")

                    # ‡πÇ‡∏ä‡∏ß‡πå Top 1 ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ sub-criteria (‡∏™‡∏∏‡∏î‡∏¢‡∏≠‡∏î‡∏°‡∏≤‡∏Å)
                    preview = []
                    for key in sorted(final_map_to_write.keys())[:5]:  # ‡πÅ‡∏™‡∏î‡∏á‡πÅ‡∏Ñ‡πà 5 ‡∏≠‡∏±‡∏ô‡πÅ‡∏£‡∏Å
                        entries = final_map_to_write[key]
                        if entries:
                            top = entries[0]
                            score = top.get("relevance_score", "-")
                            preview.append(f"{key}: {top['filename'][:50]} ({score})")
                    if preview:
                        logger.info(f"   Top evidence preview ‚Üí {', '.join(preview[:3])}{'...' if len(preview)>3 else ''}")

            except TimeoutError:
                logger.critical(f"[EVIDENCE] Lock timeout! Another process may be stuck: {lock_path}")
                raise
            except Exception as e:
                logger.critical("[EVIDENCE] FATAL SAVE ERROR")
                logger.exception(e)
                raise
            finally:
                if tmp_path and os.path.exists(tmp_path):
                    try: os.unlink(tmp_path)
                    except: pass
                logger.debug(f"[EVIDENCE] Lock released: {lock_path}")


    def _load_evidence_map(self, is_for_merge: bool = False):
        """
        Safe load of persistent evidence map. Always returns dict.
        is_for_merge: If True, suppresses "No existing evidence map" INFO log.
        """
        path = self.evidence_map_path

        if not os.path.exists(path):
            if not is_for_merge:
                self.logger.info("[EVIDENCE] No existing evidence map ‚Äì starting empty.")
            return {}

        try:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
            if not is_for_merge:
                self.logger.info(f"[EVIDENCE] Loaded evidence map: {len(data)} entries")
            return data

        except Exception as e:
            self.logger.error(f"[EVIDENCE] Failed to load evidence map from {path}: {e}")
            return {}

    def _set_mock_handlers(self, mode: str):
        """Replaces real LLM/RAG functions with mock versions."""
        if mode == "control" or mode == "random":
            if hasattr(seam_mocking, 'evaluate_with_llm_CONTROLLED_MOCK'):
                self.llm_evaluator = seam_mocking.evaluate_with_llm_CONTROLLED_MOCK
            if hasattr(seam_mocking, 'retrieve_context_with_filter_MOCK'):
                self.rag_retriever = seam_mocking.retrieve_context_with_filter_MOCK
            if hasattr(seam_mocking, 'create_structured_action_plan_MOCK'):
                self.action_plan_generator = seam_mocking.create_structured_action_plan_MOCK
            if hasattr(seam_mocking, 'set_mock_control_mode'):
                seam_mocking.set_mock_control_mode(mode == "control") 

        logger.warning(f"Engine is running in MOCK mode: {mode}")

    def _get_pdca_phase(self, level: int) -> str:
        """Helper to get the PDCA phase string from the map."""
        return PDCA_PHASE_MAP.get(level, f"Level {level} Requirement")
    

    def _get_level_constraint_prompt(self, level: int) -> str:
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á Prompt Constraint ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Å‡∏±‡∏ö‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ß‡∏∏‡∏í‡∏¥‡∏†‡∏≤‡∏ß‡∏∞‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô
        """
        if level == 1:
            return "‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏ñ‡∏∂‡∏á '‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢/‡∏ß‡∏¥‡∏™‡∏±‡∏¢‡∏ó‡∏±‡∏®‡∏ô‡πå', '‡∏Å‡∏≤‡∏£‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå', '‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏ó‡∏≥‡∏Å‡∏£‡∏≠‡∏ö‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á', ‡∏´‡∏£‡∏∑‡∏≠ '‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£' ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (L1-Focus)"
        elif level == 2:
            return "‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ô‡πâ‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞ '‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô', '‡∏Å‡∏≤‡∏£‡∏Ç‡∏±‡∏ö‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô', '‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡∏ò‡∏£‡∏£‡∏°', ‡∏´‡∏£‡∏∑‡∏≠ '‡∏Å‡∏≤‡∏£‡∏°‡∏µ‡∏™‡πà‡∏ß‡∏ô‡∏£‡πà‡∏ß‡∏°' ‡∏ï‡∏≤‡∏°‡πÅ‡∏ú‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (L2-Focus)"
        elif level == 3:
            # üö® HARD RULE: ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏ä‡πâ L3 Logic (Check/Act Focus) ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏° Context ‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡∏°‡πà
            return """
‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î (HARD RULE: L3 CHECK/ACT FOCUS):
1. ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô L3 ‡∏ô‡∏µ‡πâ **‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô '‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö (Check)' ‡πÅ‡∏•‡∏∞ '‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á (Act)' ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô**
2. ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÅ‡∏•‡πâ‡∏ß: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏ï‡πâ‡∏ô‡∏Ç‡∏≠‡∏á Context ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î (Priority 1)
3. ‡∏´‡∏≤‡∏Å‡∏û‡∏ö‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ **[SIMULATED_L3_EVIDENCE]** ‡πÉ‡∏´‡πâ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô **‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö** ‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠‡πÑ‡∏î‡πâ‡∏ã‡∏∂‡πà‡∏á‡∏ñ‡∏π‡∏Å‡∏™‡∏£‡∏∏‡∏õ‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô Check/Act ‡∏à‡∏£‡∏¥‡∏á (‡∏à‡∏±‡∏î‡πÄ‡∏õ‡πá‡∏ô Priority 1)
4. ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô Plan ‡πÅ‡∏•‡∏∞ Do ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏ï‡∏≠‡∏ô‡∏ó‡πâ‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á Context **‡∏´‡πâ‡∏≤‡∏°‡∏ô‡∏≥‡∏°‡∏≤‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤** ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à **FAIL** ‡∏´‡∏≤‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô Check/Act ‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô
5. ‡∏´‡∏≤‡∏Å‡∏Ç‡∏≤‡∏î‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô **Check** ‡∏´‡∏£‡∏∑‡∏≠ **Act** ‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠ (‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏à‡∏≤‡∏Å Simulated Evidence ‡∏´‡∏£‡∏∑‡∏≠‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á) ‡πÉ‡∏´‡πâ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÄ‡∏õ‡πá‡∏ô **‚ùå FAIL** ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô L3 ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡∏à‡∏£‡∏¥‡∏á
(L3-Focus: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏° ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏• ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á)
"""
        elif level == 4:
            return "‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ñ‡∏∂‡∏á '‡∏Å‡∏≤‡∏£‡∏ö‡∏π‡∏£‡∏ì‡∏≤‡∏Å‡∏≤‡∏£', '‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á', ‡∏´‡∏£‡∏∑‡∏≠ '‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå' ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (L4-Focus)"
        elif level == 5:
            return "‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ñ‡∏∂‡∏á '‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°', '‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡πà‡∏≤‡∏ó‡∏≤‡∏á‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à', ‡∏´‡∏£‡∏∑‡∏≠ '‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß' ‡πÇ‡∏î‡∏¢‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô (L5-Focus)"
        else:
            return ""
        

    def _classify_pdca_phase_for_chunk(
        self, 
        chunk_text: str
    ) -> Literal["Plan", "Do", "Check", "Act", "Other"]:
        """
        ‡πÉ‡∏ä‡πâ LLM ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏∞‡∏¢‡∏∞‡πÉ‡∏î‡∏£‡∏∞‡∏¢‡∏∞‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏Ç‡∏≠‡∏á PDCA ‡∏´‡∏£‡∏∑‡∏≠ 'Other'
        """
        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏ü‡∏™ PDCA
        pdca_phases_th = ["‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô", "‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥", "‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö", "‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á"]
        
        # 1. System Prompt ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢: ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö JSON 100%
        system_prompt = (
            "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô PDCA Cycle\n"
            "‡∏†‡∏≤‡∏£‡∏Å‡∏¥‡∏à‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô ‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏±‡∏î‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ß‡πà‡∏≤‡πÄ‡∏ô‡πâ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡πÉ‡∏î‡∏Ç‡∏≠‡∏á PDCA\n"
            f"‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏´‡∏ô‡∏∂‡πà‡∏á‡πÉ‡∏ô: {', '.join(pdca_phases_th)} ‡∏´‡∏£‡∏∑‡∏≠ '‡∏≠‡∏∑‡πà‡∏ô‡πÜ'\n\n"
            "‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡∏î‡πâ‡∏ß‡∏¢ **JSON Object ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô** ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö:\n"
            "{\"phase\": \"‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô\"}\n"
            "‡∏´‡∏£‡∏∑‡∏≠ {\"phase\": \"‡∏≠‡∏∑‡πà‡∏ô‡πÜ\"}\n"
            "‡∏´‡πâ‡∏≤‡∏°‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏≠‡∏Å JSON ‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≤‡∏î"
        )

        # 2. User Prompt: ‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô + ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
        user_prompt = (
            f"‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô:\n\"\"\"\n{chunk_text.strip()}\n\"\"\"\n\n"
            "‡∏Ñ‡∏≥‡∏ô‡∏¥‡∏¢‡∏≤‡∏°‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏ü‡∏™:\n"
            "- ‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô: ‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢, ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå, ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢, ‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô, ‡∏Ñ‡∏ì‡∏∞‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£\n"
            "- ‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥: ‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£, ‡∏ù‡∏∂‡∏Å‡∏≠‡∏ö‡∏£‡∏°, ‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£, ‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏£‡∏∞‡∏ö‡∏ö, ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á\n"
            "- ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö: ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏ú‡∏•, ‡∏ß‡∏±‡∏î‡∏ú‡∏•, ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô, ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏†‡∏≤‡∏¢‡πÉ‡∏ô, ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n"
            "- ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á: ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç, ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á, ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡πÉ‡∏´‡∏°‡πà, Lesson Learned, ‡∏õ‡∏¥‡∏î‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á\n\n"
            "‡∏ï‡∏≠‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞ JSON:"
        )
        
        try:
            raw_response = _fetch_llm_response(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                temperature=0.0,  # ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å! ‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô
                max_retries=2,
                llm_executor=self.llm
            )

            if not raw_response:
                return "Other"

            # ‡∏î‡∏∂‡∏á JSON ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏Ç‡πá‡∏á‡πÅ‡∏Å‡∏£‡πà‡∏á (‡πÉ‡∏ä‡πâ _robust_extract_json ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß!)
            parsed = _robust_extract_json(raw_response)
            
            # ‡∏ñ‡πâ‡∏≤ _robust_extract_json ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ fallback ‡∏î‡πâ‡∏ß‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ‡πÄ‡∏ö‡∏™‡∏¥‡∏Å
            if not parsed or not isinstance(parsed, dict):
                # ‡∏•‡∏≠‡∏á‡∏î‡∏∂‡∏á‡∏î‡πâ‡∏ß‡∏¢ regex ‡∏á‡πà‡∏≤‡∏¢ ‡πÜ
                import re
                match = re.search(r'"phase"\s*:\s*"([^"]+)"', raw_response, re.IGNORECASE)
                if match:
                    phase_th = match.group(1).strip()
                else:
                    phase_th = "‡∏≠‡∏∑‡πà‡∏ô‡πÜ"
            else:
                phase_th = parsed.get("phase", parsed.get("classification", "‡∏≠‡∏∑‡πà‡∏ô‡πÜ"))
                phase_th = str(phase_th).strip()

            # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô Literal ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
            mapping = {
                "‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô": "Plan",
                "‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥": "Do",
                "‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö": "Check",
                "‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á": "Act",
                "‡∏≠‡∏∑‡πà‡∏ô‡πÜ": "Other",
                "‡∏≠‡∏∑‡πà‡∏ô": "Other",
                "other": "Other"
            }
            result = mapping.get(phase_th, "Other")
            
            self.logger.debug(f"PDCA Classification: '{phase_th}' ‚Üí {result}")
            return result

        except Exception as e:
            self.logger.error(f"PDCA Classification failed: {e}\nRaw: {raw_response[:200]}")
            return "Other"

    # -------------------- Statement Preparation & Filtering Helpers --------------------
    def _flatten_rubric_to_statements(self) -> List[Dict[str, Any]]:
        """
        Transforms the hierarchical rubric structure loaded in self.rubric
        into a flat list of statements ready for assessment.
        """
        if not self.rubric:
            self.logger.warning("Cannot flatten rubric: self.rubric is empty.")
            return []
            
        data = deepcopy(self.rubric)
        extracted_list = []
        
        if not isinstance(data, dict):
             self.logger.error("Rubric data structure is invalid (expected dict of criteria).")
             return []
             
        for criteria_id, criteria_data in data.items():
            # üéØ FIX 1: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Criteria Data
            if not isinstance(criteria_data, dict):
                self.logger.warning(f"Skipping malformed criteria entry: {criteria_id} (not a dict).")
                continue
                
            sub_criteria_map = criteria_data.get('subcriteria', {})
            criteria_name = criteria_data.get('name')
            
            # üéØ FIX 2: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Sub-criteria Map
            if not isinstance(sub_criteria_map, dict):
                 self.logger.warning(f"Skipping criteria {criteria_id}: 'subcriteria' is not a dictionary.")
                 continue

            for sub_id, sub_data in sub_criteria_map.items():
                
                # üéØ FIX 3: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ sub_data ‡πÄ‡∏õ‡πá‡∏ô Dictionary ‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô TypeError)
                if not isinstance(sub_data, dict):
                    self.logger.warning(
                        f"Skipping malformed sub-criteria entry: {criteria_id}.{sub_id} "
                        f"is not a dictionary (found type: {type(sub_data).__name__})."
                    )
                    continue
                
                # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤ Metadata ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
                sub_data['criteria_id'] = criteria_id
                sub_data['criteria_name'] = criteria_name
                sub_data['sub_id'] = sub_id 
                sub_data['sub_criteria_name'] = sub_data.get('name', criteria_name + ' sub')
                if 'weight' not in sub_data:
                    sub_data['weight'] = criteria_data.get('weight', 0)
                extracted_list.append(sub_data)

        # Re-check and re-sort levels
        final_list = []
        for sub_criteria in extracted_list: 
            
            # üéØ FIX 4: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á Level ‡∏à‡∏≤‡∏Å Dict ‡∏ó‡∏µ‡πà‡∏°‡∏µ Key ‡πÄ‡∏õ‡πá‡∏ô String ‡πÄ‡∏õ‡πá‡∏ô List
            if "levels" in sub_criteria and isinstance(sub_criteria["levels"], dict):
                levels_list = []
                for level_str, statement in sub_criteria["levels"].items():
                    try:
                        # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÅ‡∏õ‡∏•‡∏á Level Key ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô Integer
                        level_int = int(level_str)
                        if isinstance(statement, str):
                            levels_list.append({"level": level_int, "statement": statement})
                        else:
                            self.logger.warning(f"Level {level_str} statement in {sub_criteria.get('sub_id')} is not a string.")
                    except ValueError:
                        self.logger.error(f"Invalid level key '{level_str}' found in {sub_criteria.get('sub_id', 'UNKNOWN_ID')}. Skipping.")
                        continue
                        
                sub_criteria["levels"] = levels_list
            
            # ‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏£‡∏∞‡∏î‡∏±‡∏ö
            if "levels" in sub_criteria and isinstance(sub_criteria["levels"], list):
                 sub_criteria["levels"].sort(key=lambda x: x.get("level", 0))
                 final_list.append(sub_criteria)
            else:
                 self.logger.warning(f"Sub-criteria {sub_criteria.get('sub_id', 'UNKNOWN_ID')} missing 'levels' list.")


        return final_list

    def _load_initial_evidence_info(self) -> Set[str]:
        """Retrieves the set of all available stable document IDs in the VectorStore."""
        if self.config.mock_mode != "none":
            return {"00000000-0000-0000-0000-000000000001"}
        return set() 

    def _apply_strict_filter(self, statements: List[Dict[str, Any]], available_evidence_ids: Set[str]) -> List[Dict[str, Any]]:
        """
        Filters out statements that have no specified evidence_doc_ids 
        that match any ID found in the available_evidence_ids set.
        """
        if not available_evidence_ids:
            logger.warning("Strict Filter bypassed: No available evidence IDs loaded.")
            return statements

        filtered_statements = []
        for stmt in statements:
            required_ids = set(stmt.get('evidence_doc_ids', []))
            
            if not required_ids or required_ids.isdisjoint(available_evidence_ids):
                 logger.debug(f"Strict Filter: Skipping {stmt['sub_id']} L{stmt['level']} (No evidence match)")
                 continue
            
            filtered_statements.append(stmt)
            
        return filtered_statements
    

    # -------------------- Evidence Classification Helper --------------------

    def _get_pdca_blocks_from_evidences(
        self, 
        top_evidences: List[Dict[str, Any]], 
        level: int # level ‡∏¢‡∏±‡∏á‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ logging/context ‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï
    ) -> Tuple[str, str, str, str, str]:
        """
        Groups retrieved evidence chunks into PDCA phases based on the 'pdca_tag' 
        generated by the LLM classifier. This replaces the old index-based heuristic.

        Args:
            top_evidences: List of retrieved evidence dictionaries, each containing 'text' and 'pdca_tag'.

        Returns:
            A tuple of aggregated context strings: (plan_blocks, do_blocks, check_blocks, act_blocks, other_blocks)
        """
        logger = logging.getLogger(__name__)

        # 1. Initialize groupings
        pdca_groups = defaultdict(list)
        
        # 2. Group chunks based on the 'pdca_tag'
        for i, doc in enumerate(top_evidences):
            # üìå Use the classified tag directly. Fallback to 'Other' if tag is missing.
            tag = doc.get('pdca_tag', 'Other')
            
            # üìå Format the chunk before appending to the group list
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÄ‡∏î‡∏¥‡∏° (i+1) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ trace ‡∏´‡∏≤‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
            formatted_chunk = f"--- [Chunk {i+1} | Tag: {tag}] ---\n{doc.get('text', '')}\n"
            pdca_groups[tag].append(formatted_chunk)

        # 3. Aggregate groups into single strings
        plan_blocks = "\n\n".join(pdca_groups.get('Plan', []))
        do_blocks = "\n\n".join(pdca_groups.get('Do', []))
        check_blocks = "\n\n".join(pdca_groups.get('Check', []))
        act_blocks = "\n\n".join(pdca_groups.get('Act', []))
        other_blocks = "\n\n".join(pdca_groups.get('Other', []))

        logger.debug(
            f"  > PDCA Blocks Grouped (L{level}): "
            f"P={len(pdca_groups.get('Plan', []))}, D={len(pdca_groups.get('Do', []))}, "
            f"C={len(pdca_groups.get('Check', []))}, A={len(pdca_groups.get('Act', []))}, "
            f"Other={len(pdca_groups.get('Other', []))}"
        )

        return plan_blocks, do_blocks, check_blocks, act_blocks, other_blocks

    def _get_mapped_uuids_and_priority_chunks(
                self, 
                sub_id: str, 
                level: int, 
                statement_text: str, 
                level_constraint: str,
                vectorstore_manager: Optional['VectorStoreManager']
            ) -> Tuple[List[str], List[Dict[str, Any]]]:
        """
        1. Gathers all PASSED Stable Chunk UUIDs (doc_id) from L1 up to L[level-1]. 
        2. Fetches limited priority RAG chunks (Hybrid Retrieval) 
        based on the gathered Chunk UUIDs.
        
        Returns: (mapped_chunk_uuids: list[str], priority_docs: list[dict])
        """
        
        all_priority_items: List[Dict[str, Any]] = [] 
        
        # üìå DEBUG: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Ç‡∏≠‡∏á evidence_map ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡∏∂‡∏á
        logger.info(f"DEBUG: EVIDENCE MAP KEYS BEFORE PRIORITY SEARCH (L{level}): {sorted(self.evidence_map.keys())}")
        
        # 1. ‡∏ß‡∏ô‡∏ã‡πâ‡∏≥‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà PASS ‡∏à‡∏≤‡∏Å Level 1 ‡∏à‡∏ô‡∏ñ‡∏∂‡∏á Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ (L1 -> L[level - 1])
        for prev_level in range(1, level): 
            prev_map_key = f"{sub_id}.L{prev_level}"
            
            # üéØ ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å self.evidence_map (‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÉ‡∏ô Sequential)
            items_from_map = self.evidence_map.get(prev_map_key, [])
            all_priority_items.extend(items_from_map)

        
        # 2. ‡πÅ‡∏õ‡∏•‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô Chunk UUID (String) ‡πÅ‡∏•‡∏∞ Dedup
        # üü¢ FIX 2: ‡πÄ‡∏ô‡πâ‡∏ô‡πÉ‡∏ä‡πâ Chunk UUID ‡∏´‡∏£‡∏∑‡∏≠ Stable Doc UUID ‡πÄ‡∏õ‡πá‡∏ô ID ‡∏´‡∏•‡∏±‡∏Å
        doc_ids_for_dedup: List[str] = [
            (
                item.get('chunk_uuid') 
                or item.get('stable_doc_uuid') # <-- ‡πÄ‡∏ô‡πâ‡∏ô Stable Doc UUID
                or item.get('doc_id')
            )
            for item in all_priority_items
            if isinstance(item, dict) and (
                item.get('chunk_uuid') or item.get('stable_doc_uuid') or item.get('doc_id')
            )
        ]

        mapped_chunk_uuids: List[str] = list(set([uid for uid in doc_ids_for_dedup if uid is not None])) # ‡∏Å‡∏£‡∏≠‡∏á None ‡∏≠‡∏≠‡∏Å
        num_historical_chunks = len(mapped_chunk_uuids)

        priority_docs = [] 
        
        if num_historical_chunks > 0:
            levels_logged = f"L1-L{level-1}" if level > 1 else "L0 (Should not happen)"
            logger.critical(f"üß≠ DEBUG: Priority Search initiated with {num_historical_chunks} historical Chunk UUIDs ({levels_logged}).") 
            logger.info(f"‚úÖ Hybrid Mapping: Found {num_historical_chunks} pre-mapped Chunk UUIDs from {levels_logged} for {sub_id}. Prioritizing these.")
            
            if vectorstore_manager:
                try:
                    # Assuming enhance_query_for_statement is available
                    rag_queries_for_vsm = enhance_query_for_statement(
                        statement_text=statement_text,
                        sub_id=sub_id, 
                        statement_id=sub_id, 
                        level=level, 
                        enabler_id=self.enabler_id,
                        focus_hint=level_constraint 
                    )
                    
                    doc_type = self.doc_type 
                    
                    # 3.1 ‡∏î‡∏∂‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ï‡∏≤‡∏° Chunk UUIDs ‡∏ó‡∏µ‡πà‡∏û‡∏ö
                    retrieved_docs_result = retrieve_context_by_doc_ids(
                        doc_uuids=mapped_chunk_uuids, # <-- ‡πÉ‡∏ä‡πâ Chunk/Stable Doc UUIDs
                        doc_type=doc_type,
                        enabler=self.enabler_id,
                        vectorstore_manager=vectorstore_manager
                    )
                    
                    initial_priority_chunks: List[Dict[str, Any]] = retrieved_docs_result.get("top_evidences", [])
                    
                    if initial_priority_chunks:
                        # Rerank ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Chunk ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
                        reranker = get_global_reranker() 
                        rerank_query = rag_queries_for_vsm[0] 
                        
                        # ‡∏™‡∏£‡πâ‡∏≤‡∏á LcDocument list ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Rerank (‡∏ï‡πâ‡∏≠‡∏á‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤ LcDocument)
                        lc_docs_for_rerank = [
                            LcDocument(
                                page_content=d.get('content') or d.get('text', ''), 
                                metadata={
                                    **d, 
                                    'relevance_score': 1.0 
                                }
                            ) 
                            for d in initial_priority_chunks
                        ]
                        
                        if reranker and hasattr(reranker, 'compress_documents'):
                            reranked_docs = reranker.compress_documents(
                                query=rerank_query,
                                documents=lc_docs_for_rerank,
                                top_n=self.PRIORITY_CHUNK_LIMIT 
                            )

                            # === ‡∏ß‡∏¥‡∏ä‡∏≤‡∏°‡∏≤‡∏£‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏Ü‡πà‡∏≤ 0.0000 ‡∏ï‡∏•‡∏≠‡∏î‡∏Å‡∏≤‡∏• ===
                            # ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô relevance_score ‡∏Å‡∏•‡∏±‡∏ö‡∏•‡∏á metadata ‡∏Å‡πà‡∏≠‡∏ô

                            # üõë [‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç V3] ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏û‡∏∂‡πà‡∏á reranker.scores ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏ú‡πà‡∏≤‡∏ô reranked_docs 
                            # ‡πÅ‡∏•‡∏∞‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏î‡∏∂‡∏á score ‡∏à‡∏≤‡∏Å metadata ‡∏Ç‡∏≠‡∏á‡∏°‡∏±‡∏ô
                            
                            scores = []
                            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Reranker ‡πÑ‡∏î‡πâ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô score ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô Document.metadata ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                            for doc in reranked_docs:
                                # score ‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏à‡∏≤‡∏Å reranker ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô metadata ‡∏†‡∏≤‡∏¢‡πÉ‡∏ï‡πâ key 'relevance_score' ‡∏´‡∏£‡∏∑‡∏≠ 'score'
                                score = doc.metadata.get('relevance_score') or doc.metadata.get('score', 0.0)
                                scores.append(float(score))
                                # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô relevance_score ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô metadata
                                doc.metadata["relevance_score"] = float(score)

                            if scores: # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ Scores ‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡πÑ‡∏î‡πâ
                                
                                # üü¢ [‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç] 1. Log ‡∏™‡∏£‡∏∏‡∏õ‡∏î‡πâ‡∏ß‡∏¢ INFO ‡πÅ‡∏•‡∏∞ CRITICAL
                                num_reranked = len(reranked_docs)
                                logger.info(f"‚ú® Reranking success ({sub_id} L{level}) ‚Üí Prioritized {num_reranked} chunks. Logging top scores:")
                                # üéØ Log 2 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏ó‡πà‡∏≤‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£!
                                logger.critical(f"‚ú® RERANK SCORE LOG (PRIORITY CHUNKS) ({sub_id} L{level}) ‚Üí Logging top {min(5, num_reranked)} scores:")
                                
                                for i in range(len(reranked_docs)):
                                    doc = reranked_docs[i]
                                    score = scores[i] # ‡πÉ‡∏ä‡πâ score ‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß
                                    
                                    # üü¢ [‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç] 2. ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡πÅ‡∏•‡∏∞ Log ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏î‡πâ‡∏ß‡∏¢ CRITICAL
                                    if i < 5: 
                                        filename = doc.metadata.get('filename', doc.metadata.get('source_filename', 'N/A'))
                                        doc_id_full = doc.metadata.get('doc_id', doc.metadata.get('chunk_uuid', 'N/A'))
                                        
                                        # ‡∏ï‡∏±‡∏î Chunk ID ‡πÉ‡∏´‡πâ‡∏™‡∏±‡πâ‡∏ô‡∏•‡∏á
                                        if len(doc_id_full) > 8 and '_' in doc_id_full:
                                            doc_id_short = doc_id_full.split('_')[0][:8]
                                        else:
                                            doc_id_short = doc_id_full[:8]
                                            
                                        logger.critical(f"  > Rerank #{i+1}: {doc_id_short} ({filename}) | Score: {float(score):.4f}")

                            # ‡πÅ‡∏õ‡∏•‡∏á‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô dict ‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ 'score' ‡∏ó‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡πÄ‡∏Å‡πà‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô
                            priority_docs = []
                            for d in reranked_docs:
                                # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å metadata ‡πÄ‡∏î‡∏¥‡∏°
                                item = dict(d.metadata)
                                # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
                                item.update({
                                    'content': d.page_content,
                                    'text': d.page_content,
                                    # ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î: score ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏≤‡∏ó‡πâ‡∏≤‡∏¢‡∏™‡∏∏‡∏î ‡πÅ‡∏•‡∏∞‡∏ó‡∏±‡∏ö‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô
                                    'score': float(d.metadata.get('relevance_score', 0.0)),
                                    'relevance_score': float(d.metadata.get('relevance_score', 0.0))
                                })
                                priority_docs.append(item)
                            # ========================================

                            logger.critical(f"DEBUG: Limited and prioritized {len(priority_docs)} chunks from {num_historical_chunks} mapped UUIDs.")
                        else:
                            # fallback ‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏°‡πà‡∏°‡∏µ reranker
                            priority_docs = initial_priority_chunks[:self.PRIORITY_CHUNK_LIMIT]
                            # ‡πÅ‡∏°‡πâ fallback ‡∏Å‡πá‡∏¢‡∏±‡∏á‡πÉ‡∏™‡πà score ‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö
                            for item in priority_docs:
                                if 'score' not in item:
                                    item['score'] = 0.0

                except Exception as e:
                    logger.error(f"Error fetching/reranking priority chunks for {sub_id}: {e}")
                    priority_docs = [] 
        
        # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ Chunk UUIDs ‡πÅ‡∏•‡∏∞ Chunks ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏î‡∏∂‡∏á‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÅ‡∏•‡πâ‡∏ß
        return mapped_chunk_uuids, priority_docs


    # -------------------- Calculation Helpers (ADDED) --------------------
    def _calculate_weighted_score(self, highest_full_level: int, weight: int) -> float:
        """
        Calculates the weighted score based on the highest full level achieved.
        Score is calculated by: (Level / 5) * Weight
        """
        MAX_LEVEL_CALC = 5  
        
        if highest_full_level <= 0:
            return 0.0
        
        level_for_calc = min(highest_full_level, MAX_LEVEL_CALC)
        score = (level_for_calc / MAX_LEVEL_CALC) * weight
        return score

    def _calculate_overall_stats(self, target_sub_id: str = "all"):
            """
            Calculates the total weighted score, total possible weight, and overall maturity score/level.
            """
            total_weighted_score = 0.0
            total_possible_weight = 0.0
            assessed_count = 0
            
            for result in self.final_subcriteria_results:
                if target_sub_id.lower() != "all" and result.get('sub_criteria_id') != target_sub_id:
                    continue

                weighted_score = result.get('weighted_score', 0.0)
                weight = result.get('weight', 0)
                
                total_weighted_score += weighted_score
                total_possible_weight += weight
                assessed_count += 1
                
            overall_maturity_score_avg = 0.0
            overall_maturity_level = "N/A"
            overall_progress_percent = 0.0 # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏ó‡∏µ‡πà 0.0
            
            if total_possible_weight > 0:
                overall_progress_percent = total_weighted_score / total_possible_weight
                
                MAX_LEVEL_STATS = 5 
                overall_maturity_score_avg = overall_progress_percent * MAX_LEVEL_STATS 

                # üü¢ FIX: Completed Logic for Maturity Level Determination
                if overall_maturity_score_avg >= 4.5:
                    overall_maturity_level = "L5"
                elif overall_maturity_score_avg >= 3.5:
                    overall_maturity_level = "L4"
                elif overall_maturity_score_avg >= 2.5:
                    overall_maturity_level = "L3"
                elif overall_maturity_score_avg >= 1.5:
                    overall_maturity_level = "L2"
                elif overall_maturity_score_avg >= 0.5:
                    overall_maturity_level = "L1"
                else:
                    overall_maturity_level = "L0"
            
            self.total_stats = {
                "Overall Maturity Score (Avg.)": overall_maturity_score_avg,
                "Overall Maturity Level (Weighted)": overall_maturity_level,
                "Number of Sub-Criteria Assessed": assessed_count,
                "Total Weighted Score Achieved": total_weighted_score,
                "Total Possible Weight": total_possible_weight,
                "Overall Progress Percentage (0.0 - 1.0)": overall_progress_percent,
                "percentage_achieved_run": overall_progress_percent * 100,
                "total_subcriteria": len(self.rubric),
                "target_level": self.config.target_level
            }
            return self.total_stats
            
    def _export_results(self, results: dict, enabler: str, sub_criteria_id: str, target_level: int, export_dir: str = "assessment_results") -> str:
        """
        Exports the final assessment results to a JSON file.
        
        Args:
            results: The dictionary containing the final assessment summary and results.
            enabler: The enabler ID (e.g., KM).
            sub_criteria_id: The specific sub-criteria ID being run (e.g., 2.2).
            target_level: The target level for the assessment.
            export_dir: The directory to save the output file.
            
        Returns:
            The path to the saved JSON file.
        """
        if not os.path.exists(export_dir):
            os.makedirs(export_dir)
            
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå: assessment_results_KM_2.2_YYYYMMDD_HHMMSS.json
        file_name = f"assessment_results_{enabler}_{sub_criteria_id}_{timestamp}.json"
        full_path = os.path.join(export_dir, file_name)

        # Note: results dict should contain 'summary' and 'sub_criteria_results' keys
        # Update summary fields based on the engine data
        results['summary']['enabler'] = enabler
        results['summary']['sub_criteria_id'] = sub_criteria_id
        results['summary']['target_level'] = target_level
        results['summary']['Number of Sub-Criteria Assessed'] = len(results['sub_criteria_results'])

        try:
            with open(full_path, 'w', encoding='utf-8') as f:
                # ‡πÉ‡∏ä‡πâ indent=4 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢
                json.dump(results, f, ensure_ascii=False, indent=4)
            
            logging.info(f"üíæ Successfully exported final results to: {full_path}")
            return full_path
        
        except Exception as e:
            logging.error(f"‚ùå Failed to export results to {full_path}: {e}")
            return ""
        
    # -------------------- Multiprocessing Worker Method --------------------
    def _assess_single_sub_criteria_worker(self, args) -> Tuple[List[Dict[str, Any]], Dict[str, Any], Dict[str, List[Dict[str, Any]]]]:
        """
        Worker function for multiprocessing. 
        ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏¢‡πà‡∏≠‡∏¢‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î ‡πÇ‡∏î‡∏¢‡∏î‡∏∂‡∏á‡∏ö‡∏£‡∏¥‡∏ö‡∏ó (Evidence) ‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ LLM ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô
        
        Args:
            args: tuple(statement_data, engine_config_dict)
        Returns:
            - raw_results_for_sub: list of final results for each level
            - final_sub_result: summary of sub-criteria evaluation
            - level_evidences: dict of evidences to merge later in main process (‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Metadata)
        """
        statement_data, engine_config_dict = args

        # Unpack engine config
        llm_executor = engine_config_dict['llm_executor']
        enabler = engine_config_dict['enabler']
        doc_type = engine_config_dict['doc_type']
        vectorstore_manager = engine_config_dict['vectorstore_manager']
        mapped_uuids = engine_config_dict.get('mapped_uuids')
        priority_docs_input = engine_config_dict.get('priority_docs_input')
        contextual_rules_prompt = engine_config_dict.get('contextual_rules_prompt', "")
        
        # üü¢ NEW: Unpack Rerank Gate Constants
        # NOTE: RERANK_THRESHOLD ‡πÅ‡∏•‡∏∞ MAX_EVI_STR_CAP ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÉ‡∏ä‡πâ‡πÉ‡∏ô _calculate_evidence_strength_cap()
        
        # ‡∏î‡∏∂‡∏á Baseline Context ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å Hydrate (‡∏°‡∏µ Text) ‡∏à‡∏≤‡∏Å Main Process
        previous_levels_evidence_full = engine_config_dict.get('previous_levels_evidence_full', []) 

        # Statement metadata
        level = int(statement_data.get("level", 0))
        statement_text = statement_data.get("statement", "")
        sub_criteria_name = statement_data.get("sub_criteria_name", "")
        pdca_phase = statement_data.get("pdca_phase", "")
        level_constraint = statement_data.get("level_constraint", "")
        sub_id = statement_data.get("sub_criteria_id", statement_data.get("sub_id", ""))

        # Determine retrieval/evaluation functions (Assuming existence of these helpers)
        if level <= 2:
            retrieval_func = self.retrieve_context_for_low_levels 
            evaluation_func = self.evaluate_with_llm_low_level
            top_k = 5
        else:
            retrieval_func = self.retrieve_context_with_filter
            evaluation_func = self.evaluate_with_llm
            top_k = 10

        # Build enhanced query for RAG (Assuming existence of this helper)
        rag_query_list = self.enhance_query_for_statement(
            statement_text=statement_text,
            sub_id=sub_id,
            statement_id=statement_data.get('statement_id', sub_id),
            level=level,
            enabler_id=enabler,
            focus_hint=level_constraint,
            llm_executor=llm_executor
        )
        rag_query = rag_query_list[0] if rag_query_list else statement_text

        # Retrieval
        # NOTE: Assuming vectorstore_manager is part of 'self' or passed correctly to the helper
        self.logger.info(f" ¬† > Starting assessment for {sub_id} L{level} (Attempt: 1)...") # <-- Re-added the missing log
        retrieval_result = retrieval_func(
            query=rag_query,
            doc_type=doc_type,
            enabler=enabler,
            vectorstore_manager=vectorstore_manager,
            top_k=top_k,
            mapped_uuids=mapped_uuids,
            priority_docs_input=priority_docs_input,
            sub_id=sub_id,
            level=level
        )

        top_evidences = retrieval_result.get("top_evidences", [])
        aggregated_context = retrieval_result.get("aggregated_context", "")

        # üü¢ DEBUG: ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÅ‡∏™‡∏î‡∏á Key ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Ç‡∏≠‡∏á retrieval_result (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å!)
        self.logger.critical(f"üîë DEBUG L{level}: RETRIEVAL KEYS (All): {list(retrieval_result.keys())}") 
        if top_evidences: 
            self.logger.critical(
                f"DEBUG L{level}: Inspecting first document (Type: {type(top_evidences[0])})"
                f" | EVIDENCE KEYS: {list(top_evidences[0].keys())}"
            )
        # üü¢ END DEBUG

        # ------------------ üü¢ Relevant Score Gate Logic (NEW) ------------------
        # üéØ FIX: ‡πÉ‡∏ä‡πâ Helper Function ‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡πÅ‡∏•‡∏∞‡∏Å‡∏≥‡∏´‡∏ô‡∏î Cap
        evi_cap_data = self._calculate_evidence_strength_cap(
            top_evidences=top_evidences,
            level=level,
        )
        
        # ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å Helper ‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡∏ï‡πà‡∏≠
        highest_rerank_score = evi_cap_data['highest_rerank_score']
        max_score_source = evi_cap_data['max_score_source']
        is_capped = evi_cap_data['is_capped']
        max_evi_str_for_prompt = evi_cap_data['max_evi_str_for_prompt']
        
        # NOTE: ‡∏Å‡∏≤‡∏£ Log ‡∏Å‡∏≤‡∏£ Cap ‡πÑ‡∏î‡πâ‡∏¢‡πâ‡∏≤‡∏¢‡πÑ‡∏õ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô _calculate_evidence_strength_cap ‡πÅ‡∏•‡πâ‡∏ß

        # ------------------ END: Relevant Score Gate Logic ------------------
        
        # Build multichannel context (Assuming existence of this helper)
        channels = self.build_multichannel_context_for_level(
            level=level, 
            top_evidences=top_evidences, 
            previous_levels_evidence=previous_levels_evidence_full 
        )

        # Evaluate statement (Assuming existence of these helpers)
        evaluation_result = evaluation_func(
            context=channels.get("direct_context", "") or aggregated_context,
            sub_criteria_name=sub_criteria_name,
            level=level,
            statement_text=statement_text,
            sub_id=sub_id,
            llm_executor=llm_executor,
            pdca_phase=pdca_phase,
            level_constraint=level_constraint,
            contextual_rules_prompt=contextual_rules_prompt,
            baseline_summary=channels.get("baseline_summary", ""),
            aux_summary=channels.get("aux_summary", ""),
            # üü¢ Pass max_evidence_strength
            max_evidence_strength=max_evi_str_for_prompt
        )

        # Summarize context for report (Assuming existence of this helper)
        summary_result = self.create_context_summary_llm(
            context=channels.get("direct_context", "") or aggregated_context,
            sub_criteria_name=sub_criteria_name,
            level=level,
            sub_id=sub_id,
            llm_executor=llm_executor
        )

        # Prepare evidences to return (for main process to merge)
        level_key = f"{sub_id}.L{level}"
        level_evidences = {
            level_key: [
                {
                    "doc_id": ev.get("doc_id"),
                    "filename": ev.get("source_filename") or ev.get("source") or ev.get("filename"),
                    "relevance_score": ev.get("rerank_score", ev.get("score", 0.0)), # ‡πÉ‡∏ä‡πâ rerank_score ‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏à‡∏≤‡∏Å top_evidences ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á (‡∏ã‡∏∂‡πà‡∏á‡∏ñ‡∏π‡∏Å patch ‡∏°‡∏≤‡∏à‡∏≤‡∏Å retrieval_func ‡πÅ‡∏•‡πâ‡∏ß)
                    "mapper_type": "AI_GENERATED", 
                    "timestamp": datetime.now().isoformat()
                }
                for ev in top_evidences if ev.get("doc_id")
            ]
        }

        # Final result dict
        final_sub_result = {
            "sub_criteria_id": sub_id,
            "sub_criteria_name": sub_criteria_name,
            "level": level,
            "statement": statement_text,
            "pdca_phase": pdca_phase,
            "llm_result": evaluation_result,
            "used_doc_ids": [d.get("doc_id") for d in top_evidences if d.get("doc_id")],
            "channels_debug": channels.get("debug_meta", {}),
            "summary": summary_result,
            
            # üü¢ Relevant Score Gate Metadata
            "max_relevant_score": evi_cap_data['highest_rerank_score'],
            "max_relevant_source": evi_cap_data['max_score_source'],
            "is_evidence_strength_capped": evi_cap_data['is_capped'],
            "max_evidence_strength_used": evi_cap_data['max_evi_str_for_prompt'],
        }

        raw_results_for_sub = [final_sub_result]

        return raw_results_for_sub, final_sub_result, level_evidences

    def _run_sub_criteria_assessment_worker(
            self,
            sub_criteria: Dict[str, Any],
        ) -> Tuple[Dict[str, Any], Dict[str, List[Dict[str, Any]]]]:
            """
            ‡∏£‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô L1-L5 ‡πÅ‡∏ö‡∏ö sequential ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö sub-criteria ‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏ï‡∏±‡∏ß
            ‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á evidence map ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡πÉ‡∏´‡πâ main process ‡∏£‡∏ß‡∏°
            """
            sub_id = sub_criteria['sub_id']
            sub_criteria_name = sub_criteria['sub_criteria_name']
            sub_weight = sub_criteria.get('weight', 0)

            MAX_L1_ATTEMPTS = 2
            highest_full_level = 0
            is_passed_current_level = True
            raw_results_for_sub_seq: List[Dict[str, Any]] = []

            self.logger.info(f"[WORKER START] Assessing Sub-Criteria: {sub_id} - {sub_criteria_name} (Weight: {sub_weight})")

            # ‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï temp_map_for_save ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ worker ‡∏ô‡∏µ‡πâ (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Parallel!)
            self.temp_map_for_save = {}

            # 1. Loop ‡∏ú‡πà‡∏≤‡∏ô‡∏ó‡∏∏‡∏Å Level (L1 ‚Üí L5)
            for statement_data in sub_criteria.get('levels', []):
                level = statement_data.get('level')
                if level is None or level > self.config.target_level:
                    continue

                # Dependency check: ‡∏ñ‡πâ‡∏≤ level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ fail ‚Üí cap ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà
                dependency_failed = level > 1 and not is_passed_current_level
                previous_level = level - 1
                persistence_key = f"{sub_id}.L{previous_level}"
                sequential_chunk_uuids = self.evidence_map.get(persistence_key, [])

                level_result = {}
                level_temp_map: List[Dict[str, Any]] = []

                # --- ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å _run_single_assessment (‡∏£‡∏±‡∏ö 2 ‡∏Ñ‡πà‡∏≤: result, temp_map) ---
                if level >= 3:
                    # L3-L5: ‡πÉ‡∏ä‡πâ RetryPolicy
                    wrapper = self.retry_policy.run(
                        fn=lambda attempt: self._run_single_assessment(
                            sub_criteria=sub_criteria,
                            statement_data=statement_data,
                            vectorstore_manager=self.vectorstore_manager,
                            sequential_chunk_uuids=sequential_chunk_uuids
                        ),
                        level=level,
                        statement=statement_data.get('statement', ''),
                        context_blocks={"sequential_chunk_uuids": sequential_chunk_uuids},
                        logger=self.logger
                    )

                    # ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: wrapper.result ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô tuple (result, temp_map)
                    if isinstance(wrapper, RetryResult) and wrapper.result is not None:
                        level_result = wrapper.result
                        level_temp_map = level_result.get("temp_map_for_level", []) # <-- ‡∏î‡∏∂‡∏á List Evidence ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤
                    else:
                        level_result = {}
                        level_temp_map = []

                else:
                    # L1-L2: ‡∏•‡∏≠‡∏á‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î 2 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á
                    for attempt in range(MAX_L1_ATTEMPTS):
                        level_result = self._run_single_assessment(
                            sub_criteria=sub_criteria,
                            statement_data=statement_data,
                            vectorstore_manager=self.vectorstore_manager,
                            sequential_chunk_uuids=sequential_chunk_uuids
                        )
                        level_temp_map = level_result.get("temp_map_for_level", []) # <-- ‡∏î‡∏∂‡∏á List Evidence ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤
                        if level_result.get('is_passed', False):
                            break

                # ‡πÉ‡∏ä‡πâ result ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏°‡∏≤
                result_to_process = level_result or {}
                result_to_process.setdefault("used_chunk_uuids", [])

                # ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô pass/fail ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ (‡∏£‡∏ß‡∏° dependency cap)
                is_passed_llm = result_to_process.get('is_passed', False)
                is_passed_final = is_passed_llm and not dependency_failed

                result_to_process['is_passed'] = is_passed_final
                result_to_process['is_capped'] = is_passed_llm and not is_passed_final
                result_to_process['pdca_score_required'] = get_correct_pdca_required_score(level)

                # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å evidence ‡∏•‡∏á temp_map_for_save ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠ PASS ‡∏à‡∏£‡∏¥‡∏á
                if is_passed_final and level_temp_map and isinstance(level_temp_map, list):
                
                    # üü¢ FIX 1: ‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ Lookup Filename ‡∏à‡∏≤‡∏Å doc_id_to_filename_map ‡∏ñ‡πâ‡∏≤ filename ‡πÄ‡∏õ‡πá‡∏ô 'Unknown' ‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡∏≤‡∏î‡∏´‡∏≤‡∏¢
                    resolved_temp_map = []
                    for ev in level_temp_map:
                        filename = ev.get("filename")
                        doc_id = ev.get("doc_id")
                        
                        # ‡∏´‡∏≤‡∏Å filename ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á (e.g., 'Unknown' ‡∏´‡∏£‡∏∑‡∏≠ None) ‡πÉ‡∏´‡πâ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏° lookup
                        if not filename or filename == "Unknown":
                            # ‡πÉ‡∏ä‡πâ‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á Engine ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å doc_id
                            # NOTE: self.doc_id_to_filename_map ‡∏Ñ‡∏∑‡∏≠ map ‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ß‡πâ‡∏ï‡∏≠‡∏ô Engine Init
                            resolved_filename = self.doc_id_to_filename_map.get(doc_id) 
                            if resolved_filename:
                                ev['filename'] = resolved_filename
                                self.logger.debug(f"Resolved 'Unknown' filename for {doc_id} to {resolved_filename}")
                            else:
                                self.logger.warning(f"Could not find filename for doc_id: {doc_id} in mapping. Keeping filename: {filename}")
                        
                        resolved_temp_map.append(ev)
                        
                    current_key = f"{sub_id}.L{level}"
                    self.temp_map_for_save[current_key] = resolved_temp_map # ‡πÉ‡∏ä‡πâ resolved_temp_map ‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏•‡πâ‡∏ß
                    self.logger.info(f"[EVIDENCE SAVED] {current_key} ‚Üí {len(resolved_temp_map)} chunks")

                    # üéØ FIX SEQUENTIAL DEPENDENCY: ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï self.evidence_map ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡πÉ‡∏ô Sequential Mode
                    if self.is_sequential:
                        self.evidence_map[current_key] = resolved_temp_map # ‡πÉ‡∏ä‡πâ resolved_temp_map ‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏•‡πâ‡∏ß
                        self.logger.info(f"[SEQUENTIAL UPDATE] {current_key} added to engine's main evidence_map for L{level+1} dependency.")
                    # END FIX

                # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö level ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
                is_passed_current_level = is_passed_final

                # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏•‡∏á raw results
                result_to_process.setdefault("level", level)
                result_to_process["execution_index"] = len(raw_results_for_sub_seq)
                raw_results_for_sub_seq.append(result_to_process)

                # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï highest level
                if is_passed_final:
                    highest_full_level = level
                else:
                    self.logger.info(f"[WORKER STOP] {sub_id} failed at L{level}. Highest achieved: L{highest_full_level}")
                    # break  # ‡∏´‡∏¢‡∏∏‡∏î‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡πÄ‡∏°‡∏∑‡πà‡∏≠ fail 
                    pass

            # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏• sub-criteria
            weighted_score = self._calculate_weighted_score(highest_full_level, sub_weight)
            num_passed = sum(1 for r in raw_results_for_sub_seq if r.get("is_passed", False))

            sub_summary = {
                "num_statements": len(raw_results_for_sub_seq),
                "num_passed": num_passed,
                "num_failed": len(raw_results_for_sub_seq) - num_passed,
                "pass_rate": round(num_passed / len(raw_results_for_sub_seq), 4) if raw_results_for_sub_seq else 0.0
            }

            final_sub_result = {
                "sub_criteria_id": sub_id,
                "sub_criteria_name": sub_criteria_name,
                "highest_full_level": highest_full_level,
                "weight": sub_weight,
                "target_level_achieved": highest_full_level >= self.config.target_level,
                "weighted_score": weighted_score,
                "action_plan": [],
                "raw_results_ref": raw_results_for_sub_seq,
                "sub_summary": sub_summary,
            }

            # final_temp_map = self.temp_map_for_save  # ‡∏™‡πà‡∏á‡∏Å‡∏•‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á dict
            # ‡πÄ‡∏õ‡πá‡∏ô
            final_temp_map = {}
            if self.is_sequential:
                # ‡πÉ‡∏ô sequential ‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ self.evidence_map ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß
                # ‡πÅ‡∏ï‡πà‡∏™‡πà‡∏á snapshot ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
                for key in self.evidence_map:
                    if key.startswith(sub_criteria['sub_id'] + "."):
                        final_temp_map[key] = self.evidence_map[key]
            else:
                final_temp_map = self.temp_map_for_save.copy()

            self.logger.info(f"[WORKER END] {sub_id} | Highest: L{highest_full_level} | Evidence keys: {len(final_temp_map)}")
            self.logger.debug(f"Evidence keys returned: {list(final_temp_map.keys())}")

            return final_sub_result, final_temp_map

    def _calculate_evidence_strength_cap(
        self,
        top_evidences: List[Union[Dict[str, Any], Any]],  # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á dict ‡πÅ‡∏•‡∏∞ LcDocument
        level: int,
    ) -> Dict[str, Any]:
        """
        Relevant Score Gate ‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô DEBUG FINAL: ‡∏î‡∏∂‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏à‡∏≤‡∏Å metadata, top-level key/attribute, ‡πÅ‡∏•‡∏∞ Regex fallback ‡∏ó‡∏µ‡πà‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°
        """

        highest_rerank_score = 0.0
        max_score_source = "N/A"

        score_keys = [
            "relevance_score", "rerank_score", "score", 
            "_rerank_score_force", "_rerank_score", 
            "Score", "RelevanceScore"
        ]
        
        # üí° ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤ Threshold ‡πÅ‡∏•‡∏∞ Cap ‡∏à‡∏≤‡∏Å Attribute ‡∏Ç‡∏≠‡∏á Class
        threshold = getattr(self, "RERANK_THRESHOLD", 0.5) 
        cap_value = getattr(self, "MAX_EVI_STR_CAP", 3.0)
        
        # üí° Fallback: ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Attribute ‡πÉ‡∏´‡πâ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å config/global_vars
        if not isinstance(threshold, (int, float)):
            from config.global_vars import RERANK_THRESHOLD as G_RERANK_THRESHOLD
            from config.global_vars import MAX_EVI_STR_CAP as G_MAX_EVI_STR_CAP
            threshold = G_RERANK_THRESHOLD
            cap_value = G_MAX_EVI_STR_CAP


        for doc in top_evidences:
            
            # # -------------------- DEBUGGING BLOCK (START) --------------------
            # if doc is top_evidences[0]:
            #     self.logger.critical(f"DEBUG L{level}: Inspecting first document (Type: {type(doc)})")
                
            #     if isinstance(doc, dict):
            #         content = doc.get("text", "")
            #         tail_content = content[-200:] if len(content) > 200 else content
            #         self.logger.critical(f"DEBUG L{level}: Dict keys: {list(doc.keys())}")
            #         self.logger.critical(f"DEBUG L{level}: END OF 'text' content (last 200 chars): \n***\n{tail_content}\n***")
            #     else:
            #         try:
            #             doc_attrs = [attr for attr in dir(doc) if not attr.startswith('_') and not callable(getattr(doc, attr))]
            #             self.logger.critical(f"DEBUG L{level}: Doc public attributes (potential score location): {doc_attrs}")
            #         except:
            #             self.logger.critical(f"DEBUG L{level}: Cannot inspect attributes of this object type.")
            # # -------------------- DEBUGGING BLOCK (END) --------------------
            
            page_content = ""
            metadata = {}
            
            # ‚îÄ‚îÄ‚îÄ 1. ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô metadata + content ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏∏‡∏Å‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á) ‚îÄ‚îÄ‚îÄ
            if isinstance(doc, dict):
                metadata = doc.get("metadata", {}) 
                page_content = doc.get("page_content", "") or doc.get("text", "") or doc.get("content", "")
            else:
                metadata = getattr(doc, "metadata", {})
                page_content = getattr(doc, "page_content", "") or getattr(doc, "text", "")

            # ‚îÄ‚îÄ‚îÄ 2. ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô (‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö top-level key/attribute ‡πÅ‡∏•‡∏∞ metadata) ‚îÄ‚îÄ‚îÄ
            current_score = 0.0
            
            for key in score_keys:
                score_val = None
                
                if key in metadata:
                    score_val = metadata[key]
                
                if score_val is None:
                    if isinstance(doc, dict):
                        score_val = doc.get(key)
                    else:
                        score_val = getattr(doc, key, None)

                if score_val is not None:
                    try:
                        current_score = float(score_val)
                        if current_score > 0:
                            break
                    except (ValueError, TypeError):
                        continue
            
            # ‚îÄ‚îÄ‚îÄ 3. Fallback: ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å‡∏ó‡πâ‡∏≤‡∏¢ content (Aggressive Regex) ‚îÄ‚îÄ‚îÄ
            if current_score == 0.0 and page_content and isinstance(page_content, str):
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ re import ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ
                try:
                    import re
                    tail = page_content[-1000:]
                    patterns = [
                        r"Relevance[ :]+([0-9]*\.?[0-9]+)",
                        r"Score[ :]+([0-9]*\.?[0-9]+)",
                        r"Re:[ ]*([0-9]*\.?[0-9]+)",
                        r"\[Relevance: ([0-9]*\.?[0-9]+)\]",
                        r"\[Score: ([0-9]*\.?[0-9]+)\]",
                        r"rerank_score['\"]?\s*:\s*([0-9]*\.?[0-9]+)",
                        r"\|\s*([0-9]*\.?[0-9]+)\s*\|",
                        r"\s+([0-9]\.[0-9]+)$",
                    ]
                    for pat in patterns:
                        m = re.search(pat, tail, re.IGNORECASE)
                        if m:
                            try:
                                current_score = float(m.group(1))
                                break
                            except:
                                continue
                except ImportError:
                    # ‡∏ñ‡πâ‡∏≤ re ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å import, ‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏õ
                    pass


            # ‚îÄ‚îÄ‚îÄ 4. ‡∏î‡∏∂‡∏á source ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‚îÄ‚îÄ‚îÄ
            source = (
                metadata.get("source_filename") or metadata.get("filename") or
                doc.get("source_filename") or doc.get("filename") or 
                doc.get("source") or doc.get("doc_id") or
                "N/A"
            )

            # ‚îÄ‚îÄ‚îÄ 5. ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î ‚îÄ‚îÄ‚îÄ
            if current_score > highest_rerank_score:
                highest_rerank_score = current_score
                max_score_source = source

        # ‚îÄ‚îÄ‚îÄ 6. Relevant Score Gate + Log ‚îÄ‚îÄ‚îÄ
        
        # NOTE: ‡πÉ‡∏ä‡πâ threshold ‡πÅ‡∏•‡∏∞ cap_value ‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏°‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
        if highest_rerank_score < threshold:
            max_evi_str_for_prompt = cap_value
            is_capped = True
            self.logger.warning(
                f"üö® Evi Str CAPPED L{level}: "
                f"Rerank {highest_rerank_score:.4f} (‡∏à‡∏≤‡∏Å '{max_score_source}') "
                f"< {threshold} ‚Üí ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏ó‡∏µ‡πà {cap_value}"
            )
        else:
            max_evi_str_for_prompt = 10.0
            is_capped = False
            self.logger.info(
                f"‚úÖ Evi Str FULL L{level}: "
                f"Rerank {highest_rerank_score:.4f} (‡∏à‡∏≤‡∏Å '{max_score_source}') "
                f">= {threshold} ‚Üí ‡∏õ‡∏•‡πà‡∏≠‡∏¢‡πÄ‡∏ï‡πá‡∏° 10.0"
            )

        return {
            "is_capped": is_capped,
            "max_evi_str_for_prompt": max_evi_str_for_prompt,
            "highest_rerank_score": round(float(highest_rerank_score), 4),
            "max_score_source": max_score_source,
        }
        

    def run_assessment(
            self,
            target_sub_id: str = "all",
            export: bool = False,
            vectorstore_manager: Optional['VectorStoreManager'] = None,
            sequential: bool = False,
            document_map: Optional[Dict[str, str]] = None, # üü¢ FIX: ‡∏£‡∏±‡∏ö document_map
        ) -> Dict[str, Any]:
        """
        Main runner ‡∏Ç‡∏≠‡∏á Assessment Engine
        ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á Parallel ‡πÅ‡∏•‡∏∞ Sequential 100%
        ‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏ß‡πà‡∏≤ evidence_map ‡∏Ñ‡∏£‡∏ö‡∏ó‡∏∏‡∏Å‡∏Å‡∏£‡∏ì‡∏µ
        """

        start_ts = time.time()
        self.is_sequential = sequential

        # ============================== 1. Filter Rubric ==============================
        if target_sub_id.lower() == "all":
            sub_criteria_list = self._flatten_rubric_to_statements() # üü¢ NOTE: ‡πÉ‡∏ä‡πâ _flatten_rubric_to_statements ‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏•‡πâ‡∏ß
        else:
            # üü¢ NOTE: ‡πÉ‡∏ä‡πâ _flatten_rubric_to_statements ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á List ‡∏Å‡πà‡∏≠‡∏ô
            all_statements = self._flatten_rubric_to_statements()
            sub_criteria_list = [
                s for s in all_statements if s.get('sub_id') == target_sub_id
            ]
            if not sub_criteria_list:
                self.logger.error(f"Sub-Criteria ID '{target_sub_id}' not found in rubric.")
                return {"error": f"Sub-Criteria ID '{target_sub_id}' not found."}

        # Reset states
        self.raw_llm_results = []
        self.final_subcriteria_results = []

        # ‡πÇ‡∏´‡∏•‡∏î evidence map ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß (‡πÑ‡∏°‡πà clear!)
        if os.path.exists(self.evidence_map_path):
            loaded = self._load_evidence_map()
            if loaded:
                self.evidence_map = loaded
                self.logger.info(f"Resumed from existing evidence map: {len(self.evidence_map)} keys")
            else:
                self.evidence_map = {}
        else:
            self.evidence_map = {}

        if not sequential:
            self.logger.info("[PARALLEL MODE] Starting parallel assessment...")

        run_parallel = (target_sub_id.lower() == "all") and not (sequential or export)

        # ============================== 2. Run Assessment ==============================
        if run_parallel:
            # --------------------- PARALLEL MODE ---------------------
            self.logger.info("Starting Parallel Assessment with Multiprocessing...")
            worker_args = [(
                sub_data,
                self.config.enabler,
                self.config.target_level,
                self.config.mock_mode,
                self.evidence_map_path,
                self.config.model_name,
                self.config.temperature
            ) for sub_data in sub_criteria_list]

            try:
                # ‡πÉ‡∏ä‡πâ self.logger ‡πÅ‡∏ó‡∏ô logger (‡∏ñ‡πâ‡∏≤ logger ‡πÄ‡∏õ‡πá‡∏ô global)
                self.logger.info(f"Using {max(1, os.cpu_count() - 1)} processes...")
                pool_ctx = multiprocessing.get_context('spawn')
                with pool_ctx.Pool(processes=max(1, os.cpu_count() - 1)) as pool:
                    # NOTE: _static_worker_process ‡∏à‡∏∞‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ (sub_result, temp_map_from_worker)
                    results_list = pool.map(_static_worker_process, worker_args)
            except Exception as e:
                self.logger.critical(f"Multiprocessing failed: {e}")
                raise

            # ‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å worker
            for result_tuple in results_list:
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å worker ‡∏Å‡πà‡∏≠‡∏ô unpack
                if not isinstance(result_tuple, tuple) or len(result_tuple) != 2:
                    self.logger.error(f"Worker returned invalid result structure: {result_tuple}")
                    continue
                
                sub_result, temp_map_from_worker = result_tuple

                if isinstance(temp_map_from_worker, dict):
                    # ‡∏£‡∏ß‡∏° Evidence Map ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å Worker
                    for level_key, evidence_list in temp_map_from_worker.items():
                        if isinstance(evidence_list, list) and evidence_list:
                            current_list = self.evidence_map.setdefault(level_key, [])
                            current_list.extend(evidence_list)
                            self.logger.info(f"AGGREGATED: +{len(evidence_list)} ‚Üí {level_key} "
                                        f"(total: {len(current_list)})")

                # ‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô
                raw_refs = sub_result.get("raw_results_ref", [])
                self.raw_llm_results.extend(raw_refs if isinstance(raw_refs, list) else [])
                self.final_subcriteria_results.append(sub_result)


        else:
            # --------------------- SEQUENTIAL MODE ---------------------
            mode_desc = target_sub_id if target_sub_id != "all" else "All Sub-Criteria (Sequential)"
            self.logger.info(f"Starting Sequential Assessment: {mode_desc}")

            # üéØ FIX: ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç load_all_vectorstores ‡πÉ‡∏´‡πâ‡∏°‡∏µ tenant ‡πÅ‡∏•‡∏∞ year
            local_vsm = vectorstore_manager or (
                load_all_vectorstores(
                    doc_types=[EVIDENCE_DOC_TYPES], 
                    evidence_enabler=self.config.enabler,
                    tenant=self.config.tenant,  # <--- NEW: Argument ‡∏ó‡∏µ‡πà‡∏´‡∏≤‡∏¢‡πÑ‡∏õ
                    year=self.config.year       # <--- NEW: Argument ‡∏ó‡∏µ‡πà‡∏´‡∏≤‡∏¢‡πÑ‡∏õ
                )
                if self.config.mock_mode == "none" else None
            )
            self.vectorstore_manager = local_vsm

            for sub_criteria in sub_criteria_list:
                sub_result, final_temp_map = self._run_sub_criteria_assessment_worker(sub_criteria)
                self.raw_llm_results.extend(sub_result.get("raw_results_ref", []))
                self.final_subcriteria_results.append(sub_result)

        # ============================== 3. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Evidence Map ==============================
        if self.evidence_map:
            self._save_evidence_map(map_to_save=self.evidence_map)
            total_items = sum(len(v) for v in self.evidence_map.values())
            self.logger.info(f"Persisted final evidence map | Keys: {len(self.evidence_map)} | "
                            f"Items: {total_items} | Size: ~{total_items * 0.35:.1f} KB")

        # ============================== 4. ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏• & Export ==============================
        self._calculate_overall_stats(target_sub_id)

        final_results = {
            "summary": self.total_stats,
            "sub_criteria_results": self.final_subcriteria_results,
            "raw_llm_results": self.raw_llm_results,
            "run_time_seconds": round(time.time() - start_ts, 2),
            "timestamp": datetime.now().isoformat(),
        }

        if export:
            export_path = self._export_results(
                results=final_results,
                enabler=self.config.enabler,
                sub_criteria_id=target_sub_id if target_sub_id != "all" else "ALL",
                target_level=self.config.target_level
            )
            final_results["export_path_used"] = export_path
            final_results["evidence_map"] = deepcopy(self.evidence_map)
            self.logger.info(f"Exported full results ‚Üí {export_path}")

        return final_results

    def _run_single_assessment(
        self,
        sub_criteria: Dict[str, Any],
        statement_data: Dict[str, Any],
        vectorstore_manager: Optional['VectorStoreManager'],
        sequential_chunk_uuids: Optional[List[str]] = None,
        attempt: int = 1 # ‡πÄ‡∏û‡∏¥‡πà‡∏° attempt ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö RetryPolicy ‡πÉ‡∏ô L3-L5
    ) -> Dict[str, Any]:
        """
        ‡∏£‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô Level ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (L1-L5) ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå
        - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤ LLM ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô 'int' object ‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô dict ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
        - üü¢ NEW: Implement Relevant Score Gate
        """

        start_time = time.time() # ‡πÄ‡∏û‡∏¥‡πà‡∏° start_time ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì duration
        sub_id = sub_criteria['sub_id']
        level = statement_data['level']
        statement_text = statement_data['statement']
        sub_criteria_name = sub_criteria['sub_criteria_name']
        statement_id = statement_data.get('statement_id', sub_id)
        
        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Ç‡∏≠‡∏á duration (‡∏ñ‡πâ‡∏≤‡πÄ‡∏Å‡∏¥‡∏î error ‡∏Å‡πà‡∏≠‡∏ô llm call)
        retrieval_duration = 0.0 
        llm_duration = 0.0
        rag_query = statement_text # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô

        self.logger.info(f"  > Starting assessment for {sub_id} L{level} (Attempt: {attempt})...")

        # ==================== 1. PDCA & Level Prompt ====================
        pdca_phase = self._get_pdca_phase(level)
        level_constraint = self._get_level_constraint_prompt(level)
        contextual_rules_prompt = self._get_contextual_rules_prompt(sub_id, level)
        full_focus_hint = level_constraint + contextual_rules_prompt

        # ==================== 2. Hybrid Retrieval Setup ====================
        mapped_stable_doc_ids, priority_docs = self._get_mapped_uuids_and_priority_chunks(
            sub_id=sub_id,
            level=level,
            statement_text=statement_text,
            level_constraint=level_constraint,
            vectorstore_manager=vectorstore_manager
        )

        # ==================== 3. Enhance Query ====================
        rag_query_list = enhance_query_for_statement(
            statement_text=statement_text,
            sub_id=sub_id,
            statement_id=statement_id,
            level=level,
            enabler_id=self.enabler_id,
            focus_hint=full_focus_hint,
            llm_executor=self.llm
        )
        rag_query = rag_query_list[0] if rag_query_list else statement_text

        # ==================== 4. LLM Evaluator Setup ====================
        llm_evaluator_to_use = self.llm_evaluator
        if level <= 2:
            llm_evaluator_to_use = evaluate_with_llm_low_level

        # ==================== 5. RAG Retrieval ====================
        retrieval_start = time.time()
        try:
            retrieval_result = self.rag_retriever(
                query=rag_query_list,
                doc_type=EVIDENCE_DOC_TYPES,
                enabler=self.enabler_id,
                sub_id=sub_id,
                level=level,
                vectorstore_manager=vectorstore_manager,
                mapped_uuids=mapped_stable_doc_ids,
                priority_docs_input=priority_docs,
                sequential_chunk_uuids=sequential_chunk_uuids
            )
        except Exception as e:
            self.logger.error(f"RAG retrieval failed for {sub_id} L{level}: {e}")
            return self._create_error_result(level, 'RAG Retrieval Error', start_time, 0.0)

        retrieval_duration = time.time() - retrieval_start
        top_evidences = retrieval_result.get("top_evidences", [])
        used_chunk_uuids = retrieval_result.get("used_chunk_uuids", [])

        # ==================== 6. ‡∏î‡∏∂‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏à‡∏≤‡∏Å Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ ====================
        try:
            previous_levels_raw = self._collect_previous_level_evidences(sub_id, current_level=level)
        except Exception as e:
            self.logger.error(f"Failed to collect previous evidences: {e}")
            previous_levels_raw = {}

        previous_levels_evidence_full = []
        for ev_list in previous_levels_raw.values():
            for ev in ev_list:
                doc_id = ev.get("doc_id") or ev.get("chunk_uuid")
                if not doc_id or str(doc_id).startswith("HASH-"):
                    continue
                previous_levels_evidence_full.append(ev)

        # ==================== 6a. Sequential fallback ====================
        if level > 1 and self.is_sequential:
            current_ids = {d.get("doc_id") or d.get("chunk_uuid") for d in top_evidences}
            for ev in previous_levels_evidence_full:
                ev_id = ev.get("doc_id") or ev.get("chunk_uuid")
                if ev_id not in current_ids:
                    fallback_ev = ev.copy()
                    fallback_ev["pdca_tag"] = "Baseline"
                    top_evidences.append(fallback_ev)

        # ==================== 7. ‡∏™‡∏£‡πâ‡∏≤‡∏á Multi-Channel Context ====================
        channels = build_multichannel_context_for_level(
            level=level,
            top_evidences=top_evidences,
            previous_levels_evidence=previous_levels_evidence_full,
            max_main_context_tokens=3000,
            max_summary_sentences=4
        )

        debug = channels.get("debug_meta", {})
        self.logger.info(
            f"  > Context built ‚Üí Direct: {debug.get('direct_count',0)}, "
            f"Aux: {debug.get('aux_count',0)}, "
            f"Baseline: {len(previous_levels_evidence_full)} files "
            f"from {len(previous_levels_raw)} previous levels"
        )

        # ==================== 8. LLM Evaluation ====================
        
        # üü¢ NEW: 8.1. Relevant Score Gate - Calculate Max Evidence Strength
        # self.logger.critical(f"FINAL DEBUG L{level}: DUMPING RAW top_evidences[0] JSON:")
        try:
            # ‡πÉ‡∏ä‡πâ json.dumps ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏õ‡∏•‡∏á Object/Dict ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏õ‡πá‡∏ô String
            # ‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ getattr() ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
            if isinstance(top_evidences[0], dict):
                raw_doc_data = top_evidences[0]
            else:
                raw_doc_data = {'page_content': getattr(top_evidences[0], 'page_content', 'N/A'),
                                'metadata': getattr(top_evidences[0], 'metadata', {}),
                                'score': getattr(top_evidences[0], 'score', 'N/A')}
            
            # self.logger.critical(json.dumps(raw_doc_data, indent=2, ensure_ascii=False))
        except Exception as e:
             self.logger.critical(f"FINAL DEBUG L{level}: FAILED TO DUMP RAW DOC: {e}")

        evi_cap_data = self._calculate_evidence_strength_cap(top_evidences, level)
        max_evi_str_for_prompt = evi_cap_data['max_evi_str_for_prompt']
        
        context_parts = [
            f"--- DIRECT EVIDENCE (L{level})---\n{channels.get('direct_context','')}",
            f"--- AUXILIARY EVIDENCE ---\n{channels.get('aux_summary','')}",
            f"--- BASELINE FROM PREVIOUS LEVELS ---\n{channels.get('baseline_summary','‡πÑ‡∏°‡πà‡∏°‡∏µ')}"
        ]
        final_llm_context = "\n\n".join([p for p in context_parts if p.strip()])

        llm_start = time.time()
        try:
            llm_result = llm_evaluator_to_use(
                context=final_llm_context,
                sub_criteria_name=sub_criteria_name,
                level=level,
                statement_text=statement_text,
                sub_id=sub_id,
                pdca_phase=pdca_phase,
                level_constraint=level_constraint,
                contextual_rules=contextual_rules_prompt,
                llm_executor=self.llm,
                # üü¢ NEW: ‡∏™‡πà‡∏á‡∏Ñ‡πà‡∏≤ Max Evi Str Cap ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô LLM Evaluator
                max_evidence_strength=max_evi_str_for_prompt # <--- ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÉ‡∏ô evaluate_with_llm
            )
        except Exception as e:
            self.logger.error(f"LLM Call failed for {sub_id} L{level}: {e}")
            llm_result = {}
        
        llm_duration = time.time() - llm_start

        # =====================================================================================
        # üéØ FINAL FIX: ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏±‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç (int) ‡∏Å‡πà‡∏≠‡∏ô RETURN 
        # (‡πÉ‡∏ä‡πâ calculate_pdca_breakdown_and_pass_status ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏á Business Logic ‡πÑ‡∏ß‡πâ)
        # =====================================================================================
        
        is_numeric_result = isinstance(llm_result, (int, float)) or \
                            (isinstance(llm_result, str) and str(llm_result).strip().isdigit())
                            
        if is_numeric_result:
            level_num = int(llm_result)
            self.logger.warning(
                f"üö® L{level} LLM returned ONLY number {level_num} (Type: {type(llm_result).__name__}). "
                f"Converting to standardized dict format to prevent RetryPolicy crash."
            )
            
            # *** FIX: ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏á Logic ‡∏Å‡∏≤‡∏£ PASS/FAIL ‡πÅ‡∏•‡∏∞ PDCA Breakdown ***
            try:
                # ‡πÉ‡∏ä‡πâ calculate_pdca_breakdown_and_pass_status ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ Logic L5/L4 >= 4 ‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
                # NOTE: ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö max_evi_str_cap_for_llm ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Evidence Strength Default
                pdca_breakdown_data, is_passed_num, _ = calculate_pdca_breakdown_and_pass_status(level_num, level) 
            except NameError:
                self.logger.error("calculate_pdca_breakdown_and_pass_status function is missing from scope.")
                is_passed_num = level_num >= level
                pdca_breakdown_data = {}

            status_num = "PASS" if is_passed_num else "FAIL"
            # *******************************************************************************

            return {
                "sub_criteria_id": sub_id,
                "statement_id": statement_id,
                "level": level,
                "statement": statement_text,
                "pdca_phase": pdca_phase,
                "llm_score": level_num,
                "pdca_breakdown": pdca_breakdown_data, # ‡πÉ‡∏ä‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô
                "is_passed": is_passed_num,             # ‡πÉ‡∏ä‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô
                "status": status_num,                   # ‡πÉ‡∏ä‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô
                "score": level_num,
                "llm_result_full": {"raw_number": level_num, "raw_type": type(llm_result).__name__},
                "retrieval_duration_s": round(retrieval_duration, 2),
                "llm_duration_s": round(llm_duration, 2),
                "top_evidences_ref": [],
                "temp_map_for_level": [],
                "evidence_strength": self.MAX_EVI_STR_CAP if is_passed_num else 0.0, # ‡πÉ‡∏ä‡πâ Evi Str Cap ‡πÄ‡∏õ‡πá‡∏ô Default
                "ai_confidence": "HIGH" if is_passed_num else "LOW",
                "evidence_count": 0,
                "pdca_coverage": 0.0,
                "direct_evidence_count": 0,
                "rag_query": rag_query,
                "full_context_meta": debug,
                # üü¢ NEW: Relevant Score Gate Metadata (‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å capiing)
                "max_relevant_score": evi_cap_data['highest_rerank_score'],
                "max_relevant_source": evi_cap_data['max_score_source'],
                "is_evidence_strength_capped": evi_cap_data['is_capped'],
                "max_evidence_strength_used": max_evi_str_for_prompt,
            }
        # =====================================================================================

        # ==================== 9-10. Scoring & Pass/Fail ====================
        
        if not isinstance(llm_result, dict):
            self.logger.error(
                f"üö® LLM parsing failed for {statement_id} L{level}. Received unexpected type: {type(llm_result).__name__}. Setting FAIL defaults."
            )
            llm_result = {}
        
        llm_score = llm_result.get('score', 0)
        # ‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏≤‡∏Å Global Scope ‡∏´‡∏£‡∏∑‡∏≠ Scope ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÑ‡∏ß‡πâ
        pdca_breakdown, is_passed, _ = calculate_pdca_breakdown_and_pass_status(llm_score, level)
        status = "PASS" if is_passed else "FAIL"

        # -------------------- 11. SAVE EVIDENCE MAP (PASS ONLY) --------------------
        temp_map_for_level = None
        evidence_entries = []
        
        if is_passed and top_evidences:
            seen = set()
            
            def safe_float(val, default=0.0):
                """Convert val to float safely, fallback to default if fails"""
                try:
                    return float(val)
                except (TypeError, ValueError):
                    return default

            for ev in top_evidences:
                doc_id = ev.get("doc_id") or ev.get("chunk_uuid")
                
                # ‡∏Å‡∏£‡∏≠‡∏á ID ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏•‡∏∞ Chunk ‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥
                if not doc_id or str(doc_id).startswith(("TEMP-", "HASH-")) or doc_id in seen:
                    continue

                # --- START: SCORE EXTRACTION REVISED ---
                score = 0.0
                metadata = ev.get("metadata", {}) or {}
                filename_to_use = ev.get("source_filename") or metadata.get("source_filename") or ""

                # üéØ Priority 1: ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å 'relevance_score' ‡πÅ‡∏•‡∏∞ 'score' ‡∏ó‡∏µ‡πà Reranker/Retriever ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÑ‡∏ß‡πâ
                score_sources = [
                    ev.get("relevance_score"), ev.get("score"),
                    metadata.get("relevance_score"), metadata.get("score"),
                    ev.get("rerank_score"), metadata.get("rerank_score"),
                    metadata.get("_rerank_score_force") # ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏™‡πà
                ]
                
                for s in score_sources:
                    score = max(score, safe_float(s))
                    
                # üéØ Priority 2: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö distance (ChromaDB Similarity)
                distance = metadata.get("distance") or ev.get("distance")
                if distance is not None:
                    try:
                        distance_val = safe_float(distance)
                        similarity = round(1.0 - distance_val, 4)
                        score = max(score, similarity)
                    except (TypeError, ValueError):
                        pass

                # üéØ Priority 3: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå (‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏î‡∏¥‡∏°)
                if "|SCORE:" in filename_to_use:
                    try:
                        score_str = filename_to_use.split("|SCORE:")[1].split("|")[0]
                        filename_score = safe_float(score_str)
                        score = max(score, filename_score)
                        filename_to_use = filename_to_use.split("|SCORE:")[0] 
                    except Exception:
                        pass
                
                # üí° FIX: ‡∏ñ‡πâ‡∏≤ score ‡∏¢‡∏±‡∏á‡πÄ‡∏õ‡πá‡∏ô 0.0 ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà Baseline Chunk ‡πÉ‡∏´‡πâ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏õ‡πá‡∏ô 0.5 (Evidence Default)
                if score == 0.0 and (ev.get("pdca_tag") != "Baseline"):
                    score = 0.5 
                    
                score = round(score, 4)
                # --- END: SCORE EXTRACTION REVISED ---
                
                if not filename_to_use:
                    filename_to_use = ev.get("source_filename") or ev.get("source") or ev.get("filename") or metadata.get("source_filename") or metadata.get("filename") or "UNKNOWN_FILE"

                evidence_entries.append({
                    "doc_id": doc_id,
                    "filename": filename_to_use,
                    "mapper_type": "AI_GENERATED", 
                    "timestamp": datetime.now().isoformat(), 
                    "relevance_score": score, # üí° FIX 2: ‡πÉ‡∏ä‡πâ score ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÅ‡∏•‡πâ‡∏ß
                    "chunk_uuid": doc_id,
                })
                seen.add(doc_id) # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏ô seen
            
            # -------------------- 12. Calculate PDCA Coverage & Strength --------------------
            direct_count = channels.get("debug_meta", {}).get("direct_count", 0)
            
            avg_score = sum(entry.get("relevance_score", 0.0) for entry in evidence_entries) / len(evidence_entries) if evidence_entries else 0.0
            
            pdca_coverage = sum(1 for score in pdca_breakdown.values() if score > 0) / 4.0 # ‡πÉ‡∏ä‡πâ Logic ‡πÄ‡∏î‡∏¥‡∏°

            # üí° FIX: ‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡∏Ñ‡∏π‡∏ì‡∏à‡∏≤‡∏Å 1.5 ‡πÄ‡∏õ‡πá‡∏ô 2.0 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Evidence Strength ‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏™‡∏π‡∏á‡∏Ç‡∏∂‡πâ‡∏ô
            # üí° FIX 2: ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö Evidence Strength ‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡πÄ‡∏Å‡∏¥‡∏ô‡∏Ñ‡πà‡∏≤ max_evi_str_for_prompt (‡∏à‡∏≤‡∏Å Capping)
            evidence_strength_raw = (avg_score * 10.0) * (pdca_coverage * 2.0)
            
            evidence_strength = min(
                max_evi_str_for_prompt, # <--- üü¢ ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Capped ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
                evidence_strength_raw
            )

            ai_confidence = "HIGH" if evidence_strength >= 8.0 and is_passed else \
                            "MEDIUM" if evidence_strength >= 5.5 else "LOW"

            evidence_count_for_level = len(evidence_entries)
            
            # -------------------- 13. Prepare temp_map_for_level and Finalize Evidence --------------------
            evidence_entries.sort(key=lambda x: x.get("relevance_score", 0.0), reverse=True)

            final_k_reranked = self.config.final_k_reranked if hasattr(self.config, 'final_k_reranked') else 5
            evidence_entries = evidence_entries[:final_k_reranked]

            temp_map_for_level = [
                {
                    "doc_id": entry["doc_id"],
                    "filename": entry["filename"],
                    "mapper_type": entry["mapper_type"],
                    "timestamp": entry["timestamp"],
                    "relevance_score": entry["relevance_score"],
                    "chunk_uuid": entry["chunk_uuid"],
                }
                for entry in evidence_entries
            ]
        
        else:
            evidence_entries = []
            temp_map_for_level = []
            evidence_strength = 0.0
            ai_confidence = "LOW"
            pdca_coverage = 0.0
            direct_count = 0
            evidence_count_for_level = 0
        
        # ==================== 14. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ ====================
        final_result = {
            "sub_criteria_id": sub_id,
            "statement_id": statement_id,
            "level": level,
            "statement": statement_text,
            "pdca_phase": pdca_phase,
            "llm_score": llm_score,
            "pdca_breakdown": pdca_breakdown,
            "is_passed": is_passed,
            "status": status,
            "score": llm_score,
            "llm_result_full": llm_result,
            "retrieval_duration_s": round(retrieval_duration, 2),
            "llm_duration_s": round(llm_duration, 2),
            "top_evidences_ref": self._resolve_evidence_filenames(evidence_entries), 
            "temp_map_for_level": temp_map_for_level,
            "evidence_strength": round(evidence_strength, 1),
            "ai_confidence": ai_confidence,
            "evidence_count": evidence_count_for_level,
            "pdca_coverage": round(pdca_coverage, 4), 
            "direct_evidence_count": direct_count,
            "rag_query": rag_query,
            "full_context_meta": debug,
            
            # üü¢ NEW: Relevant Score Gate Metadata
            "max_relevant_score": evi_cap_data['highest_rerank_score'],
            "max_relevant_source": evi_cap_data['max_score_source'],
            "is_evidence_strength_capped": evi_cap_data['is_capped'],
            "max_evidence_strength_used": max_evi_str_for_prompt,
        }

        # üü¢ [‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç] 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£ icon_status
        icon_status = "‚úÖ" if status == "PASS" else "‚ùå"

        # üü¢ [‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç] 2. ‡∏ô‡∏≥ icon_status ‡πÑ‡∏õ‡πÉ‡∏™‡πà‡πÉ‡∏ô Log
        self.logger.info(f"  > Assessment {sub_id} L{level} completed ‚Üí {icon_status} {status} (Score: {llm_score:.1f} | Evi Str: {final_result['evidence_strength']:.1f} | Conf: {ai_confidence})")

        return final_result