# core/seam_assessment.py

import sys
import json
import logging
import time
import os
from typing import List, Dict, Any, Optional, Union, Tuple, Set, Final, Literal
from collections import defaultdict
from datetime import datetime
from dataclasses import dataclass, field
import multiprocessing # NEW: Import for parallel execution
from functools import partial
import pathlib, uuid
from langchain_core.documents import Document as LcDocument
from core.retry_policy import RetryPolicy, RetryResult
from copy import deepcopy
import tempfile
import shutil
# from json_extractor import _robust_extract_json
from .json_extractor import _robust_extract_json
from filelock import FileLock  # ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á: pip install filelock
import re
import hashlib


# -------------------- PATH SETUP & IMPORTS --------------------
try:
    PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
    if PROJECT_ROOT not in sys.path:
        sys.path.append(PROJECT_ROOT)

    from config.global_vars import (
        # ‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß
        EXPORTS_DIR, MAX_LEVEL, INITIAL_LEVEL, FINAL_K_RERANKED,
        RUBRIC_FILENAME_PATTERN, RUBRIC_CONFIG_DIR, DEFAULT_ENABLER,
        EVIDENCE_DOC_TYPES, INITIAL_TOP_K,
        EVIDENCE_MAPPING_FILENAME_SUFFIX,
        LIMIT_CHUNKS_PER_PRIORITY_DOC,
        IS_LOG_L3_CONTEXT,
        PRIORITY_CHUNK_LIMIT,
        DEFAULT_TENANT,
        DEFAULT_YEAR,
        RERANK_THRESHOLD,
        MAX_EVI_STR_CAP,
        DEFAULT_LLM_MODEL_NAME,
        LLM_TEMPERATURE,
        MAX_PARALLEL_WORKERS,
        MIN_RERANK_SCORE_TO_KEEP,
        MIN_RETRY_SCORE,
        MIN_RELEVANCE_THRESHOLD,

        # === ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ ===
        CONTEXT_CAP_L3_PLUS,                 # ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô _run_single_assessment ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö cap context L3+
        CRITICAL_CA_THRESHOLD,               # ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô critical C/A evidence summary ‡πÅ‡∏•‡∏∞ override logic
        MAX_RETRIEVAL_ATTEMPTS,              # ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô adaptive RAG loop
        HYBRID_VECTOR_WEIGHT,                # ‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ‡πÉ‡∏ô rag_retriever ‡∏´‡∏£‡∏∑‡∏≠ hybrid setup
        HYBRID_BM25_WEIGHT,                  # ‡πÄ‡∏ä‡πà‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
        CHUNK_SIZE,                          # ‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ‡πÉ‡∏ô hydration ‡∏´‡∏£‡∏∑‡∏≠ chunking
        CHUNK_OVERLAP,                       # ‡πÄ‡∏ä‡πà‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
        REQUIRED_PDCA,                 # <--- ‡πÄ‡∏û‡∏¥‡πà‡∏°
        CORRECT_PDCA_SCORES_MAP,       # <--- ‡πÄ‡∏û‡∏¥‡πà‡∏°
        PDCA_PRIORITY_ORDER,           # <--- ‡πÄ‡∏û‡∏¥‡πà‡∏°
        BASE_PDCA_KEYWORDS,            # <--- ‡πÄ‡∏û‡∏¥‡πà‡∏°
        PDCA_LEVEL_SYNONYMS,
        ENABLE_HARD_FAIL_LOGIC,
        ENABLE_CONTEXTUAL_RULE_OVERRIDE
    )
    
    
    from core.llm_data_utils import ( 
        create_structured_action_plan, evaluate_with_llm,
        retrieve_context_with_filter, retrieve_context_for_low_levels,
        evaluate_with_llm_low_level, LOW_LEVEL_K, 
        set_mock_control_mode as set_llm_data_mock_mode,
        create_context_summary_llm,
        retrieve_context_by_doc_ids,
        _fetch_llm_response,
        build_multichannel_context_for_level
    )
    from core.vectorstore import VectorStoreManager, load_all_vectorstores, get_global_reranker 
    from core.seam_prompts import PDCA_PHASE_MAP 
    from core.action_plan_schema import ActionPlanActions  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô

    # üéØ FIX: Import ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô Path Utility ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
    from utils.path_utils import (
        get_mapping_file_path, 
        get_evidence_mapping_file_path, 
        get_contextual_rules_file_path,
        get_doc_type_collection_key,
        get_assessment_export_file_path,
        get_export_dir,
        get_rubric_file_path # <--- ‡∏ï‡πâ‡∏≠‡∏á Import ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏î‡πâ‡∏ß‡∏¢
    )

    import assessments.seam_mocking as seam_mocking 
    
except ImportError as e:
    # -------------------- Fallback Code (Same as previous) --------------------
    print(f"FATAL ERROR: Failed to import required modules. Error: {e}", file=sys.stderr)
    
    # Define placeholder variables if imports fail
    EXPORTS_DIR = "exports"
    MAX_LEVEL = 5
    INITIAL_LEVEL = 1
    FINAL_K_RERANKED = 3
    RUBRIC_FILENAME_PATTERN = "{tenant}_{enabler}_rubric.json"
    RUBRIC_CONFIG_DIR = "config/rubrics"
    DEFAULT_ENABLER = "KM"
    EVIDENCE_DOC_TYPES = "evidence"
    INITIAL_TOP_K = 10
    
    def create_structured_action_plan(*args, **kwargs): return [{"Phase": "Mock Plan", "Goal": "Resolve issue"}]
    def evaluate_with_llm(*args, **kwargs): return {"score": 1, "reason": "Mock pass", "is_passed": True}
    def retrieve_context_with_filter(*args, **kwargs): return {"top_evidences": [], "aggregated_context": "Mock Context"}
    def retrieve_context_for_low_levels(*args, **kwargs): return {"top_evidences": [], "aggregated_context": "Mock Low Context"}
    def evaluate_with_llm_low_level(*args, **kwargs): return {"score": 1, "reason": "Mock pass L1/L2", "is_passed": True}
    LOW_LEVEL_K = 2
    def set_llm_data_mock_mode(mode): pass
    class VectorStoreManager: pass
    def load_all_vectorstores(*args, **kwargs): return VectorStoreManager()
    PDCA_PHASE_MAP = {1: "Plan", 2: "Do", 3: "Check", 4: "Act", 5: "Innovate"}
    class seam_mocking:
        @staticmethod
        def evaluate_with_llm_CONTROLLED_MOCK(*args, **kwargs): return {"score": 0, "reason": "Mock fail", "is_passed": False}
        @staticmethod
        def retrieve_context_with_filter_MOCK(*args, **kwargs): return {"top_evidences": [], "aggregated_context": "Mock Context"}
        @staticmethod
        def create_structured_action_plan_MOCK(*args, **kwargs): return [{"Phase": "Mock Plan", "Goal": "Resolve issue"}]
        @staticmethod
        def set_mock_control_mode(mode): pass
    
    # üìå Placeholder functions for path_utils if the main import fails
    def get_mapping_file_path(*args, **kwargs): return "config/mapping/default/mapping.json"
    def get_evidence_mapping_file_path(*args, **kwargs): return "config/mapping/default/evidence_mapping.json"
    def get_contextual_rules_file_path(*args, **kwargs): return "config/rubrics/default/contextual_rules.json"
    def get_rubric_file_path(*args, **kwargs): return "config/rubrics/default/rubric.json"
    
    if "FATAL ERROR" in str(e):
        pass 
    # ---------------------------------------------------------------------- 


logger = logging.getLogger(__name__)
if not logger.handlers:
    logging.basicConfig(level=logging.DEBUG, format="%(asctime)s - %(levelname)s - %(message)s")



# -----------------------------------------------------------------
# üéØ FUNCTION: classify_by_keyword (Heuristic PDCA Classification)
# -----------------------------------------------------------------
def classify_by_keyword(
    text: str, 
    sub_id: str = None, 
    level: int = None,
    contextual_rules_map: dict = None
) -> str:
    """Heuristic PDCA Classification v13 ‚Äì Custom P>D>C>A priority + Fallback"""
    if not text:
        return 'Other'
    
    text_lower = text.lower()
    
    # === ‡∏Ç‡∏±‡πâ‡∏ô 1: ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏° Custom Keywords ‡∏à‡∏≤‡∏Å contextual_rules.json (‡∏Ñ‡∏£‡∏ö 4 phase) ===
    custom_keywords = defaultdict(list)
    
    if contextual_rules_map and sub_id:
        rules = contextual_rules_map.get(sub_id, {})
        
        # ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏° keywords ‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å Level ‡∏ó‡∏µ‡πà‡∏°‡∏µ (L1-L5)
        for l_key, l_rules in rules.items():
            if l_key.startswith("L"):
                # Plan Keywords (L1)
                planning_kw = l_rules.get("plan_keywords", "")
                if planning_kw:
                    custom_keywords['Plan'].extend([kw.strip().lower() for kw in planning_kw.split(",")])
                
                # Do Keywords (L2, L3)
                do_kw = l_rules.get("do_keywords", "")
                if do_kw:
                    custom_keywords['Do'].extend([kw.strip().lower() for kw in do_kw.split(",")])

                # Check Keywords (L3, L4, L5)
                check_kw = l_rules.get("check_keywords", "")
                if check_kw:
                    custom_keywords['Check'].extend([kw.strip().lower() for kw in check_kw.split(",")])
                
                # Act Keywords (L4, L5)
                act_kw = l_rules.get("act_keywords", "")
                if act_kw:
                    custom_keywords['Act'].extend([kw.strip().lower() for kw in act_kw.split(",")])

        # ‡∏Å‡∏£‡∏≠‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏•‡∏∑‡∏≠ Keyword ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô
        for key in custom_keywords:
            custom_keywords[key] = list(set(filter(None, custom_keywords[key])))

    # === ‡∏Ç‡∏±‡πâ‡∏ô 2: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Custom Keywords (‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÉ‡∏´‡∏°‡πà: Plan > Do > Check > Act) ===
    
    # üìå FIX: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö P ‡∏Å‡πà‡∏≠‡∏ô A ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô L1 P/D (‡πÄ‡∏ä‡πà‡∏ô ‡πÅ‡∏ú‡∏ô) ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å Tag ‡πÄ‡∏õ‡πá‡∏ô A/C ‡∏á‡πà‡∏≤‡∏¢‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
    
    # Plan
    if custom_keywords['Plan']:
        if any(kw in text_lower for kw in custom_keywords['Plan'] if kw):
            return 'Plan'
    # Do
    if custom_keywords['Do']:
        if any(kw in text_lower for kw in custom_keywords['Do'] if kw):
            return 'Do'
            
    # Check
    if custom_keywords['Check']:
        if any(kw in text_lower for kw in custom_keywords['Check'] if kw):
            return 'Check'
    # Act
    if custom_keywords['Act']:
        if any(kw in text_lower for kw in custom_keywords['Act'] if kw):
            return 'Act'

    
    # === ‡∏Ç‡∏±‡πâ‡∏ô 3: Fallback ‡∏î‡πâ‡∏ß‡∏¢ BASE_PDCA_KEYWORDS (‡πÉ‡∏ä‡πâ Act ‡∏Å‡πà‡∏≠‡∏ô Check ‡∏Å‡πà‡∏≠‡∏ô Do ‡∏Å‡πà‡∏≠‡∏ô Plan ‡∏ï‡∏≤‡∏° PDCA_PRIORITY_ORDER) ===
    # Fallback ‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡πÉ‡∏ä‡πâ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏ô‡πâ‡∏ô A/C ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏õ‡πá‡∏ô Logic ‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏à‡∏≤‡∏Å BASE_PDCA_KEYWORDS
    for tag in PDCA_PRIORITY_ORDER: 
        for pattern in BASE_PDCA_KEYWORDS[tag]:
            if re.search(pattern, text, re.IGNORECASE):
                return tag
    
    return 'Other'

def get_correct_pdca_required_score(level: int) -> int:
    """‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡πÑ‡∏î‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡∏ú‡πà‡∏≤‡∏ô Level ‡∏ô‡∏±‡πâ‡∏ô ‡πÜ"""
    if level == 1:
        return 1
    elif level == 2:
        return 2
    # ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: L3, L4, L5 ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏° PDCA ‡∏Ñ‡∏£‡∏ö‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÉ‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå 
    # (P, D, C, A = 2 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ï‡πà‡∏≠‡πÅ‡∏Å‡∏ô)
    elif level == 3: # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ P, D, C ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 1.0/2.0 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏° 4 (‡∏ñ‡πâ‡∏≤ L3 ‡πÄ‡∏ô‡πâ‡∏ô C ‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏î‡πâ P, D, C)
        return 4
    elif level == 4: # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ P, D, C, A ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏° 6 (‡∏ñ‡πâ‡∏≤ L4 ‡πÄ‡∏ô‡πâ‡∏ô A ‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏î‡πâ P, D, C, A)
        return 6
    elif level == 5: # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏ï‡πá‡∏°‡∏ó‡∏∏‡∏Å‡πÅ‡∏Å‡∏ô ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏° 8
        return 8
    return 8


# üìå ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç Type Hint ‡πÅ‡∏•‡∏∞ Arguments ‡∏Ç‡∏≠‡∏á Tuple ‡πÉ‡∏´‡πâ‡∏£‡∏ß‡∏° config parameter ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (10 elements)
def _static_worker_process(worker_input_tuple: Tuple[
    Dict[str, Any], str, int, str, str, str, float, float, int, Optional[Dict[str, str]]
]) -> Dict[str, Any]:
    """
    Static worker function for multiprocessing pool. 
    It reconstructs SeamAssessment in the new process and executes the assessment 
    for a single sub-criteria.
    
    Args:
        worker_input_tuple: (sub_criteria_data, enabler: str, target_level: int, mock_mode: str, 
                             evidence_map_path: str, model_name: str, temperature: float, 
                             min_retry_score: float, max_retrieval_attempts: int,
                             document_map: Optional[Dict[str, str]]) 

    Returns:
        Dict[str, Any]: Final result of the sub-criteria assessment.
    """
    
    # üü¢ NEW FIX: PATH SETUP ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Worker Process
    # ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ path ‡∏ã‡πâ‡∏≥‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤ worker process ‡πÄ‡∏´‡πá‡∏ô package ‡∏´‡∏•‡∏±‡∏Å
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
    if project_root not in sys.path:
        sys.path.append(project_root)
        
    worker_logger = logging.getLogger(__name__)

    try:
        # üü¢ FIX: Unpack ‡∏Ñ‡πà‡∏≤ Primitives ‡∏ó‡∏±‡πâ‡∏á 10 ‡∏ï‡∏±‡∏ß
        (
            sub_criteria_data, 
            enabler, 
            target_level, 
            mock_mode, 
            evidence_map_path, 
            model_name, 
            temperature,
            min_retry_score,            # ‚¨ÖÔ∏è NEW CONFIG (8th element)
            max_retrieval_attempts,     # ‚¨ÖÔ∏è NEW CONFIG (9th element)
            document_map,                # (10th element)
            action_plan_model
        ) = worker_input_tuple
    except ValueError as e:
        # ‡πÉ‡∏ä‡πâ len(worker_input_tuple) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£ Debug ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô
        worker_logger.critical(f"Worker input tuple unpack failed (expected 10 elements, got {len(worker_input_tuple)}): {e}")
        return {"error": f"Invalid worker input: {e}"}
        
    # 1. Reconstruct Config 
    try:
        # üü¢ FIX: ‡∏™‡∏£‡πâ‡∏≤‡∏á AssessmentConfig ‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏ô Worker Process ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏™‡πà‡∏Ñ‡πà‡∏≤ config ‡πÉ‡∏´‡∏°‡πà
        # (Tenant/Year ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Default ‡∏à‡∏≤‡∏Å AssessmentConfig)
        worker_config = AssessmentConfig(
            enabler=enabler,
            target_level=target_level,
            mock_mode=mock_mode,
            model_name=model_name, 
            temperature=temperature,
            min_retry_score=min_retry_score,            # ‚¨ÖÔ∏è Pass new config
            max_retrieval_attempts=max_retrieval_attempts # ‚¨ÖÔ∏è Pass new config
        )
    except Exception as e:
        worker_logger.critical(f"Failed to reconstruct AssessmentConfig in worker: {e}")
        return {
            "sub_criteria_id": sub_criteria_data.get('sub_id', 'UNKNOWN'),
            "error": f"Config reconstruction failed: {e}"
        }

    # 2. Re-instantiate SeamAssessment 
    try:
        # üü¢ FIX (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç): ‡∏™‡πà‡∏á document_map ‡πÅ‡∏•‡∏∞ worker_config ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô SEAMPDCAEngine
        # SEAMPDCAEngine ‡∏à‡∏∞‡πÉ‡∏ä‡πâ worker_config ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ min_retry_score ‡πÅ‡∏•‡∏∞ max_retrieval_attempts
        worker_instance = SEAMPDCAEngine(
            config=worker_config, 
            evidence_map_path=evidence_map_path, 
            llm_instance=None,              # LLM ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å Initialized ‡πÉ‡∏ô Engine ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ
            vectorstore_manager=None,       # VSM ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å Initialized ‡πÉ‡∏ô Engine ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ
            # doc_type ‡∏ï‡πâ‡∏≠‡∏á‡∏ñ‡∏π‡∏Å set ‡πÉ‡∏ô SEAMPDCAEngine constructor (‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ Default)
            logger_instance=worker_logger,
            document_map=document_map, # ‚¨ÖÔ∏è ‡∏™‡πà‡∏á document_map ‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏á Unpack ‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤
            ActionPlanActions=action_plan_model
        )
    except Exception as e:
        worker_logger.critical(f"FATAL: SEAMPDCAEngine instantiation failed in worker: {e}")
        return {
            "sub_criteria_id": sub_criteria_data.get('sub_id', 'UNKNOWN'),
            "error": f"Engine initialization failed: {e}"
        }
    
    # 3. Execute the worker logic
    return worker_instance._run_sub_criteria_assessment_worker(sub_criteria_data)

def merge_evidence_mappings(results_list: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
    """
    ‡∏£‡∏ß‡∏° evidence_mapping dictionaries ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å Worker ‡∏ó‡∏∏‡∏Å‡∏ï‡∏±‡∏ß 
    """
    merged_mapping = defaultdict(list)
    for result in results_list:
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å Worker ‡∏°‡∏µ Key 'evidence_mapping' ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if 'evidence_mapping' in result and isinstance(result['evidence_mapping'], dict):
            # ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏ú‡πà‡∏≤‡∏ô Key/Value ‡∏Ç‡∏≠‡∏á Worker ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡∏±‡∏ß
            for level_key, evidence_list in result['evidence_mapping'].items():
                # ‡πÉ‡∏ä‡πâ .extend() ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ú‡∏ô‡∏ß‡∏Å‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
                if isinstance(evidence_list, list):
                    merged_mapping[level_key].extend(evidence_list)
    
    # ‡πÅ‡∏õ‡∏•‡∏á defaultdict ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô dict ‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤
    return dict(merged_mapping)

# =================================================================
# üéØ NEW: Deterministic Fallback Logic (Post-Processing)
# =================================================================
def post_process_llm_result(llm_output: Dict[str, Any], level: int) -> Dict[str, Any]:
    """
    FINAL DETERMINISTIC POST-PROCESSOR v20 ‚Äî ULTIMATE VICTORY EDITION
    ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà: 15 ‡∏ò.‡∏Ñ. 2568 ‡πÄ‡∏ß‡∏•‡∏≤ 08:30 ‡∏ô. ‚Äî ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç PDCA Logic ‡πÅ‡∏•‡∏∞ Floating Point Precision
    ‡∏ú‡∏π‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á: ‡∏û‡∏µ‡πà + ‡∏ú‡∏° + Gemini
    """
    logger = logging.getLogger(__name__)
    
    # üìå FIX 1: Ensure scores are float for calculations and round them immediately for output (Floating Point Precision Fix)
    p = round(float(llm_output.get("P_Plan_Score", 0)), 1)
    d = round(float(llm_output.get("D_Do_Score", 0)), 1)
    c = round(float(llm_output.get("C_Check_Score", 0)), 1)
    a = round(float(llm_output.get("A_Act_Score", 0)), 1)
    pdca_real_sum = p + d + c + a
    llm_score = llm_output.get("score", 0)

    # SE-AM Threshold ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á 100% (‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà user ‡πÉ‡∏´‡πâ‡∏°‡∏≤)
    # L1: 1 (P>=1)
    # L2: 2 (P=2)
    # L3: 4 (P=2, D=2)
    # L4: 6 (P=2, D=2, C=1, A=1) 
    # L5: 8 (P=2, D=2, C=2, A=2)
    threshold_map = {1: 1, 2: 2, 3: 4, 4: 6, 5: 8} 
    threshold = threshold_map.get(level, 2)

    # 1. ‡∏à‡∏±‡∏ö LLM ‡πÇ‡∏Å‡∏á (‡πÉ‡∏ä‡πâ PDCA Sum ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à)
    if float(llm_score) != pdca_real_sum: 
        logger.critical(
            f"PDCA MISMATCH EXECUTED L{level} | "
            f"LLM ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÇ‡∏Å‡∏á ‚Üí score={llm_score} ‡πÅ‡∏ï‡πà PDCA ‡∏à‡∏£‡∏¥‡∏á={pdca_real_sum} "
            f"‚Üí FORCE OVERRIDE!"
        )
        llm_output["score"] = pdca_real_sum
        llm_output["original_score"] = llm_score
        llm_output["pdca_enforced"] = True

    # 2. ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö is_passed ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏£‡∏¥‡∏á (‡πÉ‡∏ä‡πâ PDCA Sum ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö Threshold ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á)
    real_pass = pdca_real_sum >= threshold
    
    # üéØ CRITICAL FIX 2: ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö FAIL ‡∏´‡∏≤‡∏Å‡∏Ç‡∏≤‡∏î‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç (C/A) ‡πÅ‡∏°‡πâ PDCA Sum ‡∏à‡∏∞‡∏ñ‡∏∂‡∏á‡πÄ‡∏Å‡∏ì‡∏ë‡πå
    # (Bug ‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ L3/L4/L5 ‡∏ú‡πà‡∏≤‡∏ô ‡∏ó‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà C=0 ‡∏´‡∏£‡∏∑‡∏≠ A=0)
    if real_pass:
        if level == 3:
            # L3 ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ C > 0 (‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 1.0 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô)
            if c <= 0.0:
                logger.warning(f"üö® L3 FAIL OVERRIDE: C_Check_Score is {c:.1f} (Must be > 0.0).")
                real_pass = False
        elif level == 4:
            # L4 ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ A > 0 (‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 1.0 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô)
            if a <= 0.0:
                logger.warning(f"üö® L4 FAIL OVERRIDE: A_Act_Score is {a:.1f} (Must be > 0.0).")
                real_pass = False
        elif level == 5:
            # L5 ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ C >= 2.0 ‡πÅ‡∏•‡∏∞ A >= 2.0
            if c < 2.0 or a < 2.0:
                logger.warning(f"üö® L5 FAIL OVERRIDE: L5 requires C={c:.1f} and A={a:.1f} (Must be >= 2.0 each).")
                real_pass = False

    if llm_output.get("is_passed") != real_pass:
        logger.critical(f"FORCING is_passed = {real_pass} (PDCA={pdca_real_sum} ‚â• {threshold}) [Post-Logic Check]")
        llm_output["is_passed"] = real_pass

    # 3. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå ‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å Round ‡πÅ‡∏•‡πâ‡∏ß
    llm_output.update({
        "P_Plan_Score": p, # <--- ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà Round ‡πÅ‡∏•‡πâ‡∏ß
        "D_Do_Score": d,
        "C_Check_Score": c,
        "A_Act_Score": a,
        "pdca_breakdown": {"P": p, "D": d, "C": c, "A": a},
        "pdca_sum": pdca_real_sum,
        "pass_threshold": threshold,
        "final_score": round(pdca_real_sum, 2), # <--- Round final_score
        "final_passed": real_pass
    })

    return llm_output

# =================================================================
# Configuration Class
# =================================================================
@dataclass
class AssessmentConfig:
    """Configuration for the SEAM PDCA Assessment Run."""
    
    # ------------------ 1. Assessment Context ------------------
    enabler: str = DEFAULT_ENABLER
    tenant: str = DEFAULT_TENANT
    year: int = DEFAULT_YEAR
    target_level: int = MAX_LEVEL
    mock_mode: str = "none" # 'none', 'random', 'control'
    force_sequential: bool = field(default=False) # Flag ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏£‡∏±‡∏ô‡πÅ‡∏ö‡∏ö Sequential

    # ------------------ 2. LLM Configuration (Configurable) ------------------
    # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Default ‡∏à‡∏≤‡∏Å global_vars.py
    model_name: str = DEFAULT_LLM_MODEL_NAME 
    temperature: float = LLM_TEMPERATURE

    # ------------------ 3. Adaptive RAG Retrieval Configuration ------------------
    # üü¢ NEW: ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô Rerank ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Adaptive Loop (MIN_RETRY_SCORE)
    # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Default 0.65 ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏û‡∏ö‡πÉ‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î Logic
    min_retry_score: float = MIN_RETRY_SCORE
    # üü¢ NEW: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≠‡∏ö‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á Adaptive RAG Loop (MAX_RETRIEVAL_ATTEMPTS)
    # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Default 3
    max_retrieval_attempts: int = 3
    
    # ------------------ 4. Export Configuration ------------------
    export_output: bool = field(default=False) # Flag ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏¥‡∏î/‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£ Export ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    export_path: str = "" # Path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå Export (‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å)


# =================================================================
# SEAM Assessment Engine (PDCA Focused)
# =================================================================
class SEAMPDCAEngine:
    
    L1_INITIAL_TOP_K_RAG: int = 50 
    MIN_RERANK_SCORE_TO_KEEP: Final[float] = MIN_RERANK_SCORE_TO_KEEP
    
    def __init__(
        self, 
        config: AssessmentConfig,
        llm_instance: Any = None, 
        logger_instance: logging.Logger = None,
        rag_retriever_instance: Any = None,
        doc_type: str = EVIDENCE_DOC_TYPES, 
        vectorstore_manager: Optional['VectorStoreManager'] = None,
        evidence_map_path: Optional[str] = None,
        document_map: Optional[Dict[str, str]] = None,
        is_parallel_all_mode: bool = False,
        sub_id: str = 'all'
    ):
        # =======================================================
        # üéØ Logger Setup (‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å‡∏™‡∏∏‡∏î)
        # =======================================================
        if logger_instance is not None:
            self.logger = logger_instance
        else:
            self.logger = logging.getLogger(__name__).getChild(
                f"Engine|{config.enabler}|{config.tenant}/{config.year}"
            )
        
        self.logger.info(f"Initializing SEAMPDCAEngine for {config.enabler} ({config.tenant}/{config.year})")

        # =======================================================
        # Core Configuration
        # =======================================================
        self.config = config
        self.enabler_id = config.enabler
        self.target_level = config.target_level
        self.sub_id = sub_id

        # Load rubric
        self.rubric = self._load_rubric()

        # Vectorstore & Doc Type
        self.vectorstore_manager = vectorstore_manager
        self.doc_type = doc_type

        # Constants
        self.FINAL_K_RERANKED = FINAL_K_RERANKED
        self.PRIORITY_CHUNK_LIMIT = PRIORITY_CHUNK_LIMIT

        # LLM
        self.llm = llm_instance

        # Mode flags
        self.is_sequential = config.force_sequential
        self.is_parallel_all_mode = is_parallel_all_mode
        self.logger.info(
            f"Engine mode: {'FULL PARALLEL (stateless)' if is_parallel_all_mode else 'SEQUENTIAL/MIXED (with hydration)'}"
        )

        self.required_pdca_map = REQUIRED_PDCA
        self.base_pdca_keywords = BASE_PDCA_KEYWORDS

        # Retry policy
        self.retry_policy = RetryPolicy(
            max_attempts=3,
            base_delay=2.0,
            jitter=True,
            escalate_context=True,
            shorten_prompt_on_fail=True,
            exponential_backoff=True,
        )

        # Thresholds
        self.RERANK_THRESHOLD: float = RERANK_THRESHOLD
        self.MAX_EVI_STR_CAP: float = MAX_EVI_STR_CAP

        # =======================================================
        # Persistent Evidence Mapping
        # =======================================================
        if evidence_map_path:
            self.evidence_map_path = evidence_map_path
        else:
            self.evidence_map_path = get_evidence_mapping_file_path(
                tenant=self.config.tenant,
                year=self.config.year,
                enabler=self.enabler_id
            )

        self.evidence_map: Dict[str, List[Dict]] = {}
        self.temp_map_for_save: Dict[str, List[Dict]] = {}

        # self.RERANK_THRESHOLD = 0.5
        # self.MAX_EVI_STR_CAP = 3.0

        # Load contextual rules and existing evidence map
        self.contextual_rules_map: Dict[str, Dict[str, Any]] = self._load_contextual_rules_map()
        self.evidence_map = self._load_evidence_map()

        self.logger.info(f"Persistent Map Path: {self.evidence_map_path}")
        self.logger.info(f"Loaded {len(self.evidence_map)} existing evidence entries.")

        # =======================================================
        # Function Pointers (with mocking support)
        # =======================================================
        self.llm_evaluator = evaluate_with_llm
        self.rag_retriever = retrieve_context_with_filter
        self.create_structured_action_plan = create_structured_action_plan

        if config.mock_mode in ["random", "control"]:
            self._set_mock_handlers(config.mock_mode)

        if config.mock_mode == "control":
            self.logger.info("Enabling global LLM data utils mock control mode.")
            set_llm_data_mock_mode(True)
        elif config.mock_mode == "random":
            self.logger.warning("Mock mode 'random' not fully implemented. Using default behavior.")

        # =======================================================
        # Lazy Initialization
        # =======================================================
        if self.llm is None:
            self._initialize_llm_if_none()
        if self.vectorstore_manager is None:
            self._initialize_vsm_if_none()

        # Force reload doc mapping to prevent hydration issues in workers
        if self.vectorstore_manager and not getattr(self.vectorstore_manager, '_doc_id_mapping', None):
            self.vectorstore_manager._load_doc_id_mapping()
            self.logger.info(
                f"Forced reload Doc ID Mapping: {len(self.vectorstore_manager._doc_id_mapping)} docs, "
                f"{len(self.vectorstore_manager._uuid_to_doc_id)} chunks"
            )

        # =======================================================
        # Document Map Loading (Filename Resolution)
        # =======================================================
        map_to_use: Dict[str, str] = document_map or {}

        if not map_to_use:
            mapping_path = get_mapping_file_path(
                self.doc_type,
                tenant=self.config.tenant,
                year=self.config.year,
                enabler=self.enabler_id
            )
            self.logger.info(f"Loading document_map from: {mapping_path}")

            try:
                with open(mapping_path, 'r', encoding='utf-8') as f:
                    doc_map_raw = json.load(f)
                map_to_use = {
                    doc_id: data.get("file_name", doc_id)
                    for doc_id, data in doc_map_raw.items()
                }
                self.logger.info(f"Loaded {len(map_to_use)} document mappings.")
            except FileNotFoundError:
                self.logger.warning(f"Document mapping file not found: {mapping_path}")
            except Exception as e:
                self.logger.error(f"Failed to load document map: {e}")

        self.doc_id_to_filename_map: Dict[str, str] = map_to_use
        self.document_map: Dict[str, str] = self.doc_id_to_filename_map

        if not self.doc_id_to_filename_map:
            self.logger.warning("Document ID ‚Üí Filename map is empty. Filename resolution limited.")

        self.ActionPlanActions = ActionPlanActions  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ
        if self.ActionPlanActions is None:
            self.ActionPlanActions = ActionPlanActions
            
        self.logger.info(f"Engine initialized: Enabler={self.enabler_id}, Mock={config.mock_mode}")
    
    def _initialize_llm_if_none(self):
        """Initializes LLM instance if self.llm is None."""
        if self.llm is None:
            self.logger.warning("‚ö†Ô∏è Initializing LLM: model=%s, temperature=%s", 
                                self.config.model_name, self.config.temperature)
            try:
                # üü¢ FIX: Import ‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ create_llm_instance
                from models.llm import create_llm_instance 
                self.llm = create_llm_instance( 
                    model_name=self.config.model_name,
                    temperature=self.config.temperature
                )
                self.logger.info("‚úÖ LLM Instance created successfully: %s (Temp: %s)", 
                                 self.config.model_name, self.config.temperature)
            except Exception as e:
                self.logger.error(f"FATAL: Could not initialize LLM: {e}")
                raise


    def _initialize_vsm_if_none(self):
        """
        Initializes VectorStoreManager if self.vectorstore_manager is None.
        Handles multi-tenant/multi-year vector store loading.
        """
        # NOTE: Assumes EVIDENCE_DOC_TYPES is imported from config.global_vars
        if self.vectorstore_manager is None:
            self.logger.info("Loading central evidence vectorstore(s)...")
            try:
                # üéØ FIX: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô evidence_enabler ‡πÄ‡∏õ‡πá‡∏ô enabler_filter ‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≤‡∏° load_all_vectorstores
                self.vectorstore_manager = load_all_vectorstores(
                    doc_types=[EVIDENCE_DOC_TYPES], 
                    enabler_filter=self.enabler_id, # <--- **‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ!**
                    tenant=self.config.tenant, 
                    year=self.config.year       
                )
                
                # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÇ‡∏´‡∏•‡∏î Doc ID Map ‡∏ã‡πâ‡∏≥‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Map ‡∏´‡∏≤‡∏¢‡πÉ‡∏ô Worker (Safety Net)
                if self.vectorstore_manager:
                    # NOTE: ‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å‡∏à‡∏∞‡∏ó‡∏≥‡∏†‡∏≤‡∏¢‡πÉ‡∏ô VSM.__init__ 
                    # ‡πÅ‡∏ï‡πà‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ã‡πâ‡∏≥‡∏ô‡∏µ‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏ß‡πà‡∏≤ Map ‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
                    self.vectorstore_manager._load_doc_id_mapping() 

                # ‡πÇ‡∏Ñ‡πâ‡∏î Log ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô
                len_retrievers = 0
                if self.vectorstore_manager and hasattr(self.vectorstore_manager, '_multi_doc_retriever') and self.vectorstore_manager._multi_doc_retriever:
                     # üí° ‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á _all_retrievers ‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡∏ú‡πà‡∏≤‡∏ô self.vectorstore_manager._multi_doc_retriever._all_retrievers
                     len_retrievers = len(
                        self.vectorstore_manager._multi_doc_retriever._all_retrievers
                    )
                     self.logger.info("‚úÖ MultiDocRetriever loaded with %s collections and cached in VSM.", 
                                 len_retrievers) 
                else:
                    self.logger.warning("VectorStoreManager loaded but MultiDocRetriever is None or missing expected attributes.")
                
                if len_retrievers == 0:
                    self.logger.error("FATAL: VectorStoreManager initialized but loaded 0 vector store collections. Check data path.")
                    raise ValueError("0 vector store collections loaded. Cannot proceed with assessment.")


            except Exception as e:
                # üìå Log ‡πÄ‡∏î‡∏¥‡∏°: ERROR - FATAL: Could not initialize VectorStoreManager: load_all_vectorstores() got an unexpected keyword argument 'evidence_enabler'
                # üìå ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏•‡πâ‡∏ß: ‡∏à‡∏∞‡πÄ‡∏à‡∏≠‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° Error ‡∏ó‡∏µ‡πà‡πÅ‡∏ó‡πâ‡∏à‡∏£‡∏¥‡∏á (‡πÄ‡∏ä‡πà‡∏ô No collections found)
                self.logger.error(f"FATAL: Could not initialize VectorStoreManager: {e}")
                raise # Re-raise the exception to ‡∏´‡∏¢‡∏∏‡∏î‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°

    def _get_applicable_contextual_rule(self, sub_id: str, level: int) -> Optional[Dict[str, Any]]:
        """
        ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Contextual Rule ‡∏ó‡∏µ‡πà‡∏°‡∏µ target_sub_criteria ‡πÅ‡∏•‡∏∞ target_level ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô
        """
        # self.contextual_rules_map ‡∏Ñ‡∏∑‡∏≠ dict ‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î‡∏à‡∏≤‡∏Å pea_km_contextual_rules.json
        for rule_name, rule_data in self.contextual_rules_map.items():
            if (rule_data.get('target_sub_criteria') == sub_id and 
                rule_data.get('target_level') == level):
                
                # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ä‡∏∑‡πà‡∏≠ Rule ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô Data ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏†‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏á
                rule_data['name'] = rule_name 
                return rule_data
        return None

    def _check_contextual_rule_condition(
        self, 
        condition: Dict[str, Any], 
        sub_id: str, 
        level: int, 
        previous_levels_evidence_dict: Dict[str, List[Dict[str, Any]]], 
        top_evidences: List[Dict[str, Any]]
    ) -> bool:
        """
        ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô Rule Condition (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö 'and' ‡πÅ‡∏•‡∏∞‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏¢‡πà‡∏≠‡∏¢)
        """
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç 'and' (‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô Contextual Rule ‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ‡πÅ‡∏Ñ‡πà 'and')
        if 'and' in condition:
            for sub_condition in condition['and']:
                
                # 1. check_passed_levels (‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö L1, L2 ‡∏ï‡πâ‡∏≠‡∏á PASS)
                if 'check_passed_levels' in sub_condition:
                    required_levels = sub_condition['check_passed_levels']
                    for required_level in required_levels:
                        if not self._is_previous_level_passed(sub_id, required_level):
                            self.logger.debug(f"Rule Condition Failed: {required_level} not passed.")
                            return False # ‡∏ñ‡πâ‡∏≤ L1/L2 ‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç Fail
                
                # 2. check_missing_pdca (‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏Å‡∏¥‡∏î Evidence Gap ‡πÉ‡∏ô D/C ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà)
                if 'check_missing_pdca' in sub_condition:
                    # Logic ‡∏ô‡∏µ‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏†‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å‡πÉ‡∏ô _run_single_assessment (missing_tags) 
                    # ‡πÅ‡∏ï‡πà‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ã‡πâ‡∏≥‡πÑ‡∏î‡πâ ‡πÅ‡∏ï‡πà‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ ‡πÉ‡∏´‡πâ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡∏ñ‡πâ‡∏≤‡∏°‡∏≤‡∏ñ‡∏∂‡∏á‡∏Ç‡∏±‡πâ‡∏ô‡∏ô‡∏µ‡πâ‡πÅ‡∏•‡πâ‡∏ß missing_tags ‡∏ñ‡∏π‡∏Å‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡πâ‡∏ß
                    pass
                
                # 3. check_evidence_exists (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô C/A ‡∏ó‡∏µ‡πà‡∏°‡∏µ Rerank Score ‡∏™‡∏π‡∏á)
                if 'check_evidence_exists' in sub_condition:
                    required_phases = sub_condition['check_evidence_exists'].get('phase', [])
                    min_rerank = sub_condition['check_evidence_exists'].get('min_rerank', globals().get('CRITICAL_CA_THRESHOLD', 0.65))
                    min_count = sub_condition['check_evidence_exists'].get('min_count', 1)
                    
                    found_count = 0
                    
                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÉ‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏ô‡∏û‡∏ö‡πÉ‡∏ô Level ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô (top_evidences)
                    for doc in top_evidences:
                        if doc.get('pdca_tag') in required_phases and doc.get('rerank_score', 0.0) >= min_rerank:
                            found_count += 1
                            
                    if found_count < min_count:
                        self.logger.debug(f"Rule Condition Failed: Found only {found_count} of required {min_count} {required_phases} with Rerank >= {min_rerank}.")
                        return False # ‡∏ñ‡πâ‡∏≤‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô C/A Critical ‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç Fail

            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏¢‡πà‡∏≠‡∏¢‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô 'and' ‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏£‡∏¥‡∏á
            return True 
        
        return False # ‡∏ñ‡πâ‡∏≤ Rule Format ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á

    def _is_previous_level_passed(self, sub_id: str, level: int) -> bool:
        """
        Helper: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡∏ú‡πà‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≤‡∏Å self.assessment_results_map
        (‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö check_passed_levels ‡πÉ‡∏ô Contextual Rule)
        """
        # Key ‡πÉ‡∏ô assessment_results_map ‡∏°‡∏±‡∏Å‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô '3.1.L1', '3.1.L2'
        key = f"{sub_id}.L{level}"
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏≠‡∏á Level ‡∏ô‡∏±‡πâ‡∏ô‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà ‡πÅ‡∏•‡∏∞‡∏°‡∏µ is_passed ‡πÄ‡∏õ‡πá‡∏ô True
        result = self.assessment_results_map.get(key)
        
        return result is not None and result.get('is_passed', False)

    def enhance_query_for_statement(
        self,
        statement_text: str,
        sub_id: str,
        statement_id: str,
        level: int,
        focus_hint: str,
    ) -> List[str]:
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á Query ‡∏ó‡∏µ‡πà‡∏â‡∏•‡∏≤‡∏î ‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏° PDCA ‡∏ó‡∏∏‡∏Å‡∏î‡πâ‡∏≤‡∏ô
        ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Contextual Map ‡∏ó‡∏µ‡πà Engine ‡∏ñ‡∏∑‡∏≠‡∏Ñ‡∏£‡∏≠‡∏á (self.contextual_rules_map)
        """
        logger = logging.getLogger(__name__)
        logger.info(f"Generating queries for {self.enabler_id} - {sub_id} L{level}")

        # --- 1. PDCA Synonyms ‡∏ï‡∏≤‡∏° Level (‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å Global Constant) ---
        current_synonyms = PDCA_LEVEL_SYNONYMS.get(level, "")

        # --- 2. ‡∏î‡∏∂‡∏á Keyword ‡∏à‡∏≤‡∏Å Contextual Map ‡∏ó‡∏µ‡πà Engine ‡∏ñ‡∏∑‡∏≠‡∏Ñ‡∏£‡∏≠‡∏á ---
        custom_keywords_list = []
        
        contextual_map = self.contextual_rules_map
        enabler_id = self.enabler_id

        # ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á Map ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Sub-Criteria ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô
        if contextual_map and sub_id in contextual_map:
            sub_map = contextual_map[sub_id]

            # Logic ‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á Keywords ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏Å‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö L1-L5 (‡πÄ‡∏ô‡πâ‡∏ô Cross-Phase Keyword)
            if level == 1:
                custom_keywords_list.append(sub_map.get('L1', {}).get('plan_keywords', ''))
            elif level == 2:
                custom_keywords_list.append(sub_map.get('L2', {}).get('do_keywords', ''))
            elif level == 3: # L3: Check & Do
                # ‡∏î‡∏∂‡∏á Keywords ‡∏Ç‡∏≠‡∏á C ‡πÅ‡∏•‡∏∞ D ‡∏°‡∏≤‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ä‡πà‡∏ß‡∏¢ RAG/Reranker
                custom_keywords_list.append(sub_map.get('L3', {}).get('check_keywords', ''))
                custom_keywords_list.append(sub_map.get('L3', {}).get('do_keywords', ''))
            elif level == 4: # L4: Check & Act
                custom_keywords_list.append(sub_map.get('L4', {}).get('act_keywords', ''))
                custom_keywords_list.append(sub_map.get('L4', {}).get('check_keywords', ''))
            elif level == 5: # L5: Act & Check
                custom_keywords_list.append(sub_map.get('L5', {}).get('act_keywords', ''))
                custom_keywords_list.append(sub_map.get('L5', {}).get('check_keywords', ''))

        # ‡∏£‡∏ß‡∏° Custom Keywords ‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ö Synonyms
        custom_keywords_str = ", ".join(filter(None, custom_keywords_list))
        if custom_keywords_str:
            current_synonyms += f", {custom_keywords_str}"

        # --- 3. Base Query (‡∏´‡∏•‡∏±‡∏Å) ---
        base_query = f"**{statement_text}** {sub_id} L{level} {enabler_id} ‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: {current_synonyms}"
        queries = [base_query]

        # --- 4. Dedicated Queries ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö L3+ ---
        if level >= 3:
            # Query 1: ‡πÄ‡∏ô‡πâ‡∏ô C (‡∏Å‡∏≤‡∏£‡∏ß‡∏±‡∏î‡∏ú‡∏•/‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô)
            queries.append(
                f"‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏• ‡∏Å‡∏≤‡∏£‡∏ß‡∏±‡∏î‡∏ú‡∏• KPI Audit ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô {statement_text} {sub_id} ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡∏õ‡∏µ ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á"
            )
            # Query 2: ‡πÄ‡∏ô‡πâ‡∏ô A (‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç)
            queries.append(
                f"‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç Corrective Action ‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö {statement_text} {sub_id} ‡∏ï‡∏≤‡∏°‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô"
            )
            # Query 3: ‡∏£‡∏ß‡∏° PDCA 4 ‡∏î‡πâ‡∏≤‡∏ô (D, C, A) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏£‡∏ö‡∏ß‡∏á‡∏à‡∏£
            queries.append(
                f"‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£ ‡∏Å‡∏≤‡∏£‡∏ß‡∏±‡∏î‡∏ú‡∏• ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á {statement_text} (PDCA) {sub_id} {enabler_id}"
            )

        # --- 5. L5 Special ---
        if level == 5:
            # Query 4: ‡πÄ‡∏ô‡πâ‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏±‡πà‡∏á‡∏¢‡∏∑‡∏ô (Optimization)
            queries.append(
                f"‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏° ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏±‡πà‡∏á‡∏¢‡∏∑‡∏ô Best Practice ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏• {statement_text} {sub_id} {enabler_id}"
            )
            # Query 5: ‡πÄ‡∏ô‡πâ‡∏ô‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏ö‡∏ó‡∏ß‡∏ô (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ï‡πà‡∏≠ 1.1)
            queries.append(
                f"‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏° ‡∏î‡∏π‡πÅ‡∏• ‡πÅ‡∏•‡∏∞‡∏™‡∏ô‡∏±‡∏ö‡∏™‡∏ô‡∏∏‡∏ô {statement_text} ‡∏Å‡∏≤‡∏£‡∏ó‡∏ö‡∏ó‡∏ß‡∏ô‡∏ß‡∏¥‡∏™‡∏±‡∏¢‡∏ó‡∏±‡∏®‡∏ô‡πå"
            )

        # --- 6. ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô + Log ---
        queries = queries[:6]
        logger.info(f"Generated {len(queries)} queries for {enabler_id} - {sub_id} L{level}")
        return queries
    

    def _resolve_evidence_filenames(self, evidence_entries: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á
        1. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà doc_id ‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ 'UNKNOWN-' (‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏†‡∏≤‡∏¢‡πÉ‡∏ô/‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£)
        2. ‡πÅ‡∏õ‡∏•‡∏á doc_id (‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô Hash/UUID) ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏î‡πâ ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ doc_id_to_filename_map
        """
        
        resolved_entries = []
        
        for entry in evidence_entries:
            # ‡πÉ‡∏ä‡πâ deepcopy ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
            resolved_entry = deepcopy(entry)
            doc_id = resolved_entry.get("doc_id", "")
            current_filename = resolved_entry.get("filename", "") # ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏î‡∏¥‡∏°‡∏à‡∏≤‡∏Å Metadata (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
            
            # --- 1. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏ì‡∏µ UNKNOWN- (AI-GENERATED or Lost Source) ---
            if doc_id.startswith("UNKNOWN-"):
                resolved_entry["filename"] = f"AI-GENERATED-REF-{doc_id.split('-')[-1]}"
                resolved_entries.append(resolved_entry)
                continue

            # --- 2. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏ì‡∏µ Doc ID (Hash/UUID) ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ---
            if doc_id:
                # A. ‡∏•‡∏≠‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å Map
                if doc_id in self.doc_id_to_filename_map:
                    resolved_entry["filename"] = self.doc_id_to_filename_map[doc_id]
                    resolved_entries.append(resolved_entry)
                    continue

                # B. ‡∏ñ‡πâ‡∏≤‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ (Map Fail)
                else:
                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏Å‡∏±‡∏ö Metadata ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏™‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                    is_generic_name = (
                        not current_filename.strip() or # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô String ‡∏ß‡πà‡∏≤‡∏á
                        current_filename.lower() == "unknown" or
                        # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Hash/UUID 64 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•
                        re.match(r"^[0-9a-f]{64}(\.pdf|\.txt)?$", current_filename, re.IGNORECASE)
                    )
                    
                    if is_generic_name:
                        # ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå Fallback ‡∏ó‡∏µ‡πà‡∏™‡∏∑‡πà‡∏≠‡∏ß‡πà‡∏≤ Map ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
                        resolved_entry["filename"] = f"MAPPING-FAILED-{doc_id[:8]}..."
                        self.logger.warning(f"Failed to map doc_id {doc_id[:8]}... to filename. Using fallback.")
                        
            # --- 3. ‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏°‡πà‡∏°‡∏µ Doc ID ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ (‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÄ‡∏õ‡πá‡∏ô Unknown) ---
            elif not doc_id and (not current_filename.strip() or current_filename.lower() == "unknown"):
                # ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ doc_id ‡πÅ‡∏•‡∏∞ filename ‡πÄ‡∏î‡∏¥‡∏°‡∏Å‡πá‡πÄ‡∏õ‡πá‡∏ô Unknown/Empty
                resolved_entry["filename"] = "MISSING-SOURCE-METADATA"
                self.logger.error("Evidence found with no doc_id and generic filename.")
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏° entry ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ (‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà)
            resolved_entries.append(resolved_entry)

        return resolved_entries
    
    # -------------------- Contextual Rules Handlers (FIXED) --------------------
    def _load_contextual_rules_map(self) -> Dict[str, Dict[str, str]]:
        """
        Loads the contextual rules JSON file using the path generated by 
        utils.path_utils.get_contextual_rules_file_path.
        """
        
        try:
            # üéØ ‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏à‡∏≤‡∏Å path_utils ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Path ‡πÄ‡∏≠‡∏á
            filepath = get_contextual_rules_file_path(
                tenant=self.config.tenant,
                enabler=self.enabler_id
            )
        except ImportError:
            self.logger.error("‚ùå FATAL: Cannot import get_contextual_rules_file_path. Check utils/path_utils.py import.")
            return {}

        # 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if not os.path.exists(filepath):
            # ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ get_contextual_rules_file_path ‡∏ó‡∏≥‡πÅ‡∏ó‡∏ô
            self.logger.info(f"‚ö†Ô∏è Contextual Rules file not found at: {filepath}. Using empty map.")
            return {}

        self.logger.info(f"‚úÖ Contextual Rules loaded from: {filepath}")
        
        # 2. ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
                self.logger.info(f"‚úÖ Loaded Contextual Rules: {len(data)} sub-criteria rules.")
                return data
        except json.JSONDecodeError as e:
            self.logger.error(f"‚ùå Failed to parse Contextual Rules JSON from {filepath}: {e}")
            return {}
        except Exception as e:
            self.logger.error(f"‚ùå Failed to load Contextual Rules from {filepath}: {e}")
            return {}
    

    # ----------------------------------------------------------------------
    # üéØ FINAL FIX 2.3: Manual Map Reload Function (inside SEAMPDCAEngine)
    # ----------------------------------------------------------------------

    def _collect_previous_level_evidences(self, sub_id: str, current_level: int) -> Dict[str, List[Dict]]:
        """
        ‡∏î‡∏∂‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏à‡∏≤‡∏Å Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ (L1 ‚Üí L2, L2 ‚Üí L3 ‡∏Ø‡∏•‡∏Ø) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô Baseline Context

        Final Production Version - 14 ‡∏ò.‡∏Ñ. 2568 (ULTIMATE VICTORY EDITION)
        - ‡πÄ‡∏û‡∏¥‡πà‡∏° is_parallel_all_mode ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ç‡πâ‡∏≤‡∏° hydration ‡πÉ‡∏ô full parallel mode (by design)
        - ‡∏£‡∏±‡∏Å‡∏©‡∏≤ Heuristic Fallback ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ CRITICAL MAPPING FAILURE
        """
        
        # --------------------------------------------------------------
        # 1. ‡∏Ç‡πâ‡∏≤‡∏° Hydration ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô Full Parallel Mode (by design)
        # --------------------------------------------------------------
        if self.is_parallel_all_mode:
            self.logger.info("FULL PARALLEL MODE: Skipping previous level evidence hydration (stateless by design)")
            return {}  # ‡∏Ñ‡∏∑‡∏ô map ‡∏ß‡πà‡∏≤‡∏á ‚Üí ‡πÑ‡∏°‡πà hydrate priority chunks ‡∏à‡∏≤‡∏Å level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤

        # --------------------------------------------------------------
        # 2. ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏° evidence ‡∏à‡∏≤‡∏Å Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏ô Sub-Criteria ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
        # --------------------------------------------------------------
        collected = {}
        for key, ev_list in self.evidence_map.items():
            if (key.startswith(f"{sub_id}.L") and 
                isinstance(ev_list, list) and 
                ev_list):
                try:
                    level_num = int(key.split(".L")[-1])
                    if level_num < current_level:
                        collected[key] = ev_list
                except (ValueError, IndexError):
                    continue

        if not collected:
            self.logger.info("No previous level evidences found for hydration.")
            return {}

        # --------------------------------------------------------------
        # 3. ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏° Stable IDs + Chunk UUIDs (cleaned)
        # --------------------------------------------------------------
        stable_ids = set()
        chunk_uuids_clean = set()

        for ev_list in collected.values():
            for ev in ev_list:
                sid = ev.get("stable_doc_uuid") or ev.get("doc_id")
                if isinstance(sid, str) and len(sid) == 64 and sid.isalnum():
                    stable_ids.add(sid)
                
                cid = ev.get("chunk_uuid")
                if isinstance(cid, str) and len(cid.replace("-", "")) >= 64:
                    chunk_uuids_clean.add(cid.replace("-", ""))

        if not stable_ids and not chunk_uuids_clean:
            self.logger.info("No valid IDs found for hydration.")
            return collected

        # --------------------------------------------------------------
        # 4. ‡πÅ‡∏õ‡∏•‡∏á Stable ‚Üí Chunk UUIDs (‡πÄ‡∏û‡∏∑‡πà‡∏≠ Log ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏Ç‡∏≠‡∏á‡πÇ‡∏Ñ‡πâ‡∏î)
        # --------------------------------------------------------------
        vsm = self.vectorstore_manager
        final_uuids = list(chunk_uuids_clean)
        self.logger.info(f"HYDRATION ‚Üí Resolved {len(final_uuids)} unique chunk UUIDs ‚Üí fetching full text...")

        stable_ids_list = list(stable_ids)
        if not stable_ids_list:
            self.logger.warning("No Stable IDs resolved for VSM hydration call.")
            return collected

        # --------------------------------------------------------------
        # 5. ‡∏î‡∏∂‡∏á full chunks (‡πÉ‡∏ä‡πâ Stable IDs 64-char)
        # --------------------------------------------------------------
        try:
            full_chunks = vsm.get_documents_by_id(stable_ids_list, self.doc_type, self.enabler_id) 
            self.logger.info(f"HYDRATION success: Retrieved {len(full_chunks)} full chunks (via Stable ID search)")
            
        except Exception as e:
            self.logger.error(f"Hydration failed in VSM call (get_documents_by_id): {e}", exc_info=True)
            return collected

        # --------------------------------------------------------------
        # 6. ‡∏™‡∏£‡πâ‡∏≤‡∏á map ‡πÅ‡∏•‡∏∞ hydrate text ‡∏û‡∏£‡πâ‡∏≠‡∏° Fallback Logic (FINAL FIX 27.0)
        # --------------------------------------------------------------
        chunk_map = {}  # Key: Cleaned V4 UUID (without dashes)
        total_retrieved = len(full_chunks)
        
        # 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á Map (Key: V4 UUID Cleaned)
        for idx, chunk in enumerate(full_chunks):
            meta = getattr(chunk, "metadata", {})
            cid_raw = meta.get("chunk_uuid")
            cid = (cid_raw or "").replace("-", "") 
            
            if cid:
                chunk_map[cid] = {
                    "text": chunk.page_content,
                    "metadata": meta
                }
            else:
                self.logger.error(f"CRITICAL HYDRATION ERROR: Retrieved chunk {idx+1}/{total_retrieved} has NO or empty 'chunk_uuid' in metadata. Skipping this chunk.")

        self.logger.info(f"DEBUG: Chunk Map built with {len(chunk_map)}/{total_retrieved} entries.")

        hydrated = {}
        restored = 0
        total = sum(len(v) for v in collected.values())

        for key, ev_list in collected.items():
            new_list = []
            for ev in ev_list:
                new_ev = ev.copy()
                data = None
                
                # ID ‡∏à‡∏≤‡∏Å evidence level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤
                cid_l1 = (ev.get("chunk_uuid") or "").replace("-", "") 
                sid_l1 = ev.get("stable_doc_uuid") or ev.get("doc_id") 
                vsm_mapping_failed = True

                # --- 1. Primary Lookup ---
                if cid_l1:
                    data = chunk_map.get(cid_l1)

                # --- 2. Fallback: VSM Mapping ---
                if not data and sid_l1 and vsm and hasattr(vsm, '_doc_id_mapping') and vsm._doc_id_mapping:
                    if cid_l1 and not data:
                        self.logger.warning(f"Hydration Check: Primary L1 chunk_uuid '{cid_l1[:8]}...' NOT found in map. Starting Stable ID Fallback...")
                    
                    if sid_l1 in vsm._doc_id_mapping:
                        vsm_mapping_failed = False
                        for v4_uuid_raw in vsm._doc_id_mapping[sid_l1].get("chunk_uuids", []):
                            v4_uuid_cleaned = v4_uuid_raw.replace("-", "")
                            if v4_uuid_cleaned in chunk_map:
                                data = chunk_map[v4_uuid_cleaned]
                                self.logger.info(f"‚úÖ Fallback SUCCESS (VSM Map): Matched via V4 UUID '{v4_uuid_cleaned[:8]}...'")
                                break
                    else:
                        self.logger.warning(f"Hydration Check: Stable ID {sid_l1[:8]}... NOT found in VSM Doc ID Mapping.")

                # --- 3. HEURISTIC FALLBACK (Final Bypass) ---
                if not data:
                    if not vsm_mapping_failed:
                        self.logger.warning(f"‚ö†Ô∏è VSM Map exists but failed. Attempting Heuristic Match by Stable ID.")
                    
                    for retrieved_chunk_data in chunk_map.values():
                        retrieved_sid = retrieved_chunk_data["metadata"].get("stable_doc_uuid")
                        if retrieved_sid == sid_l1:
                            data = retrieved_chunk_data
                            new_ev["chunk_uuid"] = retrieved_chunk_data["metadata"].get("chunk_uuid", new_ev["chunk_uuid"])
                            self.logger.info(f"üü¢ Heuristic SUCCESS: Restored using matching Stable ID {sid_l1[:8]}...")
                            break

                # --------------------------------------------------------------------------
                if data:
                    new_ev["text"] = data["text"]
                    new_ev.update({k: v for k, v in data["metadata"].items() 
                                if k not in ["text", "page_content"]})
                    restored += 1
                    
                    # üéØ CRITICAL FIX: ‡πÄ‡∏û‡∏¥‡πà‡∏° Flag is_baseline
                    new_ev["is_baseline"] = True 
                    
                else:
                    sid_l1 = ev.get("stable_doc_uuid") or ev.get("doc_id")
                    self.logger.error(f"‚ùå CRITICAL MAPPING FAILURE: Could not restore chunk (Stable ID: {sid_l1[:8] if sid_l1 else 'N/A'}...) from {len(chunk_map)} retrieved chunks.")
                    new_ev["is_baseline"] = False # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏õ‡πá‡∏ô False ‡∏´‡∏≤‡∏Å‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ hydrate
                
                new_list.append(new_ev)
            hydrated[key] = new_list
                
        self.logger.info(f"BASELINE HYDRATED ‚Üí {restored}/{total} chunks restored with full text (including fallback)")
        return hydrated

    def _get_contextual_rules_prompt(self, sub_id: str, level: int) -> str:
        """
        Retrieves the specific Contextual Rule prompt for a given Sub-Criteria and Level,
        ‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á‡∏Å‡∏≤‡∏£ Inject ‡∏Å‡∏é L5 ‡∏û‡∏¥‡πÄ‡∏®‡∏© ‡∏´‡∏≤‡∏Å Level == 5
        """
        sub_id_rules = self.contextual_rules_map.get(sub_id)
        rule_text = ""
        
        # 1. ‡∏î‡∏∂‡∏á‡∏Å‡∏é‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏¢‡πà‡∏≠‡∏¢ (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
        if sub_id_rules:
            level_key = f"L{level}"
            specific_rule = sub_id_rules.get(level_key)
            if specific_rule:
                rule_text += f"\n--- ‡∏Å‡∏é‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏¢‡πà‡∏≠‡∏¢ ({sub_id} L{level}) ---\n‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Ñ‡∏ß‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ: {specific_rule}\n"
        
        # 2. **INJECT L5 SPECIAL RULE (Safe Injection)**
        # ‡πÉ‡∏™‡πà‡∏Å‡∏é Bonus 2.0 ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô L5 ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏ö‡∏Å‡∏ß‡∏ô L3/L4
        if level == 5:
            l5_bonus_rule = """
            \n--- L5 SPECIAL RULE (Innovation & Sustainability) ---
            * **L5 PASS Condition (‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö):** ‡∏´‡∏≤‡∏Å Level ‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠ **L5** ‡∏ó‡πà‡∏≤‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏° PDCA (P+D+C+A) ‡∏ï‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô L3/L4 ‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏ô‡∏û‡∏ö‡∏Å‡πà‡∏≠‡∏ô (‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î 8.0)
            * **‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç Bonus 2.0:** ‡∏´‡∏≤‡∏Å‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏° PDCA ‡πÑ‡∏î‡πâ **‚â• 7.0** **‡πÅ‡∏•‡∏∞** ‡∏ó‡πà‡∏≤‡∏ô‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠ **‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 1 ‡∏ä‡∏¥‡πâ‡∏ô** ‡πÉ‡∏ô Context ‡∏ó‡∏µ‡πà‡πÅ‡∏™‡∏î‡∏á‡∏ñ‡∏∂‡∏á:
                * (a) ‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏• KM / ‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏° (Innovation Award)
                * (b) ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡∏ó‡∏µ‡πà‡∏ß‡∏±‡∏î‡πÑ‡∏î‡πâ (ROI, Productivity, Cost Saving)
                * (c) ‡∏Å‡∏≤‡∏£‡πÄ‡∏ú‡∏¢‡πÅ‡∏û‡∏£‡πà/‡∏Å‡∏≤‡∏£‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å (External Recognition/Publication)
            * **‡πÉ‡∏´‡πâ‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡πÉ‡∏´‡πâ Bonus Score 2.0 ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ **Score ‚â• 9.0** ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡πâ‡∏á **is_passed=true** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏∞‡∏ó‡πâ‡∏≠‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏•‡∏¥‡∏® (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£ Reset ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô)
            """
            rule_text += l5_bonus_rule
            
        return rule_text

    def _load_rubric(self) -> Dict[str, Any]:
        """ Loads the SEAM rubric JSON file using path_utils. """
        
        # üéØ FIX: ‡πÉ‡∏ä‡πâ get_rubric_file_path ‡∏à‡∏≤‡∏Å path_utils ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Path ‡πÄ‡∏≠‡∏á
        filepath = None # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô UnboundLocalError
        
        try:
            # 1. ‡∏£‡∏±‡∏ö Path ‡∏à‡∏≤‡∏Å path_utils ‡∏ã‡∏∂‡πà‡∏á‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏ä‡∏µ‡πâ‡πÑ‡∏õ‡∏ó‡∏µ‡πà 'config/' ‡πÅ‡∏•‡πâ‡∏ß
            filepath = get_rubric_file_path(
                tenant=self.config.tenant,
                enabler=self.enabler_id
            )
        except Exception as e:
            # ‡∏î‡∏±‡∏Å‡∏à‡∏±‡∏ö Exception ‡∏´‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏î‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô path_utils
            self.logger.error(f"‚ùå FATAL: Error calling get_rubric_file_path: {e}")
            return {} 

        # 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if filepath is None or not os.path.exists(filepath):
            self.logger.error(f"‚ö†Ô∏è Rubric file not found at expected path: {filepath}")
            return {}

        # 3. ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå JSON
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
            self.logger.info(f"‚úÖ Rubric loaded successfully from: {filepath}")
            return data
        except json.JSONDecodeError:
            self.logger.error(f"‚ùå Error decoding Rubric JSON. File might be corrupted: {filepath}")
            return {}
        except Exception as e:
            self.logger.error(f"‚ùå Error loading Rubric file from {filepath}: {e}")
            return {}
    
    # -------------------- Helper Function for Map Processing --------------------
    def _get_level_order_value(self, level_str: str) -> int:
        """Converts Level string ('L1', 'L5') to an integer for comparison."""
        try:
            return int(level_str.upper().replace('L', ''))
        except:
            return 0

    # -------------------- Persistent Mapping Handlers (FIXED) --------------------
    def _process_temp_map_to_final_map(self, temp_map: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Converts the temporary map into the final map format for saving, 
        and filters out temporary/unresolvable evidence IDs.
        """
        working_map = temp_map or self.temp_map_for_save or {}
        final_map_for_save = {}
        total_cleaned_items = 0

        for sub_level_key, evidence_list in working_map.items():
            if isinstance(evidence_list, dict):
                evidence_list = [evidence_list]
            elif not isinstance(evidence_list, list):
                logger.warning(f"[EVIDENCE] Skipping {sub_level_key}: not a list or dict")
                continue

            clean_list = []
            seen_ids = set()
            for ev in evidence_list:
                doc_id = ev.get("doc_id")
                
                if not doc_id:
                    continue
                
                # 1. FIX: ‡∏Å‡∏£‡∏≠‡∏á ID ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß (TEMP-) ‡∏≠‡∏≠‡∏Å
                if doc_id.startswith("TEMP-"):
                    # ID ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏õ‡∏•‡∏á‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô Stable Document ID ‡πÑ‡∏î‡πâ ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÉ‡∏ä‡πâ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ
                    logger.debug(f"[EVIDENCE] Filtering out unresolvable TEMP- ID: {doc_id} for {sub_level_key}.")
                    continue 
                
                # 2. Logic ‡πÄ‡∏î‡∏¥‡∏°: ‡∏Å‡∏£‡∏≠‡∏á HASH- (Placeholder) ‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ã‡πâ‡∏≥
                if doc_id.startswith("HASH-") or doc_id in seen_ids:
                    continue
                    
                seen_ids.add(doc_id)
                clean_list.append(ev)
                total_cleaned_items += 1 

            if clean_list:
                final_map_for_save[sub_level_key] = clean_list

        logger.info(f"[EVIDENCE] Processed {len(final_map_for_save)} sub-level keys with total {total_cleaned_items} evidence items")
        return final_map_for_save

    def _clean_map_for_json(self, data: Union[Dict, List, Set, Any]) -> Union[Dict, List, Any]:
        """Recursively converts objects that cannot be serialized (like sets) into lists."""
        if isinstance(data, dict):
            return {k: self._clean_map_for_json(v) for k, v in data.items()}
        elif isinstance(data, list):
            return [self._clean_map_for_json(v) for v in data]
        elif isinstance(data, set):
            return [self._clean_map_for_json(v) for v in data]
        return data

    def _clean_temp_entries(self, evidence_map: Dict[str, List[Dict]]) -> Dict[str, List[Dict]]:
        """
        ‡∏Å‡∏£‡∏≠‡∏á TEMP-, HASH-, ‡πÅ‡∏•‡∏∞ Unknown ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å evidence map ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        ‡πÉ‡∏ä‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏ï‡∏≠‡∏ô merge ‡πÅ‡∏•‡∏∞‡∏Å‡πà‡∏≠‡∏ô save ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î 100%
        """
        if not evidence_map:
            return {}

        cleaned_map = {}
        total_removed = 0
        total_unknown_fixed = 0

        for key, entries in evidence_map.items():
            valid_entries = []
            for entry in entries:
                doc_id = entry.get("doc_id", "")

                # 1. ‡∏Å‡∏£‡∏≠‡∏á TEMP- ‡πÅ‡∏•‡∏∞ HASH- ‡∏≠‡∏≠‡∏Å‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≤‡∏î
                if str(doc_id).startswith("TEMP-") or str(doc_id).startswith("HASH-"):
                    total_removed += 1
                    continue

                # 2. ‡∏ñ‡πâ‡∏≤ doc_id ‡∏ß‡πà‡∏≤‡∏á ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏•‡∏¢ ‚Üí ‡∏ó‡∏¥‡πâ‡∏á
                if not doc_id or doc_id == "Unknown":
                    total_removed += 1
                    continue

                # 3. ‡πÅ‡∏Å‡πâ filename ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô Unknown / None / ‡∏ß‡πà‡∏≤‡∏á
                filename = entry.get("filename", "").strip()
                if not filename or filename == "Unknown" or filename.lower() == "unknown_file.pdf":
                    # ‡πÉ‡∏ä‡πâ doc_id ‡∏™‡∏±‡πâ‡∏ô ‡πÜ ‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß (‡∏î‡∏π‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤)
                    short_id = doc_id[:8]
                    entry["filename"] = f"‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á_{short_id}.pdf"
                    total_unknown_fixed += 1
                else:
                    # ‡πÄ‡∏≠‡∏≤ path ‡∏≠‡∏≠‡∏Å ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÅ‡∏Ñ‡πà‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå
                    entry["filename"] = os.path.basename(filename)

                valid_entries.append(entry)

            if valid_entries:
                cleaned_map[key] = valid_entries
            else:
                logger.debug(f"[CLEAN] Key {key} ‡∏Å‡∏•‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏ß‡πà‡∏≤‡∏á‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏£‡∏≠‡∏á ‚Üí ‡∏ñ‡∏π‡∏Å‡∏•‡∏ö‡∏≠‡∏≠‡∏Å")

        logger.info(f"[CLEANUP] ‡∏•‡∏ö TEMP-/HASH- ‡∏≠‡∏≠‡∏Å {total_removed} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | "
                    f"‡πÅ‡∏Å‡πâ Unknown filename {total_unknown_fixed} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | "
                    f"‡πÄ‡∏´‡∏•‡∏∑‡∏≠ {len(cleaned_map)} keys")

        return cleaned_map
    
    def _save_evidence_map(self, map_to_save: Optional[Dict[str, List[Dict[str, Any]]]] = None):
        """
        ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å evidence map ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢ 100% - Atomic Write + FileLock + Cleanup + Raw LLM Data
        ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å raw_llm_pdca ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÇ‡∏õ‡∏£‡πà‡∏á‡πÉ‡∏™‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏¢‡πâ‡∏≠‡∏ô‡∏Å‡∏•‡∏±‡∏ö
        """
        try:
            map_file_path = get_evidence_mapping_file_path(
                tenant=self.config.tenant,
                year=self.config.year,
                enabler=self.enabler_id
            )
        except Exception as e:
            self.logger.critical(f"[EVIDENCE] FATAL: ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Evidence Map Path ‡πÑ‡∏î‡πâ: {e}")
            raise

        lock_path = map_file_path + ".lock"
        tmp_path = None

        self.logger.info(f"[EVIDENCE] Preparing to save evidence map ‚Üí {map_file_path}")

        try:
            os.makedirs(os.path.dirname(map_file_path), exist_ok=True)

            with FileLock(lock_path, timeout=60):
                self.logger.debug("[EVIDENCE] File lock acquired.")

                # === Merge Logic ===
                if map_to_save is not None:
                    final_map_to_write = map_to_save
                else:
                    existing_map = self._load_evidence_map(is_for_merge=True) or {}
                    runtime_map = deepcopy(self.evidence_map)
                    final_map_to_write = existing_map

                    for key, new_entries in runtime_map.items():
                        entry_map = {
                            e.get("chunk_uuid", e.get("doc_id", "N/A")): e
                            for e in final_map_to_write.setdefault(key, [])
                        }
                        for new_entry in new_entries:
                            entry_id = new_entry.get("chunk_uuid", new_entry.get("doc_id", "N/A"))
                            if entry_id == "N/A" or not entry_id:
                                continue

                            new_score = new_entry.get("relevance_score", 0.0)

                            if entry_id not in entry_map:
                                entry_map[entry_id] = new_entry
                            else:
                                # Update ‡∏ñ‡πâ‡∏≤ score ‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤ ‡∏´‡∏£‡∏∑‡∏≠‡∏ñ‡πâ‡∏≤‡∏°‡∏µ raw_llm_pdca ‡∏ó‡∏µ‡πà‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Å‡∏ß‡πà‡∏≤
                                old_entry = entry_map[entry_id]
                                old_score = old_entry.get("relevance_score", 0.0)
                                if new_score > old_score:
                                    entry_map[entry_id] = new_entry

                        final_map_to_write[key] = list(entry_map.values())

                if not final_map_to_write:
                    self.logger.warning("[EVIDENCE] Nothing to save (empty map).")
                    return

                # === Cleanup + Sort ===
                final_map_to_write = self._clean_temp_entries(final_map_to_write)
                for key, entries in final_map_to_write.items():
                    entries.sort(key=lambda x: x.get("relevance_score", 0.0), reverse=True)

                # === Atomic Write ===
                with tempfile.NamedTemporaryFile(
                    mode='w', delete=False, encoding="utf-8", dir=os.path.dirname(map_file_path)
                ) as tmp_file:
                    cleaned_data = self._clean_map_for_json(final_map_to_write)
                    json.dump(cleaned_data, tmp_file, indent=4, ensure_ascii=False)
                    tmp_path = tmp_file.name

                shutil.move(tmp_path, map_file_path)
                tmp_path = None

                # === Stats ===
                total_keys = len(final_map_to_write)
                total_items = sum(len(v) for v in final_map_to_write.values())
                file_size_kb = os.path.getsize(map_file_path) / 1024
                self.logger.info(
                    f"[EVIDENCE] SAVED SUCCESSFULLY! "
                    f"Keys: {total_keys} | Items: {total_items} | Size: ~{file_size_kb:.1f} KB | Path: {map_file_path}"
                )

        except Exception as e:
            self.logger.critical("[EVIDENCE] FATAL ERROR DURING SAVE")
            self.logger.exception(e)
            raise

        finally:
            # === Cleanup lock & temp file (Double Safety) ===
            if os.path.exists(lock_path):
                try:
                    os.unlink(lock_path)
                    self.logger.debug(f"[EVIDENCE] Removed lock file: {lock_path}")
                except Exception as e:
                    self.logger.warning(f"[EVIDENCE] Failed to remove lock: {e}")

            if tmp_path and os.path.exists(tmp_path):
                try:
                    os.unlink(tmp_path)
                except:
                    pass

    def _load_evidence_map(self, is_for_merge: bool = False) -> Dict[str, List[Dict[str, Any]]]:
        """
        ‡πÇ‡∏´‡∏•‡∏î evidence map ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
        is_for_merge = True ‚Üí ‡πÑ‡∏°‡πà log "No existing map" (‡πÉ‡∏ä‡πâ‡πÉ‡∏ô save)
        """
        try:
            path = get_evidence_mapping_file_path(
                tenant=self.config.tenant,
                year=self.config.year,
                enabler=self.enabler_id
            )
        except Exception as e:
            self.logger.error(f"[EVIDENCE] FATAL: ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏î‡πâ: {e}")
            return {}

        if not os.path.exists(path):
            if not is_for_merge:
                self.logger.info("[EVIDENCE] No existing evidence map found ‚Äì starting fresh.")
            return {}

        try:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
            if not is_for_merge:
                total_items = sum(len(v) for v in data.values() if isinstance(v, list))
                self.logger.info(f"[EVIDENCE] Loaded evidence map: {len(data)} keys, {total_items} items from {path}")
            return data
        except Exception as e:
            self.logger.error(f"[EVIDENCE] Failed to load evidence map from {path}: {e}")
            return {}


    def _set_mock_handlers(self, mode: str):
        """Replaces real LLM/RAG functions with mock versions."""
        if mode == "control" or mode == "random":
            if hasattr(seam_mocking, 'evaluate_with_llm_CONTROLLED_MOCK'):
                self.llm_evaluator = seam_mocking.evaluate_with_llm_CONTROLLED_MOCK
            if hasattr(seam_mocking, 'retrieve_context_with_filter_MOCK'):
                self.rag_retriever = seam_mocking.retrieve_context_with_filter_MOCK
            if hasattr(seam_mocking, 'create_structured_action_plan_MOCK'):
                self.action_plan_generator = seam_mocking.create_structured_action_plan_MOCK
            if hasattr(seam_mocking, 'set_mock_control_mode'):
                seam_mocking.set_mock_control_mode(mode == "control") 

        logger.warning(f"Engine is running in MOCK mode: {mode}")

    def _get_pdca_phase(self, level: int) -> str:
        """Helper to get the PDCA phase string from the map."""
        return PDCA_PHASE_MAP.get(level, f"Level {level} Requirement")
    

    def _get_level_constraint_prompt(self, level: int) -> str:
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á Prompt Constraint ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Å‡∏±‡∏ö‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ß‡∏∏‡∏í‡∏¥‡∏†‡∏≤‡∏ß‡∏∞‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô
        """
        if level == 1:
            return "‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏ñ‡∏∂‡∏á '‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢/‡∏ß‡∏¥‡∏™‡∏±‡∏¢‡∏ó‡∏±‡∏®‡∏ô‡πå', '‡∏Å‡∏≤‡∏£‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå', '‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏ó‡∏≥‡∏Å‡∏£‡∏≠‡∏ö‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á', ‡∏´‡∏£‡∏∑‡∏≠ '‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£' ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (L1-Focus)"
        elif level == 2:
            return "‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ô‡πâ‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞ '‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô', '‡∏Å‡∏≤‡∏£‡∏Ç‡∏±‡∏ö‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô', '‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡∏ò‡∏£‡∏£‡∏°', ‡∏´‡∏£‡∏∑‡∏≠ '‡∏Å‡∏≤‡∏£‡∏°‡∏µ‡∏™‡πà‡∏ß‡∏ô‡∏£‡πà‡∏ß‡∏°' ‡∏ï‡∏≤‡∏°‡πÅ‡∏ú‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (L2-Focus)"
        elif level == 3:
            # üö® HARD RULE: ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏ä‡πâ L3 Logic (Check/Act Focus) ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏° Context ‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡∏°‡πà
            return """
‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î (HARD RULE: L3 CHECK/ACT FOCUS):
1. ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô L3 ‡∏ô‡∏µ‡πâ **‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô '‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö (Check)' ‡πÅ‡∏•‡∏∞ '‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á (Act)' ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô**
2. ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÅ‡∏•‡πâ‡∏ß: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏ï‡πâ‡∏ô‡∏Ç‡∏≠‡∏á Context ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î (Priority 1)
3. ‡∏´‡∏≤‡∏Å‡∏û‡∏ö‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ **[L3_SUMMARY_EVIDENCE]** ‡πÉ‡∏´‡πâ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô **‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö** ‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠‡πÑ‡∏î‡πâ‡∏ã‡∏∂‡πà‡∏á‡∏ñ‡∏π‡∏Å‡∏™‡∏£‡∏∏‡∏õ‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô Check/Act ‡∏à‡∏£‡∏¥‡∏á (‡∏à‡∏±‡∏î‡πÄ‡∏õ‡πá‡∏ô Priority 1)
4. ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô Plan ‡πÅ‡∏•‡∏∞ Do ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏ï‡∏≠‡∏ô‡∏ó‡πâ‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á Context **‡∏´‡πâ‡∏≤‡∏°‡∏ô‡∏≥‡∏°‡∏≤‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤** ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à **FAIL** ‡∏´‡∏≤‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô Check/Act ‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô
5. ‡∏´‡∏≤‡∏Å‡∏Ç‡∏≤‡∏î‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô **Check** ‡∏´‡∏£‡∏∑‡∏≠ **Act** ‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠ (‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏à‡∏≤‡∏Å Summary Evidence ‡∏´‡∏£‡∏∑‡∏≠‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á) ‡πÉ‡∏´‡πâ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÄ‡∏õ‡πá‡∏ô **‚ùå FAIL** ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô L3 ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡∏à‡∏£‡∏¥‡∏á
(L3-Focus: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏° ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏• ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á)
"""
        elif level == 4:
            return "‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ñ‡∏∂‡∏á '‡∏Å‡∏≤‡∏£‡∏ö‡∏π‡∏£‡∏ì‡∏≤‡∏Å‡∏≤‡∏£', '‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á', ‡∏´‡∏£‡∏∑‡∏≠ '‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå' ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (L4-Focus)"
        elif level == 5:
            return "‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ñ‡∏∂‡∏á '‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°', '‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡πà‡∏≤‡∏ó‡∏≤‡∏á‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à', ‡∏´‡∏£‡∏∑‡∏≠ '‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß' ‡πÇ‡∏î‡∏¢‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô (L5-Focus)"
        else:
            return ""
        

    def _classify_pdca_phase_for_chunk(
        self, 
        chunk_text: str
    ) -> Literal["Plan", "Do", "Check", "Act", "Other"]:
        """
        ‡πÉ‡∏ä‡πâ LLM ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏∞‡∏¢‡∏∞‡πÉ‡∏î‡∏£‡∏∞‡∏¢‡∏∞‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏Ç‡∏≠‡∏á PDCA ‡∏´‡∏£‡∏∑‡∏≠ 'Other'
        """
        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏ü‡∏™ PDCA
        pdca_phases_th = ["‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô", "‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥", "‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö", "‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á"]
        
        # 1. System Prompt ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢: ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö JSON 100%
        system_prompt = (
            "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô PDCA Cycle\n"
            "‡∏†‡∏≤‡∏£‡∏Å‡∏¥‡∏à‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô ‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏±‡∏î‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ß‡πà‡∏≤‡πÄ‡∏ô‡πâ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡πÉ‡∏î‡∏Ç‡∏≠‡∏á PDCA\n"
            f"‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏´‡∏ô‡∏∂‡πà‡∏á‡πÉ‡∏ô: {', '.join(pdca_phases_th)} ‡∏´‡∏£‡∏∑‡∏≠ '‡∏≠‡∏∑‡πà‡∏ô‡πÜ'\n\n"
            "‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡∏î‡πâ‡∏ß‡∏¢ **JSON Object ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô** ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö:\n"
            "{\"phase\": \"‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô\"}\n"
            "‡∏´‡∏£‡∏∑‡∏≠ {\"phase\": \"‡∏≠‡∏∑‡πà‡∏ô‡πÜ\"}\n"
            "‡∏´‡πâ‡∏≤‡∏°‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏≠‡∏Å JSON ‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≤‡∏î"
        )

        # 2. User Prompt: ‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô + ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
        user_prompt = (
            f"‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô:\n\"\"\"\n{chunk_text.strip()}\n\"\"\"\n\n"
            "‡∏Ñ‡∏≥‡∏ô‡∏¥‡∏¢‡∏≤‡∏°‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏ü‡∏™:\n"
            "- ‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô: ‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢, ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå, ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢, ‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô, ‡∏Ñ‡∏ì‡∏∞‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£\n"
            "- ‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥: ‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£, ‡∏ù‡∏∂‡∏Å‡∏≠‡∏ö‡∏£‡∏°, ‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£, ‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏£‡∏∞‡∏ö‡∏ö, ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á\n"
            "- ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö: ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏ú‡∏•, ‡∏ß‡∏±‡∏î‡∏ú‡∏•, ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô, ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏†‡∏≤‡∏¢‡πÉ‡∏ô, ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n"
            "- ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á: ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç, ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á, ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡πÉ‡∏´‡∏°‡πà, Lesson Learned, ‡∏õ‡∏¥‡∏î‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á\n\n"
            "‡∏ï‡∏≠‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞ JSON:"
        )
        
        try:
            raw_response = _fetch_llm_response(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                temperature=0.0,  # ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å! ‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô
                max_retries=2,
                llm_executor=self.llm
            )

            if not raw_response:
                return "Other"

            # ‡∏î‡∏∂‡∏á JSON ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏Ç‡πá‡∏á‡πÅ‡∏Å‡∏£‡πà‡∏á (‡πÉ‡∏ä‡πâ _robust_extract_json ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß!)
            parsed = _robust_extract_json(raw_response)
            
            # ‡∏ñ‡πâ‡∏≤ _robust_extract_json ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ fallback ‡∏î‡πâ‡∏ß‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ‡πÄ‡∏ö‡∏™‡∏¥‡∏Å
            if not parsed or not isinstance(parsed, dict):
                # ‡∏•‡∏≠‡∏á‡∏î‡∏∂‡∏á‡∏î‡πâ‡∏ß‡∏¢ regex ‡∏á‡πà‡∏≤‡∏¢ ‡πÜ
                match = re.search(r'"phase"\s*:\s*"([^"]+)"', raw_response, re.IGNORECASE)
                if match:
                    phase_th = match.group(1).strip()
                else:
                    phase_th = "‡∏≠‡∏∑‡πà‡∏ô‡πÜ"
            else:
                phase_th = parsed.get("phase", parsed.get("classification", "‡∏≠‡∏∑‡πà‡∏ô‡πÜ"))
                phase_th = str(phase_th).strip()

            # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô Literal ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
            mapping = {
                "‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô": "Plan",
                "‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥": "Do",
                "‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö": "Check",
                "‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á": "Act",
                "‡∏≠‡∏∑‡πà‡∏ô‡πÜ": "Other",
                "‡∏≠‡∏∑‡πà‡∏ô": "Other",
                "other": "Other"
            }
            result = mapping.get(phase_th, "Other")
            
            self.logger.debug(f"PDCA Classification: '{phase_th}' ‚Üí {result}")
            return result

        except Exception as e:
            self.logger.error(f"PDCA Classification failed: {e}\nRaw: {raw_response[:200]}")
            return "Other"

    # -------------------- Statement Preparation & Filtering Helpers --------------------
    def _flatten_rubric_to_statements(self) -> List[Dict[str, Any]]:
        """
        Transforms the hierarchical rubric structure loaded in self.rubric
        into a flat list of statements ready for assessment.
        """
        if not self.rubric:
            self.logger.warning("Cannot flatten rubric: self.rubric is empty.")
            return []
            
        data = deepcopy(self.rubric)
        extracted_list = []
        
        if not isinstance(data, dict):
             self.logger.error("Rubric data structure is invalid (expected dict of criteria).")
             return []
             
        for criteria_id, criteria_data in data.items():
            # üéØ FIX 1: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Criteria Data
            if not isinstance(criteria_data, dict):
                self.logger.warning(f"Skipping malformed criteria entry: {criteria_id} (not a dict).")
                continue
                
            sub_criteria_map = criteria_data.get('subcriteria', {})
            criteria_name = criteria_data.get('name')
            
            # üéØ FIX 2: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Sub-criteria Map
            if not isinstance(sub_criteria_map, dict):
                 self.logger.warning(f"Skipping criteria {criteria_id}: 'subcriteria' is not a dictionary.")
                 continue

            for sub_id, sub_data in sub_criteria_map.items():
                
                # üéØ FIX 3: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ sub_data ‡πÄ‡∏õ‡πá‡∏ô Dictionary ‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô TypeError)
                if not isinstance(sub_data, dict):
                    self.logger.warning(
                        f"Skipping malformed sub-criteria entry: {criteria_id}.{sub_id} "
                        f"is not a dictionary (found type: {type(sub_data).__name__})."
                    )
                    continue
                
                # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤ Metadata ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
                sub_data['criteria_id'] = criteria_id
                sub_data['criteria_name'] = criteria_name
                sub_data['sub_id'] = sub_id 
                sub_data['sub_criteria_name'] = sub_data.get('name', criteria_name + ' sub')
                if 'weight' not in sub_data:
                    sub_data['weight'] = criteria_data.get('weight', 0)
                extracted_list.append(sub_data)

        # Re-check and re-sort levels
        final_list = []
        for sub_criteria in extracted_list: 
            
            # üéØ FIX 4: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á Level ‡∏à‡∏≤‡∏Å Dict ‡∏ó‡∏µ‡πà‡∏°‡∏µ Key ‡πÄ‡∏õ‡πá‡∏ô String ‡πÄ‡∏õ‡πá‡∏ô List
            if "levels" in sub_criteria and isinstance(sub_criteria["levels"], dict):
                levels_list = []
                for level_str, statement in sub_criteria["levels"].items():
                    try:
                        # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÅ‡∏õ‡∏•‡∏á Level Key ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô Integer
                        level_int = int(level_str)
                        if isinstance(statement, str):
                            levels_list.append({"level": level_int, "statement": statement})
                        else:
                            self.logger.warning(f"Level {level_str} statement in {sub_criteria.get('sub_id')} is not a string.")
                    except ValueError:
                        self.logger.error(f"Invalid level key '{level_str}' found in {sub_criteria.get('sub_id', 'UNKNOWN_ID')}. Skipping.")
                        continue
                        
                sub_criteria["levels"] = levels_list
            
            # ‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏£‡∏∞‡∏î‡∏±‡∏ö
            if "levels" in sub_criteria and isinstance(sub_criteria["levels"], list):
                 sub_criteria["levels"].sort(key=lambda x: x.get("level", 0))
                 final_list.append(sub_criteria)
            else:
                 self.logger.warning(f"Sub-criteria {sub_criteria.get('sub_id', 'UNKNOWN_ID')} missing 'levels' list.")


        return final_list

    def _load_initial_evidence_info(self) -> Set[str]:
        """Retrieves the set of all available stable document IDs in the VectorStore."""
        if self.config.mock_mode != "none":
            return {"00000000-0000-0000-0000-000000000001"}
        return set() 

    def _apply_strict_filter(self, statements: List[Dict[str, Any]], available_evidence_ids: Set[str]) -> List[Dict[str, Any]]:
        """
        Filters out statements that have no specified evidence_doc_ids 
        that match any ID found in the available_evidence_ids set.
        """
        if not available_evidence_ids:
            logger.warning("Strict Filter bypassed: No available evidence IDs loaded.")
            return statements

        filtered_statements = []
        for stmt in statements:
            required_ids = set(stmt.get('evidence_doc_ids', []))
            
            if not required_ids or required_ids.isdisjoint(available_evidence_ids):
                 logger.debug(f"Strict Filter: Skipping {stmt['sub_id']} L{stmt['level']} (No evidence match)")
                 continue
            
            filtered_statements.append(stmt)
            
        return filtered_statements
    
# -------------------- Evidence Classification Helper (Optimized) --------------------
    def _get_mapped_uuids_and_priority_chunks(
        self,
        sub_id: str,
        level: int,
        statement_text: str,
        level_constraint: str,
        vectorstore_manager: Optional['VectorStoreManager']
    ) -> Tuple[List[str], List[Dict]]:
        """
        ‡∏î‡∏∂‡∏á Priority Chunks ‡∏à‡∏≤‡∏Å Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ + Hydrate ‡πÄ‡∏ï‡πá‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
        üü¢ FIX: ‡πÄ‡∏û‡∏¥‡πà‡∏° Pre-boost ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Score 0.0000
        """
        priority_chunks = []
        mapped_stable_ids = []

        # 1. ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å evidence_map (L1 ‚Üí L2, L2 ‚Üí L3 ‡∏Ø‡∏•‡∏Ø)
        for key, evidences in self.evidence_map.items():
            if key.startswith(f"{sub_id}.L") and evidences:
                try:
                    prev_level = int(key.split(".L")[-1])
                    if prev_level < level:
                        priority_chunks.extend(evidences)
                except:
                    continue

        if not priority_chunks:
            self.logger.info(f"No priority chunks found for {sub_id} L{level}")
            return mapped_stable_ids, []

        # üü¢ FIX 1: PRE-BOOST BEFORE HYDRATION
        # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Score 0.0000 ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏Å‡∏£‡∏≠‡∏á‡∏≠‡∏≠‡∏Å‡∏†‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏á
        for chunk in priority_chunks:
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ Score ‡∏´‡∏£‡∏∑‡∏≠ Score = 0 ‚Üí Set ‡∏Ñ‡πà‡∏≤ Default 0.80
            if "rerank_score" not in chunk or chunk.get("rerank_score", 0.0) == 0.0:
                chunk["rerank_score"] = 0.80
            if "score" not in chunk or chunk.get("score", 0.0) == 0.0:
                chunk["score"] = 0.80
            
            # ‚≠ê CRITICAL: ‡∏ï‡∏±‡πâ‡∏á Flag is_baseline = True ‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ
            chunk["is_baseline"] = True

        self.logger.info(f"Pre-boosted {len(priority_chunks)} priority chunks (Score set to 0.80, is_baseline=True)")

        # 2. ‡∏ó‡∏≥ Robust Hydration ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà‡πÄ‡∏•‡∏¢!
        priority_chunks = self._robust_hydrate_documents_for_priority_chunks(
            chunks_to_hydrate=priority_chunks,
            vsm=vectorstore_manager
        )

        # 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á mapped_stable_ids ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö RAG Retriever
        for chunk in priority_chunks:
            sid = chunk.get("stable_doc_uuid") or chunk.get("doc_id")
            if sid and isinstance(sid, str) and len(sid.replace("-", "")) >= 64:
                mapped_stable_ids.append(sid)

        self.logger.info(f"PRIORITY HYDRATED ‚Üí {len(priority_chunks)} chunks ready for L{level} (with full text + baseline flag)")

        return mapped_stable_ids, priority_chunks

    # -------------------- Calculation Helpers (ADDED) --------------------
    def _calculate_weighted_score(self, highest_full_level: int, weight: int) -> float:
        """
        Calculates the weighted score based on the highest full level achieved.
        Score is calculated by: (Level / 5) * Weight
        """
        MAX_LEVEL_CALC = 5  
        
        if highest_full_level <= 0:
            return 0.0
        
        level_for_calc = min(highest_full_level, MAX_LEVEL_CALC)
        score = (level_for_calc / MAX_LEVEL_CALC) * weight
        return score

    def _calculate_overall_stats(self, target_sub_id: str):
        """
        Calculates overall statistics from sub-criteria results (self.final_subcriteria_results)
        and stores them in self.total_stats.
        """
        results = self.final_subcriteria_results
        if not results:
            self.total_stats = {
                "Overall Maturity Score (Avg.)": 0.0,
                "Overall Maturity Level (Weighted)": "L0",
                "Number of Sub-Criteria Assessed": 0,
                "Total Weighted Score Achieved": 0.0,
                "Total Possible Weight": 0.0,
                "Overall Progress Percentage (0.0 - 1.0)": 0.0,
                "percentage_achieved_run": 0.0,
                "total_subcriteria": len(self._flatten_rubric_to_statements()),
                "target_level": self.config.target_level,
                "enabler": self.config.enabler,
                "sub_criteria_id": target_sub_id,
            }
            return

        # 1. Calculate Sums
        total_weighted_score_achieved = sum(r.get('weighted_score', 0) for r in results)
        total_possible_weight = sum(r.get('weight', 0) for r in results)

        # 2. Overall Maturity Score (Avg.)
        overall_avg_score = 0.0
        if total_possible_weight > 0:
            overall_avg_score = total_weighted_score_achieved / total_possible_weight
            # üü¢ FIX: ROUNDING for clean output (e.g., 1.999... -> 2.0)
            overall_avg_score = round(overall_avg_score, 2) 
        
        # 3. Overall Progress Percentage (0.0 - 1.0)
        overall_progress_percentage = 0.0
        # Assume MAX_LEVEL is 5 (‡∏´‡∏£‡∏∑‡∏≠‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å self.config ‡∏´‡∏£‡∏∑‡∏≠ global_vars)
        MAX_LEVEL = getattr(globals(), 'MAX_LEVEL', 5) 
        if total_possible_weight > 0 and MAX_LEVEL > 0:
            max_possible_score = total_possible_weight * MAX_LEVEL
            overall_progress_percentage = total_weighted_score_achieved / max_possible_score
            # üü¢ FIX: ROUNDING for clean output (4 ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡∏≠‡∏£‡πå‡πÄ‡∏ã‡πá‡∏ô‡∏ï‡πå)
            overall_progress_percentage = round(overall_progress_percentage, 4)

        # 4. Overall Maturity Level (Weighted)
        # ‡∏õ‡∏±‡∏î‡πÄ‡∏®‡∏©‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏ó‡∏µ‡πà‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÑ‡∏î‡πâ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î Level (‡πÄ‡∏ä‡πà‡∏ô 1.2 -> L1, 1.5 -> L2)
        highest_level_achieved = round(overall_avg_score)
        final_level = min(max(int(highest_level_achieved), 0), MAX_LEVEL)
        overall_level_label = f"L{final_level}"
        
        # 5. Final Percentage Achieved (0-100%)
        percentage_achieved_run = overall_progress_percentage * 100
        # üü¢ FIX: ROUNDING for clean output (1 ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö 0-100%)
        percentage_achieved_run = round(percentage_achieved_run, 1)


        self.total_stats = {
            "Overall Maturity Score (Avg.)": overall_avg_score, # <--- FIXED
            "Overall Maturity Level (Weighted)": overall_level_label,
            "Number of Sub-Criteria Assessed": len(results),
            "Total Weighted Score Achieved": round(total_weighted_score_achieved, 2), # <--- FIXED
            "Total Possible Weight": total_possible_weight,
            "Overall Progress Percentage (0.0 - 1.0)": overall_progress_percentage, # <--- FIXED
            "percentage_achieved_run": percentage_achieved_run, # <--- FIXED
            "total_subcriteria": len(self._flatten_rubric_to_statements()),
            "target_level": self.config.target_level,
            "enabler": self.config.enabler,
            "sub_criteria_id": target_sub_id,
        }
        
        self.logger.info(f"OVERALL STATS: Avg Score={overall_avg_score}, Level={overall_level_label}")

    def _export_results(self, results: dict, sub_criteria_id: str, **kwargs) -> str:
        """
        Exports the assessment results (for a specific sub-criteria or the final run) 
        to a JSON file, using utils/path_utils.py for full path determination.
        """
        
        enabler = self.enabler_id
        target_level = self.config.target_level
        
        # 1. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Path Utility (‡∏¢‡πâ‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏Å‡πà‡∏≠‡∏ô try/except)
        tenant = self.config.tenant
        year = self.config.year
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        suffix = f"assessment_results_{sub_criteria_id}_{timestamp}"

        full_path = ""
        export_dir = ""

        try:
            # 2. ‡πÉ‡∏ä‡πâ Path Utility ‡∏™‡∏£‡πâ‡∏≤‡∏á Full Path
            if self.config.export_path:
                # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î export_path (Override)
                export_dir = self.config.export_path
                file_name = f"assessment_results_{enabler}_{sub_criteria_id}_{timestamp}.json"
                full_path = os.path.join(export_dir, file_name)
            else:
                # üéØ ‡πÉ‡∏ä‡πâ get_assessment_export_file_path ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á Full Path ‡∏ï‡∏≤‡∏°‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô
                full_path = get_assessment_export_file_path(
                    tenant=tenant,
                    year=year,
                    enabler=enabler,
                    suffix=suffix,
                    ext="json"
                )
                # ‡∏î‡∏∂‡∏á export_dir ‡∏à‡∏≤‡∏Å full_path ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å get_export_dir ‡∏ã‡πâ‡∏≥
                export_dir = os.path.dirname(full_path)

        except ImportError as e:
            self.logger.error(f"‚ùå FATAL: Cannot import path_utils: {e}. Falling back to manual path.")
            
            # Fallback Logic: ‡πÉ‡∏ä‡πâ DATA_STORE_ROOT ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Path ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏î‡∏¥‡∏°
            data_store_root_path = os.environ.get('DATA_STORE_ROOT', 'data_store') 
            
            if self.config.export_path:
                export_dir = self.config.export_path
            else:
                # Fallback ‡∏™‡∏π‡πà Path ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô: data_store/tenant/exports/year/enabler
                export_dir = os.path.join(data_store_root_path, tenant, "exports", str(year), enabler)
            
            file_name = f"assessment_results_{enabler}_{sub_criteria_id}_{timestamp}.json"
            full_path = os.path.join(export_dir, file_name)
            self.logger.warning(f"‚ö†Ô∏è Using fallback path: {full_path}")


        # 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á Directory ‡∏´‡∏≤‡∏Å‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ
        if not os.path.exists(export_dir):
            try:
                os.makedirs(export_dir)
                self.logger.info(f"Created export directory: {export_dir}")
            except OSError as e:
                self.logger.error(f"‚ùå Failed to create export directory {export_dir}: {e}")
                return ""

        # 4. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°/‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ï Summary Field
        if 'summary' not in results:
            results['summary'] = {}
            
        results['summary']['enabler'] = enabler
        results['summary']['sub_criteria_id'] = sub_criteria_id
        results['summary']['target_level'] = target_level
        
        # ‡∏õ‡∏£‡∏±‡∏ö Logic ‡∏Å‡∏≤‡∏£‡∏ô‡∏±‡∏ö Sub-Criteria ‡πÉ‡∏´‡πâ‡∏ô‡∏±‡∏ö‡∏ï‡∏≤‡∏° 'sub_criteria_results' ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
        if 'sub_criteria_results' in results and isinstance(results['sub_criteria_results'], dict):
            results['summary']['Number of Sub-Criteria Assessed'] = len(results['sub_criteria_results'])
        else:
             results['summary']['Number of Sub-Criteria Assessed'] = 1 

        # 5. Export ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏õ‡∏ó‡∏µ‡πà JSON File
        try:
            with open(full_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=4)
            
            self.logger.info(f"üíæ Successfully exported results for {sub_criteria_id} to: {full_path}")
            return full_path
        
        except Exception as e:
            self.logger.error(f"‚ùå Failed to export results for {sub_criteria_id} to {full_path}: {e}")
            return ""
        
    def rephrase_query_for_retry(self, original_query: str, level: int, sub_id: str) -> str:
        """
        Helper method to slightly rephrase the query for the next retrieval attempt.
        """
        self.logger.info(f"Rephrasing query for L{level} retry: {original_query[:50]}...")
        # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö query: ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏ï‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á‡∏≠‡∏≠‡∏Å ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ LLM ‡∏ä‡πà‡∏ß‡∏¢ rephrase 
        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÅ‡∏Ñ‡πà‡∏Ñ‡∏∑‡∏ô query ‡πÄ‡∏î‡∏¥‡∏° ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ '‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î'
        if level >= 3:
            # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Level ‡∏™‡∏π‡∏á‡πÜ ‡∏•‡∏≠‡∏á‡πÄ‡∏ô‡πâ‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ó‡∏µ‡πà‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡∏Ç‡∏∂‡πâ‡∏ô
            return f"‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö {original_query}"
        return original_query
    
    def _create_error_result(
        self,
        level: int,
        error_message: str,
        start_time: float,
        retrieval_duration: float,
        sub_id: str,
        statement_id: str,
        statement_text: str,
        llm_duration: float = 0.0,
    ) -> Dict[str, Any]:
        """
        Generates a standard error dictionary for when RAG or LLM fails.
        """
        # NOTE: Assumes self._get_pdca_phase is defined
        try:
            pdca_phase = self._get_pdca_phase(level) 
        except Exception:
            pdca_phase = "N/A"
            
        pdca_breakdown = {p: 0 for p in ['P', 'D', 'C', 'A']}
        total_duration = time.time() - start_time

        self.logger.error(f"FATAL ERROR RESULT for {sub_id} L{level}: {error_message}")

        return {
            "sub_criteria_id": sub_id,
            "statement_id": statement_id,
            "level": level,
            "statement": statement_text,
            "pdca_phase": pdca_phase,
            "llm_score": 0.0,
            "pdca_breakdown": pdca_breakdown,
            "is_passed": False,
            "status": "FAIL",
            "score": 0.0,
            "llm_result_full": {"error": error_message, "details": "Assessment skipped due to critical failure."},
            "retrieval_duration_s": round(retrieval_duration, 2),
            "llm_duration_s": round(llm_duration, 2),
            "top_evidences_ref": [],
            "temp_map_for_level": [],
            "evidence_strength": 0.0,
            "ai_confidence": "LOW",
            "evidence_count": 0,
            "pdca_coverage": 0.0,
            "direct_evidence_count": 0,
            "rag_query": statement_text,
            "full_context_meta": {"error_type": "Critical Failure"},
            # üü¢ NEW: Relevant Score Gate Metadata (Set to default error values)
            "max_relevant_score": 0.0,
            "max_relevant_source": "ERROR_HANDLING",
            "is_evidence_strength_capped": False,
            "max_evidence_strength_used": 0.0,
            "total_run_time_s": round(total_duration, 2)
        }

    def _run_sub_criteria_assessment_worker(
        self,
        sub_criteria: Dict[str, Any],
    ) -> Tuple[Dict[str, Any], Dict[str, List[Dict[str, Any]]]]:
        """
        ‡∏£‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô L1-L5 ‡πÅ‡∏ö‡∏ö sequential (‡∏´‡∏£‡∏∑‡∏≠‡πÅ‡∏ö‡∏ö‡πÄ‡∏ï‡πá‡∏°‡πÉ‡∏ô‡πÇ‡∏´‡∏°‡∏î Mixed/Parallel) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö sub-criteria ‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏ï‡∏±‡∏ß
        ‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á evidence map ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡πÉ‡∏´‡πâ main process ‡∏£‡∏ß‡∏° (‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Action Plan)
        """
        # üìå ‡πÉ‡∏ä‡πâ Global/Class Constant ‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡πÑ‡∏ß‡πâ‡πÉ‡∏ô Header ‡∏Ç‡∏≠‡∏á core/seam_assessment.py
        REQUIRED_PDCA: Final[Dict[int, Set[str]]] = globals().get('REQUIRED_PDCA', {1: {"P"}, 2: {"P", "D"}, 3: {"P", "D", "C"}, 4: {"P", "D", "C", "A"}, 5: {"P", "D", "C", "A"}})
        MAX_L1_ATTEMPTS = globals().get('MAX_L1_ATTEMPTS', 2)
        WEAK_EVIDENCE_THRESHOLD = globals().get('WEAK_EVIDENCE_THRESHOLD', 5.0)

        sub_id = sub_criteria['sub_id']
        sub_criteria_name = sub_criteria['sub_criteria_name']
        sub_weight = sub_criteria.get('weight', 0)

        
        current_sequential_pass_level = 0 
        
        # üü¢ NEW: Local State Variables for Sequential Logic (Patch 3)
        first_failed_level_local = None 
        
        is_passed_previous_level = True 
        raw_results_for_sub_seq: List[Dict[str, Any]] = []
        start_ts = time.time() 

        self.logger.info(f"[WORKER START] Assessing Sub-Criteria: {sub_id} - {sub_criteria_name} (Weight: {sub_weight})")

        # ‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï temp_map_for_save ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ worker ‡∏ô‡∏µ‡πâ
        self.temp_map_for_save = {}

        # -----------------------------------------------------------
        # 1. LOOP THROUGH LEVELS (L1 ‚Üí L5) - ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ó‡∏∏‡∏Å Level ‡πÄ‡∏™‡∏°‡∏≠
        # -----------------------------------------------------------
        for statement_data in sub_criteria.get('levels', []):
            level = statement_data.get('level')
            if level is None or level > self.config.target_level:
                continue
            
            # üõë [REVISED LOGIC]: ‡∏•‡∏ö‡∏Å‡∏≤‡∏£ Capping ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏Å LLM ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏£‡∏±‡∏ô‡∏ó‡∏∏‡∏Å Level
            
            sequential_chunk_uuids = [] 
            level_result = {}
            level_temp_map: List[Dict[str, Any]] = []

            # (‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏Å _run_single_assessment ‡∏î‡πâ‡∏ß‡∏¢ Retry Policy)
            # L3+ ‡πÉ‡∏ä‡πâ RetryPolicy, L1/L2 ‡πÉ‡∏ä‡πâ Manual Retry (MAX_L1_ATTEMPTS)
            if level >= 3:
                wrapper = self.retry_policy.run(
                    fn=lambda attempt: self._run_single_assessment(
                        sub_criteria=sub_criteria,
                        statement_data=statement_data,
                        vectorstore_manager=self.vectorstore_manager,
                        sequential_chunk_uuids=sequential_chunk_uuids 
                    ),
                    level=level,
                    statement=statement_data.get('statement', ''),
                    context_blocks={"sequential_chunk_uuids": sequential_chunk_uuids},
                    logger=self.logger
                )
                level_result = wrapper.result if isinstance(wrapper, RetryResult) and wrapper.result is not None else {}
                level_temp_map = level_result.get("temp_map_for_level", []) 
            else:
                # (‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏Å _run_single_assessment ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö L1/L2)
                for attempt_num in range(1, MAX_L1_ATTEMPTS + 1):
                    self.logger.info(f"  > Starting assessment for {sub_id} L{level} (Attempt: {attempt_num}/{MAX_L1_ATTEMPTS})...")
                    level_result = self._run_single_assessment(
                        sub_criteria=sub_criteria,
                        statement_data=statement_data,
                        vectorstore_manager=self.vectorstore_manager,
                        sequential_chunk_uuids=sequential_chunk_uuids,
                        attempt=attempt_num
                    )
                    level_temp_map = level_result.get("temp_map_for_level", []) 
                    if level_result.get('is_passed', False):
                        self.logger.info(f"  > L{level} passed on attempt {attempt_num}.")
                        break
                    elif attempt_num < MAX_L1_ATTEMPTS:
                        self.logger.warning(f"  > L{level} failed on attempt {attempt_num}. Retrying...")
                    else:
                        self.logger.error(f"  > L{level} failed all {MAX_L1_ATTEMPTS} attempts.")


            # --- 1.2 PROCESS RESULT AND HANDLE EVIDENCE ---
            result_to_process = level_result or {}
            result_to_process.setdefault("level", level) 
            result_to_process.setdefault("used_chunk_uuids", [])

            is_passed_llm = result_to_process.get('is_passed', False)
            # üõë [REVISED LOGIC]: is_passed_final ‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£‡∏ñ‡∏π‡∏Å cap ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà‡πÅ‡∏•‡πâ‡∏ß
            is_passed_final = is_passed_llm 

            result_to_process['is_passed'] = is_passed_final
            result_to_process['is_capped'] = False # ‡∏ï‡∏±‡πâ‡∏á‡πÄ‡∏õ‡πá‡∏ô False ‡πÄ‡∏™‡∏°‡∏≠
            result_to_process['is_counted'] = True # ‡∏ï‡∏±‡πâ‡∏á‡πÄ‡∏õ‡πá‡∏ô True ‡∏Å‡πà‡∏≠‡∏ô ‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô False ‡πÉ‡∏ô Logic Capping

            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å evidence ‡∏•‡∏á temp_map_for_save ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠ PASS ‡∏à‡∏£‡∏¥‡∏á
            if is_passed_final and level_temp_map and isinstance(level_temp_map, list):
                
                # üìå Logic ‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Strength ‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô
                # --- START OF FIX ---
                # 1. ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤ highest_rerank_score
                #    (‡∏Ñ‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏ñ‡∏π‡∏Å‡∏™‡πà‡∏á‡∏°‡∏≤‡∏à‡∏≤‡∏Å _run_single_assessment ‡∏†‡∏≤‡∏¢‡πÉ‡∏ï‡πâ‡∏Ñ‡∏µ‡∏¢‡πå 'max_relevant_score')
                highest_rerank = result_to_process.get('max_relevant_score', 0.0)

                # 2. ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏ä‡∏∑‡πà‡∏≠ Argument ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≤‡∏° Definition:
                max_evi_str_after_save = self._save_level_evidences_and_calculate_strength(
                    level_temp_map=level_temp_map,
                    sub_id=sub_id,
                    level=level,
                    llm_result=result_to_process, 
                    highest_rerank_score=highest_rerank 
                )
                                
                result_to_process['max_evidence_strength_used'] = max_evi_str_after_save
                
                result_to_process['evidence_strength'] = round(
                    min(max_evi_str_after_save, 10.0) if is_passed_final else 0.0, 1
                )
                
            # üü¢ NEW LOGIC: Update Sequential State (Patch 3 Logic)
            if first_failed_level_local is not None:
                # üí° Level ‡∏ó‡∏µ‡πà‡∏ï‡∏≤‡∏°‡∏°‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏õ‡πá‡∏ô GAP_ONLY
                result_to_process["evaluation_mode"] = "GAP_ONLY"
                result_to_process["is_counted"] = False
                result_to_process["is_passed"] = False # ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°
                result_to_process["cap_reason"] = (
                    f"Gap analysis after sequential fail at L{first_failed_level_local}"
                )
                self.logger.info(f"  > L{level} marked as GAP_ONLY (Fail at L{first_failed_level_local}).")
            
            elif not is_passed_final and first_failed_level_local is None:
                # 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£ Fail ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å (‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏Ñ‡∏¢ Fail)
                first_failed_level_local = level
                # üí° Level ‡∏ô‡∏µ‡πâ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤ Fail ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏∏‡∏î‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£ Capping
                self.logger.info(f"  > üõë First Sequential FAIL detected at L{level}. (Setting first_failed_level_local={level})")
            
            elif is_passed_final:
                # 2. ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï Level ‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î (‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô Level ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ)
                if level == current_sequential_pass_level + 1:
                    current_sequential_pass_level = level
                    self.logger.info(f"  > Sequential PASS L{level}. Current Highest Pass: L{current_sequential_pass_level}")
                else:
                    # ‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≤‡∏° Level (‡πÄ‡∏ä‡πà‡∏ô L1 ‡∏ú‡πà‡∏≤‡∏ô, L3 ‡∏ñ‡∏π‡∏Å‡∏£‡∏±‡∏ô ‡πÅ‡∏ï‡πà L2 ‡∏ñ‡∏π‡∏Å Skip)
                    # üõë [REVISED LOGIC]: ‡πÉ‡∏ä‡πâ Logic ‡∏ô‡∏µ‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÉ‡∏ô‡πÇ‡∏´‡∏°‡∏î SEQUENTIAL ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô 
                    # ‡πÉ‡∏ô‡πÇ‡∏´‡∏°‡∏î MIXED/PARALLEL ‡∏õ‡∏•‡πà‡∏≠‡∏¢‡πÉ‡∏´‡πâ‡∏°‡∏±‡∏ô‡∏ú‡πà‡∏≤‡∏ô‡πÑ‡∏õ
                    if self.is_sequential: # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏´‡∏°‡∏î sequential ‡∏à‡∏£‡∏¥‡∏á‡πÜ
                        self.logger.warning(f"  > Sequential BREAK detected at L{level}. Capping at {current_sequential_pass_level}")
                        first_failed_level_local = current_sequential_pass_level + 1 # Force cap ‡∏ó‡∏µ‡πà Level ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏Ç‡πâ‡∏≤‡∏°/‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô
                        result_to_process["is_counted"] = False
                        result_to_process["is_passed"] = False


            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏•‡∏á raw results
            result_to_process["execution_index"] = len(raw_results_for_sub_seq)
            raw_results_for_sub_seq.append(result_to_process)
        
        # -----------------------------------------------------------
        # 2. CALCULATE SUMMARY
        # -----------------------------------------------------------
        # üìå highest_full_level ‡∏Ñ‡∏∑‡∏≠ Level ‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î (current_sequential_pass_level)
        highest_full_level = current_sequential_pass_level

        # (‡πÇ‡∏Ñ‡πâ‡∏î _calculate_weighted_score)
        weighted_score = self._calculate_weighted_score(highest_full_level, sub_weight)
        weighted_score = round(weighted_score, 2)

        # üìå num_passed ‡∏ï‡πâ‡∏≠‡∏á‡∏ô‡∏±‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Level ‡∏ó‡∏µ‡πà "is_counted" ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà False (‡∏´‡∏£‡∏∑‡∏≠ is_passed ‡πÄ‡∏õ‡πá‡∏ô True ‡πÉ‡∏ô Logic ‡πÄ‡∏î‡∏¥‡∏°)
        num_passed = sum(1 for r in raw_results_for_sub_seq if r.get("is_passed", False) and r.get("is_counted", True))

        sub_summary = {
            "num_statements": len(raw_results_for_sub_seq),
            "num_passed": num_passed,
            "num_failed": len(raw_results_for_sub_seq) - num_passed,
            "pass_rate": round(num_passed / len(raw_results_for_sub_seq), 4) if raw_results_for_sub_seq else 0.0
        }

        
        # -----------------------------------------------------------
        # 3. GENERATE ACTION PLAN (POST-PROCESSING) üöÄ
        # -----------------------------------------------------------

        target_next_level = highest_full_level + 1 if highest_full_level < 5 else 5
        
        statements_for_action_plan = []
        
        for r in raw_results_for_sub_seq:
            is_passed = r.get('is_passed', False)
            evidence_strength = r.get('evidence_strength', 10.0)

            # 1. Fail ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å Cap (‡∏Ñ‡∏∑‡∏≠ Fail ‡πÅ‡∏£‡∏Å)
            if not is_passed and r.get('evaluation_mode') != "GAP_ONLY":
                r['recommendation_type'] = 'FAILED'
                statements_for_action_plan.append(r)
                continue
            
            # 1.1 Fail ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô GAP_ONLY (Level ‡∏ó‡∏µ‡πà‡∏ï‡∏≤‡∏°‡∏°‡∏≤)
            if r.get('evaluation_mode') == "GAP_ONLY":
                r['recommendation_type'] = 'GAP_ANALYSIS' # ‡πÉ‡∏ä‡πâ Recommendation type ‡πÉ‡∏´‡∏°‡πà‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Gap
                statements_for_action_plan.append(r)
                continue

            # 2. ‡∏ú‡πà‡∏≤‡∏ô‡πÅ‡∏ï‡πà‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏≠‡πà‡∏≠‡∏ô
            if is_passed and evidence_strength < WEAK_EVIDENCE_THRESHOLD: # ‡∏™‡∏°‡∏°‡∏ï‡∏¥ WEAK_EVIDENCE_THRESHOLD = 5.0
                r['recommendation_type'] = 'WEAK_EVIDENCE' 
                statements_for_action_plan.append(r) # ‚úÖ Statement ‡∏ó‡∏µ‡πà Pass ‡πÅ‡∏ï‡πà‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏≠‡πà‡∏≠‡∏ô ‡∏ñ‡∏π‡∏Å‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡πÅ‡∏•‡πâ‡∏ß!
        
        action_plan_result = []

        try:
            # üî• ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á:
            action_plan_result = self.create_structured_action_plan( # ‡πÉ‡∏ä‡πâ self.create_structured_action_plan
                recommendation_statements=statements_for_action_plan, 
                sub_id=sub_id,
                sub_criteria_name=sub_criteria_name, 
                target_level=target_next_level, 
                llm_executor=self.llm,
                ActionPlanActions=self.ActionPlanActions
            )
            
        except Exception as e:
            self.logger.error(f"Failed to generate Action Plan for {sub_id}: {e}")
            action_plan_result = [{
                "Phase": "Error", 
                "Goal": "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á Action Plan ‡πÑ‡∏î‡πâ", 
                "Actions": [{
                    "Statement_ID": "ERROR", 
                    "Recommendation": f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ LLM ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Action Plan: {str(e)}"
                }]
            }]


        # -----------------------------------------------------------
        # 4. FINAL RESULT
        # -----------------------------------------------------------
        
        final_temp_map = {}
        # üí° Logic ‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô Main Process
        if self.is_sequential or self.is_parallel_all_mode: # ‡πÉ‡∏ô‡πÇ‡∏´‡∏°‡∏î Parallel ‡∏Å‡πá‡∏ï‡πâ‡∏≠‡∏á‡∏™‡πà‡∏á map ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢
            for key in self.evidence_map:
                # ‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö Sub-Criteria ‡∏ô‡∏µ‡πâ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
                if key.startswith(sub_criteria['sub_id'] + "."):
                    final_temp_map[key] = self.evidence_map[key]
        else:
            # ‡πÇ‡∏´‡∏°‡∏î Mixed (‡∏´‡∏£‡∏∑‡∏≠‡πÇ‡∏´‡∏°‡∏î‡πÄ‡∏Å‡πà‡∏≤)
            final_temp_map = self.temp_map_for_save.copy()


        final_sub_result = {
            "sub_criteria_id": sub_id,
            "sub_criteria_name": sub_criteria_name,
            "highest_full_level": highest_full_level,
            "weight": sub_weight,
            "target_level_achieved": highest_full_level >= self.config.target_level,
            "weighted_score": weighted_score,
            "action_plan": action_plan_result, 
            "raw_results_ref": raw_results_for_sub_seq,
            "sub_summary": sub_summary,
            "worker_duration_s": round(time.time() - start_ts, 2)
        }

        self.logger.info(f"[WORKER END] {sub_id} | Highest: L{highest_full_level} | Action Plans: {len(action_plan_result)} phase(s) | Duration: {final_sub_result['worker_duration_s']:.2f}s")

        return final_sub_result, final_temp_map

    def _save_level_evidences_and_calculate_strength(
        self, 
        level_temp_map: List[Dict[str, Any]], 
        sub_id: str, 
        level: int, 
        llm_result: Dict[str, Any],
        highest_rerank_score: float = 0.0
    ) -> float:
        """
        [CRITICAL FIX 25.0] 
        ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô level ‡∏ô‡∏±‡πâ‡∏ô‡πÜ ‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà self.evidence_map/temp_map
        ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Evidence Strength (Evi Str)
        
        ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å: Chunk UUID ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ‡∏ó‡∏≥‡πÉ‡∏´‡πâ L2, L3 Hydration ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß (0 chunks restored).
        """
        # üìå Map Key ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö Evidence ‡πÉ‡∏ô self.evidence_map ‡πÅ‡∏•‡∏∞ self.temp_map_for_save
        map_key = f"{sub_id}.L{level}"
        new_evidence_list: List[Dict[str, Any]] = []
        
        # 1. ‡∏ß‡∏ô‡∏ã‡πâ‡∏≥‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô
        for chunk in level_temp_map:
            
            # üéØ CRITICAL FIX 25.0: ‡∏î‡∏∂‡∏á Chunk UUID ‡πÅ‡∏•‡∏∞ Stable Doc ID ‡πÅ‡∏¢‡∏Å‡∏à‡∏≤‡∏Å‡∏Å‡∏±‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
            chunk_uuid_key = chunk.get("chunk_uuid") 
            stable_doc_uuid_key = chunk.get("stable_doc_uuid") or chunk.get("doc_id")

            # Fallback Logic: ‡∏ñ‡πâ‡∏≤ Chunk UUID ‡∏´‡∏≤‡∏¢ ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ Stable Doc UUID ‡πÅ‡∏ó‡∏ô (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ entry ‡∏ß‡πà‡∏≤‡∏á)
            if not chunk_uuid_key and stable_doc_uuid_key:
                chunk_uuid_key = stable_doc_uuid_key 
                self.logger.warning(f"‚ö†Ô∏è [EVI SAVE] Missing chunk_uuid. Falling back to Stable ID: {chunk_uuid_key[:8]}...")

            if not stable_doc_uuid_key or not chunk_uuid_key:
                 self.logger.error(f"‚ùå [EVI SAVE] Cannot determine required IDs for chunk. Skipping.")
                 continue

            # 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á Evidence Entry
            evidence_entry = {
                "sub_id": sub_id,
                "level": level,
                "relevance_score": chunk.get("rerank_score", chunk.get("score", 0.0)),
                "doc_id": stable_doc_uuid_key,          # <--- [FIXED] Stable ID (Document ID)
                "stable_doc_uuid": stable_doc_uuid_key, # <--- Stable ID (Document ID)
                "chunk_uuid": chunk_uuid_key,           # <--- [FIXED] Unique Chunk ID (‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Hydration)
                "source": chunk.get("source", "N/A"),
                "source_filename": chunk.get("filename", "N/A"),
                "pdca_tag": chunk.get("pdca_tag", "Other"), 
                "status": "PASS", 
                "timestamp": datetime.now().isoformat(),
            }
            new_evidence_list.append(evidence_entry)
            
        # 3. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Evidence Strength (Evi Str)
        # üìå ‡πÉ‡∏ä‡πâ self._calculate_evidence_strength_cap ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏û‡∏î‡∏≤‡∏ô‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô (Cap)
        evi_cap_data = self._calculate_evidence_strength_cap(
            top_evidences=new_evidence_list, # ‡πÉ‡∏ä‡πâ List ‡∏Ç‡∏≠‡∏á Evidence ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏Å‡∏£‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÅ‡∏•‡πâ‡∏ß
            level=level,
            highest_rerank_score=highest_rerank_score
        )
        
        max_evi_str_for_prompt = evi_cap_data['max_evi_str_for_prompt']

        # 4. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏Ç‡πâ‡∏≤ Map (‡πÉ‡∏ä‡πâ setdefault ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤ List ‡∏ñ‡∏π‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡∏∂‡πâ‡∏ô)
        # *Note: ‡πÉ‡∏ô‡πÇ‡∏´‡∏°‡∏î Worker/Parallel, self.evidence_map ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏£‡∏ß‡∏°‡πÉ‡∏ô Process ‡∏´‡∏•‡∏±‡∏Å*
        current_map = self.evidence_map.setdefault(map_key, [])
        current_map.extend(new_evidence_list)
        
        # 5. ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï Temp Map (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Worker Mode: ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏•‡∏±‡∏ö‡πÉ‡∏ô Process)
        temp_map = self.temp_map_for_save.setdefault(map_key, [])
        temp_map.extend(new_evidence_list)
        
        # 6. Log ‡∏™‡∏£‡∏∏‡∏õ
        self.logger.info(f"[EVIDENCE SAVED] {map_key} ‚Üí {len(new_evidence_list)} chunks")
        self.logger.info(f"[SEQUENTIAL UPDATE] {map_key} added to engine's main evidence_map for L{level+1} dependency.")
        
        # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ max_evi_str_for_prompt ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï final_results
        return evi_cap_data['max_evi_str_for_prompt']
        
    def _calculate_evidence_strength_cap(
        self,
        top_evidences: List[Union[Dict[str, Any], Any]],
        level: int,
        highest_rerank_score: Optional[float] = None
    ) -> Dict[str, Any]:
        """
        Relevant Score Gate ‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô FINAL: ‡∏î‡∏∂‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏à‡∏≤‡∏Å metadata, top-level key/attribute, ‡πÅ‡∏•‡∏∞ Regex fallback ‡∏ó‡∏µ‡πà‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°
        ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏±‡∏Å‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î 'max_evi_str_for_prompt' ‡πÇ‡∏î‡∏¢‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å Rerank Score ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
        """

        score_keys = [
            "rerank_score", "score", "relevance_score", # ‡∏à‡∏±‡∏î rerank_score ‡∏°‡∏≤‡∏Å‡πà‡∏≠‡∏ô
            "_rerank_score_force", "_rerank_score", 
            "Score", "RelevanceScore"
        ]
        
        # ‚îÄ‚îÄ‚îÄ 1. ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤ Threshold ‡πÅ‡∏•‡∏∞ Cap ‡∏à‡∏≤‡∏Å Attribute/Fallback (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô) ‚îÄ‚îÄ‚îÄ
        # üí° ‡πÉ‡∏ä‡πâ Attribute ‡∏Ç‡∏≠‡∏á Class ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å (‡∏ã‡∏∂‡πà‡∏á‡∏ñ‡∏π‡∏Å‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å global_vars ‡πÉ‡∏ô __init__)
        threshold = getattr(self, "RERANK_THRESHOLD", 0.5) 
        cap_value = getattr(self, "MAX_EVI_STR_CAP", 3.0)
        
        # üí° Fallback ‡∏à‡∏≤‡∏Å globals() (‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà __init__ ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤)
        if threshold == 0.5:
            threshold = globals().get('RERANK_THRESHOLD', 0.5)
        if cap_value == 3.0:
            cap_value = globals().get('MAX_EVI_STR_CAP', 3.0)

        # üü¢ FIX 1: ‡πÉ‡∏ä‡πâ highest_rerank_score ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏ï‡∏±‡πâ‡∏á‡∏ï‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
        max_score_found = highest_rerank_score if highest_rerank_score is not None else 0.0
        max_score_source = "Adaptive_RAG_Loop" if highest_rerank_score is not None else "N/A"


        for doc in top_evidences:
            
            page_content = ""
            metadata = {}
            current_score = 0.0 # ‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£

            # ‚îÄ‚îÄ‚îÄ 2. ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô metadata + content ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏∏‡∏Å‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á) ‚îÄ‚îÄ‚îÄ
            if isinstance(doc, dict):
                # ‡πÉ‡∏ä‡πâ doc.get() ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô KeyError/AttributeError
                metadata = doc.get("metadata", {}) 
                page_content = doc.get("page_content", "") or doc.get("text", "") or doc.get("content", "")
            else:
                # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Langchain Document ‡∏´‡∏£‡∏∑‡∏≠ Object ‡∏≠‡∏∑‡πà‡∏ô‡πÜ
                metadata = getattr(doc, "metadata", {})
                page_content = getattr(doc, "page_content", "") or getattr(doc, "text", "")

            # ‚îÄ‚îÄ‚îÄ 3. ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô (‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö top-level key/attribute ‡πÅ‡∏•‡∏∞ metadata) ‚îÄ‚îÄ‚îÄ
            for key in score_keys:
                score_val = None
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÉ‡∏ô metadata
                score_val = metadata.get(key)
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÉ‡∏ô doc object/dict
                if score_val is None:
                    if isinstance(doc, dict):
                        score_val = doc.get(key)
                    else:
                        score_val = getattr(doc, key, None)
                
                # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô float
                if score_val is not None:
                    try:
                        temp_score = float(score_val)
                        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏°‡∏≤‡∏™‡∏°‡πÄ‡∏´‡∏ï‡∏∏‡∏™‡∏°‡∏ú‡∏•‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà (‡∏Ñ‡∏ß‡∏£‡∏≠‡∏¢‡∏π‡πà‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á 0 ‡∏ñ‡∏∂‡∏á 1.0)
                        if 0.0 < temp_score <= 1.0: 
                            if temp_score > current_score:
                                current_score = temp_score
                                break # ‡∏´‡∏¢‡∏∏‡∏î‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏û‡∏ö‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ
                    except (ValueError, TypeError):
                        continue
            
            # ‚îÄ‚îÄ‚îÄ 4. Fallback: ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å‡∏ó‡πâ‡∏≤‡∏¢ content (Aggressive Regex) ‚îÄ‚îÄ‚îÄ
            if current_score == 0.0 and page_content and isinstance(page_content, str):
                try:
                    # üìå ‡πÉ‡∏ä‡πâ re.search ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å import ‡πÉ‡∏ô header
                    tail = page_content[-1000:]
                    patterns = [
                        r"Relevance[ :]+([0-9]*\.?[0-9]+)",
                        r"Score[ :]+([0-9]*\.?[0-9]+)",
                        r"Re:[ ]*([0-9]*\.?[0-9]+)",
                        r"\[Relevance: ([0-9]*\.?[0-9]+)\]",
                        r"\[Score: ([0-9]*\.?[0-9]+)\]",
                        r"rerank_score['\"]?\s*:\s*([0-9]*\.?[0-9]+)",
                        r"\|\s*([0-9]*\.?[0-9]+)\s*\|",
                        r"\s+([0-9]\.[0-9]+)$",
                    ]
                    import re # Ensure re is imported if it's not a class method
                    for pat in patterns:
                        m = re.search(pat, tail, re.IGNORECASE)
                        if m:
                            try:
                                temp_score = float(m.group(1))
                                if 0.0 < temp_score <= 1.0: # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï
                                    if temp_score > current_score:
                                        current_score = temp_score
                                        break
                            except:
                                continue
                except Exception:
                    # ‡∏Å‡∏£‡∏ì‡∏µ re ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å import ‡∏´‡∏£‡∏∑‡∏≠ error ‡∏≠‡∏∑‡πà‡∏ô‡πÜ
                    pass 

            # üî¥ FIX: ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô (Score Clamp) 
            # (‡∏Å‡∏£‡∏ì‡∏µ‡∏î‡∏∂‡∏á‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô > 1.0 ‡∏à‡∏≤‡∏Å metadata/top-level key ‡∏ã‡∏∂‡πà‡∏á‡∏≠‡∏≤‡∏à‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà 0-1)
            if current_score > 1.0:
                self.logger.warning(f"üö® Score Clamp L{level}: Detected score {current_score:.4f} > 1.0. Assuming score not in 0-1 range and ignoring.")
                current_score = 0.0 # ‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï ‡∏ñ‡πâ‡∏≤‡πÄ‡∏Å‡∏¥‡∏ô 1.0 ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà Rerank/Relevance 0-1

            # ‚îÄ‚îÄ‚îÄ 5. ‡∏î‡∏∂‡∏á source ‚îÄ‚îÄ‚îÄ
            source = (
                metadata.get("source_filename") or metadata.get("filename") or
                doc.get("source_filename") or doc.get("filename") or 
                doc.get("source") or doc.get("doc_id") or
                "N/A"
            )

            # ‚îÄ‚îÄ‚îÄ 6. ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î (‡∏û‡∏£‡πâ‡∏≠‡∏° Log ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ‡∏Å‡∏≤‡∏£ Override) ‚îÄ‚îÄ‚îÄ
            if current_score > max_score_found:
                # üü¢ FIX 2: ‡πÄ‡∏û‡∏¥‡πà‡∏° Log ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ‡∏Å‡∏≤‡∏£ Override ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏à‡∏≤‡∏Å Loop
                if highest_rerank_score is not None and current_score > highest_rerank_score:
                    self.logger.critical(f"‚ö†Ô∏è Score Override: Found hidden score {current_score:.4f} > Loop score {highest_rerank_score:.4f} from source: {source}")

                max_score_found = current_score
                max_score_source = source

        # ‚îÄ‚îÄ‚îÄ 7. Relevant Score Gate + Log ‚îÄ‚îÄ‚îÄ
        
        if max_score_found < threshold: 
            max_evi_str_for_prompt = cap_value
            is_capped = True
            self.logger.warning(
                f"üö® Evi Str CAPPED L{level}: "
                f"Rerank {max_score_found:.4f} (‡∏à‡∏≤‡∏Å '{max_score_source}') "
                f"< {threshold} ‚Üí ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏ó‡∏µ‡πà {cap_value}"
            )
        else:
            max_evi_str_for_prompt = 10.0
            is_capped = False
            self.logger.info(
                f"‚úÖ Evi Str FULL L{level}: "
                f"Rerank {max_score_found:.4f} (‡∏à‡∏≤‡∏Å '{max_score_source}') "
                f">= {threshold} ‚Üí ‡∏õ‡∏•‡πà‡∏≠‡∏¢‡πÄ‡∏ï‡πá‡∏° 10.0"
            )

        return {
            "is_capped": is_capped,
            "max_evi_str_for_prompt": max_evi_str_for_prompt,
            "highest_rerank_score": round(float(max_score_found), 4), 
            "max_score_source": max_score_source,
        }
        

    def _robust_hydrate_documents_for_priority_chunks(
        self, 
        chunks_to_hydrate: List[Dict], 
        vsm: Optional['VectorStoreManager'],
        current_sub_id: Optional[str] = None
    ) -> List[Dict]:
        
        """
        Hydrates priority chunks using robust Stable ID fallback logic
        and guarantees the 'text' key is present in all returned chunks.
        Preserves and reinforces the 'is_baseline' flag.
        """
        
        # 1. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î active_sub_id (Fix 5)
        # ‡πÉ‡∏ä‡πâ current_sub_id ‡∏Å‡πà‡∏≠‡∏ô, ‡∏ï‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ self.sub_id, ‡πÅ‡∏•‡∏∞ Fallback ‡πÑ‡∏õ‡∏ó‡∏µ‡πà config
        active_sub_id = current_sub_id or getattr(self, 'sub_id', None)
        if not active_sub_id:
            self.logger.error("Hydration failed: Missing sub_id.")
            active_sub_id = getattr(self, 'config', {}).get('sub_id') or 'unknown'
            
        if active_sub_id == 'unknown':
            self.logger.error("FATAL: Hydration cannot proceed without sub_id. Returning unhydrated chunks.")
            return chunks_to_hydrate 
        
        if not chunks_to_hydrate or not vsm:
            return chunks_to_hydrate

        # 1. Collect Stable/Chunk IDs
        stable_ids = set()
        for chunk in chunks_to_hydrate:
            # ‡πÉ‡∏ä‡πâ Stable ID ‡∏´‡∏£‡∏∑‡∏≠ Chunk UUID ‡πÄ‡∏õ‡πá‡∏ô ID ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ Hydration
            sid = chunk.get("stable_doc_uuid") or chunk.get("doc_id") or chunk.get("chunk_uuid") 
            if sid and isinstance(sid, str) and len(sid.replace("-", "")) >= 32:
                stable_ids.add(sid)

        hydrated_priority_docs = []
        restored_count = 0
        total_count = len(chunks_to_hydrate)

        # -------------------- FALLBACK LOGIC 1 (No Valid IDs) --------------------
        if not stable_ids:
            self.logger.info("No Stable IDs found for Priority Chunk hydration. Boosting existing chunks.")
            for chunk in chunks_to_hydrate:
                new_chunk = chunk.copy()
                new_chunk["is_baseline"] = True # Fix 3
                if "text" in new_chunk and new_chunk["text"].strip():
                    # Re-tag PDCA (Fix 5: ‡πÉ‡∏ä‡πâ active_sub_id)
                    new_tag = classify_by_keyword(
                        new_chunk["text"],
                        sub_id=active_sub_id, 
                        contextual_rules_map=self.contextual_rules_map
                    )
                    if new_tag != 'Other':
                        new_chunk["pdca_tag"] = new_tag
                    new_chunk["rerank_score"] = max(new_chunk.get("rerank_score", 0.0), 0.95)
                    new_chunk["score"] = max(new_chunk.get("score", 0.0), 0.95)
                hydrated_priority_docs.append(new_chunk)
            
            self.logger.info(f"PRIORITY HYDRATION ‚Üí Fallback boost used. Final chunks: {len(hydrated_priority_docs)} (all baseline).")
            
            # üü¢ FIX 7: ‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡∏µ‡∏¢‡πå 'text' ‡∏Å‡πà‡∏≠‡∏ô return
            return self._guarantee_text_key(hydrated_priority_docs)


        # -------------------- HYDRATION LOGIC --------------------
        self.logger.info(f"Attempting to hydrate {len(stable_ids)} documents...")
        stable_id_map = defaultdict(list)
        
        try:
            # üü¢ FIX 6.1: ‡πÅ‡∏Å‡πâ‡∏ä‡∏∑‡πà‡∏≠‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô VSM
            # get_documents_by_id ‡∏Ñ‡∏ß‡∏£‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô Map ‡∏´‡∏£‡∏∑‡∏≠ List[LcDocument] ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô Map ‡πÑ‡∏î‡πâ
            retrieved_docs = vsm.get_documents_by_id(list(stable_ids), doc_type=self.doc_type, enabler=self.config.enabler)
            
            # ‡πÅ‡∏õ‡∏•‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å LcDocument ‡πÄ‡∏õ‡πá‡∏ô Map ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ö Logic ‡πÄ‡∏î‡∏¥‡∏°
            # ‡πÄ‡∏£‡∏≤‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á LcDocument ‡πÄ‡∏õ‡πá‡∏ô Dict ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà
            for doc in retrieved_docs:
                sid = doc.metadata.get("stable_doc_uuid") or doc.metadata.get("doc_id")
                if sid:
                    # Store as Dict with 'text' key for consistency
                    stable_id_map[sid].append({
                        "text": doc.page_content,
                        "metadata": doc.metadata
                    })
            
        except Exception as e:
            self.logger.error(f"VSM failed to get documents by ID: {e}. Falling back to boosting only.")
            
            # üü¢ FIX 6.2: VSM Fallback Logic (‡πÉ‡∏ä‡πâ‡πÇ‡∏Ñ‡πâ‡∏î Fallback 1 ‡∏ã‡πâ‡∏≥)
            hydrated_priority_docs_fallback = []
            self.logger.info("PRIORITY HYDRATION ‚Üí VSM Fallback. Boosting existing chunks.")
            for chunk in chunks_to_hydrate:
                new_chunk = chunk.copy()
                new_chunk["is_baseline"] = True
                if "text" in new_chunk and new_chunk["text"].strip():
                    new_tag = classify_by_keyword(
                        new_chunk["text"],
                        sub_id=active_sub_id, 
                        contextual_rules_map=self.contextual_rules_map
                    )
                    if new_tag != 'Other':
                        new_chunk["pdca_tag"] = new_tag
                    new_chunk["rerank_score"] = max(new_chunk.get("rerank_score", 0.0), 0.95)
                    new_chunk["score"] = max(new_chunk.get("score", 0.0), 0.95)
                hydrated_priority_docs_fallback.append(new_chunk)
            
            self.logger.info(f"PRIORITY HYDRATION ‚Üí Fallback boost used. Final chunks: {len(hydrated_priority_docs_fallback)} (all baseline).")
            
            # üü¢ FIX 7: ‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡∏µ‡∏¢‡πå 'text' ‡∏Å‡πà‡∏≠‡∏ô return
            return self._guarantee_text_key(hydrated_priority_docs_fallback)
        
        
        # 3. Hydrate + Boost + Re-tag PDCA 
        seen_signatures = set()
        for chunk in chunks_to_hydrate:
            new_chunk = chunk.copy()
            new_chunk["is_baseline"] = True # Fix 3
            
            sid = new_chunk.get("stable_doc_uuid") or new_chunk.get("doc_id")

            hydrated = False
            if sid and sid in stable_id_map and stable_id_map[sid]:
                best_match = stable_id_map[sid][0]
                
                # ‡∏î‡∏∂‡∏á Text
                new_chunk["text"] = best_match["text"] 
                # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï Metadata
                new_chunk.update({k: v for k, v in best_match["metadata"].items()
                                if k not in ["text", "page_content"]})
                
                # ‡∏£‡∏±‡∏Å‡∏©‡∏≤ chunk_uuid ‡πÄ‡∏î‡∏¥‡∏°‡πÑ‡∏ß‡πâ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
                original_uuid = new_chunk.get("chunk_uuid")
                if original_uuid:
                    new_chunk["chunk_uuid"] = original_uuid

                restored_count += 1
                hydrated = True
                new_chunk["rerank_score"] = 1.0
                new_chunk["score"] = 1.0

            # Re-tag PDCA from full text
            if "text" in new_chunk and new_chunk["text"].strip():
                new_tag = classify_by_keyword(
                    new_chunk["text"], 
                    sub_id=active_sub_id, # Fix 5
                    contextual_rules_map=self.contextual_rules_map
                )
                if new_tag != 'Other':
                    old_tag = new_chunk.get("pdca_tag", "Other")
                    new_chunk["pdca_tag"] = new_tag
                    if old_tag == 'Other':
                        self.logger.debug(f"Re-tagged priority chunk as '{new_tag}' (from 'Other')")

            # Dedup
            signature = (sid, new_chunk.get("text", "")[:200])
            if signature in seen_signatures:
                continue
            seen_signatures.add(signature)

            # Boost ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà hydrated ‡πÅ‡∏ï‡πà‡∏°‡∏µ text
            if not hydrated and "text" in new_chunk:
                new_chunk["rerank_score"] = max(new_chunk.get("rerank_score", 0.0), 0.95)
                new_chunk["score"] = max(new_chunk.get("score", 0.0), 0.95)

            # Boost ‡∏ô‡πâ‡∏≠‡∏¢‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ text (‡∏´‡∏£‡∏∑‡∏≠ Hydration ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß)
            if "text" not in new_chunk:
                self.logger.warning(f"Priority chunk (SID: {sid[:8] if sid else 'N/A'}...) has no text after hydration attempt.")
                new_chunk["rerank_score"] = max(new_chunk.get("rerank_score", 0.0), 0.80)
                new_chunk["score"] = max(new_chunk.get("score", 0.0), 0.80)

            hydrated_priority_docs.append(new_chunk)

        # -------------------- FINAL GUARANTEE (Fix 7) --------------------
        return self._guarantee_text_key(hydrated_priority_docs, total_count=total_count, restored_count=restored_count)

    def _guarantee_text_key(self, chunks: List[Dict], total_count: int = 0, restored_count: int = 0) -> List[Dict]:
        """Ensures every chunk has a 'text' key to prevent downstream KeyError."""
        final_chunks_guaranteed = []
        guaranteed_count = 0
        for chunk in chunks:
            if 'text' not in chunk:
                chunk['text'] = ""
                guaranteed_count += 1
                self.logger.debug(f"Guaranteed 'text' key for chunk (ID: {chunk.get('chunk_uuid', 'N/A')[:8]})")
            final_chunks_guaranteed.append(chunk)

        baseline_count = len([d for d in final_chunks_guaranteed if d.get('is_baseline', False)])
        
        # ‡∏ñ‡πâ‡∏≤‡∏ñ‡∏π‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏à‡∏≤‡∏Å Hydration Loop ‡∏à‡∏∞‡πÅ‡∏™‡∏î‡∏á Log ‡∏™‡∏£‡∏∏‡∏õ
        if total_count > 0:
            self.logger.info(
                f"PRIORITY HYDRATION SUMMARY ‚Üí Restored {restored_count}/{total_count} chunks with full text. "
                f"Final priority chunks: {len(final_chunks_guaranteed)} ({baseline_count} marked as baseline). Guaranteed 'text' key for {guaranteed_count} chunks."
            )
        
        return final_chunks_guaranteed
    
    def _get_keywords_for_phase(self, sub_id: str, level: int, phase: str = "Plan") -> str:
        """
        ‡∏î‡∏∂‡∏á keywords ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö phase (Plan, Do, Check, Act) ‡∏à‡∏≤‡∏Å contextual_rules.json
        ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: level-specific ‚Üí sub-specific ‚Üí enabler default ‚Üí fallback
        """
        keywords = []
        sub_rules = self.contextual_rules_map.get(sub_id, {})
        
        phase_key = f"{phase.lower()}_keywords"
        
        # 1. Level-specific (L1, L2, ...)
        level_key = f"L{level}"
        level_rules = sub_rules.get(level_key, {})
        if phase_key in level_rules:
            keywords = level_rules[phase_key]
        
        # 2. Sub-specific fallback
        elif phase_key in sub_rules:
            keywords = sub_rules[phase_key]
        
        # 3. Enabler default
        elif "_enabler_defaults" in self.contextual_rules_map:
            default_keywords = self.contextual_rules_map["_enabler_defaults"].get(phase_key)
            if default_keywords:
                keywords = default_keywords
        
        # 4. Global fallback (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏•‡∏¢)
        if not keywords:
            fallback_map = {
                "plan": ["‡∏ß‡∏¥‡∏™‡∏±‡∏¢‡∏ó‡∏±‡∏®‡∏ô‡πå", "‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢", "‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á", "‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢", "‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå", "‡πÅ‡∏ú‡∏ô"],
                "do": ["‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£", "‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥", "‡∏à‡∏±‡∏î‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°", "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£", "‡∏Ç‡∏±‡∏ö‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô"],
                "check": ["‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö", "‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô", "‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°", "‡∏ß‡∏±‡∏î‡∏ú‡∏•", "‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö"],
                "act": ["‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á", "‡∏û‡∏±‡∏í‡∏ô‡∏≤", "‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç", "‡∏ô‡∏≥‡πÑ‡∏õ‡πÉ‡∏ä‡πâ", "‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô"]
            }
            keywords = fallback_map.get(phase.lower(), [])
        
        return ", ".join(keywords)


    def _get_pdca_blocks_from_evidences(
            self, 
            evidences: List[Dict], 
            baseline_evidences: Dict[str, List[Dict]], 
            level: int,
            sub_id: str,
            contextual_rules_map: Dict[str, Any]
        ) -> Tuple[str, str, str, str, str]:
            """
            Groups evidences by PDCA tags and formats them into context blocks
            for the LLM prompt.

            Args:
                evidences: The list of all relevant evidence chunks (new and previous levels).
                baseline_evidences: Dictionary of baseline evidences collected from previous levels.
                level: The current assessment level (L1-L5).
                sub_id: The ID of the sub-criteria being assessed.
                contextual_rules_map: Rules for keyword classification.

            Returns:
                A tuple containing (plan_blocks, do_blocks, check_blocks, act_blocks, other_blocks)
                as formatted strings.
            """
            
            # üü¢ FIX 9: ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏Ñ‡∏µ‡∏¢‡πå 'text' ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÄ‡∏°‡∏ò‡∏≠‡∏î 
            # (‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤ self._guarantee_text_key ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏Ñ‡∏•‡∏≤‡∏™‡∏´‡∏•‡∏±‡∏Å)
            evidences = self._guarantee_text_key(evidences)
            
            # üü¢ NEW FIX 12: ‡∏Å‡∏£‡∏≠‡∏á Chunk ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ Text ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ Evidences ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
            original_count = len(evidences)
            evidences = [
                chunk for chunk in evidences 
                if chunk.get("text", "").strip() # ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞ chunk ‡∏ó‡∏µ‡πà text ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà string ‡∏ß‡πà‡∏≤‡∏á/space
            ]
            if len(evidences) < original_count:
                # üí° ‡πÉ‡∏ä‡πâ self.logger.debug ‡πÅ‡∏ó‡∏ô self.logger.info
                self.logger.debug(f"PDCA Pre-Filter: Removed {original_count - len(evidences)} chunks with empty text.")

            # ------------------ Local Helper Function: _create_block ------------------
            def _create_block(tag: str, chunks: List[Dict]) -> str:
                """Helper to format a list of chunks into a single text block."""
                if not chunks:
                    return ""

                block_content = []
                sorted_chunks = sorted(chunks, key=lambda x: x.get("rerank_score", 0.0), reverse=True)
                
                for i, chunk in enumerate(sorted_chunks):
                    # üéØ ‡∏£‡∏ß‡∏° Header ‡πÅ‡∏•‡∏∞ Content ‡πÉ‡∏ô Block ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
                    header = f"### [{tag} {i+1}/{len(sorted_chunks)}]"
                    content = chunk['text'].strip() # ‡πÉ‡∏ä‡πâ strip() ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î
                    
                    block_content.append(f"{header}\n{content}")
                
                # ‡πÉ‡∏ä‡πâ \n---\n ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÅ‡∏ö‡πà‡∏á Block
                return "\n---\n".join(block_content)
            # --------------------------------------------------------------------------

            # 1. Group evidences by PDCA tag (using the consolidated list)
            pdca_groups = defaultdict(list)
            
            # 2. Tag/Re-tag and Group the evidences
            for chunk in evidences:
                # ‡πÉ‡∏ä‡πâ classify_by_keyword ‡πÄ‡∏û‡∏∑‡πà‡∏≠ re-tag ‡∏´‡∏≤‡∏Å LLM ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏´‡πâ tag ‡∏°‡∏≤ ‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô
                # (‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤ classify_by_keyword ‡∏ñ‡∏π‡∏Å Import ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡πá‡∏ô method ‡∏Ç‡∏≠‡∏á self)
                if 'pdca_tag' not in chunk or not chunk['pdca_tag'].strip() or chunk['pdca_tag'] == 'Other':
                    new_tag = classify_by_keyword(
                        chunk.get("text", ""), 
                        sub_id=sub_id, 
                        contextual_rules_map=contextual_rules_map
                    )
                    chunk['pdca_tag'] = new_tag
                
                tag = chunk['pdca_tag']
                
                # ‚≠ê CRITICAL: ‡∏£‡∏ß‡∏° Baseline Evidences ‡∏à‡∏≤‡∏Å level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô‡∏Å‡∏•‡∏∏‡πà‡∏° PDCA 
                # (‡∏û‡∏ß‡∏Å‡∏ô‡∏µ‡πâ‡∏Ñ‡∏ß‡∏£‡∏ñ‡∏π‡∏Å re-tag ‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏ô _collect_previous_level_evidences)
                
                if tag in ["Plan", "Do", "Check", "Act", "Other"]:
                    pdca_groups[tag].append(chunk)

            # 3. Create blocks from grouped evidences
            plan_blocks = _create_block("Plan", pdca_groups["Plan"])
            do_blocks = _create_block("Do", pdca_groups["Do"])
            check_blocks = _create_block("Check", pdca_groups["Check"])
            act_blocks = _create_block("Act", pdca_groups["Act"])
            other_blocks = _create_block("Other", pdca_groups["Other"])

            return plan_blocks, do_blocks, check_blocks, act_blocks, other_blocks
                
    def run_assessment(
        self,
        target_sub_id: str = "all",
        export: bool = False,
        vectorstore_manager: Optional['VectorStoreManager'] = None,
        sequential: bool = False,
        document_map: Optional[Dict[str, str]] = None,
    ) -> Dict[str, Any]:
        """
        Main runner ‡∏Ç‡∏≠‡∏á Assessment Engine
        ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á Parallel ‡πÅ‡∏•‡∏∞ Sequential 100%
        ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Evidence Map ‡πÄ‡∏™‡∏°‡∏≠ (‡πÅ‡∏°‡πâ‡∏£‡∏±‡∏ô sub ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß)
        """
        start_ts = time.time()
        self.is_sequential = sequential

        # ============================== 1. Filter Rubric ==============================
        if target_sub_id.lower() == "all":
            sub_criteria_list = self._flatten_rubric_to_statements()
        else:
            all_statements = self._flatten_rubric_to_statements()
            sub_criteria_list = [
                s for s in all_statements if s.get('sub_id') == target_sub_id
            ]
            if not sub_criteria_list:
                self.logger.error(f"Sub-Criteria ID '{target_sub_id}' not found in rubric.")
                return {"error": f"Sub-Criteria ID '{target_sub_id}' not found."}

        # Reset states
        self.raw_llm_results = []
        self.final_subcriteria_results = []

        # ‡πÇ‡∏´‡∏•‡∏î evidence map ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß (‡πÑ‡∏°‡πà clear!)
        if os.path.exists(self.evidence_map_path):
            loaded = self._load_evidence_map()
            if loaded:
                self.evidence_map = loaded
                self.logger.info(f"Resumed from existing evidence map: {len(self.evidence_map)} keys")
            else:
                self.evidence_map = {}
        else:
            self.evidence_map = {}

        # --------------------- ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Max Workers ---------------------
        max_workers = globals().get('MAX_PARALLEL_WORKERS', 4)
        if not isinstance(max_workers, int) or max_workers <= 0:
            max_workers = 4
            self.logger.warning(f"Invalid MAX_PARALLEL_WORKERS in config ‚Üí using safe default: {max_workers}")
        self.logger.info(f"Using max_workers = {max_workers}")

        run_parallel = (target_sub_id.lower() == "all") and not sequential

        # ============================== 2. Run Assessment ==============================
        if run_parallel:
            self.logger.info(f"Starting Parallel Assessment with {max_workers} processes")
            worker_args = [(
                sub_data,
                self.config.enabler,
                self.config.target_level,
                self.config.mock_mode,
                self.evidence_map_path,
                self.config.model_name,
                self.config.temperature,
                getattr(self.config, 'MIN_RETRY_SCORE', 0.50),
                getattr(self.config, 'MAX_RETRIEVAL_ATTEMPTS', 3),
                document_map or self.document_map,
                ActionPlanActions  # <--- ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ï‡∏±‡∏ß‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô element ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
            ) for sub_data in sub_criteria_list]

            try:
                with multiprocessing.get_context('spawn').Pool(processes=max_workers) as pool:
                    results_list = pool.map(_static_worker_process, worker_args)
            except Exception as e:
                self.logger.critical(f"Multiprocessing failed: {e}")
                raise

            for result_tuple in results_list:
                if not isinstance(result_tuple, tuple) or len(result_tuple) != 2:
                    continue
                sub_result, temp_map_from_worker = result_tuple

                if isinstance(temp_map_from_worker, dict):
                    for level_key, evidence_list in temp_map_from_worker.items():
                        current_list = self.evidence_map.setdefault(level_key, [])
                        current_list.extend(evidence_list)

                raw_refs = sub_result.get("raw_results_ref", [])
                self.raw_llm_results.extend(raw_refs if isinstance(raw_refs, list) else [])
                self.final_subcriteria_results.append(sub_result)

        else:
            # --------------------- SEQUENTIAL MODE ---------------------
            mode_desc = target_sub_id if target_sub_id != "all" else "All Sub-Criteria"
            self.logger.info(f"Starting Sequential Assessment: {mode_desc}")

            local_vsm = vectorstore_manager or (
                load_all_vectorstores(
                    doc_types=[EVIDENCE_DOC_TYPES],
                    enabler_filter=self.config.enabler,
                    tenant=self.config.tenant,
                    year=self.config.year,
                ) if self.config.mock_mode == "none" else None
            )
            self.vectorstore_manager = local_vsm

            if self.vectorstore_manager:
                self.vectorstore_manager.logger = self.logger
                self.logger.info("Assigned Engine logger to VectorStoreManager")

            for sub_criteria in sub_criteria_list:
                sub_result, final_temp_map = self._run_sub_criteria_assessment_worker(sub_criteria)
                self.raw_llm_results.extend(sub_result.get("raw_results_ref", []))
                self.final_subcriteria_results.append(sub_result)

        # ============================== 3. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Evidence Map (‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏™‡∏°‡∏≠) ==============================
        if self.evidence_map:
            self.logger.info(f"[EVIDENCE] Attempting to persist Evidence Map ‚Üí {self.evidence_map_path}")
            try:
                self._save_evidence_map(map_to_save=self.evidence_map)
                total_items = sum(len(v) for v in self.evidence_map.values() if isinstance(v, list))
                self.logger.info(
                    f"Evidence Map SAVED SUCCESSFULLY | Keys: {len(self.evidence_map)} | "
                    f"Items: {total_items} | Path: {self.evidence_map_path}"
                )
            except Exception as e:
                self.logger.critical(f"FAILED TO SAVE Evidence Map ‚Üí {self.evidence_map_path}")
                self.logger.exception(e)
        else:
            self.logger.warning("[EVIDENCE] Evidence Map is EMPTY ‚Üí Nothing to save")

        # ============================== 4. ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏• & Export ==============================
        self._calculate_overall_stats(target_sub_id)

        final_results = {
            "summary": self.total_stats,
            "sub_criteria_results": self.final_subcriteria_results,
            "raw_llm_results": self.raw_llm_results,
            "run_time_seconds": round(time.time() - start_ts, 2),
            "timestamp": datetime.now().isoformat(),
        }

        if export:
            export_path = self._export_results(
                results=final_results,
                enabler=self.config.enabler,
                sub_criteria_id=target_sub_id if target_sub_id != "all" else "ALL",
                target_level=self.config.target_level
            )
            final_results["export_path_used"] = export_path
            final_results["evidence_map_snapshot"] = deepcopy(self.evidence_map)

        return final_results

    # -------------------- _run_single_assessment (FINAL REVISED VERSION) --------------------
    def _run_single_assessment(
        self,
        sub_criteria: Dict[str, Any],
        statement_data: Dict[str, Any],
        vectorstore_manager: Optional['VectorStoreManager'],
        sequential_chunk_uuids: Optional[List[str]] = None,
        attempt: int = 1
    ) -> Dict[str, Any]:
        """
        ‡∏£‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô Level ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (L1-L5) ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå
        *FIXED: Adaptive Filtering Fallback Logic, PDCA Gate & Weighted Strength*
        """
    
        start_time = time.time()
        sub_id = sub_criteria['sub_id']
        level = statement_data['level']
        statement_text = statement_data['statement']
        sub_criteria_name = sub_criteria['sub_criteria_name']
        statement_id = statement_data.get('statement_id', sub_id)

        self.logger.info(f"  > Starting assessment for {sub_id} L{level} (Attempt: {attempt})...")

        # ==================== 1. PDCA & Keywords ====================
        pdca_phase = self._get_pdca_phase(level)
        level_constraint = self._get_level_constraint_prompt(level)

        context_rules = self.contextual_rules_map.get(sub_id, {})
        must_include_keywords = ", ".join(context_rules.get("must_include_keywords", []))
        avoid_keywords = ", ".join(context_rules.get("avoid_keywords", []))

        planning_keywords = self._get_keywords_for_phase(sub_id, level, "Plan")
        # üìå NEW: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Contextual Rule ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
        # ‡∏î‡∏∂‡∏á Rule ‡∏ó‡∏µ‡πà‡∏°‡∏µ level ‡πÅ‡∏•‡∏∞ sub_id ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
        contextual_rule = self._get_applicable_contextual_rule(sub_id, level)

        # ==================== 2. Hybrid Retrieval Setup ====================
        mapped_stable_doc_ids, priority_docs_unhydrated = self._get_mapped_uuids_and_priority_chunks(
            sub_id=sub_id,
            level=level,
            statement_text=statement_text,
            level_constraint=level_constraint,
            vectorstore_manager=vectorstore_manager
        )

        priority_docs = self._robust_hydrate_documents_for_priority_chunks(
            chunks_to_hydrate=priority_docs_unhydrated,
            vsm=vectorstore_manager,
            current_sub_id=sub_id
        )

        # ==================== 3. Enhance Query ====================
        rag_query_list = self.enhance_query_for_statement( 
            statement_text=statement_text,
            sub_id=sub_id,
            statement_id=statement_id,
            level=level,
            focus_hint=level_constraint,
            # ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏™‡πà‡∏á enabler_id ‡πÅ‡∏•‡∏∞ contextual_map ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ñ‡∏π‡∏Å‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏ú‡πà‡∏≤‡∏ô self ‡πÅ‡∏•‡πâ‡∏ß
        )
        rag_query = rag_query_list[0] if rag_query_list else statement_text

        # ==================== 4. LLM Evaluator ====================
        llm_evaluator_to_use = evaluate_with_llm_low_level if level <= 2 else self.llm_evaluator

        # ==================== 5. ADAPTIVE RAG LOOP (FIXED: Filter Score 0.0000) ====================
        highest_rerank_score = 0.0
        final_top_evidences = []
        retrieval_start = time.time()
        loop_attempt = 1

        while loop_attempt <= MAX_RETRIEVAL_ATTEMPTS:
            self.logger.info(
                f"  > RAG Retrieval {sub_id} L{level} (Attempt: {loop_attempt}/{MAX_RETRIEVAL_ATTEMPTS}). "
                f"Best score so far: {highest_rerank_score:.4f}"
            )

            query_input = rag_query_list if loop_attempt == 1 and rag_query_list else [rag_query]

            try:
                retrieval_result = self.rag_retriever(
                    query=query_input,
                    doc_type=EVIDENCE_DOC_TYPES,
                    enabler=self.config.enabler,
                    sub_id=sub_id,
                    level=level,
                    vectorstore_manager=vectorstore_manager,
                    mapped_uuids=mapped_stable_doc_ids,
                    priority_docs_input=priority_docs,
                    sequential_chunk_uuids=sequential_chunk_uuids,
                )
            except Exception as e:
                self.logger.error(f"RAG retrieval failed: {e}")
                break

            top_evidences_current = retrieval_result.get("top_evidences", [])

            current_max_score = max(
                (ev.get("rerank_score") or ev.get("score", 0.0) for ev in top_evidences_current),
                default=0.0
            )
            
            # üü¢ FIX: ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏û‡∏ö Rerank Score ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà 0.0 ‡πÉ‡∏´‡πâ‡∏Å‡∏£‡∏≠‡∏á Chunks ‡∏ó‡∏µ‡πà Score 0.0000 ‡∏≠‡∏≠‡∏Å (‡∏°‡∏≤‡∏à‡∏≤‡∏Å Priority Chunks ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏£‡∏ß‡∏°‡∏°‡∏≤)
            if current_max_score > 0.0001: 
                original_count = len(top_evidences_current)
                top_evidences_current = [
                    ev for ev in top_evidences_current 
                    if (ev.get("rerank_score") or ev.get("score", 0.0)) > 0.0001
                ]
                if len(top_evidences_current) < original_count:
                    # ‚úÖ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô INFO ‡πÄ‡∏õ‡πá‡∏ô DEBUG
                    self.logger.debug(f"Filtered out {original_count - len(top_evidences_current)} chunks with Score 0.0000 (assumed Priority Chunks).")
            # -----------------------------------------------------------------------------------------------------------------------------------


            priority_max_score = max(
                (doc.get("rerank_score") or doc.get("score", 0.0) for doc in priority_docs),
                default=0.0
            )
            overall_max_score = max(current_max_score, priority_max_score)

            self.logger.info(
                f"  > Attempt {loop_attempt} ‚Üí New: {current_max_score:.4f} | Priority: {priority_max_score:.4f} | "
                f"Overall: {overall_max_score:.4f}"
            )

            if overall_max_score > highest_rerank_score:
                highest_rerank_score = overall_max_score
                final_top_evidences = top_evidences_current
                if loop_attempt > 1:
                    self.logger.info(f"  > Retrieval improved: New overall best {highest_rerank_score:.4f}")

            if highest_rerank_score >= MIN_RETRY_SCORE:
                self.logger.info(f"  > Adaptive Retrieval L{level}: Score {highest_rerank_score:.4f} ‚â• {MIN_RETRY_SCORE} ‚Üí STOP")
                break

            if loop_attempt < MAX_RETRIEVAL_ATTEMPTS:
                rag_query = f"‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {statement_text} ‡πÉ‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó {level_constraint}"

            loop_attempt += 1

        retrieval_duration = time.time() - retrieval_start
        top_evidences = final_top_evidences 

        # ==================== 6. Adaptive Filtering ====================
        filtered = []
        original_top_evidences = top_evidences 

        for doc in original_top_evidences:
            score = doc.get('rerank_score', doc.get('score', 0.0))
            is_baseline = doc.get('is_baseline', False)  # ‚≠ê Check Baseline Flag
            doc_id = doc.get('chunk_uuid') or doc.get('doc_id') or 'UNKNOWN'
            
            # üü¢ FIX 2: ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô Baseline Evidence ‚Üí ‡∏ú‡πà‡∏≤‡∏ô‡∏ó‡∏±‡∏ô‡∏ó‡∏µ (‡πÑ‡∏°‡πà‡πÄ‡∏ä‡πá‡∏Ñ Threshold)
            if is_baseline:
                filtered.append(doc)
                self.logger.debug(f"‚úÖ Baseline chunk kept (ID: {doc_id[:8]}...) | Score {score:.4f}")
                continue
            
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà Baseline ‚Üí ‡πÄ‡∏ä‡πá‡∏Ñ Score ‡∏ï‡∏≤‡∏° Threshold ‡∏õ‡∏Å‡∏ï‡∏¥
            if score >= globals()['MIN_RERANK_SCORE_TO_KEEP']:
                filtered.append(doc)
            else:
                # ‚úÖ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô INFO ‡πÄ‡∏õ‡πá‡∏ô DEBUG
                self.logger.debug(f"Filtering out chunk (ID: {doc_id[:8]}...) | Score {score:.4f}")

        # Fallback Logic ‡∏ñ‡πâ‡∏≤‡∏Å‡∏£‡∏≠‡∏á‡∏´‡∏°‡∏î
        if not filtered and original_top_evidences:
            # üü¢ ‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡πÄ‡∏õ‡πá‡∏ô WARNING ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô
            self.logger.warning(
                f"  > (L{level}) Adaptive Filtering removed all chunks. "
                f"Using all {len(original_top_evidences)} original chunks for PDCA grouping (Fallback)."
            )
            top_evidences = original_top_evidences
        elif not filtered and not original_top_evidences:
            top_evidences = [] 
        else:
            top_evidences = filtered 

        # ‚úÖ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô INFO ‡πÄ‡∏õ‡πá‡∏ô DEBUG
        self.logger.debug(f"Adaptive Filter L{level}: Kept {len(top_evidences)}/{len(original_top_evidences)} chunks "
                        f"({len([d for d in top_evidences if d.get('is_baseline')])} baseline)")

        # ==================== 6.5. üü¢ FIX 4: Robust Hydration for Filtered Chunks ====================
        if top_evidences and vectorstore_manager:
            # ‚úÖ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô INFO ‡πÄ‡∏õ‡πá‡∏ô DEBUG
            self.logger.debug(f"FIX 4: Running Robust Hydration for {len(top_evidences)} filtered chunks (New + Baseline)...")
            
            # **CRITICAL FIX**: ‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÄ‡∏û‡∏∑‡πà‡∏≠ Hydrate
            top_evidences = self._robust_hydrate_documents_for_priority_chunks(
                chunks_to_hydrate=top_evidences,
                vsm=vectorstore_manager
            )
            
            # ‚úÖ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô INFO ‡πÄ‡∏õ‡πá‡∏ô DEBUG
            self.logger.debug(f"FIX 4: Hydration complete. Final chunks with text: {len([c for c in top_evidences if 'text' in c and c['text'].strip()])}")

        # ==================== 7. Baseline from Previous Levels ====================
        # üéØ FIX 1: Initial Value ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô Dictionary ‡πÄ‡∏õ‡∏•‡πà‡∏≤ {} ‡πÄ‡∏™‡∏°‡∏≠
        previous_levels_evidence_dict = {} 
        # List ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤ build_multichannel_context_for_level
        previous_levels_evidence_list = [] 

        if level > 1 and not self.is_parallel_all_mode:
            # _collect_previous_level_evidences ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô Dict[str, List[Dict]]
            previous_levels_evidence_dict = self._collect_previous_level_evidences(sub_id, current_level=level)
        
        # ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏à‡∏≤‡∏Å Dictionary ‡πÄ‡∏Ç‡πâ‡∏≤ List ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Summary Context
        for lst in previous_levels_evidence_dict.values():
            previous_levels_evidence_list.extend(lst)

        # ==================== 8. Build Context ====================
        # üü¢ FIX 10.1: ‡∏£‡∏ß‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏´‡∏°‡πà (top_evidences) ‡∏Å‡∏±‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤
        # NOTE: top_evidences ‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏≠‡∏á _robust_hydrate_documents_for_priority_chunks
        total_evidences = top_evidences + previous_levels_evidence_list 

        # üü¢ FIX 10.2: ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏Ñ‡∏µ‡∏¢‡πå 'text' ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏£‡∏ß‡∏° (‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÄ‡∏Å‡πà‡∏≤ L1)
        total_evidences = self._guarantee_text_key(total_evidences) 
        
        plan_blocks, do_blocks, check_blocks, act_blocks, other_blocks = self._get_pdca_blocks_from_evidences(
            total_evidences,
            # üéØ FIX 2: ‡∏™‡πà‡∏á Dictionary ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ .values() ‡πÉ‡∏ô‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏î Error
            baseline_evidences=previous_levels_evidence_dict, 
            level=level,
            sub_id=sub_id,
            contextual_rules_map=self.contextual_rules_map
        )

        direct_context = "\n\n".join(filter(None, [
            plan_blocks,
            do_blocks,
            check_blocks,
            act_blocks,
            other_blocks
        ]))

        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Context Length Limit ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö L3 ‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏õ
        max_context_length = None
        if level >= 3:
            max_context_length = CONTEXT_CAP_L3_PLUS 
            self.logger.info(f"Context Cap set for L{level}: {max_context_length} characters.")

        # --- CRITICAL C/A EVIDENCE SUMMARY ---
        critical_evidence_summary = ""
        if level >= 2:
            # CRITICAL_SCORE_THRESHOLD = globals()['CRITICAL_CA_THRESHOLD'] 
            
            critical_chunks = [
                doc for doc in top_evidences 
                if doc.get('pdca_tag') in ['Check', 'Act'] and doc.get('rerank_score', 0.0) >= CRITICAL_CA_THRESHOLD
            ]
            
            if critical_chunks:
                self.logger.critical(f"Found {len(critical_chunks)} CRITICAL C/A chunks (Score >= {CRITICAL_CA_THRESHOLD}) for L{level}.")
                summary_text = "\n".join([
                    f"- [{doc['pdca_tag']} | Score: {doc.get('rerank_score'):.4f}] {doc['text'][:180].strip()}..." 
                    for doc in critical_chunks
                ])
                critical_evidence_summary = f"--- CRITICAL C/A EVIDENCE (SCORE > {CRITICAL_CA_THRESHOLD}) ---\n{summary_text}"
            else:
                self.logger.info(f"No CRITICAL C/A chunks found (Score < {CRITICAL_CA_THRESHOLD}) for L{level}.")
                
        # üü¢ ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£ aux_summary ‡πÅ‡∏•‡∏∞ baseline_summary ‡∏ñ‡∏π‡∏Å‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà 
        channels = build_multichannel_context_for_level(
            level=level,
            top_evidences=top_evidences,
            # üéØ FIX 3: ‡∏™‡πà‡∏á List ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô build_multichannel_context_for_level
            previous_levels_evidence=previous_levels_evidence_list, 
            max_main_context_tokens=3000,
            max_summary_sentences=4,
            max_context_length=max_context_length
        )
        aux_summary = channels.get('aux_summary', '‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏£‡∏≠‡∏á')
        baseline_summary = channels.get('baseline_summary', '‡πÑ‡∏°‡πà‡∏°‡∏µ')

        # üü¢ ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£ final_llm_context ‡∏ñ‡∏π‡∏Å‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà
        final_llm_context = "\n\n".join(filter(None, [
            f"--- DIRECT EVIDENCE (L{level} | PDCA Structured)---\n{direct_context}",
            critical_evidence_summary, 
            f"--- AUXILIARY EVIDENCE SUMMARY ---\n{aux_summary}",
            f"--- BASELINE FROM PREVIOUS LEVELS SUMMARY ---\n{baseline_summary}"
        ]))

        if not final_llm_context.strip():
            final_llm_context = "--- ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á ---"
            self.logger.warning(f"No context generated for {sub_id} L{level}")
        elif max_context_length and len(final_llm_context) > max_context_length:
            self.logger.warning(f"Final LLM Context for L{level} still exceeded Cap. Length: {len(final_llm_context)} (Cap: {max_context_length})")

        self.logger.debug(f"--- LLM CONTEXT (L{level}) --- \n{final_llm_context}")

        # ==================== 9. Evidence Strength Calculation & PDCA Gate ====================
        
        # --- PDCA Completeness Check --- (Patch 1: PDCA Gate)
        available_tags = set()
        if plan_blocks.strip(): available_tags.add("P")
        if do_blocks.strip(): available_tags.add("D")
        if check_blocks.strip(): available_tags.add("C")
        if act_blocks.strip(): available_tags.add("A")

        missing_tags = REQUIRED_PDCA.get(level, set()) - available_tags
        
        # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
        ai_confidence = "HIGH"
        is_hard_fail_pdca = False
        
        # (‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ Logic ‡πÄ‡∏î‡∏¥‡∏° ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ max_evi_str_for_prompt ‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô‡∏à‡∏≤‡∏Å Rerank)
        evi_cap_data = self._calculate_evidence_strength_cap(
            top_evidences=top_evidences,
            level=level,
            highest_rerank_score=highest_rerank_score
        )
        max_evi_str_for_prompt = evi_cap_data['max_evi_str_for_prompt']

        # ----------------------------------------------------------------------------------------------------
        # üü¢ NEW: CONTEXTUAL RULE HARD FAIL OVERRIDE CHECK
        # ----------------------------------------------------------------------------------------------------
        is_contextual_override_active = False
        
        # üéØ Check 1: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ Contextual Rule ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if contextual_rule and globals().get('ENABLE_CONTEXTUAL_RULE_OVERRIDE', False):
            # üéØ Check 2: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Rule ‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó OVERRIDE_ON_PDCA_GAP ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            if contextual_rule['rule_type'] == 'SCORE_OVERRIDE_ON_PDCA_GAP':
                
                # üéØ Check 3: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏Ç‡∏≠‡∏á Rule ‡∏î‡πâ‡∏ß‡∏¢‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
                if self._check_contextual_rule_condition(contextual_rule['condition'], sub_id, level, previous_levels_evidence_dict, top_evidences):
                    
                    self.logger.critical(f"  > CONTEXTUAL OVERRIDE L{level}: Rule '{contextual_rule['name']}' matched conditions (PDCA Gap Bypass).")
                    is_contextual_override_active = True
        
        # ----------------------------------------------------------------------------------------------------

        if missing_tags:
            self.logger.warning(
                f"  > PDCA INCOMPLETE for L{level} | Missing: {sorted(missing_tags)} | "
                f"Forcing PARTIAL evidence strength & confidence"
            )
            # Force partial strength cap
            max_evi_str_for_prompt = min(max_evi_str_for_prompt, 3.0)
            ai_confidence = "LOW"
            
            # Hard rule for L3+ (strict SE-AM)
            if level >= 3 and (("C" in missing_tags) or ("A" in missing_tags)):
                if not is_contextual_override_active:
                    # üî¥ ‡∏ó‡∏≥ Hard Fail ‡∏ñ‡πâ‡∏≤ Rule ‡πÑ‡∏°‡πà Override
                    self.logger.critical(f"  > HARD FAIL L{level}: Missing critical closed-loop PDCA phase(s): {missing_tags} - Skipping LLM call.")
                    is_hard_fail_pdca = True
                else:
                    # üü¢ Bypass Hard Fail ‡πÅ‡∏ï‡πà‡πÉ‡∏ä‡πâ Logic ‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏à‡∏≤‡∏Å Rule ‡πÅ‡∏ó‡∏ô
                    self.logger.warning(f"  > HARD FAIL AVOIDED: Contextual Rule Bypassed PDCA Hard Fail Logic.")
                    # ‡πÑ‡∏°‡πà‡∏ï‡∏±‡πâ‡∏á is_hard_fail_pdca = True ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÇ‡∏Ñ‡πâ‡∏î‡∏£‡∏±‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏î‡πâ ‡πÅ‡∏ï‡πà‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ Rule Score ‡πÅ‡∏ó‡∏ô
                
        else:
            # ‚úÖ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô INFO ‡πÄ‡∏õ‡πá‡∏ô DEBUG
            self.logger.debug(f"  > PDCA COMPLETE for L{level}: {sorted(available_tags)}")


        # --- Weighted Evidence Strength --- (Patch 2: Weighted Strength)
        if level in REQUIRED_PDCA and available_tags:
            pdca_completeness_ratio = len(available_tags) / len(REQUIRED_PDCA.get(level))
            
            # ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏Ç‡∏≠‡∏á PDCA (0.6 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö L3+ ‡πÅ‡∏•‡∏∞ 0.3 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö L1/L2)
            weight_pdca = 0.6 if level >= 3 else 0.3
            
            # highest_rerank_score ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ Rerank Score ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
            adjusted_score = (
                (1 - weight_pdca) * highest_rerank_score 
                + weight_pdca * pdca_completeness_ratio
            )
            
            # ‡πÉ‡∏ä‡πâ adjusted_score ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î Evidence Strength ‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤ Prompt (‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 10.0)
            # ‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ min ‡∏Å‡∏±‡∏ö max_evi_str_for_prompt ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏ñ‡∏π‡∏Å cap 3.0 ‡∏à‡∏≤‡∏Å Hard Fail ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤
            final_strength_from_weighted = min(adjusted_score * 10.0, 10.0)
            max_evi_str_for_prompt = min(max_evi_str_for_prompt, final_strength_from_weighted)
            
            # ‚úÖ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô INFO ‡πÄ‡∏õ‡πá‡∏ô DEBUG
            self.logger.debug(
                f"  > Weighted Evidence Score L{level}: Rerank={highest_rerank_score:.2f} "
                f"| PDCA Ratio={pdca_completeness_ratio:.2f} | Final Strength={max_evi_str_for_prompt:.1f}"
            )
        
        # ==================== 10. LLM Evaluation ====================
        llm_start = time.time()
        llm_result = {}
        is_passed_llm = False # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô

        if not is_hard_fail_pdca:
            try:
                llm_kwargs = {
                    "context": final_llm_context,
                    "sub_criteria_name": sub_criteria_name,
                    "level": level,
                    "statement_text": statement_text,
                    "sub_id": sub_id,
                    "pdca_phase": pdca_phase,
                    "level_constraint": level_constraint,
                    "must_include_keywords": must_include_keywords,
                    "avoid_keywords": avoid_keywords,
                    "max_rerank_score": highest_rerank_score,
                    "max_evidence_strength": max_evi_str_for_prompt, # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏•‡πâ‡∏ß
                    "llm_executor": self.llm,
                    "contextual_rules_map": self.contextual_rules_map,
                    "enabler_id": self.config.enabler,
                    "ai_confidence": ai_confidence # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏•‡πâ‡∏ß
                }

                if level <= 2:
                    llm_kwargs["planning_keywords"] = planning_keywords

                llm_result = llm_evaluator_to_use(**llm_kwargs)
                is_passed_llm = llm_result.get('is_passed', False)
            except Exception as e:
                self.logger.error(f"LLM Call failed: {e}")
                llm_result = {}
                is_passed_llm = False
        else:
            self.logger.info(f"  > Skipping LLM call for L{level} due to PDCA Hard Fail.")

        llm_duration = time.time() - llm_start

        # ==================== 11. Post-Processing & Scoring (STRICT PDCA OVERRIDE APPLIED) ====================
        if not isinstance(llm_result, dict):
            llm_result = {}

        llm_result = post_process_llm_result(llm_result, level)

        # üìå ‡∏õ‡∏£‡∏±‡∏ö Final Correction ‡πÉ‡∏´‡πâ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Hard Fail
        if is_hard_fail_pdca:
            # ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å LLM, score ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô 0 ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
            final_score = 0.0
            is_passed = False
            final_pdca_breakdown = {'P': 0.0, 'D': 0.0, 'C': 0.0, 'A': 0.0}
            self.logger.warning(f"  > L{level} Final Score is 0.0 due to PDCA HARD FAIL.")
        
        # üü¢ NEW: CONTEXTUAL OVERRIDE SCORING
        elif is_contextual_override_active:
            action = contextual_rule['action']
            if action.get('bypass_hard_fail', False):
                
                # ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÑ‡∏ß‡πâ‡πÉ‡∏ô Rule ‡πÅ‡∏ó‡∏ô LLM
                final_pdca_breakdown = action.get('force_llm_input_scores', {'P': 0.0, 'D': 0.0, 'C': 0.0, 'A': 0.0})
                
                # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Reason/Extraction ‡∏à‡∏≤‡∏Å Rule Comment
                llm_result['reason'] = action.get('comment', 'Bypassed by Contextual Rule due to PDCA Gap.')
                llm_result['extraction_c'] = f"OVERRIDE: {final_pdca_breakdown.get('C', 0.0)}"
                llm_result['extraction_a'] = f"OVERRIDE: {final_pdca_breakdown.get('A', 0.0)}"

                final_score = sum(final_pdca_breakdown.values())
                required_score_for_level = get_correct_pdca_required_score(level)
                is_passed = final_score >= required_score_for_level

                self.logger.critical(f"  > L{level} Contextual Override Success. Score: {final_score:.1f} (Rule: {final_pdca_breakdown})")

        # -------------------------------------------------------------------------------------------

        else:
            # ‡πÉ‡∏ä‡πâ Logic ‡πÄ‡∏î‡∏¥‡∏° (Post-process ‡πÅ‡∏•‡∏∞ Override C/A Score ‡∏à‡∏≤‡∏Å LLM)
            final_pdca_breakdown = llm_result.get('pdca_breakdown', {})
            
            C_KEYWORDS = BASE_PDCA_KEYWORDS['Check']
            A_KEYWORDS = BASE_PDCA_KEYWORDS['Act']
            
            # --- C/A SCORE OVERRIDE (FORCE UP) ---
            
            # 1. ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô C (Check) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö L3, L4, L5
            if level >= 3 and final_pdca_breakdown.get('C', 0) < 2:
                
                is_c_evidence_found = any(
                    chunk.get('pdca_tag') == 'Check' or 
                    any(k in chunk.get('text', '') for k in C_KEYWORDS)
                    for chunk in top_evidences
                )
                
                is_p_d_ok = final_pdca_breakdown.get('P', 0) >= 1 and final_pdca_breakdown.get('D', 0) >= 1
                
                if is_c_evidence_found and is_p_d_ok:
                    final_pdca_breakdown['C'] = 2.0
                    self.logger.warning(f"  > L{level} C Score OVERRIDE: Forced to 2.0 due to evidence/keywords 'Check' found.")
                elif level == 3 and is_c_evidence_found:
                    final_pdca_breakdown['C'] = 2.0
                    self.logger.warning(f"  > L3 C Score OVERRIDE: Forced to 2.0 (L3 Focus) due to evidence/keywords 'Check' found.")


            # 2. ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô A (Act) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö L4, L5
            if level >= 4 and final_pdca_breakdown.get('A', 0) < 2:
                
                is_a_evidence_found = any(
                    chunk.get('pdca_tag') == 'Act' or 
                    any(k in chunk.get('text', '') for k in A_KEYWORDS)
                    for chunk in top_evidences
                )
                
                if is_a_evidence_found and final_pdca_breakdown.get('C', 0) == 2.0: 
                    final_pdca_breakdown['A'] = 2.0
                    self.logger.warning(f"  > L{level} A Score OVERRIDE: Forced to 2.0 due to evidence/keywords 'Act' found and C is 2.0.")

            # 3. Final Correction & Calculation after Override UP
            
            if level == 3:
                if final_pdca_breakdown.get('A', 0) > 0:
                    self.logger.warning(f"  > L3 PDCA Correction: A_Act_Score must be 0. Correcting.")
                    final_pdca_breakdown['A'] = 0.0

            final_score = sum(final_pdca_breakdown.values())
            
            required_score_for_level = get_correct_pdca_required_score(level)
            is_passed = final_score >= required_score_for_level


        # -------------------------------------------------------------
        
        status = "PASS" if is_passed else "FAIL"
        # üìå Final Evidence Strength ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å Cap ‡πÅ‡∏•‡πâ‡∏ß (max_evi_str_for_prompt)
        evidence_strength = max_evi_str_for_prompt if is_passed else 0.0
        
        # üìå Final AI Confidence ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏õ‡∏£‡∏±‡∏ö‡∏à‡∏≤‡∏Å Patch 1 ‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏à‡∏≤‡∏Å Strength ‡πÉ‡∏´‡∏°‡πà
        if is_passed:
            # ‡πÉ‡∏ä‡πâ Logic ‡πÄ‡∏î‡∏¥‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Confidence ‡∏ñ‡πâ‡∏≤ PASS ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà Hard Fail
            # üî¥ FIX: ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç Syntax Error ‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ
            ai_confidence = "HIGH" if evidence_strength >= 8 else "MEDIUM" if evidence_strength >= 5.5 else "LOW"
        elif is_hard_fail_pdca:
            ai_confidence = "LOW"
            
        icon = "üü¢" if is_passed else "üî¥"

        self.logger.info(
            f"  > Assessment {sub_id} L{level} completed ‚Üí {icon} {status} "
            f"(Score: {final_score:.1f} | Evi Str: {evidence_strength:.1f} | Conf: {ai_confidence})"
        )

        # ‚úÖ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô INFO ‡πÄ‡∏õ‡πá‡∏ô DEBUG
        self.logger.debug(
            f"  > Context Built L{level}: Direct chunks={len(top_evidences)}, "
            f"Aux={'‡∏°‡∏µ' if aux_summary != '‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏£‡∏≠‡∏á' else '‡πÑ‡∏°‡πà‡∏°‡∏µ'}, "
            f"Baseline={'‡∏°‡∏µ' if '‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏à‡∏≤‡∏Å Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤' not in baseline_summary else '‡πÑ‡∏°‡πà‡∏°‡∏µ'}"
        )

        # === ‡πÅ‡∏Å‡πâ pdca_tag ‡πÉ‡∏ô temp_map_for_level ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏ó‡∏µ‡πà re-tag ‡πÅ‡∏•‡πâ‡∏ß ===
        for chunk in top_evidences:
            if "text" in chunk and chunk["text"].strip():
                # üìå ‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤ classify_by_keyword ‡∏ñ‡∏π‡∏Å Import ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡πá‡∏ô method ‡∏Ç‡∏≠‡∏á class
                new_tag = classify_by_keyword(chunk["text"], sub_id=sub_id, contextual_rules_map=self.contextual_rules_map)
                if new_tag != 'Other':
                    chunk["pdca_tag"] = new_tag
        # ==================================================================================

        return {
            "sub_criteria_id": sub_id,
            "level": level,
            "statement_id": statement_id,
            "statement_text": statement_text,
            "is_passed": is_passed,
            "score": round(final_score, 1),
            "pdca_breakdown": final_pdca_breakdown,
            "reason": llm_result.get('reason', "No reason provided"),
            "extraction_c": llm_result.get('extraction_c', "-"),
            "extraction_a": llm_result.get('extraction_a', "-"),
            "evidence_strength": evidence_strength,
            "ai_confidence": ai_confidence,
            "max_relevant_score": highest_rerank_score,
            "temp_map_for_level": top_evidences,
            "duration": time.time() - start_time,
            "retrieval_duration": retrieval_duration,
            "llm_duration": llm_duration,
        }