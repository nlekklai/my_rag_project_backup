# core/seam_assessment.py

import sys
import json
import logging
import time
import os
from typing import List, Dict, Any, Optional, Union, Tuple, Set, Final, Literal
from collections import defaultdict
from datetime import datetime
from dataclasses import dataclass, field
import multiprocessing # NEW: Import for parallel execution
from functools import partial
from core.llm_data_utils import enhance_query_for_statement
import pathlib, uuid
from langchain_core.documents import Document as LcDocument
from core.retry_policy import RetryPolicy, RetryResult
from copy import deepcopy
import tempfile
import shutil
# from json_extractor import _robust_extract_json
from .json_extractor import _robust_extract_json
from filelock import FileLock  # ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á: pip install filelock


# -------------------- PATH SETUP & IMPORTS --------------------
try:
    PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
    if PROJECT_ROOT not in sys.path:
        sys.path.append(PROJECT_ROOT)

    from config.global_vars import (
        EXPORTS_DIR, MAX_LEVEL, INITIAL_LEVEL, FINAL_K_RERANKED,
        RUBRIC_FILENAME_PATTERN, RUBRIC_CONFIG_DIR, DEFAULT_ENABLER,
        EVIDENCE_DOC_TYPES, INITIAL_TOP_K,
        EVIDENCE_MAPPING_FILENAME_SUFFIX,
        LIMIT_CHUNKS_PER_PRIORITY_DOC,
        IS_LOG_L3_CONTEXT,
        PRIORITY_CHUNK_LIMIT
    )
    
    from core.llm_data_utils import ( 
        create_structured_action_plan, evaluate_with_llm,
        retrieve_context_with_filter, retrieve_context_for_low_levels,
        evaluate_with_llm_low_level, LOW_LEVEL_K, 
        set_mock_control_mode as set_llm_data_mock_mode,
        create_context_summary_llm,
        retrieve_context_by_doc_ids,
        _fetch_llm_response,
        build_multichannel_context_for_level
    )
    from core.vectorstore import VectorStoreManager, load_all_vectorstores, get_global_reranker 
    from core.seam_prompts import PDCA_PHASE_MAP 
        
    import assessments.seam_mocking as seam_mocking 
    
except ImportError as e:
    print(f"FATAL ERROR: Failed to import required modules. Error: {e}", file=sys.stderr)
    
    # Define placeholder variables if imports fail
    EXPORTS_DIR = "exports"
    MAX_LEVEL = 5
    INITIAL_LEVEL = 1
    FINAL_K_RERANKED = 3
    RUBRIC_FILENAME_PATTERN = "{enabler}_rubric.json"
    RUBRIC_CONFIG_DIR = "config/rubrics"
    DEFAULT_ENABLER = "KM"
    EVIDENCE_DOC_TYPES = "evidence"
    INITIAL_TOP_K = 10
    
    def create_structured_action_plan(*args, **kwargs): return [{"Phase": "Mock Plan", "Goal": "Resolve issue"}]
    def evaluate_with_llm(*args, **kwargs): return {"score": 1, "reason": "Mock pass", "is_passed": True}
    def retrieve_context_with_filter(*args, **kwargs): return {"top_evidences": [], "aggregated_context": "Mock Context"}
    def retrieve_context_for_low_levels(*args, **kwargs): return {"top_evidences": [], "aggregated_context": "Mock Low Context"}
    def evaluate_with_llm_low_level(*args, **kwargs): return {"score": 1, "reason": "Mock pass L1/L2", "is_passed": True}
    LOW_LEVEL_K = 2
    def set_llm_data_mock_mode(mode): pass
    class VectorStoreManager: pass
    def load_all_vectorstores(*args, **kwargs): return VectorStoreManager()
    PDCA_PHASE_MAP = {1: "Plan", 2: "Do", 3: "Check", 4: "Act", 5: "Innovate"}
    class seam_mocking:
        @staticmethod
        def evaluate_with_llm_CONTROLLED_MOCK(*args, **kwargs): return {"score": 0, "reason": "Mock fail", "is_passed": False}
        @staticmethod
        def retrieve_context_with_filter_MOCK(*args, **kwargs): return {"top_evidences": [], "aggregated_context": "Mock Context"}
        @staticmethod
        def create_structured_action_plan_MOCK(*args, **kwargs): return [{"Phase": "Mock Plan", "Goal": "Resolve issue"}]
        @staticmethod
        def set_mock_control_mode(mode): pass
    
    if "FATAL ERROR" in str(e):
        pass 


logger = logging.getLogger(__name__)
if not logger.handlers:
    logging.basicConfig(level=logging.DEBUG, format="%(asctime)s - %(levelname)s - %(message)s")

# =================================================================
# üü¢ FIX: Helper Function for PDCA Calculation (Priority 1 Part 2 & Priority 2)
# NOTE: ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡πÅ‡∏õ‡∏•‡∏á llm_score (1-5) ‡πÄ‡∏õ‡πá‡∏ô PDCA Breakdown ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
# =================================================================
LEVEL_PHASE_MAP = {
    1: ['P'],
    2: ['P', 'D'],
    3: ['P', 'D', 'C'],
    4: ['P', 'D', 'C', 'A'],
    5: ['P', 'D', 'C', 'A'] # L5 ‡πÉ‡∏ä‡πâ P, D, C, A ‡πÄ‡∏ä‡πà‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö L4 ‡πÅ‡∏ï‡πà‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏ï‡πá‡∏°‡∏≠‡∏≤‡∏à‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô
}

# ----------------------------------------------------------------------
# NEW CONSTANT: ‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô PDCA ‡∏ó‡∏µ‡πà '‡∏ú‡πà‡∏≤‡∏ô' ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ Level (Achieved Score)
# ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Achieved Score (Sum of P,D,C,A) ‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö Required Score (R)
# L1 (R=1, A=1): P=1
# L2 (R=2, A=2): P=1, D=1
# L3 (R=4, A=4): P=1, D=1, C=1, A=1
# L4 (R=6, A=6): P=2, D=2, C=1, A=1
# L5 (R=8, A=8): P=2, D=2, C=2, A=2
# ----------------------------------------------------------------------
CORRECT_PDCA_SCORES_MAP: Final[Dict[int, Dict[str, int]]] = {
    1: {'P': 1, 'D': 0, 'C': 0, 'A': 0},
    2: {'P': 1, 'D': 1, 'C': 0, 'A': 0},
    3: {'P': 1, 'D': 1, 'C': 1, 'A': 1},
    4: {'P': 2, 'D': 2, 'C': 1, 'A': 1},
    5: {'P': 2, 'D': 2, 'C': 2, 'A': 2},
}

# üü¢ NOTE: ‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£ Global ‡∏´‡∏£‡∏∑‡∏≠ Config Flag ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏¥‡∏î/‡∏õ‡∏¥‡∏î‡πÇ‡∏´‡∏°‡∏î‡∏ô‡∏µ‡πâ
# ‡πÄ‡∏ä‡πà‡∏ô: IS_L3_DEBUG_TEST = True 
# ‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏™‡πà‡∏á‡∏Ñ‡πà‡∏≤‡∏ô‡∏µ‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà build_simulated_l3_evidence (‡πÄ‡∏ä‡πà‡∏ô via debug_mode argument)

def build_simulated_l3_evidence(check_blocks: list[dict]) -> str:

    if not check_blocks:
        return ""

    # --- Original Dynamic Logic ---
    source_files = ", ".join(sorted({b["file"] for b in check_blocks}))
    extracted_summary = "\n\n".join(
        f"- ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå {b['file']}:\n{b['content'][:600]}"
        for b in check_blocks
    )

    return f"""
[SIMULATED_L3_EVIDENCE]
‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö (Check Phase) ‡∏û‡∏ö‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå: {source_files}
... (‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏Ç‡∏≠‡∏á‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏î‡∏¥‡∏°)
""".strip()

def build_ordered_context(level: int,
                          plan_blocks: list[dict],
                          do_blocks: list[dict],
                          check_blocks: list[dict],
                          act_blocks: list[dict],
                          other_blocks: list[dict]) -> str:
    def fmt(blocks):
        return "\n\n".join(
            f"[{b.get('file', 'Unknown File')}]\n{b.get('content', b.get('text', ''))}" for b in blocks
        )

    if level == 3:
        # L3: Check/Act Priority 1, Plan/Do/Other ‡∏ï‡πà‡∏≠‡∏ó‡πâ‡∏≤‡∏¢
        ordered = [
            fmt(check_blocks),
            fmt(act_blocks),
            fmt(plan_blocks),
            fmt(do_blocks),
            fmt(other_blocks)
        ]
    else:
        # Default: Plan -> Do -> Check -> Act -> Other
        ordered = [
            fmt(plan_blocks),
            fmt(do_blocks),
            fmt(check_blocks),
            fmt(act_blocks),
            fmt(other_blocks)
        ]

    return "\n\n".join([o for o in ordered if o])


def calculate_pdca_breakdown_and_pass_status(llm_score: int, level: int) -> Tuple[Dict[str, int], bool, float]:
    """
    ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì PDCA breakdown, is_passed status, ‡πÅ‡∏•‡∏∞ raw_pdca_score (Achieved Score) 
    ‡πÇ‡∏î‡∏¢‡πÅ‡∏õ‡∏•‡∏á‡∏à‡∏≤‡∏Å llm_score (1-5) ‡πÅ‡∏•‡∏∞ Level ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô 
    
    ‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£:
    - L1 ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ llm_score >= 1
    - L2 ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ llm_score >= 2
    - L3 ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ llm_score >= 3
    - L4 ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ llm_score >= 4
    - L5 ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ llm_score >= 4 
    """
    pdca_map: Dict[str, int] = {'P': 0, 'D': 0, 'C': 0, 'A': 0}
    is_passed: bool = False
    raw_pdca_score: float = 0.0
    
    # 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ PASS (‡πÉ‡∏ä‡πâ Logic ‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏Å‡∏≥‡∏´‡∏ô‡∏î)
    if level == 5:
        if llm_score >= 4:
            is_passed = True
    elif level == 4:
        if llm_score >= 4:
            is_passed = True
    elif level == 3:
        if llm_score >= 3:
            is_passed = True
    elif level == 2:
        if llm_score >= 2:
            is_passed = True
    elif level == 1:
        if llm_score >= 1:
            is_passed = True

    # 2. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì PDCA Breakdown ‡πÅ‡∏•‡∏∞ raw_pdca_score (Achieved Score)
    if is_passed:
        # *** REVISED LOGIC: ‡πÉ‡∏ä‡πâ CORRECT_PDCA_SCORES_MAP ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô P, D, C, A ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ***
        correct_scores = CORRECT_PDCA_SCORES_MAP.get(level, pdca_map) 
        pdca_map.update(correct_scores)
        
        # raw_pdca_score (Achieved Score) ‡∏à‡∏∞‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö Required Score (R) ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ú‡πà‡∏≤‡∏ô
        raw_pdca_score = float(sum(pdca_map.values()))
    
    return pdca_map, is_passed, raw_pdca_score

def get_correct_pdca_required_score(level: int) -> int:
    """
    ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤ Required Score (R) ‡∏ï‡∏≤‡∏° Level ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå SE-AM:
    L1=1, L2=2, L3=4, L4=6, L5=8
    """
    # ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ô‡∏µ‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß
    if level == 1:
        return 1
    elif level == 2:
        return 2
    elif level == 3:
        return 4
    elif level == 4:
        return 6
    elif level == 5:
        return 8
    # ‡∏Å‡∏£‡∏ì‡∏µ Level ‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î
    return 8


# üìå ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç Type Hint ‡πÅ‡∏•‡∏∞ Arguments ‡∏Ç‡∏≠‡∏á Tuple ‡πÉ‡∏´‡πâ‡∏£‡∏ß‡∏° LLM parameters (7 elements)
def _static_worker_process(worker_input_tuple: Tuple[Dict[str, Any], str, int, str, str, str, float]) -> Dict[str, Any]:
    """
    Static worker function for multiprocessing pool. 
    It reconstructs SeamAssessment in the new process and executes the assessment 
    for a single sub-criteria.
    
    Args:
        worker_input_tuple: (sub_criteria_data, enabler: str, target_level: int, mock_mode: str, evidence_map_path: str, model_name: str, temperature: float) 

    Returns:
        Dict[str, Any]: Final result of the sub-criteria assessment.
    """
    
    # üü¢ NEW FIX: PATH SETUP ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Worker Process
    # ‡∏ó‡∏≥‡πÉ‡∏´‡πâ Worker Process ‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å Root Directory ‡∏Ç‡∏≠‡∏á‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå ‡πÄ‡∏û‡∏∑‡πà‡∏≠ Import modules ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡πÑ‡∏î‡πâ (‡πÄ‡∏ä‡πà‡∏ô models.llm)
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
    if project_root not in sys.path:
        sys.path.append(project_root)
        
    # NOTE: logger ‡∏Ñ‡∏ß‡∏£‡∏ñ‡∏π‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏ô Worker process
    worker_logger = logging.getLogger(__name__)

    try:
        # üü¢ FIX: Unpack ‡∏Ñ‡πà‡∏≤ Primitives ‡∏ó‡∏±‡πâ‡∏á 7 ‡∏ï‡∏±‡∏ß (‡∏£‡∏ß‡∏° LLM parameters)
        sub_criteria_data, enabler, target_level, mock_mode, evidence_map_path, model_name, temperature = worker_input_tuple
    except ValueError as e:
        worker_logger.critical(f"Worker input tuple unpack failed: {e}")
        return {"error": f"Invalid worker input: {e}"}
        
    # 1. Reconstruct Config ‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤ Primitives (The Robust Fix)
    try:
        # üü¢ FIX: ‡∏™‡∏£‡πâ‡∏≤‡∏á AssessmentConfig ‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏ô Worker Process ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡πà‡∏á LLM parameters
        # (AssessmentConfig ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ field model_name ‡πÅ‡∏•‡∏∞ temperature ‡πÅ‡∏•‡πâ‡∏ß)
        worker_config = AssessmentConfig(
            enabler=enabler,
            target_level=target_level,
            mock_mode=mock_mode,
            model_name=model_name, 
            temperature=temperature
            # force_sequential ‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡πÉ‡∏ô worker
        )
    except Exception as e:
        worker_logger.critical(f"Failed to reconstruct AssessmentConfig in worker: {e}")
        # Return ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î
        return {
            "sub_criteria_id": sub_criteria_data.get('sub_id', 'UNKNOWN'),
            "error": f"Config reconstruction failed: {e}"
        }

    # 2. Re-instantiate SeamAssessment (LLM ‡πÅ‡∏•‡∏∞ VSM ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏ô Worker)
    try:
        # Worker Instance ‡∏à‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏Å _initialize_llm_if_none() ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á LLM/VSM ‡πÉ‡∏´‡∏°‡πà
        # (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤ _initialize_llm_if_none ‡∏ñ‡∏π‡∏Å‡πÅ‡∏Å‡πâ‡πÉ‡∏´‡πâ Import ‡∏à‡∏≤‡∏Å models.llm ‡πÅ‡∏•‡πâ‡∏ß)
        worker_instance = SEAMPDCAEngine(
            config=worker_config, 
            evidence_map_path=evidence_map_path, 
            llm_instance=None, # ‡πÉ‡∏´‡πâ Worker ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà
            vectorstore_manager=None, # ‡πÉ‡∏´‡πâ Worker ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà
            logger_instance=worker_logger
        )
    except Exception as e:
        worker_logger.critical(f"FATAL: SEAMPDCAEngine instantiation failed in worker: {e}")
        return {
            "sub_criteria_id": sub_criteria_data.get('sub_id', 'UNKNOWN'),
            "error": f"Engine initialization failed: {e}"
        }
    
    # 3. Execute the worker logic
    # ‡πÄ‡∏°‡∏ò‡∏≠‡∏î‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏£‡∏±‡∏ô Logic L1-L5 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Sub-Criteria ‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß
    return worker_instance._run_sub_criteria_assessment_worker(sub_criteria_data)


def merge_evidence_mappings(results_list: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
    """
    ‡∏£‡∏ß‡∏° evidence_mapping dictionaries ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å Worker ‡∏ó‡∏∏‡∏Å‡∏ï‡∏±‡∏ß 
    """
    merged_mapping = defaultdict(list)
    for result in results_list:
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å Worker ‡∏°‡∏µ Key 'evidence_mapping' ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if 'evidence_mapping' in result and isinstance(result['evidence_mapping'], dict):
            # ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏ú‡πà‡∏≤‡∏ô Key/Value ‡∏Ç‡∏≠‡∏á Worker ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡∏±‡∏ß
            for level_key, evidence_list in result['evidence_mapping'].items():
                # ‡πÉ‡∏ä‡πâ .extend() ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ú‡∏ô‡∏ß‡∏Å‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
                if isinstance(evidence_list, list):
                    merged_mapping[level_key].extend(evidence_list)
    
    # ‡πÅ‡∏õ‡∏•‡∏á defaultdict ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô dict ‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤
    return dict(merged_mapping)

# =================================================================
# Configuration Class
# =================================================================
@dataclass
class AssessmentConfig:
    """Configuration for the SEAM PDCA Assessment Run."""
    enabler: str = DEFAULT_ENABLER
    target_level: int = MAX_LEVEL
    mock_mode: str = "none" # 'none', 'random', 'control'
    force_sequential: bool = field(default=False) # Flag to force sequential ru
    # üü¢ FIX: ‡πÄ‡∏û‡∏¥‡πà‡∏° LLM Configuration Fields ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô Dataclass
    model_name: str = "llama3.1:8b" # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ default ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÉ‡∏ä‡πâ
    temperature: float = 0.0


# =================================================================
# SEAM Assessment Engine (PDCA Focused)
# =================================================================
class SEAMPDCAEngine:
    
    # üéØ Mapping for RAG Query Augmentation at Level 1 (Plan)
    ENABLER_L1_AUGMENTATION = {
        "KM": "‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢ ‡∏ß‡∏¥‡∏™‡∏±‡∏¢‡∏ó‡∏±‡∏®‡∏ô‡πå ‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå ‡πÅ‡∏ú‡∏ô‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏∏‡πà‡∏á‡∏°‡∏±‡πà‡∏ô",
        "HCM": "‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏• ‡πÅ‡∏ú‡∏ô‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡∏ô ‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏• ‡∏Å‡∏≤‡∏£‡∏°‡∏µ‡∏™‡πà‡∏ß‡∏ô‡∏£‡πà‡∏ß‡∏°‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£",
        "DT": "‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏î‡∏¥‡∏à‡∏¥‡∏ó‡∏±‡∏• ‡πÅ‡∏ú‡∏ô‡πÅ‡∏°‡πà‡∏ö‡∏ó‡∏î‡∏¥‡∏à‡∏¥‡∏ó‡∏±‡∏• ‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á IT ‡πÅ‡∏ú‡∏ô‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ",
        "SP": "‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£ ‡πÅ‡∏ú‡∏ô‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£ ‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå ‡πÅ‡∏ú‡∏ô‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå",
        "DEFAULT": "‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢ ‡∏ß‡∏¥‡∏™‡∏±‡∏¢‡∏ó‡∏±‡∏®‡∏ô‡πå ‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå ‡πÅ‡∏ú‡∏ô‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏∏‡πà‡∏á‡∏°‡∏±‡πà‡∏ô" 
    }
    
    L1_INITIAL_TOP_K_RAG: int = 50 
    
    def __init__(
        self, 
        config: 'AssessmentConfig',
        llm_instance: Any = None, 
        logger_instance: logging.Logger = None,
        rag_retriever_instance: Any = None,
        # üü¢ FIX #1: ‡πÄ‡∏û‡∏¥‡πà‡∏° doc_type 
        doc_type: str = EVIDENCE_DOC_TYPES, 
        # üü¢ FIX #2: ‡πÄ‡∏û‡∏¥‡πà‡∏° vectorstore_manager
        vectorstore_manager: Optional['VectorStoreManager'] = None,
        # üìå FIX #3 (‡πÉ‡∏´‡∏°‡πà): ‡πÄ‡∏û‡∏¥‡πà‡∏° evidence_map_path ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å Worker Process
        evidence_map_path: Optional[str] = None 
    ):

            self.config = config
            self.enabler_id = config.enabler
            self.target_level = config.target_level
            self.rubric = self._load_rubric()
            
            # üü¢ ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤ VSM ‡πÅ‡∏•‡∏∞ doc_type
            self.vectorstore_manager = vectorstore_manager
            self.doc_type = doc_type

            self.FINAL_K_RERANKED = FINAL_K_RERANKED
            self.PRIORITY_CHUNK_LIMIT = PRIORITY_CHUNK_LIMIT

            # üü¢ ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤ LLM ‡πÅ‡∏•‡∏∞ Logger
            self.llm = llm_instance           
            self.logger = logger_instance if logger_instance is not None else logging.getLogger(__name__)

            # üü¢ Disable Strict Filter
            self.initial_evidence_ids: Set[str] = self._load_initial_evidence_info()
            all_statements = self._flatten_rubric_to_statements()
            initial_count = len(all_statements)

            self.logger.info(f"DEBUG: Statements found: {initial_count}. Strict Filter is **DISABLED**.")

            self.statements_to_assess = all_statements
            self.logger.info(f"DEBUG: Statements selected for assessment: {len(self.statements_to_assess)} (Skipped: {initial_count - len(self.statements_to_assess)})")

            # Assessment results storage
            self.raw_llm_results: List[Dict[str, Any]] = []
            self.final_subcriteria_results: List[Dict[str, Any]] = []
            self.total_stats: Dict[str, Any] = {}

            self.is_sequential = False  

            self.retry_policy = RetryPolicy(
                max_attempts=3,            
                base_delay=2.0,            
                jitter=True,               
                escalate_context=True,     
                shorten_prompt_on_fail=True,  
                exponential_backoff=True,  
            )

            # üìå Persistent Mapping Configuration
            
            # 1. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Evidence Map Path
            # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏°‡∏≤‡∏à‡∏≤‡∏Å Worker (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ) ‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤ Default
            if evidence_map_path:
                self.evidence_map_path = evidence_map_path
            else:
                map_filename = f"{self.enabler_id.lower()}{EVIDENCE_MAPPING_FILENAME_SUFFIX}"
                # üîπ ‡πÉ‡∏ä‡πâ Absolute Path ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤ CWD ‡πÑ‡∏°‡πà‡∏Ñ‡∏á‡∏ó‡∏µ‡πà
                self.evidence_map_path = os.path.join(PROJECT_ROOT, RUBRIC_CONFIG_DIR, map_filename)

            
            # 2. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Attribute ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Persistent Mapping
            self.evidence_map: Dict[str, List[str]] = {}
            self.temp_map_for_save: Dict[str, List[str]] = {}

            self.contextual_rules_map: Dict[str, Dict[str, str]] = self._load_contextual_rules_map()
            
            # 3. ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà 
            self.evidence_map = self._load_evidence_map()

            self.logger.info(f"Persistent Map Path set to: {self.evidence_map_path}")
            self.logger.info(f"Loaded {len(self.evidence_map)} existing evidence entries into self.evidence_map.")
            
            # Mock function pointers (will point to real functions by default)
            self.llm_evaluator = evaluate_with_llm
            self.rag_retriever = retrieve_context_with_filter
            self.action_plan_generator = create_structured_action_plan

            # Apply mocking if enabled
            if config.mock_mode in ["random", "control"]:
                self._set_mock_handlers(config.mock_mode)

            # Set global mock control mode for llm_data_utils if using 'control'
            if config.mock_mode == "control":
                logger.info("Enabling global LLM data utils mock control mode.")
                set_llm_data_mock_mode(True)
            elif config.mock_mode == "random":
                logger.warning("Mock mode 'random' is not fully implemented. Using 'control' logic if available.")
                if hasattr(seam_mocking, 'set_mock_control_mode'):
                    seam_mocking.set_mock_control_mode(False)
                    set_llm_data_mock_mode(False)

            # üìå ‡πÇ‡∏´‡∏•‡∏î LLM ‡πÅ‡∏•‡∏∞ VSM ‡∏´‡∏≤‡∏Å‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ
            if self.llm is None: self._initialize_llm_if_none()
            if self.vectorstore_manager is None: self._initialize_vsm_if_none()
            
            self.logger.info(f"Engine initialized for Enabler: {self.enabler_id}, Mock Mode: {config.mock_mode}")

    def _initialize_llm_if_none(self):
        """Initializes LLM instance if self.llm is None."""
        if self.llm is None:
            self.logger.warning("‚ö†Ô∏è Initializing LLM: model=%s, temperature=%s", 
                                self.config.model_name, self.config.temperature)
            try:
                # üü¢ FIX: Import ‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ create_llm_instance
                from models.llm import create_llm_instance 
                self.llm = create_llm_instance( 
                    model_name=self.config.model_name,
                    temperature=self.config.temperature
                )
                self.logger.info("‚úÖ LLM Instance created successfully: %s (Temp: %s)", 
                                 self.config.model_name, self.config.temperature)
            except Exception as e:
                self.logger.error(f"FATAL: Could not initialize LLM: {e}")
                raise


    def _initialize_vsm_if_none(self):
        """Initializes VectorStoreManager if self.vectorstore_manager is None."""
        if self.vectorstore_manager is None:
            self.logger.info("Loading central evidence vectorstore(s)...")
            try:
                self.vectorstore_manager = load_all_vectorstores(
                    doc_types=[EVIDENCE_DOC_TYPES], 
                    evidence_enabler=self.enabler_id
                )
                
                # üìå FINAL FIX: ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á MultiDocRetriever (Private Attribute) 
                # ‡πÅ‡∏•‡∏∞‡∏ï‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ _all_retrievers (Private Attribute)
                len_retrievers = len(
                    self.vectorstore_manager._multi_doc_retriever._all_retrievers
                )
                
                self.logger.info("‚úÖ MultiDocRetriever loaded with %s collections and cached in VSM.", 
                                 len_retrievers) 
            except Exception as e:
                self.logger.error(f"FATAL: Could not initialize VectorStoreManager: {e}")
                raise
        
    # -------------------- Contextual Rules Handlers (NEW) --------------------
    def _load_contextual_rules_map(self) -> Dict[str, Dict[str, str]]:
        """Loads the Sub-Criteria Contextual Rules (Layer 2) map."""
        map_filename = f"{self.enabler_id.lower()}_contextual_rules.json"
        # üìå NOTE: ‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£ global PROJECT_ROOT ‡πÅ‡∏•‡∏∞ RUBRIC_CONFIG_DIR
        filepath = os.path.join(PROJECT_ROOT, RUBRIC_CONFIG_DIR, map_filename)
        
        if not os.path.exists(filepath):
            logger.warning(f"‚ö†Ô∏è Contextual Rules map not found at {filepath}. Using empty map.")
            return {}

        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
                logger.info(f"‚úÖ Loaded Contextual Rules map from {filepath}. ({len(data)} sub-criteria rules)")
                # Map Structure: {'2.2': {'L1': 'Rule Text for L1', 'L3': 'Rule Text for L3'}, ...}
                return data
        except Exception as e:
            logger.error(f"‚ùå Failed to load Contextual Rules map. Error: {e}")
            return {}


    def _collect_previous_level_evidences(self, sub_id: str, current_level: int, vectorstore_manager: Optional['VectorStoreManager'] = None) -> Dict[str, List[Dict]]:
        """
        ‡∏î‡∏∂‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô (Metadata/DocIDs) ‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏à‡∏≤‡∏Å Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î 
        ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏≥‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô Context (Baseline) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Level ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
        
        ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ Hydration (‡∏î‡∏∂‡∏á Text) ‡∏à‡∏≤‡∏Å VSM ‡πÉ‡∏ô‡πÇ‡∏´‡∏°‡∏î Parallel (‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏à‡∏≤‡∏Å Main Process ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Context)
        
        Args:
            sub_id (str): Sub-Criteria ID ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô (‡πÄ‡∏ä‡πà‡∏ô "3.1").
            current_level (int): Level ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô (‡πÄ‡∏ä‡πà‡∏ô L2).
            vectorstore_manager (Optional['VectorStoreManager']): VSM ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏∂‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ (Text) 
                                                                 ‡πÉ‡∏ô‡πÇ‡∏´‡∏°‡∏î Parallel/Worker (‡∏ñ‡πâ‡∏≤ Main Process ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Context)
        Returns:
            Dict[str, List[Dict]]: Dictionary ‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡πÑ‡∏î‡πâ, ‡πÇ‡∏î‡∏¢ Key ‡∏Ñ‡∏∑‡∏≠ "sub_id.L#".
                                   ‡∏≠‡∏≤‡∏à‡∏°‡∏µ Text ‡∏£‡∏ß‡∏°‡∏≠‡∏¢‡∏π‡πà‡∏î‡πâ‡∏ß‡∏¢‡∏´‡∏≤‡∏Å‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ Hydration.
        """
        collected = {}

        # 1. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å
        # ‡πÉ‡∏ô Sequential Mode ‡∏´‡∏£‡∏∑‡∏≠ Parallel Main Process ‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ self.evidence_map 
        # (‡∏ã‡∏∂‡πà‡∏á‡πÉ‡∏ô Sequential ‡∏°‡∏±‡∏ô‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÑ‡∏õ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏¢‡πÜ)
        source_map = self.evidence_map
        source_name = "evidence_map (SEQ/PAR Main)"

        # 2. ‡∏Å‡∏£‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏° Metadata (DocID/Filename)
        for key, evidence_list in source_map.items():
            # ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Key ‡∏ó‡∏µ‡πà‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ sub_id ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô Level
            if key.startswith(f"{sub_id}.L") and isinstance(evidence_list, list):
                try:
                    level_num = int(key.split(".L")[-1])
                    
                    # <<< ‡∏à‡∏∏‡∏î‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                    if level_num < current_level: 
                        collected[key] = evidence_list 
                except:
                    continue
        
        # 3. HYDRATION (‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á Text): ‡πÉ‡∏ô‡πÇ‡∏´‡∏°‡∏î Parallel (is_sequential=False) 
        #    ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á vectorstore_manager ‡∏°‡∏≤ (‡∏´‡∏°‡∏≤‡∏¢‡∏ñ‡∏∂‡∏á Main Process ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Context) 
        #    ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏î‡∏∂‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏ï‡πá‡∏°‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á‡πÑ‡∏õ‡πÉ‡∏´‡πâ Worker ‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô Baseline Context
        
        is_worker_context_preparation = not self.is_sequential and vectorstore_manager is not None

        if is_worker_context_preparation and collected and vectorstore_manager:
            all_uuids = [ev['doc_id'] for ev_list in collected.values() for ev in ev_list if 'doc_id' in ev]
            
            try:
                # ‡∏î‡∏∂‡∏á Chunks ‡πÄ‡∏ï‡πá‡∏°‡πÜ ‡∏à‡∏≤‡∏Å VSM
                full_chunks = vectorstore_manager.retrieve_chunks_by_uuids(all_uuids) 
            except Exception as e:
                self.logger.error(f"Failed to retrieve full chunks for baseline in PAR mode: {e}")
                full_chunks = []

            full_chunk_map = {c.get('doc_id') or c.get('chunk_uuid'): c for c in full_chunks}
            
            hydrated_collected = {}
            for key, ev_list in collected.items():
                hydrated_list = []
                for ev_metadata in ev_list:
                    doc_id = ev_metadata.get('doc_id')
                    full_chunk = full_chunk_map.get(doc_id)
                    
                    if full_chunk and full_chunk.get('text'):
                        # ‡∏£‡∏ß‡∏° Metadata ‡πÄ‡∏î‡∏¥‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ö Text ‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏°‡∏≤
                        chunk_to_add = full_chunk.copy()
                        chunk_to_add.update(ev_metadata) 
                        hydrated_list.append(chunk_to_add)
                
                if hydrated_list:
                    hydrated_collected[key] = hydrated_list
            
            collected = hydrated_collected # ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ Text ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡πá‡∏ô Baseline Context
        
        # 4. Debug Log
        total_files = sum(len(v) for v in collected.values())
        self.logger.info(
            f"BASELINE LOADED ‚Üí Mode: {'SEQ' if self.is_sequential else 'PAR'} | "
            f"Source: {source_name} | "
            f"Found {len(collected)} levels | "
            f"Keys: {sorted(collected.keys())} | "
            f"Total files: {total_files}"
        )

        return collected

    def _get_contextual_rules_prompt(self, sub_id: str, level: int) -> str:
        """
        Retrieves the specific Contextual Rule prompt for a given Sub-Criteria and Level.
        """
        sub_id_rules = self.contextual_rules_map.get(sub_id)
        if sub_id_rules:
            level_key = f"L{level}"
            rule_text = sub_id_rules.get(level_key)
            if rule_text:
                # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏™‡πà‡πÉ‡∏ô Prompt
                return f"\n--- ‡∏Å‡∏é‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏¢‡πà‡∏≠‡∏¢ ({sub_id} L{level}) ---\n‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Ñ‡∏ß‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ: {rule_text}\n"
        return ""

    def _load_rubric(self) -> List[Dict[str, Any]]:
        """Loads the SE-AM Rubric JSON file."""
        filename = RUBRIC_FILENAME_PATTERN.format(enabler=self.enabler_id.lower())
        filepath = os.path.join(PROJECT_ROOT, RUBRIC_CONFIG_DIR, filename) 
        
        if not os.path.exists(filepath):
            logger.error(f"Rubric file not found for {self.enabler_id}: {filepath}")
            if self.config.mock_mode != "none":
                logger.warning("Using minimal mock rubric for testing.")
                return [{
                    "sub_id": "1.1", "name": "Mock Sub-Criteria 1.1", "weight": 4, 
                    "levels": [{"level": 1, "statement": "Mock L1 statement"}, {"level": 2, "statement": "Mock L2 statement"}]
                }]
            raise FileNotFoundError(f"Rubric not found at {filepath}")

        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
            
            # --- FIX: Transform Dictionary Root to List of Sub-Criteria ---
            if isinstance(data, dict):
                logger.info("Rubric file detected as Dictionary root. Extracting Sub-Criteria list.")
                extracted_list = []
                for criteria_id, criteria_data in data.items():
                    sub_criteria_map = criteria_data.get('subcriteria', {})
                    criteria_name = criteria_data.get('name')
                    
                    for sub_id, sub_data in sub_criteria_map.items():
                        sub_data['criteria_id'] = criteria_id
                        sub_data['criteria_name'] = criteria_name
                        sub_data['sub_id'] = sub_id 
                        sub_data['sub_criteria_name'] = sub_data.get('name', criteria_name + ' sub') 
                        
                        if 'weight' not in sub_data:
                            sub_data['weight'] = criteria_data.get('weight', 0)
                        
                        extracted_list.append(sub_data)
                data = extracted_list

            if not isinstance(data, list):
                raise ValueError(f"Rubric file {filepath} has invalid root structure (expected list after transformation).")

            # Check for missing levels and sort, and transform levels dict to list
            for sub_criteria in data:
                if "levels" in sub_criteria and isinstance(sub_criteria["levels"], dict):
                    levels_list = []
                    for level_str, statement in sub_criteria["levels"].items():
                        levels_list.append({"level": int(level_str), "statement": statement})
                    sub_criteria["levels"] = levels_list
                
                if "levels" in sub_criteria and isinstance(sub_criteria["levels"], list):
                    sub_criteria["levels"].sort(key=lambda x: x.get("level", 0))
            
            return data
    
    # -------------------- Helper Function for Map Processing --------------------
    def _get_level_order_value(self, level_str: str) -> int:
        """Converts Level string ('L1', 'L5') to an integer for comparison."""
        try:
            return int(level_str.upper().replace('L', ''))
        except:
            return 0

    # -------------------- Persistent Mapping Handlers (FIXED) --------------------
    def _process_temp_map_to_final_map(self, temp_map: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Converts the temporary map into the final map format for saving, 
        and filters out temporary/unresolvable evidence IDs.
        """
        working_map = temp_map or self.temp_map_for_save or {}
        final_map_for_save = {}
        total_cleaned_items = 0

        for sub_level_key, evidence_list in working_map.items():
            if isinstance(evidence_list, dict):
                evidence_list = [evidence_list]
            elif not isinstance(evidence_list, list):
                logger.warning(f"[EVIDENCE] Skipping {sub_level_key}: not a list or dict")
                continue

            clean_list = []
            seen_ids = set()
            for ev in evidence_list:
                doc_id = ev.get("doc_id")
                
                if not doc_id:
                    continue
                
                # 1. FIX: ‡∏Å‡∏£‡∏≠‡∏á ID ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß (TEMP-) ‡∏≠‡∏≠‡∏Å
                if doc_id.startswith("TEMP-"):
                    # ID ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏õ‡∏•‡∏á‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô Stable Document ID ‡πÑ‡∏î‡πâ ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÉ‡∏ä‡πâ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ
                    logger.debug(f"[EVIDENCE] Filtering out unresolvable TEMP- ID: {doc_id} for {sub_level_key}.")
                    continue 
                
                # 2. Logic ‡πÄ‡∏î‡∏¥‡∏°: ‡∏Å‡∏£‡∏≠‡∏á HASH- (Placeholder) ‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ã‡πâ‡∏≥
                if doc_id.startswith("HASH-") or doc_id in seen_ids:
                    continue
                    
                seen_ids.add(doc_id)
                clean_list.append(ev)
                total_cleaned_items += 1 

            if clean_list:
                final_map_for_save[sub_level_key] = clean_list

        logger.info(f"[EVIDENCE] Processed {len(final_map_for_save)} sub-level keys with total {total_cleaned_items} evidence items")
        return final_map_for_save

    def _clean_map_for_json(self, data: Union[Dict, List, Set, Any]) -> Union[Dict, List, Any]:
        """Recursively converts objects that cannot be serialized (like sets) into lists."""
        if isinstance(data, dict):
            return {k: self._clean_map_for_json(v) for k, v in data.items()}
        elif isinstance(data, list):
            return [self._clean_map_for_json(v) for v in data]
        elif isinstance(data, set):
            return [self._clean_map_for_json(v) for v in data]
        return data

    def _clean_temp_entries(self, evidence_map: Dict[str, List[Dict]]) -> Dict[str, List[Dict]]:
        """
        ‡∏Å‡∏£‡∏≠‡∏á TEMP-, HASH-, ‡πÅ‡∏•‡∏∞ Unknown ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å evidence map ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        ‡πÉ‡∏ä‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏ï‡∏≠‡∏ô merge ‡πÅ‡∏•‡∏∞‡∏Å‡πà‡∏≠‡∏ô save ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î 100%
        """
        if not evidence_map:
            return {}

        cleaned_map = {}
        total_removed = 0
        total_unknown_fixed = 0

        for key, entries in evidence_map.items():
            valid_entries = []
            for entry in entries:
                doc_id = entry.get("doc_id", "")

                # 1. ‡∏Å‡∏£‡∏≠‡∏á TEMP- ‡πÅ‡∏•‡∏∞ HASH- ‡∏≠‡∏≠‡∏Å‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≤‡∏î
                if str(doc_id).startswith("TEMP-") or str(doc_id).startswith("HASH-"):
                    total_removed += 1
                    continue

                # 2. ‡∏ñ‡πâ‡∏≤ doc_id ‡∏ß‡πà‡∏≤‡∏á ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏•‡∏¢ ‚Üí ‡∏ó‡∏¥‡πâ‡∏á
                if not doc_id or doc_id == "Unknown":
                    total_removed += 1
                    continue

                # 3. ‡πÅ‡∏Å‡πâ filename ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô Unknown / None / ‡∏ß‡πà‡∏≤‡∏á
                filename = entry.get("filename", "").strip()
                if not filename or filename == "Unknown" or filename.lower() == "unknown_file.pdf":
                    # ‡πÉ‡∏ä‡πâ doc_id ‡∏™‡∏±‡πâ‡∏ô ‡πÜ ‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß (‡∏î‡∏π‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤)
                    short_id = doc_id[:8]
                    entry["filename"] = f"‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á_{short_id}.pdf"
                    total_unknown_fixed += 1
                else:
                    # ‡πÄ‡∏≠‡∏≤ path ‡∏≠‡∏≠‡∏Å ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÅ‡∏Ñ‡πà‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå
                    entry["filename"] = os.path.basename(filename)

                valid_entries.append(entry)

            if valid_entries:
                cleaned_map[key] = valid_entries
            else:
                logger.debug(f"[CLEAN] Key {key} ‡∏Å‡∏•‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏ß‡πà‡∏≤‡∏á‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏£‡∏≠‡∏á ‚Üí ‡∏ñ‡∏π‡∏Å‡∏•‡∏ö‡∏≠‡∏≠‡∏Å")

        logger.info(f"[CLEANUP] ‡∏•‡∏ö TEMP-/HASH- ‡∏≠‡∏≠‡∏Å {total_removed} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | "
                    f"‡πÅ‡∏Å‡πâ Unknown filename {total_unknown_fixed} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | "
                    f"‡πÄ‡∏´‡∏•‡∏∑‡∏≠ {len(cleaned_map)} keys")

        return cleaned_map


    def _save_evidence_map(self, map_to_save: Optional[Dict[str, List[Dict[str, Any]]]] = None):
        """
        Saves the evidence map to a persistent JSON file using atomic write + FileLock.
        """
        map_file_path = self.evidence_map_path
        lock_path = map_file_path + ".lock"
        tmp_path = None

        logger.info(f"[EVIDENCE] Evidence map target path: {map_file_path}")

        try:
            # 1. ‡πÉ‡∏ä‡πâ FileLock ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô
            logger.debug(f"[EVIDENCE] Acquiring file lock: {lock_path}")
            with FileLock(lock_path, timeout=60):
                logger.debug("[EVIDENCE] Lock acquired. Proceeding with save...")

                # === ‡πÄ‡∏£‡∏¥‡πà‡∏° FIX LOGIC MERGE & FILTER ===
                if map_to_save is not None:
                    # ‡∏Å‡∏£‡∏ì‡∏µ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á map ‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á (‡πÉ‡∏ä‡πâ‡∏Å‡∏£‡∏ì‡∏µ‡∏û‡∏¥‡πÄ‡∏®‡∏©)
                    final_map_to_write = map_to_save
                    logger.debug("[EVIDENCE] Using passed map_to_save. Skipping deep merge/filter logic.")
                else:
                    # 1. ‡πÇ‡∏´‡∏•‡∏î Map ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÄ‡∏î‡∏¥‡∏°‡∏à‡∏≤‡∏Å Disk (‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠ 3.1.L1-L5)
                    existing_map_from_disk = self._load_evidence_map(is_for_merge=True) or {}
                    
                    # 2. Map ‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏á‡∏£‡∏±‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡πÉ‡∏ô‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥ (Worker Process ‡πÄ‡∏û‡∏¥‡πà‡∏á‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï 3.1.L1-L5)
                    map_from_runtime = deepcopy(self.evidence_map)
                    
                    # 3. [FIXED] ‡∏ú‡∏™‡∏≤‡∏ô (Merge): ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Map ‡πÄ‡∏Å‡πà‡∏≤ ‡πÅ‡∏•‡∏∞‡∏£‡∏ß‡∏° Map ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ
                    # final_map_to_write ‡∏à‡∏∞‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (3.1 + 1.1)
                    final_map_to_write = existing_map_from_disk
                    final_map_to_write.update(map_from_runtime) # üëà ‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏´‡∏•‡∏±‡∏Å: ‡∏£‡∏ß‡∏° Map ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î

                    # 4. [FIXED] ‡∏Å‡∏£‡∏≠‡∏á TEMP- ID ‡∏à‡∏≤‡∏Å Map ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏£‡∏ß‡∏°‡πÅ‡∏•‡πâ‡∏ß
                    final_map_to_write = self._process_temp_map_to_final_map(final_map_to_write)
                    # ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å merge ‡πÄ‡∏™‡∏£‡πá‡∏à
                    final_map_to_write = self._clean_temp_entries(final_map_to_write)
                    
                    logger.debug(f"[DEBUG] Final Map keys count: {len(final_map_to_write.keys())}") # üëà Log ‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô
                # === ‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î FIX LOGIC MERGE & FILTER ===
                
                if not final_map_to_write:
                    logger.warning("[EVIDENCE] final_map_to_write is empty. Skipping save.")
                    return

                # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° directory
                target_dir = os.path.dirname(map_file_path)
                os.makedirs(target_dir, exist_ok=True)

                # ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß‡∏Å‡πà‡∏≠‡∏ô (Atomic Write)
                with tempfile.NamedTemporaryFile(
                    mode='w', delete=False, encoding="utf-8", dir=target_dir
                ) as tmp_file:
                    map_to_write_cleaned = self._clean_map_for_json(final_map_to_write)
                    json.dump(map_to_write_cleaned, tmp_file, indent=4, ensure_ascii=False)
                    tmp_path = tmp_file.name

                # ‡∏¢‡πâ‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏£‡∏¥‡∏á (atomic)
                shutil.move(tmp_path, map_file_path)
                tmp_path = None

                logger.info(f"[EVIDENCE] Evidence map saved successfully to: {map_file_path}")
                logger.info(f"[DEBUG] Final file size: {os.path.getsize(map_file_path)} bytes")
                
                items_count = sum(len(v) for v in final_map_to_write.values())
                logger.info(f"Persisted final evidence map | Keys: {len(final_map_to_write.keys())} | Items: {items_count} | Size: ~{(os.path.getsize(map_file_path) / 1024):.1f} KB")

        except TimeoutError:
            logger.critical(f"[EVIDENCE] Could not acquire lock within 60s: {lock_path}")
            logger.critical("[EVIDENCE] Another process is holding the lock ‚Äî possible stuck process!")
            raise
        except Exception as e:
            logger.critical("FATAL FILE WRITE ERROR - CHECK LOG TRACE")
            logger.exception(f"[EVIDENCE] Failed to save map: {e}")
            raise
        finally:
            if tmp_path and os.path.exists(tmp_path):
                try:
                    os.unlink(tmp_path)
                    logger.debug(f"[EVIDENCE] Cleaned up temp file: {tmp_path}")
                except Exception:
                    pass
            logger.debug(f"[EVIDENCE] File lock released: {lock_path}")

    def _load_evidence_map(self, is_for_merge: bool = False):
        """
        Safe load of persistent evidence map. Always returns dict.
        is_for_merge: If True, suppresses "No existing evidence map" INFO log.
        """
        path = self.evidence_map_path

        if not os.path.exists(path):
            if not is_for_merge:
                self.logger.info("[EVIDENCE] No existing evidence map ‚Äì starting empty.")
            return {}

        try:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
            if not is_for_merge:
                self.logger.info(f"[EVIDENCE] Loaded evidence map: {len(data)} entries")
            return data

        except Exception as e:
            self.logger.error(f"[EVIDENCE] Failed to load evidence map from {path}: {e}")
            return {}

    def _set_mock_handlers(self, mode: str):
        """Replaces real LLM/RAG functions with mock versions."""
        if mode == "control" or mode == "random":
            if hasattr(seam_mocking, 'evaluate_with_llm_CONTROLLED_MOCK'):
                self.llm_evaluator = seam_mocking.evaluate_with_llm_CONTROLLED_MOCK
            if hasattr(seam_mocking, 'retrieve_context_with_filter_MOCK'):
                self.rag_retriever = seam_mocking.retrieve_context_with_filter_MOCK
            if hasattr(seam_mocking, 'create_structured_action_plan_MOCK'):
                self.action_plan_generator = seam_mocking.create_structured_action_plan_MOCK
            if hasattr(seam_mocking, 'set_mock_control_mode'):
                seam_mocking.set_mock_control_mode(mode == "control") 

        logger.warning(f"Engine is running in MOCK mode: {mode}")

    def _get_pdca_phase(self, level: int) -> str:
        """Helper to get the PDCA phase string from the map."""
        return PDCA_PHASE_MAP.get(level, f"Level {level} Requirement")
    

    def _get_level_constraint_prompt(self, level: int) -> str:
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á Prompt Constraint ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Å‡∏±‡∏ö‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ß‡∏∏‡∏í‡∏¥‡∏†‡∏≤‡∏ß‡∏∞‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô
        """
        if level == 1:
            return "‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏ñ‡∏∂‡∏á '‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢/‡∏ß‡∏¥‡∏™‡∏±‡∏¢‡∏ó‡∏±‡∏®‡∏ô‡πå', '‡∏Å‡∏≤‡∏£‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå', '‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏ó‡∏≥‡∏Å‡∏£‡∏≠‡∏ö‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á', ‡∏´‡∏£‡∏∑‡∏≠ '‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£' ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (L1-Focus)"
        elif level == 2:
            return "‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ô‡πâ‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞ '‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô', '‡∏Å‡∏≤‡∏£‡∏Ç‡∏±‡∏ö‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô', '‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡∏ò‡∏£‡∏£‡∏°', ‡∏´‡∏£‡∏∑‡∏≠ '‡∏Å‡∏≤‡∏£‡∏°‡∏µ‡∏™‡πà‡∏ß‡∏ô‡∏£‡πà‡∏ß‡∏°' ‡∏ï‡∏≤‡∏°‡πÅ‡∏ú‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (L2-Focus)"
        elif level == 3:
            # üö® HARD RULE: ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏ä‡πâ L3 Logic (Check/Act Focus) ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏° Context ‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡∏°‡πà
            return """
‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î (HARD RULE: L3 CHECK/ACT FOCUS):
1. ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô L3 ‡∏ô‡∏µ‡πâ **‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô '‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö (Check)' ‡πÅ‡∏•‡∏∞ '‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á (Act)' ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô**
2. ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÅ‡∏•‡πâ‡∏ß: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏ï‡πâ‡∏ô‡∏Ç‡∏≠‡∏á Context ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î (Priority 1)
3. ‡∏´‡∏≤‡∏Å‡∏û‡∏ö‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ **[SIMULATED_L3_EVIDENCE]** ‡πÉ‡∏´‡πâ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô **‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö** ‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠‡πÑ‡∏î‡πâ‡∏ã‡∏∂‡πà‡∏á‡∏ñ‡∏π‡∏Å‡∏™‡∏£‡∏∏‡∏õ‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô Check/Act ‡∏à‡∏£‡∏¥‡∏á (‡∏à‡∏±‡∏î‡πÄ‡∏õ‡πá‡∏ô Priority 1)
4. ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô Plan ‡πÅ‡∏•‡∏∞ Do ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏ï‡∏≠‡∏ô‡∏ó‡πâ‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á Context **‡∏´‡πâ‡∏≤‡∏°‡∏ô‡∏≥‡∏°‡∏≤‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤** ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à **FAIL** ‡∏´‡∏≤‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô Check/Act ‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô
5. ‡∏´‡∏≤‡∏Å‡∏Ç‡∏≤‡∏î‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô **Check** ‡∏´‡∏£‡∏∑‡∏≠ **Act** ‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠ (‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏à‡∏≤‡∏Å Simulated Evidence ‡∏´‡∏£‡∏∑‡∏≠‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á) ‡πÉ‡∏´‡πâ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÄ‡∏õ‡πá‡∏ô **‚ùå FAIL** ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô L3 ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡∏à‡∏£‡∏¥‡∏á
(L3-Focus: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏° ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏• ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á)
"""
        elif level == 4:
            return "‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ñ‡∏∂‡∏á '‡∏Å‡∏≤‡∏£‡∏ö‡∏π‡∏£‡∏ì‡∏≤‡∏Å‡∏≤‡∏£', '‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á', ‡∏´‡∏£‡∏∑‡∏≠ '‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå' ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (L4-Focus)"
        elif level == 5:
            return "‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ñ‡∏∂‡∏á '‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°', '‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡πà‡∏≤‡∏ó‡∏≤‡∏á‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à', ‡∏´‡∏£‡∏∑‡∏≠ '‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß' ‡πÇ‡∏î‡∏¢‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô (L5-Focus)"
        else:
            return ""
        

    def _classify_pdca_phase_for_chunk(
        self, 
        chunk_text: str
    ) -> Literal["Plan", "Do", "Check", "Act", "Other"]:
        """
        ‡πÉ‡∏ä‡πâ LLM ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏∞‡∏¢‡∏∞‡πÉ‡∏î‡∏£‡∏∞‡∏¢‡∏∞‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏Ç‡∏≠‡∏á PDCA ‡∏´‡∏£‡∏∑‡∏≠ 'Other'
        """
        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏ü‡∏™ PDCA
        pdca_phases_th = ["‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô", "‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥", "‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö", "‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á"]
        
        # 1. System Prompt ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢: ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö JSON 100%
        system_prompt = (
            "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô PDCA Cycle\n"
            "‡∏†‡∏≤‡∏£‡∏Å‡∏¥‡∏à‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô ‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏±‡∏î‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ß‡πà‡∏≤‡πÄ‡∏ô‡πâ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡πÉ‡∏î‡∏Ç‡∏≠‡∏á PDCA\n"
            f"‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏´‡∏ô‡∏∂‡πà‡∏á‡πÉ‡∏ô: {', '.join(pdca_phases_th)} ‡∏´‡∏£‡∏∑‡∏≠ '‡∏≠‡∏∑‡πà‡∏ô‡πÜ'\n\n"
            "‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡∏î‡πâ‡∏ß‡∏¢ **JSON Object ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô** ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö:\n"
            "{\"phase\": \"‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô\"}\n"
            "‡∏´‡∏£‡∏∑‡∏≠ {\"phase\": \"‡∏≠‡∏∑‡πà‡∏ô‡πÜ\"}\n"
            "‡∏´‡πâ‡∏≤‡∏°‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏≠‡∏Å JSON ‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≤‡∏î"
        )

        # 2. User Prompt: ‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô + ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
        user_prompt = (
            f"‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô:\n\"\"\"\n{chunk_text.strip()}\n\"\"\"\n\n"
            "‡∏Ñ‡∏≥‡∏ô‡∏¥‡∏¢‡∏≤‡∏°‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏ü‡∏™:\n"
            "- ‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô: ‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢, ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå, ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢, ‡πÅ‡∏ú‡∏ô‡∏á‡∏≤‡∏ô, ‡∏Ñ‡∏ì‡∏∞‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£\n"
            "- ‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥: ‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£, ‡∏ù‡∏∂‡∏Å‡∏≠‡∏ö‡∏£‡∏°, ‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£, ‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏£‡∏∞‡∏ö‡∏ö, ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á\n"
            "- ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö: ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏ú‡∏•, ‡∏ß‡∏±‡∏î‡∏ú‡∏•, ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô, ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏†‡∏≤‡∏¢‡πÉ‡∏ô, ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n"
            "- ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á: ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç, ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á, ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡πÉ‡∏´‡∏°‡πà, Lesson Learned, ‡∏õ‡∏¥‡∏î‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á\n\n"
            "‡∏ï‡∏≠‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞ JSON:"
        )
        
        try:
            raw_response = _fetch_llm_response(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                temperature=0.0,  # ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å! ‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô
                max_retries=2,
                llm_executor=self.llm
            )

            if not raw_response:
                return "Other"

            # ‡∏î‡∏∂‡∏á JSON ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏Ç‡πá‡∏á‡πÅ‡∏Å‡∏£‡πà‡∏á (‡πÉ‡∏ä‡πâ _robust_extract_json ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß!)
            parsed = _robust_extract_json(raw_response)
            
            # ‡∏ñ‡πâ‡∏≤ _robust_extract_json ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ fallback ‡∏î‡πâ‡∏ß‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ‡πÄ‡∏ö‡∏™‡∏¥‡∏Å
            if not parsed or not isinstance(parsed, dict):
                # ‡∏•‡∏≠‡∏á‡∏î‡∏∂‡∏á‡∏î‡πâ‡∏ß‡∏¢ regex ‡∏á‡πà‡∏≤‡∏¢ ‡πÜ
                import re
                match = re.search(r'"phase"\s*:\s*"([^"]+)"', raw_response, re.IGNORECASE)
                if match:
                    phase_th = match.group(1).strip()
                else:
                    phase_th = "‡∏≠‡∏∑‡πà‡∏ô‡πÜ"
            else:
                phase_th = parsed.get("phase", parsed.get("classification", "‡∏≠‡∏∑‡πà‡∏ô‡πÜ"))
                phase_th = str(phase_th).strip()

            # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô Literal ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
            mapping = {
                "‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô": "Plan",
                "‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥": "Do",
                "‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö": "Check",
                "‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á": "Act",
                "‡∏≠‡∏∑‡πà‡∏ô‡πÜ": "Other",
                "‡∏≠‡∏∑‡πà‡∏ô": "Other",
                "other": "Other"
            }
            result = mapping.get(phase_th, "Other")
            
            self.logger.debug(f"PDCA Classification: '{phase_th}' ‚Üí {result}")
            return result

        except Exception as e:
            self.logger.error(f"PDCA Classification failed: {e}\nRaw: {raw_response[:200]}")
            return "Other"

    # -------------------- Statement Preparation & Filtering Helpers --------------------
    def _flatten_rubric_to_statements(self) -> List[Dict[str, Any]]:
        """Extracts a flat list of all individual level statements from the rubric."""
        flat_list = []
        for sub_criteria in self.rubric:
            sub_id = sub_criteria['sub_id']
            sub_criteria_name = sub_criteria['sub_criteria_name']
            
            for statement_data in sub_criteria.get('levels', []):
                flat_list.append({
                    "sub_id": sub_id,
                    "sub_criteria_name": sub_criteria_name,
                    "level": statement_data['level'],
                    "statement": statement_data['statement'],
                    "evidence_doc_ids": statement_data.get('evidence_doc_ids', []) 
                })
        return flat_list

    def _load_initial_evidence_info(self) -> Set[str]:
        """Retrieves the set of all available stable document IDs in the VectorStore."""
        if self.config.mock_mode != "none":
            return {"00000000-0000-0000-0000-000000000001"}
        return set() 

    def _apply_strict_filter(self, statements: List[Dict[str, Any]], available_evidence_ids: Set[str]) -> List[Dict[str, Any]]:
        """
        Filters out statements that have no specified evidence_doc_ids 
        that match any ID found in the available_evidence_ids set.
        """
        if not available_evidence_ids:
            logger.warning("Strict Filter bypassed: No available evidence IDs loaded.")
            return statements

        filtered_statements = []
        for stmt in statements:
            required_ids = set(stmt.get('evidence_doc_ids', []))
            
            if not required_ids or required_ids.isdisjoint(available_evidence_ids):
                 logger.debug(f"Strict Filter: Skipping {stmt['sub_id']} L{stmt['level']} (No evidence match)")
                 continue
            
            filtered_statements.append(stmt)
            
        return filtered_statements
    

    # -------------------- Evidence Classification Helper --------------------

    def _get_pdca_blocks_from_evidences(
        self, 
        top_evidences: List[Dict[str, Any]], 
        level: int # level ‡∏¢‡∏±‡∏á‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ logging/context ‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï
    ) -> Tuple[str, str, str, str, str]:
        """
        Groups retrieved evidence chunks into PDCA phases based on the 'pdca_tag' 
        generated by the LLM classifier. This replaces the old index-based heuristic.

        Args:
            top_evidences: List of retrieved evidence dictionaries, each containing 'text' and 'pdca_tag'.

        Returns:
            A tuple of aggregated context strings: (plan_blocks, do_blocks, check_blocks, act_blocks, other_blocks)
        """
        logger = logging.getLogger(__name__)

        # 1. Initialize groupings
        pdca_groups = defaultdict(list)
        
        # 2. Group chunks based on the 'pdca_tag'
        for i, doc in enumerate(top_evidences):
            # üìå Use the classified tag directly. Fallback to 'Other' if tag is missing.
            tag = doc.get('pdca_tag', 'Other')
            
            # üìå Format the chunk before appending to the group list
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÄ‡∏î‡∏¥‡∏° (i+1) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ trace ‡∏´‡∏≤‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
            formatted_chunk = f"--- [Chunk {i+1} | Tag: {tag}] ---\n{doc.get('text', '')}\n"
            pdca_groups[tag].append(formatted_chunk)

        # 3. Aggregate groups into single strings
        plan_blocks = "\n\n".join(pdca_groups.get('Plan', []))
        do_blocks = "\n\n".join(pdca_groups.get('Do', []))
        check_blocks = "\n\n".join(pdca_groups.get('Check', []))
        act_blocks = "\n\n".join(pdca_groups.get('Act', []))
        other_blocks = "\n\n".join(pdca_groups.get('Other', []))

        logger.debug(
            f"  > PDCA Blocks Grouped (L{level}): "
            f"P={len(pdca_groups.get('Plan', []))}, D={len(pdca_groups.get('Do', []))}, "
            f"C={len(pdca_groups.get('Check', []))}, A={len(pdca_groups.get('Act', []))}, "
            f"Other={len(pdca_groups.get('Other', []))}"
        )

        return plan_blocks, do_blocks, check_blocks, act_blocks, other_blocks


    def _get_mapped_uuids_and_priority_chunks(
            self, 
            sub_id: str, 
            level: int, 
            statement_text: str, 
            level_constraint: str,
            vectorstore_manager: Optional['VectorStoreManager']
        ) -> Tuple[List[str], List[Dict[str, Any]]]:
            """
            1. Gathers all PASSED Stable Chunk UUIDs (doc_id) from L1 up to L[level-1]. 
            2. Fetches limited priority RAG chunks (Hybrid Retrieval) 
            based on the gathered Chunk UUIDs.
            
            Returns: (mapped_chunk_uuids: list[str], priority_docs: list[dict])
            """
            
            all_priority_items: List[Dict[str, Any]] = [] 
            
            # üìå DEBUG: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Ç‡∏≠‡∏á evidence_map ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡∏∂‡∏á
            logger.info(f"DEBUG: EVIDENCE MAP KEYS BEFORE PRIORITY SEARCH (L{level}): {sorted(self.evidence_map.keys())}")
            
            # 1. ‡∏ß‡∏ô‡∏ã‡πâ‡∏≥‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà PASS ‡∏à‡∏≤‡∏Å Level 1 ‡∏à‡∏ô‡∏ñ‡∏∂‡∏á Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ (L1 -> L[level - 1])
            for prev_level in range(1, level): 
                prev_map_key = f"{sub_id}.L{prev_level}"
                
                # üéØ ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å self.evidence_map (‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÉ‡∏ô Sequential)
                items_from_map = self.evidence_map.get(prev_map_key, [])
                all_priority_items.extend(items_from_map)
                
                # [‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á]: ‡∏•‡∏ö‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å self.temp_map_for_save ‡∏≠‡∏≠‡∏Å‡πÉ‡∏ô‡πÇ‡∏´‡∏°‡∏î Sequential ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
                # ‡πÅ‡∏ï‡πà‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏´‡∏°‡∏î Parallel, Main Process ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å self.temp_map_for_save 
                # ‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡πÉ‡∏ô all_priority_items ‡∏î‡πâ‡∏ß‡∏¢ (‡∏ã‡∏∂‡πà‡∏á Logic ‡∏ô‡∏µ‡πâ‡∏Ñ‡∏ß‡∏£‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô Main Loop ‡∏Ç‡∏≠‡∏á run_assessment)
                # ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏Ñ‡∏á‡πÑ‡∏ß‡πâ‡πÅ‡∏Ñ‡πà‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å evidence_map ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö Sequential mode ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏£‡∏±‡∏ô‡∏≠‡∏¢‡∏π‡πà

            
            # 2. ‡πÅ‡∏õ‡∏•‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô Chunk UUID (String) ‡πÅ‡∏•‡∏∞ Dedup
            doc_ids_for_dedup: List[str] = [
                item.get('doc_id') 
                for item in all_priority_items
                if isinstance(item, dict) and item.get('doc_id')
            ]

            mapped_chunk_uuids: List[str] = list(set(doc_ids_for_dedup))
            num_historical_chunks = len(mapped_chunk_uuids)

            priority_docs = [] 
            
            if num_historical_chunks > 0:
                levels_logged = f"L1-L{level-1}" if level > 1 else "L0 (Should not happen)"
                logger.critical(f"üß≠ DEBUG: Priority Search initiated with {num_historical_chunks} historical Chunk UUIDs ({levels_logged}).") 
                logger.info(f"‚úÖ Hybrid Mapping: Found {num_historical_chunks} pre-mapped Chunk UUIDs from {levels_logged} for {sub_id}. Prioritizing these.")
                
                if vectorstore_manager:
                    try:
                        # Assuming enhance_query_for_statement is available
                        rag_queries_for_vsm = enhance_query_for_statement(
                            statement_text=statement_text,
                            sub_id=sub_id, 
                            statement_id=sub_id, 
                            level=level, 
                            enabler_id=self.enabler_id,
                            focus_hint=level_constraint 
                        )
                        
                        doc_type = self.doc_type 
                        
                        # 3.1 ‡∏î‡∏∂‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ï‡∏≤‡∏° Chunk UUIDs ‡∏ó‡∏µ‡πà‡∏û‡∏ö
                        retrieved_docs_result = retrieve_context_by_doc_ids(
                            doc_uuids=mapped_chunk_uuids, # <-- ‡πÉ‡∏ä‡πâ Chunk UUIDs
                            doc_type=doc_type,
                            enabler=self.enabler_id,
                            vectorstore_manager=vectorstore_manager
                        )
                        
                        initial_priority_chunks: List[Dict[str, Any]] = retrieved_docs_result.get("top_evidences", [])
                        
                        if initial_priority_chunks:
                            # Rerank ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Chunk ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
                            reranker = get_global_reranker(self.FINAL_K_RERANKED) 
                            rerank_query = rag_queries_for_vsm[0] 
                            
                            # ‡∏™‡∏£‡πâ‡∏≤‡∏á LcDocument list ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Rerank (‡∏ï‡πâ‡∏≠‡∏á‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤ LcDocument)
                            from langchain_core.documents import Document as LcDocument

                            lc_docs_for_rerank = [
                                LcDocument(
                                    page_content=d.get('content') or d.get('text', ''), 
                                    metadata={
                                        **d, 
                                        'relevance_score': 1.0 
                                    }
                                ) 
                                for d in initial_priority_chunks
                            ]
                            
                            if reranker and hasattr(reranker, 'compress_documents'):
                                reranked_docs = reranker.compress_documents(
                                    query=rerank_query,
                                    documents=lc_docs_for_rerank,
                                    top_n=self.PRIORITY_CHUNK_LIMIT 
                                )
                                # ‡πÅ‡∏õ‡∏•‡∏á‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô Dict
                                priority_docs = [{
                                    **d.metadata, 
                                    'content': d.page_content,
                                    'text': d.page_content, 
                                    'score': d.metadata.get('relevance_score', 1.0) 
                                } for d in reranked_docs]
                            else:
                                priority_docs = initial_priority_chunks[:self.PRIORITY_CHUNK_LIMIT]

                            logger.critical(f"üß≠ DEBUG: Limited and prioritized {len(priority_docs)} chunks from {num_historical_chunks} mapped UUIDs.")

                    except Exception as e:
                        logger.error(f"Error fetching/reranking priority chunks for {sub_id}: {e}")
                        priority_docs = [] 
            
            # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ Chunk UUIDs ‡πÅ‡∏•‡∏∞ Chunks ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏î‡∏∂‡∏á‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÅ‡∏•‡πâ‡∏ß
            return mapped_chunk_uuids, priority_docs

    # -------------------- Calculation Helpers (ADDED) --------------------
    def _calculate_weighted_score(self, highest_full_level: int, weight: int) -> float:
        """
        Calculates the weighted score based on the highest full level achieved.
        Score is calculated by: (Level / 5) * Weight
        """
        MAX_LEVEL_CALC = 5  
        
        if highest_full_level <= 0:
            return 0.0
        
        level_for_calc = min(highest_full_level, MAX_LEVEL_CALC)
        score = (level_for_calc / MAX_LEVEL_CALC) * weight
        return score

    def _calculate_overall_stats(self, target_sub_id: str = "all"):
            """
            Calculates the total weighted score, total possible weight, and overall maturity score/level.
            """
            total_weighted_score = 0.0
            total_possible_weight = 0.0
            assessed_count = 0
            
            for result in self.final_subcriteria_results:
                if target_sub_id.lower() != "all" and result.get('sub_criteria_id') != target_sub_id:
                    continue

                weighted_score = result.get('weighted_score', 0.0)
                weight = result.get('weight', 0)
                
                total_weighted_score += weighted_score
                total_possible_weight += weight
                assessed_count += 1
                
            overall_maturity_score_avg = 0.0
            overall_maturity_level = "N/A"
            overall_progress_percent = 0.0 # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏ó‡∏µ‡πà 0.0
            
            if total_possible_weight > 0:
                overall_progress_percent = total_weighted_score / total_possible_weight
                
                MAX_LEVEL_STATS = 5 
                overall_maturity_score_avg = overall_progress_percent * MAX_LEVEL_STATS 

                # üü¢ FIX: Completed Logic for Maturity Level Determination
                if overall_maturity_score_avg >= 4.5:
                    overall_maturity_level = "L5"
                elif overall_maturity_score_avg >= 3.5:
                    overall_maturity_level = "L4"
                elif overall_maturity_score_avg >= 2.5:
                    overall_maturity_level = "L3"
                elif overall_maturity_score_avg >= 1.5:
                    overall_maturity_level = "L2"
                elif overall_maturity_score_avg >= 0.5:
                    overall_maturity_level = "L1"
                else:
                    overall_maturity_level = "L0"
            
            self.total_stats = {
                "Overall Maturity Score (Avg.)": overall_maturity_score_avg,
                "Overall Maturity Level (Weighted)": overall_maturity_level,
                "Number of Sub-Criteria Assessed": assessed_count,
                "Total Weighted Score Achieved": total_weighted_score,
                "Total Possible Weight": total_possible_weight,
                "Overall Progress Percentage (0.0 - 1.0)": overall_progress_percent,
                "percentage_achieved_run": overall_progress_percent * 100,
                "total_subcriteria": len(self.rubric),
                "target_level": self.config.target_level
            }
            return self.total_stats
            
    def _export_results(self, results: dict, enabler: str, sub_criteria_id: str, target_level: int, export_dir: str = "assessment_results") -> str:
        """
        Exports the final assessment results to a JSON file.
        
        Args:
            results: The dictionary containing the final assessment summary and results.
            enabler: The enabler ID (e.g., KM).
            sub_criteria_id: The specific sub-criteria ID being run (e.g., 2.2).
            target_level: The target level for the assessment.
            export_dir: The directory to save the output file.
            
        Returns:
            The path to the saved JSON file.
        """
        if not os.path.exists(export_dir):
            os.makedirs(export_dir)
            
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå: assessment_results_KM_2.2_YYYYMMDD_HHMMSS.json
        file_name = f"assessment_results_{enabler}_{sub_criteria_id}_{timestamp}.json"
        full_path = os.path.join(export_dir, file_name)

        # Note: results dict should contain 'summary' and 'sub_criteria_results' keys
        # Update summary fields based on the engine data
        results['summary']['enabler'] = enabler
        results['summary']['sub_criteria_id'] = sub_criteria_id
        results['summary']['target_level'] = target_level
        results['summary']['Number of Sub-Criteria Assessed'] = len(results['sub_criteria_results'])

        try:
            with open(full_path, 'w', encoding='utf-8') as f:
                # ‡πÉ‡∏ä‡πâ indent=4 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢
                json.dump(results, f, ensure_ascii=False, indent=4)
            
            logging.info(f"üíæ Successfully exported final results to: {full_path}")
            return full_path
        
        except Exception as e:
            logging.error(f"‚ùå Failed to export results to {full_path}: {e}")
            return ""
        
    # -------------------- Multiprocessing Worker Method --------------------
    def _assess_single_sub_criteria_worker(self, args) -> Tuple[List[Dict[str, Any]], Dict[str, Any], Dict[str, List[Dict[str, Any]]]]:
        """
        Worker function for multiprocessing. 
        ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏ñ‡∏π‡∏Å‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡∏°‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏ô‡πÉ‡∏ô Worker Process ‡πÅ‡∏¢‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏´‡∏≤‡∏Å (True Parallel) 
        ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô‡∏à‡∏∂‡∏á‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏±‡∏ö Baseline Evidence ‡∏°‡∏≤‡∏à‡∏≤‡∏Å Main Process ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
        
        Args:
            args: tuple(statement_data, engine_config_dict)
        Returns:
            - raw_results_for_sub: list of final results for each level
            - final_sub_result: summary of sub-criteria evaluation
            - level_evidences: dict of evidences to merge later in main process (‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Metadata)
                Format: { "1.1.L1": [ { "doc_id": ..., "filename": ... }, ... ] }
        """
        statement_data, engine_config_dict = args

        # Unpack engine config
        llm_executor = engine_config_dict['llm_executor']
        enabler = engine_config_dict['enabler']
        doc_type = engine_config_dict['doc_type']
        vectorstore_manager = engine_config_dict['vectorstore_manager']
        mapped_uuids = engine_config_dict.get('mapped_uuids')
        priority_docs_input = engine_config_dict.get('priority_docs_input')
        contextual_rules_prompt = engine_config_dict.get('contextual_rules_prompt', "")
        
        # <<< ‡∏à‡∏∏‡∏î‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡∏î‡∏∂‡∏á Baseline Context ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å Hydrate (‡∏°‡∏µ Text) ‡∏à‡∏≤‡∏Å Main Process
        # Main Process ‡∏ï‡πâ‡∏≠‡∏á‡∏î‡∏∂‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô L1-L(n-1) ‡πÉ‡∏™‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡πÉ‡∏ô Dict ‡∏ô‡∏µ‡πâ‡πÅ‡∏•‡πâ‡∏ß
        previous_levels_evidence_full = engine_config_dict.get('previous_levels_evidence_full', []) 
        # >>>

        # Statement metadata
        level = int(statement_data.get("level", 0))
        statement_text = statement_data.get("statement", "")
        sub_criteria_name = statement_data.get("sub_criteria_name", "")
        pdca_phase = statement_data.get("pdca_phase", "")
        level_constraint = statement_data.get("level_constraint", "")
        sub_id = statement_data.get("sub_criteria_id", statement_data.get("sub_id", ""))

        # Determine retrieval/evaluation functions
        if level <= 2:
            retrieval_func = retrieve_context_for_low_levels
            evaluation_func = evaluate_with_llm_low_level
            top_k = 5
        else:
            retrieval_func = retrieve_context_with_filter
            evaluation_func = evaluate_with_llm
            top_k = 10

        # Build enhanced query for RAG
        rag_query_list = enhance_query_for_statement(
            statement_text=statement_text,
            sub_id=sub_id,
            statement_id=statement_data.get('statement_id', sub_id),
            level=level,
            enabler_id=enabler,
            focus_hint=level_constraint,
            llm_executor=llm_executor
        )
        rag_query = rag_query_list[0] if rag_query_list else statement_text

        # Retrieval
        retrieval_result = retrieval_func(
            query=rag_query,
            doc_type=doc_type,
            enabler=enabler,
            vectorstore_manager=vectorstore_manager,
            top_k=top_k,
            mapped_uuids=mapped_uuids,
            priority_docs_input=priority_docs_input,
            sub_id=sub_id,
            level=level
        )

        top_evidences = retrieval_result.get("top_evidences", [])
        aggregated_context = retrieval_result.get("aggregated_context", "")

        # Collect previous level evidences (‡∏ñ‡∏π‡∏Å‡∏•‡∏ö‡∏≠‡∏≠‡∏Å‡πÅ‡∏•‡πâ‡∏ß)
        # ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å worker function ‡∏ô‡∏µ‡πâ‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÄ‡∏°‡∏ò‡∏≠‡∏î‡∏Ç‡∏≠‡∏á Engine class ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á

        # Build multichannel context
        # <<< ‡πÉ‡∏ä‡πâ previous_levels_evidence_full ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ö‡∏°‡∏≤‡∏à‡∏≤‡∏Å args ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
        channels = build_multichannel_context_for_level(
            level=level, 
            top_evidences=top_evidences, 
            previous_levels_evidence=previous_levels_evidence_full # <<< ‡∏™‡πà‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÄ‡∏ï‡πá‡∏°‡πÜ ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ
        )
        # >>>

        # Evaluate statement
        evaluation_result = evaluation_func(
            context=channels.get("direct_context", "") or aggregated_context,
            sub_criteria_name=sub_criteria_name,
            level=level,
            statement_text=statement_text,
            sub_id=sub_id,
            llm_executor=llm_executor,
            pdca_phase=pdca_phase,
            level_constraint=level_constraint,
            contextual_rules_prompt=contextual_rules_prompt,
            baseline_summary=channels.get("baseline_summary", ""),
            aux_summary=channels.get("aux_summary", "")
        )

        # Summarize context for report
        summary_result = create_context_summary_llm(
            context=channels.get("direct_context", "") or aggregated_context,
            sub_criteria_name=sub_criteria_name,
            level=level,
            sub_id=sub_id,
            llm_executor=llm_executor
        )

        # Prepare evidences to return (for main process to merge)
        # <<< ‡∏™‡πà‡∏á‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Metadata ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏†‡∏≤‡∏£‡∏∞‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡∏ú‡πà‡∏≤‡∏ô Process
        level_key = f"{sub_id}.L{level}"
        level_evidences = {
            level_key: [
                {
                    "doc_id": ev.get("doc_id"),
                    "filename": ev.get("filename"),
                    "mapper_type": "AI_GENERATED", 
                    "timestamp": datetime.now().isoformat() # ‡πÄ‡∏û‡∏¥‡πà‡∏° timestamp ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå
                }
                for ev in top_evidences if ev.get("doc_id")
            ]
        }
        # >>>

        # Final result dict
        final_sub_result = {
            "sub_criteria_id": sub_id,
            "sub_criteria_name": sub_criteria_name,
            "level": level,
            "statement": statement_text,
            "pdca_phase": pdca_phase,
            "llm_result": evaluation_result,
            "used_doc_ids": [d.get("doc_id") for d in top_evidences if d.get("doc_id")],
            "channels_debug": channels.get("debug_meta", {}),
            "summary": summary_result
        }

        raw_results_for_sub = [final_sub_result]

        return raw_results_for_sub, final_sub_result, level_evidences

    def _run_sub_criteria_assessment_worker(
        self,
        sub_criteria: Dict[str, Any],
    ) -> Tuple[Dict[str, Any], Dict[str, List[Dict[str, Any]]]]:
        """
        ‡∏£‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô L1-L5 ‡πÅ‡∏ö‡∏ö sequential ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö sub-criteria ‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏ï‡∏±‡∏ß
        ‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á evidence map ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡πÉ‡∏´‡πâ main process ‡∏£‡∏ß‡∏°
        """
        sub_id = sub_criteria['sub_id']
        sub_criteria_name = sub_criteria['sub_criteria_name']
        sub_weight = sub_criteria.get('weight', 0)

        MAX_L1_ATTEMPTS = 2
        highest_full_level = 0
        is_passed_current_level = True
        raw_results_for_sub_seq: List[Dict[str, Any]] = []

        self.logger.info(f"[WORKER START] Assessing Sub-Criteria: {sub_id} - {sub_criteria_name} (Weight: {sub_weight})")

        # ‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï temp_map_for_save ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ worker ‡∏ô‡∏µ‡πâ (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Parallel!)
        self.temp_map_for_save = {}

        # 1. Loop ‡∏ú‡πà‡∏≤‡∏ô‡∏ó‡∏∏‡∏Å Level (L1 ‚Üí L5)
        for statement_data in sub_criteria.get('levels', []):
            level = statement_data.get('level')
            if level is None or level > self.config.target_level:
                continue

            # Dependency check: ‡∏ñ‡πâ‡∏≤ level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ fail ‚Üí cap ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà
            dependency_failed = level > 1 and not is_passed_current_level
            previous_level = level - 1
            persistence_key = f"{sub_id}.L{previous_level}"
            sequential_chunk_uuids = self.evidence_map.get(persistence_key, [])

            level_result = {}
            level_temp_map: List[Dict[str, Any]] = []

            # --- ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å _run_single_assessment (‡∏£‡∏±‡∏ö 2 ‡∏Ñ‡πà‡∏≤: result, temp_map) ---
            if level >= 3:
                # L3-L5: ‡πÉ‡∏ä‡πâ RetryPolicy
                wrapper = self.retry_policy.run(
                    fn=lambda attempt: self._run_single_assessment(
                        sub_criteria=sub_criteria,
                        statement_data=statement_data,
                        vectorstore_manager=self.vectorstore_manager,
                        sequential_chunk_uuids=sequential_chunk_uuids
                    ),
                    level=level,
                    statement=statement_data.get('statement', ''),
                    context_blocks={"sequential_chunk_uuids": sequential_chunk_uuids},
                    logger=self.logger
                )

                # ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: wrapper.result ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô tuple (result, temp_map)
                if isinstance(wrapper, RetryResult) and wrapper.result is not None:
                    level_result = wrapper.result
                    level_temp_map = level_result.get("temp_map_for_level", []) # <-- ‡∏î‡∏∂‡∏á List Evidence ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤
                else:
                    level_result = {}
                    level_temp_map = []

            else:
                # L1-L2: ‡∏•‡∏≠‡∏á‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î 2 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á
                for attempt in range(MAX_L1_ATTEMPTS):
                    level_result = self._run_single_assessment(
                        sub_criteria=sub_criteria,
                        statement_data=statement_data,
                        vectorstore_manager=self.vectorstore_manager,
                        sequential_chunk_uuids=sequential_chunk_uuids
                    )
                    level_temp_map = level_result.get("temp_map_for_level", []) # <-- ‡∏î‡∏∂‡∏á List Evidence ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤
                    if level_result.get('is_passed', False):
                        break

            # ‡πÉ‡∏ä‡πâ result ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏°‡∏≤
            result_to_process = level_result or {}
            result_to_process.setdefault("used_chunk_uuids", [])

            # ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô pass/fail ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ (‡∏£‡∏ß‡∏° dependency cap)
            is_passed_llm = result_to_process.get('is_passed', False)
            is_passed_final = is_passed_llm and not dependency_failed

            result_to_process['is_passed'] = is_passed_final
            result_to_process['is_capped'] = is_passed_llm and not is_passed_final
            result_to_process['pdca_score_required'] = get_correct_pdca_required_score(level)

            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å evidence ‡∏•‡∏á temp_map_for_save ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠ PASS ‡∏à‡∏£‡∏¥‡∏á
            if is_passed_final and level_temp_map and isinstance(level_temp_map, list):
                current_key = f"{sub_id}.L{level}"
                self.temp_map_for_save[current_key] = level_temp_map
                self.logger.info(f"[EVIDENCE SAVED] {current_key} ‚Üí {len(level_temp_map)} chunks")

            # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö level ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
            is_passed_current_level = is_passed_final

            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏•‡∏á raw results
            result_to_process.setdefault("level", level)
            result_to_process["execution_index"] = len(raw_results_for_sub_seq)
            raw_results_for_sub_seq.append(result_to_process)

            # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï highest level
            if is_passed_final:
                highest_full_level = level
            else:
                self.logger.info(f"[WORKER STOP] {sub_id} failed at L{level}. Highest achieved: L{highest_full_level}")
                break  # ‡∏´‡∏¢‡∏∏‡∏î‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡πÄ‡∏°‡∏∑‡πà‡∏≠ fail

        # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏• sub-criteria
        weighted_score = self._calculate_weighted_score(highest_full_level, sub_weight)
        num_passed = sum(1 for r in raw_results_for_sub_seq if r.get("is_passed", False))

        sub_summary = {
            "num_statements": len(raw_results_for_sub_seq),
            "num_passed": num_passed,
            "num_failed": len(raw_results_for_sub_seq) - num_passed,
            "pass_rate": round(num_passed / len(raw_results_for_sub_seq), 4) if raw_results_for_sub_seq else 0.0
        }

        final_sub_result = {
            "sub_criteria_id": sub_id,
            "sub_criteria_name": sub_criteria_name,
            "highest_full_level": highest_full_level,
            "weight": sub_weight,
            "target_level_achieved": highest_full_level >= self.config.target_level,
            "weighted_score": weighted_score,
            "action_plan": [],
            "raw_results_ref": raw_results_for_sub_seq,
            "sub_summary": sub_summary,
        }

        # final_temp_map = self.temp_map_for_save  # ‡∏™‡πà‡∏á‡∏Å‡∏•‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á dict
        # ‡πÄ‡∏õ‡πá‡∏ô
        final_temp_map = {}
        if self.is_sequential:
            # ‡πÉ‡∏ô sequential ‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ self.evidence_map ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß
            # ‡πÅ‡∏ï‡πà‡∏™‡πà‡∏á snapshot ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
            for key in self.evidence_map:
                if key.startswith(sub_criteria['sub_id'] + "."):
                    final_temp_map[key] = self.evidence_map[key]
        else:
            final_temp_map = self.temp_map_for_save.copy()

        self.logger.info(f"[WORKER END] {sub_id} | Highest: L{highest_full_level} | Evidence keys: {len(final_temp_map)}")
        self.logger.debug(f"Evidence keys returned: {list(final_temp_map.keys())}")

        return final_sub_result, final_temp_map

    def run_assessment(
            self,
            target_sub_id: str = "all",
            export: bool = False,
            vectorstore_manager: Optional['VectorStoreManager'] = None,
            sequential: bool = False
        ) -> Dict[str, Any]:
        """
        Main runner ‡∏Ç‡∏≠‡∏á Assessment Engine
        ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á Parallel ‡πÅ‡∏•‡∏∞ Sequential 100%
        ‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏ß‡πà‡∏≤ evidence_map ‡∏Ñ‡∏£‡∏ö‡∏ó‡∏∏‡∏Å‡∏Å‡∏£‡∏ì‡∏µ
        """

        # if export:
        #     logger.info("EXPORT DETECTED ‚Üí FORCING SEQUENTIAL MODE...")
        #     sequential = True
        #     run_parallel = False
            
        start_ts = time.time()
        self.is_sequential = sequential

        # ============================== 1. Filter Rubric ==============================
        if target_sub_id.lower() == "all":
            sub_criteria_list = self.rubric
        else:
            sub_criteria_list = [
                s for s in self.rubric if s.get('sub_id') == target_sub_id
            ]
            if not sub_criteria_list:
                logger.error(f"Sub-Criteria ID '{target_sub_id}' not found in rubric.")
                return {"error": f"Sub-Criteria ID '{target_sub_id}' not found."}

        # Reset states
        self.raw_llm_results = []
        self.final_subcriteria_results = []
        # self.evidence_map.clear()  # ‡πÄ‡∏Ñ‡∏•‡∏µ‡∏¢‡∏£‡πå‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÉ‡∏´‡∏°‡πà

        # ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏î‡πâ‡∏ß‡∏¢:
        if os.path.exists(self.evidence_map_path):
            loaded = self._load_evidence_map()
            if loaded:
                self.evidence_map = loaded
                self.logger.info(f"Resumed from existing evidence map: {len(self.evidence_map)} keys")
            else:
                self.evidence_map = {}
        else:
            self.evidence_map = {}

        if not sequential:
            self.logger.info("[PARALLEL MODE] Starting parallel assessment...")

        # run_parallel = (target_sub_id.lower() == "all" and not self.config.force_sequential)
        run_parallel = (target_sub_id.lower() == "all") and not (sequential or export)

        # ============================== 2. Run Assessment ==============================
        if run_parallel:
            # --------------------- PARALLEL MODE ---------------------
            logger.info("Starting Parallel Assessment with Multiprocessing...")

            worker_args = [(
                sub_data,
                self.config.enabler,
                self.config.target_level,
                self.config.mock_mode,
                self.evidence_map_path,
                self.config.model_name,
                self.config.temperature
            ) for sub_data in sub_criteria_list]

            try:
                pool_ctx = multiprocessing.get_context('spawn')
                with pool_ctx.Pool(processes=max(1, os.cpu_count() - 1)) as pool:
                    results_list = pool.map(_static_worker_process, worker_args)
            except Exception as e:
                logger.critical(f"Multiprocessing failed: {e}")
                raise

            # ‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å worker
            for sub_result, temp_map_from_worker in results_list:
                # ‡∏£‡∏ß‡∏° Evidence Map
                if isinstance(temp_map_from_worker, dict):
                    for level_key, evidence_list in temp_map_from_worker.items():
                        if isinstance(evidence_list, list) and evidence_list:
                            current_list = self.evidence_map.setdefault(level_key, [])
                            current_list.extend(evidence_list)
                            self.logger.info(f"AGGREGATED: +{len(evidence_list)} ‚Üí {level_key} "
                                           f"(total: {len(current_list)})")

                # ‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏≠‡∏∑‡πà‡∏ô ‡πÜ
                raw_refs = sub_result.get("raw_results_ref", [])
                self.raw_llm_results.extend(raw_refs if isinstance(raw_refs, list) else [])
                self.final_subcriteria_results.append(sub_result)

        else:
            # --------------------- SEQUENTIAL MODE ---------------------
            mode_desc = target_sub_id if target_sub_id != "all" else "All Sub-Criteria (Sequential)"
            self.logger.info(f"Starting Sequential Assessment: {mode_desc}")

            # ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î: ‡∏≠‡∏¢‡πà‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á temp_map_for_save ‡πÄ‡∏•‡∏¢‡πÉ‡∏ô Sequential
            # ‡πÅ‡∏•‡∏∞‡∏≠‡∏¢‡πà‡∏≤‡∏£‡∏±‡∏ö temp_map_from_worker ‡∏°‡∏≤‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏ô‡∏±‡πâ‡∏ô!

            local_vsm = vectorstore_manager or (
                load_all_vectorstores(doc_types=[EVIDENCE_DOC_TYPES], evidence_enabler=self.config.enabler)
                if self.config.mock_mode == "none" else None
            )
            self.vectorstore_manager = local_vsm

            for sub_criteria in sub_criteria_list:
                # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å worker ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏™‡∏ô‡πÉ‡∏à temp_map_from_worker ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ Sequential ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ï‡∏£‡∏á‡πÉ‡∏ô evidence_map ‡πÅ‡∏•‡πâ‡∏ß
                sub_result, _ = self._run_sub_criteria_assessment_worker(sub_criteria)

                # ‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏õ‡∏Å‡∏ï‡∏¥
                self.raw_llm_results.extend(sub_result.get("raw_results_ref", []))
                self.final_subcriteria_results.append(sub_result)

        # ============================== 3. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Evidence Map ==============================
        if self.evidence_map:
            self._save_evidence_map(map_to_save=self.evidence_map)
            total_items = sum(len(v) for v in self.evidence_map.values())
            self.logger.info(f"Persisted final evidence map | Keys: {len(self.evidence_map)} | "
                            f"Items: {total_items} | Size: ~{total_items * 0.35:.1f} KB")

        # ============================== 4. ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏• & Export ==============================
        self._calculate_overall_stats(target_sub_id)

        final_results = {
            "summary": self.total_stats,
            "sub_criteria_results": self.final_subcriteria_results,
            "raw_llm_results": self.raw_llm_results,
            "run_time_seconds": round(time.time() - start_ts, 2),
            "timestamp": datetime.now().isoformat(),
        }

        if export:
            export_path = self._export_results(
                results=final_results,
                enabler=self.config.enabler,
                sub_criteria_id=target_sub_id if target_sub_id != "all" else "ALL",
                target_level=self.config.target_level
            )
            # ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡∏£‡∏ß‡∏° evidence_map ‡∏ó‡∏∏‡∏Å‡∏Å‡∏£‡∏ì‡∏µ (‡∏ó‡∏±‡πâ‡∏á sequential ‡πÅ‡∏•‡∏∞ parallel)
            final_results["export_path_used"] = export_path
            final_results["evidence_map"] = deepcopy(self.evidence_map)
            self.logger.info(f"Exported full results ‚Üí {export_path}")

        return final_results

    def _run_single_assessment(
        self,
        sub_criteria: Dict[str, Any],
        statement_data: Dict[str, Any],
        vectorstore_manager: Optional['VectorStoreManager'],
        sequential_chunk_uuids: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        ‡∏£‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô Level ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (L1-L5) ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå
        - ‡πÉ‡∏ä‡πâ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏à‡∏≤‡∏Å Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (baseline)
        - ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å evidence map ‡∏Ñ‡∏£‡∏ö‡∏ó‡∏∏‡∏Å Level
        - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Sequential & Parallel 100%
        - ‡πÑ‡∏°‡πà‡∏°‡∏µ TEMP-, HASH-, Unknown ‡∏≠‡∏µ‡∏Å‡∏ï‡πà‡∏≠‡πÑ‡∏õ
        """

        sub_id = sub_criteria['sub_id']
        level = statement_data['level']
        statement_text = statement_data['statement']
        sub_criteria_name = sub_criteria['sub_criteria_name']
        statement_id = statement_data.get('statement_id', sub_id)

        logger.info(f"  > Starting assessment for {sub_id} L{level}...")

        # ==================== 1. PDCA & Level Prompt ====================
        pdca_phase = self._get_pdca_phase(level)
        level_constraint = self._get_level_constraint_prompt(level)
        contextual_rules_prompt = self._get_contextual_rules_prompt(sub_id, level)
        full_focus_hint = level_constraint + contextual_rules_prompt

        # ==================== 2. Hybrid Retrieval Setup ====================
        mapped_stable_doc_ids, priority_docs = self._get_mapped_uuids_and_priority_chunks(
            sub_id=sub_id,
            level=level,
            statement_text=statement_text,
            level_constraint=level_constraint,
            vectorstore_manager=vectorstore_manager
        )

        # ==================== 3. Enhance Query ====================
        rag_query_list = enhance_query_for_statement(
            statement_text=statement_text,
            sub_id=sub_id,
            statement_id=statement_id,
            level=level,
            enabler_id=self.enabler_id,
            focus_hint=full_focus_hint,
            llm_executor=self.llm
        )
        rag_query = rag_query_list[0] if rag_query_list else statement_text

        # ==================== 4. LLM Evaluator Setup ====================
        llm_evaluator_to_use = self.llm_evaluator
        if level <= 2:
            llm_evaluator_to_use = evaluate_with_llm_low_level

        # ==================== 5. RAG Retrieval ====================
        retrieval_start = time.time()
        try:
            retrieval_result = self.rag_retriever(
                query=rag_query_list,
                doc_type=EVIDENCE_DOC_TYPES,
                enabler=self.enabler_id,
                sub_id=sub_id,
                level=level,
                vectorstore_manager=vectorstore_manager,
                mapped_uuids=mapped_stable_doc_ids,
                priority_docs_input=priority_docs,
                sequential_chunk_uuids=sequential_chunk_uuids
            )
        except Exception as e:
            logger.error(f"RAG retrieval failed for {sub_id} L{level}: {e}")
            retrieval_result = {"top_evidences": [], "aggregated_context": "ERROR: RAG failure.", "used_chunk_uuids": []}

        retrieval_duration = time.time() - retrieval_start
        top_evidences = retrieval_result.get("top_evidences", [])
        used_chunk_uuids = retrieval_result.get("used_chunk_uuids", [])

        # ==================== 6. ‡∏î‡∏∂‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏à‡∏≤‡∏Å Level ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ ====================
        try:
            previous_levels_raw = self._collect_previous_level_evidences(
                sub_id, current_level=level, vectorstore_manager=vectorstore_manager
            )
        except Exception as e:
            logger.error(f"Failed to collect previous evidences: {e}")
            previous_levels_raw = {}

        previous_levels_evidence_full = []
        previous_levels_filename_map = {}

        for ev_list in previous_levels_raw.values():
            for ev in ev_list:
                doc_id = ev.get("doc_id") or ev.get("chunk_uuid")
                if not doc_id or str(doc_id).startswith("HASH-"):
                    continue
                previous_levels_evidence_full.append(ev)
                filename = ev.get("source_filename") or ev.get("source") or ev.get("filename") or "UNKNOWN"
                previous_levels_filename_map[doc_id] = filename

        # ==================== 6a. Sequential fallback ====================
        if level > 1 and self.is_sequential:
            current_ids = {d.get("doc_id") or d.get("chunk_uuid") for d in top_evidences}
            for ev in previous_levels_evidence_full:
                ev_id = ev.get("doc_id") or ev.get("chunk_uuid")
                if ev_id not in current_ids:
                    fallback_ev = ev.copy()
                    fallback_ev["pdca_tag"] = "Baseline"
                    top_evidences.append(fallback_ev)

        # ==================== 7. ‡∏™‡∏£‡πâ‡∏≤‡∏á Multi-Channel Context ====================
        channels = build_multichannel_context_for_level(
            level=level,
            top_evidences=top_evidences,
            previous_levels_evidence=previous_levels_evidence_full,
            max_main_context_tokens=3000,
            max_summary_sentences=4
        )

        debug = channels.get("debug_meta", {})
        logger.info(
            f"  > Context built ‚Üí Direct: {debug.get('direct_count',0)}, "
            f"Aux: {debug.get('aux_count',0)}, "
            f"Baseline: {len(previous_levels_evidence_full)} files "
            f"from {len(previous_levels_raw)} previous levels"
        )

        # ==================== 8. LLM Evaluation ====================
        context_parts = [
            f"--- DIRECT EVIDENCE (L{level}) ---\n{channels.get('direct_context','')}",
            f"--- AUXILIARY EVIDENCE ---\n{channels.get('aux_summary','')}",
            f"--- BASELINE FROM PREVIOUS LEVELS ---\n{channels.get('baseline_summary','‡πÑ‡∏°‡πà‡∏°‡∏µ')}"
        ]
        final_llm_context = "\n\n".join([p for p in context_parts if p.strip()])

        llm_start = time.time()
        llm_result = llm_evaluator_to_use(
            context=final_llm_context,
            sub_criteria_name=sub_criteria_name,
            level=level,
            statement_text=statement_text,
            sub_id=sub_id,
            pdca_phase=pdca_phase,
            level_constraint=level_constraint,
            contextual_rules=contextual_rules_prompt,
            llm_executor=self.llm
        )
        llm_duration = time.time() - llm_start

        # ==================== 9-10. Scoring & Pass/Fail ====================
        llm_score = llm_result.get('score', 0) if llm_result else 0
        pdca_breakdown, is_passed, _ = calculate_pdca_breakdown_and_pass_status(llm_score, level)
        status = "PASS" if is_passed else "FAIL"

        # ==================== 11. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Evidence Map (‡πÄ‡∏â‡∏û‡∏≤‡∏∞ PASS) ====================
        temp_map_for_level = None
        evidence_entries = []  # ‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡∏ô‡∏µ‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÅ‡∏•‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•
        logger.critical(f"üß≠ DEBUG: Entering Evidence Save Logic for {sub_id}.L{level}. Passed: {is_passed}, Top Evidences: {len(top_evidences)}")

        if is_passed and top_evidences:
            seen = set()
            discarded_ids = []

            for ev in top_evidences:
                doc_id = ev.get("doc_id") or ev.get("chunk_uuid")
                if not doc_id or str(doc_id).startswith("TEMP-") or str(doc_id).startswith("HASH-"):
                    discarded_ids.append(f"Skipped: {doc_id}")
                    continue
                if doc_id in seen:
                    continue
                seen.add(doc_id)

                filename = (
                    ev.get("source_filename") or
                    ev.get("source") or
                    ev.get("filename") or
                    previous_levels_filename_map.get(doc_id) or
                    f"‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á_{doc_id[:8]}.pdf"
                )

                entry = {
                    "doc_id": doc_id,
                    "filename": os.path.basename(filename),
                    "mapper_type": "AI_GENERATED",
                    "timestamp": datetime.now().isoformat()
                }
                evidence_entries.append(entry)

            logger.critical(f"üß≠ DEBUG: Discarded {len(discarded_ids)} invalid entries. "
                            f"Valid entries: {len(evidence_entries)}")

            if evidence_entries:
                key = f"{sub_id}.L{level}"
                if self.is_sequential:
                    current_list = self.evidence_map.setdefault(key, [])
                    existing_ids = {item["doc_id"] for item in current_list}
                    new_entries = [e for e in evidence_entries if e["doc_id"] not in existing_ids]
                    current_list.extend(new_entries)
                    logger.info(f"DIRECT SAVE evidence_map[{key}] +{len(new_entries)} files ‚Üí total {len(current_list)}")
                else:
                    if not hasattr(self, "temp_map_for_save"):
                        self.temp_map_for_save = {}
                    self.temp_map_for_save[key] = evidence_entries

                logger.info(f"  > [EVIDENCE SAVED] {key} ‚Üí {len(evidence_entries)} files")
                temp_map_for_level = evidence_entries if not self.is_sequential else None

        # ==================== 12. Evidence Strength & Confidence ====================
        direct_count = len([d for d in top_evidences if d.get("pdca_tag") in ["P", "D", "C", "A"]])
        total_chunks = len(top_evidences)
        pdca_coverage = len({d.get("pdca_tag") for d in top_evidences if d.get("pdca_tag")})

        evidence_strength = min(10.0, 
            (direct_count * 1.8) + 
            (2.0 if total_chunks >= 20 else 1.0 if total_chunks >= 10 else 0.0) +
            (pdca_coverage * 1.5)
        )

        ai_confidence = "HIGH" if evidence_strength >= 8.0 and is_passed else \
                        "MEDIUM" if evidence_strength >= 5.5 else "LOW"

        evidence_count_for_level = len(evidence_entries)

        # ==================== 13. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ ====================
        final_result = {
            "sub_criteria_id": sub_id,
            "statement_id": statement_id,
            "level": level,
            "statement": statement_text,
            "pdca_phase": pdca_phase,
            "llm_score": llm_score,
            "pdca_breakdown": pdca_breakdown,
            "is_passed": is_passed,
            "status": status,
            "score": llm_score,
            "llm_result_full": llm_result,
            "retrieval_duration_s": round(retrieval_duration, 2),
            "llm_duration_s": round(llm_duration, 2),
            "top_evidences_ref": evidence_entries,  # ‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô ‚Üí ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô 100%
            "temp_map_for_level": temp_map_for_level,
            "evidence_strength": round(evidence_strength, 1),
            "ai_confidence": ai_confidence,
            "evidence_count": evidence_count_for_level,
            "pdca_coverage": pdca_coverage,
            "direct_evidence_count": direct_count
        }

        logger.info(f"  > Assessment {sub_id} L{level} completed ‚Üí {status} (Score: {llm_score:.1f})")
        return final_result