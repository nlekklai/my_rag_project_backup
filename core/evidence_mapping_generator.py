#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# ====================================================================
# üõ†Ô∏è WARNING SUPPRESSION BLOCK (‡∏ñ‡∏π‡∏Å‡∏¢‡πâ‡∏≤‡∏¢‡∏°‡∏≤‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏ú‡∏•‡∏Å‡πà‡∏≠‡∏ô Import ‡∏≠‡∏∑‡πà‡∏ô‡πÜ)
# ====================================================================
import warnings
import os

# 1. ‡∏õ‡∏¥‡∏î FutureWarning (‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ TRANSFORMERS_CACHE)
warnings.filterwarnings("ignore", category=FutureWarning) 

# 2. ‡∏õ‡∏¥‡∏î DeprecationWarning (‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ LangChainDeprecationWarning)
warnings.filterwarnings("ignore", category=DeprecationWarning) 

# 3. ‡∏õ‡∏¥‡∏î UserWarning ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ (‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ 'No languages specified...' ‡πÅ‡∏•‡∏∞ pypdf warnings)
warnings.filterwarnings("ignore", category=UserWarning)

# 4. ‡∏õ‡∏¥‡∏î RuntimeWarning (‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏Å‡∏£‡∏ì‡∏µ‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏Ñ‡∏≤‡∏î‡∏Ñ‡∏¥‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô)
warnings.filterwarnings("ignore", category=RuntimeWarning)

# 5. ‡∏õ‡∏¥‡∏î Hugging Face Tokenizer Parallelism Warning
os.environ["TOKENIZERS_PARALLELISM"] = "false" 

# ====================================================================

import json
import argparse
import sys
from typing import List, Dict, Any, Optional
# ‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤ import ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
from core.ingest import load_and_chunk_document 
from core.vectorstore import load_vectorstore, FINAL_K_RERANKED 
from langchain.schema import Document as LcDocument

import logging
import re

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


class EvidenceMappingGenerator:
    """
    Class ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á Suggested Mappings ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á Evidence File
    ‡∏Å‡∏±‡∏ö Statement ‡πÉ‡∏ô Vector Store ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Logic ‡∏Å‡∏≤‡∏£ Augment Query ‡πÅ‡∏ö‡∏ö Dynamic
    """
    def __init__(self, enabler_id: str):
        self.enabler_id = enabler_id.lower()
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Path ‡∏Ç‡∏≠‡∏á Statement Checklist
        self.STATEMENTS_JSON_PATH = f"evidence_checklist/{self.enabler_id}_evidence_statements_checklist.json"

        # ‡πÇ‡∏´‡∏•‡∏î Retriever ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Statement Vector Store
        self.statement_retriever = load_vectorstore(
            doc_id=f"{self.enabler_id}_statements",
            doc_types="statement"
        )
        self.statement_data = self._load_statements_data()

    def _load_statements_data(self):
        if not os.path.exists(self.STATEMENTS_JSON_PATH):
            raise FileNotFoundError(f"‚ùå Statement checklist not found at {self.STATEMENTS_JSON_PATH}")
        with open(self.STATEMENTS_JSON_PATH, 'r', encoding='utf-8') as f:
            data = json.load(f)
            logger.info(f"‚úÖ Loaded statements data from {os.path.basename(self.STATEMENTS_JSON_PATH)}.")
            return data

    def _extract_sub_criteria_id(self, doc_id: str) -> Optional[str]:
        # Logic ‡∏ô‡∏µ‡πâ‡∏ñ‡∏π‡∏Å‡∏Ñ‡∏á‡πÑ‡∏ß‡πâ ‡πÅ‡∏ï‡πà‡∏ñ‡∏π‡∏Å Bypass ‡πÉ‡∏ô _get_dynamic_augmentation
        match = re.search(r'(\d\.\d)L?', doc_id, re.IGNORECASE)
        if match:
             return match.group(1)
        match_alt = re.search(r'(\d\.\d)_L', doc_id, re.IGNORECASE)
        return match_alt.group(1) if match_alt else None

    # üåü HELPER METHOD: ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î‡πÉ‡∏´‡πâ‡∏Å‡∏±‡∏ö Query ‡∏ï‡∏≤‡∏° Level
    def _get_level_constraint_prompt(self, level: int) -> str:
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á Prompt Constraint ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ö‡∏≠‡∏Å LLM/Vector Search ‡πÉ‡∏´‡πâ‡∏Å‡∏£‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô L3/L4/L5 ‡∏≠‡∏≠‡∏Å
        ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ RAG ‡∏î‡∏∂‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Å‡∏±‡∏ö‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ß‡∏∏‡∏í‡∏¥‡∏†‡∏≤‡∏ß‡∏∞
        """
        # ‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£: ‡∏´‡πâ‡∏≤‡∏°‡∏î‡∏∂‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô
        if level == 1:
            # L1: ‡∏´‡πâ‡∏≤‡∏°‡∏î‡∏∂‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô L3-L5 (‡∏Å‡∏≤‡∏£‡∏ö‡∏π‡∏£‡∏ì‡∏≤‡∏Å‡∏≤‡∏£, ‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°, ‡∏Å‡∏≤‡∏£‡∏ß‡∏±‡∏î‡∏ú‡∏•‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß)
            return "‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö '‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô', '‡∏Å‡∏≤‡∏£‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà', ‡∏´‡∏£‡∏∑‡∏≠ '‡∏Å‡∏≤‡∏£‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô' ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö '‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á', '‡∏Å‡∏≤‡∏£‡∏ö‡∏π‡∏£‡∏ì‡∏≤‡∏Å‡∏≤‡∏£', '‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°', ‡∏´‡∏£‡∏∑‡∏≠ '‡∏Å‡∏≤‡∏£‡∏ß‡∏±‡∏î‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß' (L1-Filter)"
        elif level == 2:
            # L2: ‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï L1/L2 ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡∏´‡πâ‡∏≤‡∏°‡∏î‡∏∂‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô L4-L5
            return "‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö '‡∏Å‡∏≤‡∏£‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥', '‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô', ‡∏´‡∏£‡∏∑‡∏≠ '‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô' ‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö '‡∏Å‡∏≤‡∏£‡∏ö‡∏π‡∏£‡∏ì‡∏≤‡∏Å‡∏≤‡∏£', '‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°', ‡∏´‡∏£‡∏∑‡∏≠ '‡∏Å‡∏≤‡∏£‡∏ß‡∏±‡∏î‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß' (L2-Filter)"
        elif level == 3:
            # L3: ‡∏°‡∏∏‡πà‡∏á‡πÄ‡∏ô‡πâ‡∏ô‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ß‡∏±‡∏î‡∏ú‡∏•‡∏£‡∏∞‡∏¢‡∏∞‡∏™‡∏±‡πâ‡∏ô ‡∏´‡πâ‡∏≤‡∏° L5
            return "‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏£‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö '‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°', '‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏Å‡∏±‡∏ö‡∏î‡∏π‡πÅ‡∏•', ‡∏´‡∏£‡∏∑‡∏≠ '‡∏Å‡∏≤‡∏£‡∏ß‡∏±‡∏î‡∏ú‡∏•‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô' ‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö '‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°', ‡∏´‡∏£‡∏∑‡∏≠ '‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡πà‡∏≤‡∏ó‡∏≤‡∏á‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß' (L3-Filter)"
        elif level == 4:
            # L4: ‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô L5 (‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏ö‡∏π‡∏£‡∏ì‡∏≤‡∏Å‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á)
            return "‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ñ‡∏∂‡∏á '‡∏Å‡∏≤‡∏£‡∏ö‡∏π‡∏£‡∏ì‡∏≤‡∏Å‡∏≤‡∏£' ‡∏´‡∏£‡∏∑‡∏≠ '‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á' ‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏û‡∏¥‡∏™‡∏π‡∏à‡∏ô‡πå '‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡πà‡∏≤‡∏ó‡∏≤‡∏á‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß' (L4-Filter)"
        elif level == 5:
            # L5: ‡πÑ‡∏°‡πà‡∏à‡∏≥‡∏Å‡∏±‡∏î ‡πÅ‡∏ï‡πà‡πÄ‡∏ô‡πâ‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á L5
            return "‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î: ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ñ‡∏∂‡∏á '‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°', '‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡πà‡∏≤‡∏ó‡∏≤‡∏á‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à', ‡∏´‡∏£‡∏∑‡∏≠ '‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß' ‡πÇ‡∏î‡∏¢‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô (L5-Focus)"
        else:
            # Default ‡∏´‡∏£‡∏∑‡∏≠ Level ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏™‡∏≤‡∏Å‡∏•
            return "‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£"
        
    def _get_dynamic_augmentation(self, doc_id: str, base_query_content: str) -> str:
        """
        üõ†Ô∏è 2. ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á: ‡πÉ‡∏ä‡πâ Keywords ‡∏ó‡∏µ‡πà‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á‡∏ï‡∏≤‡∏° Enabler ID (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö 10 Enabler)
        """
        
        # üü¢ ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Keywords ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ Enabler
        ENABLER_KEYWORDS = {
            "km": [
                "‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ", "Knowledge Management", "‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢ KM", 
                "‡πÅ‡∏ú‡∏ô‡πÅ‡∏°‡πà‡∏ö‡∏ó KM", "‡∏Å‡∏≤‡∏£‡πÅ‡∏•‡∏Å‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ", "‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ"
            ],
            "hcm": [
                "‡∏Å‡∏≤‡∏£‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏ó‡∏∏‡∏ô‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå", "Human Capital", "‡πÅ‡∏ú‡∏ô‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡∏ô", 
                "‡∏Å‡∏≤‡∏£‡∏™‡∏£‡∏£‡∏´‡∏≤", "‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏ö‡∏∏‡∏Ñ‡∏•‡∏≤‡∏Å‡∏£", "Competency", "‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•"
            ],
            "sp": [
                "‡∏Å‡∏≤‡∏£‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå", "Strategy Planning", "‡∏ß‡∏¥‡∏™‡∏±‡∏¢‡∏ó‡∏±‡∏®‡∏ô‡πå", 
                "‡∏û‡∏±‡∏ô‡∏ò‡∏Å‡∏¥‡∏à", "‡πÄ‡∏õ‡πâ‡∏≤‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£", "KPI", "‡πÅ‡∏ú‡∏ô‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£"
            ],
            "dt": [
                "‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏î‡∏¥‡∏à‡∏¥‡∏ó‡∏±‡∏•", "Digital Transformation", "IT Governance", 
                "Cyber Security", "IT Roadmap", "‡∏£‡∏∞‡∏ö‡∏ö ERP", "‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏™‡∏≤‡∏£‡∏™‡∏ô‡πÄ‡∏ó‡∏®"
            ],
            "cg": [
                "‡∏ö‡∏£‡∏£‡∏©‡∏±‡∏ó‡∏†‡∏¥‡∏ö‡∏≤‡∏•", "Corporate Governance", "‡∏à‡∏£‡∏¥‡∏¢‡∏ò‡∏£‡∏£‡∏°", 
                "‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏Å‡∏±‡∏ö‡∏î‡∏π‡πÅ‡∏•‡∏Å‡∏¥‡∏à‡∏Å‡∏≤‡∏£", "‡∏Ñ‡∏ß‡∏≤‡∏°‡πÇ‡∏õ‡∏£‡πà‡∏á‡πÉ‡∏™", "‡∏Ñ‡∏ì‡∏∞‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£"
            ],
            "l": [
                "‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏Å‡∏é‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö", "Legal & Regulatory", "‡∏Å‡∏≤‡∏£‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏ï‡∏≤‡∏°‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢", 
                "‡∏Ç‡πâ‡∏≠‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö", "‡∏™‡∏±‡∏ç‡∏ç‡∏≤", "‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏î‡∏¥‡∏à‡∏¥‡∏ó‡∏±‡∏•"
            ],
            "rm&ic": [ # ‡∏Ñ‡∏á‡πÑ‡∏ß‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏•‡∏∏‡πà‡∏° RM&IC ‡∏ï‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏î‡∏¥‡∏°
                "‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á", "Risk Management", "‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏†‡∏≤‡∏¢‡πÉ‡∏ô", 
                "Internal Control", "‡πÅ‡∏ú‡∏ô‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á", "‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£"
            ],
            "scm": [
                "‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏´‡πà‡∏ß‡∏á‡πÇ‡∏ã‡πà‡∏≠‡∏∏‡∏õ‡∏ó‡∏≤‡∏ô", "Supply Chain", "‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏ã‡∏∑‡πâ‡∏≠‡∏à‡∏±‡∏î‡∏à‡πâ‡∏≤‡∏á", 
                "‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏Ñ‡∏•‡∏±‡∏á", "‡∏ú‡∏π‡πâ‡∏Ç‡∏≤‡∏¢", "‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡∏°‡∏≠‡∏ö‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤"
            ],
            "im": [
                "‡∏ô‡∏ß‡∏±‡∏ï‡∏Å‡∏£‡∏£‡∏°", "Innovation Management", "‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡∏à‡∏±‡∏¢‡πÅ‡∏•‡∏∞‡∏û‡∏±‡∏í‡∏ô‡∏≤", 
                "R&D", "‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå‡∏™‡∏¥‡∏ô‡∏ó‡∏≤‡∏á‡∏õ‡∏±‡∏ç‡∏ç‡∏≤", "‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤"
            ],
            "ia": [
                "‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏†‡∏≤‡∏¢‡πÉ‡∏ô", "Internal Audit", "‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö‡∏ó‡∏≤‡∏ô", 
                "‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö", "‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö", "‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠"
            ],
        }
        
        enabler_key = self.enabler_id.lower()
        
        if enabler_key not in ENABLER_KEYWORDS:
             logger.warning(f"‚ö†Ô∏è Warning: Enabler '{enabler_key.upper()}' not defined, using KM as default.")
             keywords = ENABLER_KEYWORDS.get("km")
        else:
             keywords = ENABLER_KEYWORDS[enabler_key]
        
        logger.warning(f"‚ö†Ô∏è Warning: Using {enabler_key.upper()}-specific keywords for augmentation.")
        
        # üü¢ ‡∏™‡∏£‡πâ‡∏≤‡∏á String ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Augmentation
        return (
            f"‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô '{self.enabler_id.upper()}' "
            f"‡∏´‡∏£‡∏∑‡∏≠ '{self.enabler_id.upper()}' ({', '.join(keywords)})"
        )
        
    def _get_statement_detail(self, content: str) -> Optional[Dict[str, str]]:
        # ‡πÇ‡∏Ñ‡πâ‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡πÉ‡∏ä‡πâ statement_data ‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î‡∏°‡∏≤‡∏à‡∏≤‡∏Å JSON ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
        for enabler_block in self.statement_data:
            sub_criteria_id = enabler_block.get("Sub_Criteria_ID")
            for i in range(1, 6):
                level_key = f"Level_{i}_Statements"
                statements_list = enabler_block.get(level_key, [])
                for j, statement_text in enumerate(statements_list):
                    clean_content = content.strip().lower()
                    clean_statement = statement_text.strip().lower()
                    
                    # 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö‡∏°‡∏µ‡πÄ‡∏•‡∏Ç‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤
                    if clean_statement and clean_statement[0].isdigit():
                        clean_statement_no_num = clean_statement.split(maxsplit=1)[-1].strip()
                        if clean_content == clean_statement_no_num.lower():
                            return {
                                "statement_key": f"{sub_criteria_id}_L{i}_{j + 1}",
                                "sub_level": f"{sub_criteria_id} Level {i} Statement {j + 1}",
                                "statement_text": statement_text
                            }
                            
                    # 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏•‡∏Ç‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤
                    if clean_content == clean_statement:
                        return {
                            "statement_key": f"{sub_criteria_id}_L{i}_{j + 1}",
                            "sub_level": f"{sub_criteria_id} Level {i} Statement {j + 1}",
                            "statement_text": statement_text
                        }
        return None

    def process_and_suggest_mapping(self,
                                    file_path: str,
                                    doc_id: Optional[str] = None,
                                    level: Optional[int] = None, # üëà ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤ Level ‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤
                                    top_k_statements: int = FINAL_K_RERANKED,
                                    similarity_threshold: float = 0.9900,
                                    suggestion_limit: int = 3) -> List[Dict[str, Any]]:
        """
        Return: List of suggested mappings (JSON serializable)
        """
        effective_doc_id = doc_id if doc_id is not None else os.path.splitext(os.path.basename(file_path))[0]
        
        docs = load_and_chunk_document(file_path=file_path, doc_id=effective_doc_id)
        if not docs:
            logger.error(f"‚ùå Failed to load or chunk the file: {file_path}")
            return []

        base_query = docs[0].page_content
        
        logger.info(f"Loaded and chunked {file_path} -> {len(docs)} chunks.")
        logger.info(f"Primary Chunk Content (Base Query): \n---START---\n{base_query}\n---END---")
        
        if len(re.sub(r'[\d\s\W]', '', base_query)) < len(base_query) * 0.1:
             logger.warning(f"‚ö†Ô∏è Warning: Base Query chunk from {effective_doc_id} appears to be mostly noise/numbers.")

        # ===================================================================================================
        # FINAL/BALANCED MODE: ‡πÉ‡∏ä‡πâ Dynamic Augmentation (Enabler-specific Keywords) ‡πÅ‡∏•‡∏∞ Level Constraint
        # ===================================================================================================
        
        # üü¢ 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á Enabler-specific Augmentation
        augmentation_keywords = self._get_dynamic_augmentation(effective_doc_id, base_query)
        
        # üü¢ 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á Level Constraint (‡∏ñ‡πâ‡∏≤ Level ‡∏ñ‡∏π‡∏Å‡∏£‡∏∞‡∏ö‡∏∏)
        level_constraint = ""
        if level is not None and 1 <= level <= 5:
            level_constraint = self._get_level_constraint_prompt(level)
            logger.info(f"Applying Level Constraint: {level_constraint}")
        
        # üü¢ 3. ‡∏£‡∏ß‡∏° Prompt ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        instruction_prompt = (
            f"‡πÇ‡∏õ‡∏£‡∏î‡∏à‡∏±‡∏î‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° Statement ‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏Å‡∏±‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏™‡∏î‡∏á‡∏ñ‡∏∂‡∏á ({augmentation_keywords}). "
            f"‡πÇ‡∏î‡∏¢‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏ß‡πà‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏±‡∏ö‡∏™‡∏ô‡∏∏‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° Statement ‡πÉ‡∏ô‡∏î‡πâ‡∏≤‡∏ô '{self.enabler_id.upper()}' ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà. "
            f"{level_constraint}"
            f"‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô: "
        )
        
        query = f"{instruction_prompt}{base_query[:1000]}"
        logger.info(f"Using {self.enabler_id.upper()} Augmented Query for RAG: '{query[:120]}...'")

        # ===================================================================================================
        
        # üìå Note: self.statement_retriever.invoke(query) ‡∏à‡∏∞‡∏ó‡∏≥‡∏Å‡∏≤‡∏£ Rerank ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô
        retrieved_statements: List[LcDocument] = self.statement_retriever.invoke(query)

        suggested_mappings = []
        for i, doc in enumerate(retrieved_statements):
            score = float(doc.metadata.get('relevance_score', 0.0))
            if score < similarity_threshold:
                # ‡∏´‡∏¢‡∏∏‡∏î‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤ Threshold
                break
            
            # ‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î Statement
            details = self._get_statement_detail(doc.page_content)
            
            if details:
                # ‡∏ô‡∏±‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞ suggestion ‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô threshold
                if len(suggested_mappings) < suggestion_limit:
                    suggested_mappings.append({
                        "suggestion_rank": i + 1,
                        "score": score,
                        "statement_key": details["statement_key"],
                        "sub_level": details["sub_level"],
                        "statement_text": details["statement_text"],
                        "justification": f"‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏ó‡∏≤‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏™‡∏π‡∏á: ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠: '{details['sub_level']}'"
                    })

        return suggested_mappings

    def process_directory(self,
                          directory: str,
                          output_file: str = "results/merged_results.json",
                          level: Optional[int] = None, # üëà ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤ Level ‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤
                          top_k: int = 7,
                          threshold: float = 0.9900,
                          suggestion_limit: int = 3):
        """
        Process all supported files in a directory and save merged results into a single JSON
        """
        os.makedirs(os.path.dirname(output_file), exist_ok=True)

        merged_results = {}
        # üõ†Ô∏è ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á: ‡πÄ‡∏û‡∏¥‡πà‡∏° .jpg, .jpeg, .png ‡πÉ‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö
        SUPPORTED_EXTENSIONS = [".pdf", ".docx", ".txt", ".xlsx", ".pptx", ".md", ".csv", ".jpg", ".jpeg", ".png"]

        for filename in os.listdir(directory):
            ext = os.path.splitext(filename)[1].lower()
            
            # üí° Skip directories and hidden files/temp files
            file_path = os.path.join(directory, filename)
            if os.path.isdir(file_path) or filename.startswith('.'):
                 continue

            if ext not in SUPPORTED_EXTENSIONS:
                logger.info(f"Skipping unsupported file type: {filename}")
                continue
            
            doc_id = os.path.splitext(filename)[0]
            
            logger.info(f"\n==================================================")
            logger.info(f"üöÄ STARTING PROCESSING: {filename}")
            logger.info(f"==================================================")

            try:
                suggested = self.process_and_suggest_mapping(
                    file_path=file_path,
                    doc_id=doc_id,
                    level=level, # üëà ‡∏™‡πà‡∏á Level ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ
                    top_k_statements=top_k,
                    similarity_threshold=threshold,
                    suggestion_limit=suggestion_limit
                )
                merged_results[doc_id] = suggested
            except Exception as e:
                logger.error(f"‚ùå Failed to process {filename}: {e}")
                merged_results[doc_id] = {"error": str(e)}

        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(merged_results, f, ensure_ascii=False, indent=2)
        logger.info(f"‚úÖ Merged results saved to {output_file}")


def main():
    parser = argparse.ArgumentParser(description="Evidence Mapping Generator CLI")
    # üõ†Ô∏è 3. ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á: ‡πÄ‡∏û‡∏¥‡πà‡∏° choices ‡∏ï‡∏≤‡∏°‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ Enabler ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
    parser.add_argument("--enabler", 
                            type=str, 
                            default="KM",
                            choices=["CG", "L", "SP", "RM&IC", "SCM", "DT", "HCM", "KM", "IM", "IA"],
                            help="The core business enabler abbreviation (e.g., 'KM', 'SCM').")
    
    # üìå 4. ‡πÄ‡∏û‡∏¥‡πà‡∏° Argument ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Level ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÉ‡∏ô Constraint Prompt
    parser.add_argument("--level", 
                            type=int, 
                            choices=[1, 2, 3, 4, 5], 
                            default=None, 
                            help="Maturity Level constraint (1-5). Used to filter out higher level statements.")
                            
    parser.add_argument("--file_path", type=str, help="Path to a single evidence file")
    
    parser.add_argument("--doc_id", type=str, help="Optional Document ID to override the filename. (e.g., KM1.1L106)")
    
    parser.add_argument("--directory", type=str, help="Directory to process all evidence files")
    parser.add_argument("--output_file", type=str, default="results/merged_results.json", help="Output JSON file path")
    parser.add_argument("--top_k", type=int, default=7, help="Top K statements to retrieve")
    parser.add_argument("--threshold", type=float, default=0.9900, help="Similarity threshold") 
    parser.add_argument("--suggestion_limit", type=int, default=3, help="Max number of suggestions") 
    args = parser.parse_args()

    try:
        generator = EvidenceMappingGenerator(enabler_id=args.enabler)
    except FileNotFoundError as e:
        print(f"‚ùå Error during initialization: {e}")
        return
    except Exception as e:
         print(f"‚ùå General Error during initialization: {e}")
         return

    if args.file_path:
        # Process single file
        result = generator.process_and_suggest_mapping(
            file_path=args.file_path,
            doc_id=args.doc_id, 
            level=args.level, # üëà ‡∏™‡πà‡∏á Level ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ
            top_k_statements=args.top_k,
            similarity_threshold=args.threshold,
            suggestion_limit=args.suggestion_limit
        )
        if isinstance(result, list) and result:
             print("================================================================================")
             print(f"‚úÖ Suggested Mappings for Evidence File '{os.path.basename(args.file_path)}' ({args.enabler}) [Level Filter: {args.level or 'None'}]:")
             print("================================================================================")
             for i, suggestion in enumerate(result):
                print(f"--- Suggestion {i + 1} (Score: {suggestion['score']:.4f}) ---")
                print(f"  Statement Key:   {suggestion['statement_key']}")
                print(f"  Sub/Level:       {suggestion['sub_level']}")
                print(f"  Statement Text:  {suggestion['statement_text']}")
                print(f"  Justification:   {suggestion['justification']}")
                print("----------------------------------------")
             print(f"Found {len(result)} suggestions (filtered by top {args.top_k}).")
        else:
             print("================================================================================")
             print(f"‚úÖ Suggested Mappings for Evidence File '{os.path.basename(args.file_path)}' ({args.enabler}) [Level Filter: {args.level or 'None'}]:")
             print("================================================================================")
             print("‚ùå No suggested mappings found above the threshold, or an error occurred.")
    elif args.directory:
        # Process all files in directory
        generator.process_directory(
            directory=args.directory,
            output_file=args.output_file,
            level=args.level, # üëà ‡∏™‡πà‡∏á Level ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ
            top_k=args.top_k,
            threshold=args.threshold,
            suggestion_limit=args.suggestion_limit
        )
    else:
        print("‚ùå Please provide either --file_path or --directory")
        sys.exit(1)


if __name__ == "__main__":
    main()
