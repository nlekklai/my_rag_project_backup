#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# ====================================================================
# ðŸ› ï¸ WARNING SUPPRESSION BLOCK (à¸–à¸¹à¸à¸¢à¹‰à¸²à¸¢à¸¡à¸²à¸—à¸µà¹ˆà¸™à¸µà¹ˆà¹€à¸žà¸·à¹ˆà¸­à¹ƒà¸«à¹‰à¸¡à¸µà¸œà¸¥à¸à¹ˆà¸­à¸™ Import à¸­à¸·à¹ˆà¸™à¹†)
# ====================================================================
import warnings
import os

# 1. à¸›à¸´à¸” FutureWarning (à¸ˆà¸±à¸”à¸à¸²à¸£ TRANSFORMERS_CACHE)
warnings.filterwarnings("ignore", category=FutureWarning) 

# 2. à¸›à¸´à¸” DeprecationWarning (à¸ˆà¸±à¸”à¸à¸²à¸£ LangChainDeprecationWarning)
warnings.filterwarnings("ignore", category=DeprecationWarning) 

# 3. à¸›à¸´à¸” UserWarning à¸—à¸±à¹ˆà¸§à¹„à¸› (à¸ˆà¸±à¸”à¸à¸²à¸£ 'No languages specified...' à¹à¸¥à¸° pypdf warnings)
warnings.filterwarnings("ignore", category=UserWarning)

# 4. à¸›à¸´à¸” RuntimeWarning (à¹€à¸œà¸·à¹ˆà¸­à¸à¸£à¸“à¸µà¸¡à¸µà¸‚à¹‰à¸­à¸œà¸´à¸”à¸žà¸¥à¸²à¸”à¸—à¸µà¹ˆà¹„à¸¡à¹ˆà¸„à¸²à¸”à¸„à¸´à¸”à¹ƒà¸™à¸à¸²à¸£à¸£à¸±à¸™)
warnings.filterwarnings("ignore", category=RuntimeWarning)

# 5. à¸›à¸´à¸” Hugging Face Tokenizer Parallelism Warning
os.environ["TOKENIZERS_PARALLELISM"] = "false" 

# ====================================================================

import json
import argparse
import sys
from typing import List, Dict, Any, Optional
# à¸•à¹‰à¸­à¸‡à¹à¸™à¹ˆà¹ƒà¸ˆà¸§à¹ˆà¸² import à¸–à¸¹à¸à¸•à¹‰à¸­à¸‡à¸•à¸²à¸¡à¹‚à¸„à¸£à¸‡à¸ªà¸£à¹‰à¸²à¸‡à¹‚à¸›à¸£à¹€à¸ˆà¸à¸•à¹Œà¸‚à¸­à¸‡à¸„à¸¸à¸“
from core.ingest import load_and_chunk_document 
from core.vectorstore import load_vectorstore, FINAL_K_RERANKED 
from langchain.schema import Document as LcDocument

import logging
import re

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


class EvidenceMappingGenerator:
    """
    Class à¸ªà¸³à¸«à¸£à¸±à¸šà¸ªà¸£à¹‰à¸²à¸‡ Suggested Mappings à¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡ Evidence File
    à¸à¸±à¸š Statement à¹ƒà¸™ Vector Store à¹‚à¸”à¸¢à¹ƒà¸Šà¹‰ Logic à¸à¸²à¸£ Augment Query à¹à¸šà¸š Dynamic
    """
    def __init__(self, enabler_id: str):
        self.enabler_id = enabler_id.lower()
        # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š Path à¸‚à¸­à¸‡ Statement Checklist
        self.STATEMENTS_JSON_PATH = f"evidence_checklist/{self.enabler_id}_evidence_statements_checklist.json"

        # à¹‚à¸«à¸¥à¸” Retriever à¸ªà¸³à¸«à¸£à¸±à¸š Statement Vector Store
        self.statement_retriever = load_vectorstore(
            doc_id=f"{self.enabler_id}_statements",
            doc_types="statement"
        )
        self.statement_data = self._load_statements_data()

    def _load_statements_data(self):
        if not os.path.exists(self.STATEMENTS_JSON_PATH):
            raise FileNotFoundError(f"âŒ Statement checklist not found at {self.STATEMENTS_JSON_PATH}")
        with open(self.STATEMENTS_JSON_PATH, 'r', encoding='utf-8') as f:
            data = json.load(f)
            logger.info(f"âœ… Loaded statements data from {os.path.basename(self.STATEMENTS_JSON_PATH)}.")
            return data

    def _extract_sub_criteria_id(self, doc_id: str) -> Optional[str]:
        # Logic à¸™à¸µà¹‰à¸–à¸¹à¸à¸„à¸‡à¹„à¸§à¹‰ à¹à¸•à¹ˆà¸–à¸¹à¸ Bypass à¹ƒà¸™ _get_dynamic_augmentation
        match = re.search(r'(\d\.\d)L?', doc_id, re.IGNORECASE)
        if match:
             return match.group(1)
        match_alt = re.search(r'(\d\.\d)_L', doc_id, re.IGNORECASE)
        return match_alt.group(1) if match_alt else None

    # ðŸŒŸ HELPER METHOD: à¸ªà¸³à¸«à¸£à¸±à¸šà¸ªà¸£à¹‰à¸²à¸‡à¸‚à¹‰à¸­à¸ˆà¸³à¸à¸±à¸”à¹ƒà¸«à¹‰à¸à¸±à¸š Query à¸•à¸²à¸¡ Level
    def _get_level_constraint_prompt(self, level: int) -> str:
        """
        à¸ªà¸£à¹‰à¸²à¸‡ Prompt Constraint à¹€à¸žà¸·à¹ˆà¸­à¸šà¸­à¸ LLM/Vector Search à¹ƒà¸«à¹‰à¸à¸£à¸­à¸‡à¸«à¸¥à¸±à¸à¸à¸²à¸™ L3/L4/L5 à¸­à¸­à¸
        à¹€à¸žà¸·à¹ˆà¸­à¹ƒà¸«à¹‰ RAG à¸”à¸¶à¸‡à¸«à¸¥à¸±à¸à¸à¸²à¸™à¸—à¸µà¹ˆà¹€à¸«à¸¡à¸²à¸°à¸ªà¸¡à¸à¸±à¸šà¸£à¸°à¸”à¸±à¸šà¸§à¸¸à¸’à¸´à¸ à¸²à¸§à¸°
        """
        # à¸«à¸¥à¸±à¸à¸à¸²à¸£: à¸«à¹‰à¸²à¸¡à¸”à¸¶à¸‡à¸«à¸¥à¸±à¸à¸à¸²à¸™à¸—à¸µà¹ˆà¸¡à¸µà¸£à¸°à¸”à¸±à¸šà¸ªà¸¹à¸‡à¸à¸§à¹ˆà¸²à¸£à¸°à¸”à¸±à¸šà¸—à¸µà¹ˆà¸à¸³à¸¥à¸±à¸‡à¸›à¸£à¸°à¹€à¸¡à¸´à¸™
        if level == 1:
            # L1: à¸«à¹‰à¸²à¸¡à¸”à¸¶à¸‡à¸«à¸¥à¸±à¸à¸à¸²à¸™ L3-L5 (à¸à¸²à¸£à¸šà¸¹à¸£à¸“à¸²à¸à¸²à¸£, à¸™à¸§à¸±à¸•à¸à¸£à¸£à¸¡, à¸à¸²à¸£à¸§à¸±à¸”à¸œà¸¥à¸£à¸°à¸¢à¸°à¸¢à¸²à¸§)
            return "à¸‚à¹‰à¸­à¸ˆà¸³à¸à¸±à¸”: à¸«à¸¥à¸±à¸à¸à¸²à¸™à¸•à¹‰à¸­à¸‡à¹€à¸à¸µà¹ˆà¸¢à¸§à¸à¸±à¸š 'à¸à¸²à¸£à¹€à¸£à¸´à¹ˆà¸¡à¸•à¹‰à¸™', 'à¸à¸²à¸£à¸¡à¸µà¸­à¸¢à¸¹à¹ˆ', à¸«à¸£à¸·à¸­ 'à¸à¸²à¸£à¸§à¸²à¸‡à¹à¸œà¸™' à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™ à¸«à¹‰à¸²à¸¡à¹€à¸à¸µà¹ˆà¸¢à¸§à¸‚à¹‰à¸­à¸‡à¸à¸±à¸š 'à¸à¸²à¸£à¸›à¸£à¸±à¸šà¸›à¸£à¸¸à¸‡à¸­à¸¢à¹ˆà¸²à¸‡à¸•à¹ˆà¸­à¹€à¸™à¸·à¹ˆà¸­à¸‡', 'à¸à¸²à¸£à¸šà¸¹à¸£à¸“à¸²à¸à¸²à¸£', 'à¸™à¸§à¸±à¸•à¸à¸£à¸£à¸¡', à¸«à¸£à¸·à¸­ 'à¸à¸²à¸£à¸§à¸±à¸”à¸œà¸¥à¸¥à¸±à¸žà¸˜à¹Œà¸£à¸°à¸¢à¸°à¸¢à¸²à¸§' (L1-Filter)"
        elif level == 2:
            # L2: à¸­à¸™à¸¸à¸à¸²à¸• L1/L2 à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™ à¸«à¹‰à¸²à¸¡à¸”à¸¶à¸‡à¸«à¸¥à¸±à¸à¸à¸²à¸™ L4-L5
            return "à¸‚à¹‰à¸­à¸ˆà¸³à¸à¸±à¸”: à¸«à¸¥à¸±à¸à¸à¸²à¸™à¸•à¹‰à¸­à¸‡à¹€à¸à¸µà¹ˆà¸¢à¸§à¸à¸±à¸š 'à¸à¸²à¸£à¸›à¸à¸´à¸šà¸±à¸•à¸´', 'à¸à¸²à¸£à¸—à¸³à¹ƒà¸«à¹‰à¹€à¸›à¹‡à¸™à¸¡à¸²à¸•à¸£à¸à¸²à¸™', à¸«à¸£à¸·à¸­ 'à¸à¸²à¸£à¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¹€à¸šà¸·à¹‰à¸­à¸‡à¸•à¹‰à¸™' à¸«à¹‰à¸²à¸¡à¹€à¸à¸µà¹ˆà¸¢à¸§à¸‚à¹‰à¸­à¸‡à¸à¸±à¸š 'à¸à¸²à¸£à¸šà¸¹à¸£à¸“à¸²à¸à¸²à¸£', 'à¸™à¸§à¸±à¸•à¸à¸£à¸£à¸¡', à¸«à¸£à¸·à¸­ 'à¸à¸²à¸£à¸§à¸±à¸”à¸œà¸¥à¸¥à¸±à¸žà¸˜à¹Œà¸£à¸°à¸¢à¸°à¸¢à¸²à¸§' (L2-Filter)"
        elif level == 3:
            # L3: à¸¡à¸¸à¹ˆà¸‡à¹€à¸™à¹‰à¸™à¹„à¸›à¸—à¸µà¹ˆà¸à¸²à¸£à¸„à¸§à¸šà¸„à¸¸à¸¡à¹à¸¥à¸°à¸à¸²à¸£à¸§à¸±à¸”à¸œà¸¥à¸£à¸°à¸¢à¸°à¸ªà¸±à¹‰à¸™ à¸«à¹‰à¸²à¸¡ L5
            return "à¸‚à¹‰à¸­à¸ˆà¸³à¸à¸±à¸”: à¸«à¸¥à¸±à¸à¸à¸²à¸™à¸„à¸§à¸£à¹€à¸à¸µà¹ˆà¸¢à¸§à¸à¸±à¸š 'à¸à¸²à¸£à¸„à¸§à¸šà¸„à¸¸à¸¡', 'à¸à¸²à¸£à¸à¸³à¸à¸±à¸šà¸”à¸¹à¹à¸¥', à¸«à¸£à¸·à¸­ 'à¸à¸²à¸£à¸§à¸±à¸”à¸œà¸¥à¹€à¸šà¸·à¹‰à¸­à¸‡à¸•à¹‰à¸™' à¸«à¹‰à¸²à¸¡à¹€à¸à¸µà¹ˆà¸¢à¸§à¸‚à¹‰à¸­à¸‡à¸à¸±à¸š 'à¸™à¸§à¸±à¸•à¸à¸£à¸£à¸¡', à¸«à¸£à¸·à¸­ 'à¸à¸²à¸£à¸ªà¸£à¹‰à¸²à¸‡à¸„à¸¸à¸“à¸„à¹ˆà¸²à¸—à¸²à¸‡à¸˜à¸¸à¸£à¸à¸´à¸ˆà¸£à¸°à¸¢à¸°à¸¢à¸²à¸§' (L3-Filter)"
        elif level == 4:
            # L4: à¸­à¸™à¸¸à¸à¸²à¸•à¸—à¸¸à¸à¸­à¸¢à¹ˆà¸²à¸‡à¸¢à¸à¹€à¸§à¹‰à¸™ L5 (à¹€à¸™à¹‰à¸™à¸à¸²à¸£à¸šà¸¹à¸£à¸“à¸²à¸à¸²à¸£à¹à¸¥à¸°à¸à¸²à¸£à¸›à¸£à¸±à¸šà¸›à¸£à¸¸à¸‡)
            return "à¸‚à¹‰à¸­à¸ˆà¸³à¸à¸±à¸”: à¸«à¸¥à¸±à¸à¸à¸²à¸™à¸„à¸§à¸£à¹à¸ªà¸”à¸‡à¸–à¸¶à¸‡ 'à¸à¸²à¸£à¸šà¸¹à¸£à¸“à¸²à¸à¸²à¸£' à¸«à¸£à¸·à¸­ 'à¸à¸²à¸£à¸›à¸£à¸±à¸šà¸›à¸£à¸¸à¸‡à¸­à¸¢à¹ˆà¸²à¸‡à¸•à¹ˆà¸­à¹€à¸™à¸·à¹ˆà¸­à¸‡' à¸«à¹‰à¸²à¸¡à¹€à¸à¸µà¹ˆà¸¢à¸§à¸‚à¹‰à¸­à¸‡à¸à¸±à¸šà¸à¸²à¸£à¸žà¸´à¸ªà¸¹à¸ˆà¸™à¹Œ 'à¸„à¸¸à¸“à¸„à¹ˆà¸²à¸—à¸²à¸‡à¸˜à¸¸à¸£à¸à¸´à¸ˆà¸£à¸°à¸¢à¸°à¸¢à¸²à¸§' (L4-Filter)"
        elif level == 5:
            # L5: à¹„à¸¡à¹ˆà¸ˆà¸³à¸à¸±à¸” à¹à¸•à¹ˆà¹€à¸™à¹‰à¸™à¹€à¸‰à¸žà¸²à¸°à¸„à¸³à¸ªà¸³à¸„à¸±à¸à¸‚à¸­à¸‡ L5
            return "à¸‚à¹‰à¸­à¸ˆà¸³à¸à¸±à¸”: à¸«à¸¥à¸±à¸à¸à¸²à¸™à¸„à¸§à¸£à¹à¸ªà¸”à¸‡à¸–à¸¶à¸‡ 'à¸™à¸§à¸±à¸•à¸à¸£à¸£à¸¡', 'à¸à¸²à¸£à¸ªà¸£à¹‰à¸²à¸‡à¸„à¸¸à¸“à¸„à¹ˆà¸²à¸—à¸²à¸‡à¸˜à¸¸à¸£à¸à¸´à¸ˆ', à¸«à¸£à¸·à¸­ 'à¸œà¸¥à¸¥à¸±à¸žà¸˜à¹Œà¸£à¸°à¸¢à¸°à¸¢à¸²à¸§' à¹‚à¸”à¸¢à¸Šà¸±à¸”à¹€à¸ˆà¸™ (L5-Focus)"
        else:
            # Default à¸«à¸£à¸·à¸­ Level à¹„à¸¡à¹ˆà¸–à¸¹à¸à¸•à¹‰à¸­à¸‡ à¹ƒà¸«à¹‰à¹ƒà¸Šà¹‰à¸„à¸³à¸„à¹‰à¸™à¸«à¸²à¸ªà¸²à¸à¸¥
            return "à¸à¸£à¸¸à¸“à¸²à¸«à¸²à¸«à¸¥à¸±à¸à¸à¸²à¸™à¸—à¸µà¹ˆà¸ªà¸­à¸”à¸„à¸¥à¹‰à¸­à¸‡à¸à¸±à¸šà¹€à¸à¸“à¸‘à¹Œà¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¸à¸²à¸£"
        
    def _get_dynamic_augmentation(self, doc_id: str, base_query_content: str) -> str:
        """
        ðŸ› ï¸ 2. à¸›à¸£à¸±à¸šà¸›à¸£à¸¸à¸‡: à¹ƒà¸Šà¹‰ Keywords à¸—à¸µà¹ˆà¹€à¸‰à¸žà¸²à¸°à¹€à¸ˆà¸²à¸°à¸ˆà¸‡à¸•à¸²à¸¡ Enabler ID (à¸£à¸­à¸‡à¸£à¸±à¸š 10 Enabler)
        """
        
        # ðŸŸ¢ à¸à¸³à¸«à¸™à¸” Keywords à¸ªà¸³à¸«à¸£à¸±à¸šà¹à¸•à¹ˆà¸¥à¸° Enabler
        ENABLER_KEYWORDS = {
            "km": [
                "à¸à¸²à¸£à¸ˆà¸±à¸”à¸à¸²à¸£à¸„à¸§à¸²à¸¡à¸£à¸¹à¹‰", "Knowledge Management", "à¸™à¹‚à¸¢à¸šà¸²à¸¢ KM", 
                "à¹à¸œà¸™à¹à¸¡à¹ˆà¸šà¸— KM", "à¸à¸²à¸£à¹à¸¥à¸à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰", "à¸à¸²à¸£à¹€à¸à¹‡à¸šà¸„à¸§à¸²à¸¡à¸£à¸¹à¹‰"
            ],
            "hcm": [
                "à¸à¸²à¸£à¸šà¸£à¸´à¸«à¸²à¸£à¸—à¸¸à¸™à¸¡à¸™à¸¸à¸©à¸¢à¹Œ", "Human Capital", "à¹à¸œà¸™à¸à¸³à¸¥à¸±à¸‡à¸„à¸™", 
                "à¸à¸²à¸£à¸ªà¸£à¸£à¸«à¸²", "à¸à¸²à¸£à¸žà¸±à¸’à¸™à¸²à¸šà¸¸à¸„à¸¥à¸²à¸à¸£", "Competency", "à¸à¸²à¸£à¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¸œà¸¥"
            ],
            "sp": [
                "à¸à¸²à¸£à¸§à¸²à¸‡à¹à¸œà¸™à¸à¸¥à¸¢à¸¸à¸—à¸˜à¹Œ", "Strategy Planning", "à¸§à¸´à¸ªà¸±à¸¢à¸—à¸±à¸¨à¸™à¹Œ", 
                "à¸žà¸±à¸™à¸˜à¸à¸´à¸ˆ", "à¹€à¸›à¹‰à¸²à¸›à¸£à¸°à¸ªà¸‡à¸„à¹Œà¸­à¸‡à¸„à¹Œà¸à¸£", "KPI", "à¹à¸œà¸™à¸›à¸à¸´à¸šà¸±à¸•à¸´à¸à¸²à¸£"
            ],
            "dt": [
                "à¹€à¸—à¸„à¹‚à¸™à¹‚à¸¥à¸¢à¸µà¸”à¸´à¸ˆà¸´à¸—à¸±à¸¥", "Digital Transformation", "IT Governance", 
                "Cyber Security", "IT Roadmap", "à¸£à¸°à¸šà¸š ERP", "à¹€à¸—à¸„à¹‚à¸™à¹‚à¸¥à¸¢à¸µà¸ªà¸²à¸£à¸ªà¸™à¹€à¸—à¸¨"
            ],
            "cg": [
                "à¸šà¸£à¸£à¸©à¸±à¸—à¸ à¸´à¸šà¸²à¸¥", "Corporate Governance", "à¸ˆà¸£à¸´à¸¢à¸˜à¸£à¸£à¸¡", 
                "à¸à¸²à¸£à¸à¸³à¸à¸±à¸šà¸”à¸¹à¹à¸¥à¸à¸´à¸ˆà¸à¸²à¸£", "à¸„à¸§à¸²à¸¡à¹‚à¸›à¸£à¹ˆà¸‡à¹ƒà¸ª", "à¸„à¸“à¸°à¸à¸£à¸£à¸¡à¸à¸²à¸£"
            ],
            "l": [
                "à¸à¸Žà¸«à¸¡à¸²à¸¢à¹à¸¥à¸°à¸à¸Žà¸£à¸°à¹€à¸šà¸µà¸¢à¸š", "Legal & Regulatory", "à¸à¸²à¸£à¸›à¸à¸´à¸šà¸±à¸•à¸´à¸•à¸²à¸¡à¸à¸Žà¸«à¸¡à¸²à¸¢", 
                "à¸‚à¹‰à¸­à¸šà¸±à¸‡à¸„à¸±à¸š", "à¸ªà¸±à¸à¸à¸²", "à¸à¸Žà¸«à¸¡à¸²à¸¢à¸”à¸´à¸ˆà¸´à¸—à¸±à¸¥"
            ],
            "rm&ic": [ # à¸„à¸‡à¹„à¸§à¹‰à¹€à¸›à¹‡à¸™à¸à¸¥à¸¸à¹ˆà¸¡ RM&IC à¸•à¸²à¸¡à¸à¸²à¸£à¸à¸³à¸«à¸™à¸”à¹€à¸”à¸´à¸¡
                "à¸šà¸£à¸´à¸«à¸²à¸£à¸„à¸§à¸²à¸¡à¹€à¸ªà¸µà¹ˆà¸¢à¸‡", "Risk Management", "à¸à¸²à¸£à¸„à¸§à¸šà¸„à¸¸à¸¡à¸ à¸²à¸¢à¹ƒà¸™", 
                "Internal Control", "à¹à¸œà¸™à¸šà¸£à¸´à¸«à¸²à¸£à¸„à¸§à¸²à¸¡à¸•à¹ˆà¸­à¹€à¸™à¸·à¹ˆà¸­à¸‡", "à¸„à¸§à¸²à¸¡à¹€à¸ªà¸µà¹ˆà¸¢à¸‡à¸­à¸‡à¸„à¹Œà¸à¸£"
            ],
            "scm": [
                "à¸à¸²à¸£à¸ˆà¸±à¸”à¸à¸²à¸£à¸«à¹ˆà¸§à¸‡à¹‚à¸‹à¹ˆà¸­à¸¸à¸›à¸—à¸²à¸™", "Supply Chain", "à¸à¸²à¸£à¸ˆà¸±à¸”à¸‹à¸·à¹‰à¸­à¸ˆà¸±à¸”à¸ˆà¹‰à¸²à¸‡", 
                "à¸šà¸£à¸´à¸«à¸²à¸£à¸„à¸¥à¸±à¸‡", "à¸œà¸¹à¹‰à¸‚à¸²à¸¢", "à¸à¸²à¸£à¸ªà¹ˆà¸‡à¸¡à¸­à¸šà¸ªà¸´à¸™à¸„à¹‰à¸²"
            ],
            "im": [
                "à¸™à¸§à¸±à¸•à¸à¸£à¸£à¸¡", "Innovation Management", "à¸à¸²à¸£à¸§à¸´à¸ˆà¸±à¸¢à¹à¸¥à¸°à¸žà¸±à¸’à¸™à¸²", 
                "R&D", "à¸—à¸£à¸±à¸žà¸¢à¹Œà¸ªà¸´à¸™à¸—à¸²à¸‡à¸›à¸±à¸à¸à¸²", "à¸à¸²à¸£à¸ªà¸£à¹‰à¸²à¸‡à¸¡à¸¹à¸¥à¸„à¹ˆà¸²"
            ],
            "ia": [
                "à¸à¸²à¸£à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸ à¸²à¸¢à¹ƒà¸™", "Internal Audit", "à¸à¸²à¸£à¸ªà¸­à¸šà¸—à¸²à¸™", 
                "à¸œà¸¥à¸à¸²à¸£à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š", "à¸£à¸²à¸¢à¸‡à¸²à¸™à¸à¸²à¸£à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š", "à¸à¸²à¸£à¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¸„à¸§à¸²à¸¡à¹€à¸žà¸µà¸¢à¸‡à¸žà¸­"
            ],
        }
        
        enabler_key = self.enabler_id.lower()
        
        if enabler_key not in ENABLER_KEYWORDS:
             logger.warning(f"âš ï¸ Warning: Enabler '{enabler_key.upper()}' not defined, using KM as default.")
             keywords = ENABLER_KEYWORDS.get("km")
        else:
             keywords = ENABLER_KEYWORDS[enabler_key]
        
        logger.warning(f"âš ï¸ Warning: Using {enabler_key.upper()}-specific keywords for augmentation.")
        
        # ðŸŸ¢ à¸ªà¸£à¹‰à¸²à¸‡ String à¸ªà¸³à¸«à¸£à¸±à¸š Augmentation
        return (
            f"à¸«à¸¥à¸±à¸à¸à¸²à¸™à¸—à¸µà¹ˆà¹€à¸à¸µà¹ˆà¸¢à¸§à¸‚à¹‰à¸­à¸‡à¹‚à¸”à¸¢à¸•à¸£à¸‡à¸à¸±à¸šà¸à¸²à¸£à¸”à¸³à¹€à¸™à¸´à¸™à¸‡à¸²à¸™ '{self.enabler_id.upper()}' "
            f"à¸«à¸£à¸·à¸­ '{self.enabler_id.upper()}' ({', '.join(keywords)})"
        )
        
    def _get_statement_detail(self, content: str) -> Optional[Dict[str, str]]:
        # à¹‚à¸„à¹‰à¸”à¸ªà¹ˆà¸§à¸™à¸™à¸µà¹‰à¸¢à¸±à¸‡à¸„à¸‡à¹ƒà¸Šà¹‰ statement_data à¸—à¸µà¹ˆà¹‚à¸«à¸¥à¸”à¸¡à¸²à¸ˆà¸²à¸ JSON à¹€à¸žà¸·à¹ˆà¸­à¸”à¸¶à¸‡à¸£à¸²à¸¢à¸¥à¸°à¹€à¸­à¸µà¸¢à¸”
        for enabler_block in self.statement_data:
            sub_criteria_id = enabler_block.get("Sub_Criteria_ID")
            for i in range(1, 6):
                level_key = f"Level_{i}_Statements"
                statements_list = enabler_block.get(level_key, [])
                for j, statement_text in enumerate(statements_list):
                    clean_content = content.strip().lower()
                    clean_statement = statement_text.strip().lower()
                    
                    # 1. à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¹à¸šà¸šà¸¡à¸µà¹€à¸¥à¸‚à¸™à¸³à¸«à¸™à¹‰à¸²
                    if clean_statement and clean_statement[0].isdigit():
                        clean_statement_no_num = clean_statement.split(maxsplit=1)[-1].strip()
                        if clean_content == clean_statement_no_num.lower():
                            return {
                                "statement_key": f"{sub_criteria_id}_L{i}_{j + 1}",
                                "sub_level": f"{sub_criteria_id} Level {i} Statement {j + 1}",
                                "statement_text": statement_text
                            }
                            
                    # 2. à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¹à¸šà¸šà¹„à¸¡à¹ˆà¸¡à¸µà¹€à¸¥à¸‚à¸™à¸³à¸«à¸™à¹‰à¸²
                    if clean_content == clean_statement:
                        return {
                            "statement_key": f"{sub_criteria_id}_L{i}_{j + 1}",
                            "sub_level": f"{sub_criteria_id} Level {i} Statement {j + 1}",
                            "statement_text": statement_text
                        }
        return None

    def process_and_suggest_mapping(self,
                                    file_path: str,
                                    doc_id: Optional[str] = None,
                                    level: Optional[int] = None, # ðŸ‘ˆ à¸£à¸±à¸šà¸„à¹ˆà¸² Level à¹€à¸‚à¹‰à¸²à¸¡à¸²
                                    top_k_statements: int = FINAL_K_RERANKED,
                                    similarity_threshold: float = 0.9900,
                                    suggestion_limit: int = 3) -> List[Dict[str, Any]]:
        """
        Return: List of suggested mappings (JSON serializable)
        """
        effective_doc_id = doc_id if doc_id is not None else os.path.splitext(os.path.basename(file_path))[0]
        
        docs = load_and_chunk_document(file_path=file_path, doc_id=effective_doc_id)
        if not docs:
            logger.error(f"âŒ Failed to load or chunk the file: {file_path}")
            return []

        base_query = docs[0].page_content
        
        logger.info(f"Loaded and chunked {file_path} -> {len(docs)} chunks.")
        logger.info(f"Primary Chunk Content (Base Query): \n---START---\n{base_query}\n---END---")
        
        if len(re.sub(r'[\d\s\W]', '', base_query)) < len(base_query) * 0.1:
             logger.warning(f"âš ï¸ Warning: Base Query chunk from {effective_doc_id} appears to be mostly noise/numbers.")

        # ===================================================================================================
        # FINAL/BALANCED MODE: à¹ƒà¸Šà¹‰ Dynamic Augmentation (Enabler-specific Keywords) à¹à¸¥à¸° Level Constraint
        # ===================================================================================================
        
        # ðŸŸ¢ 1. à¸ªà¸£à¹‰à¸²à¸‡ Enabler-specific Augmentation
        augmentation_keywords = self._get_dynamic_augmentation(effective_doc_id, base_query)
        
        # ðŸŸ¢ 2. à¸ªà¸£à¹‰à¸²à¸‡ Level Constraint (à¸–à¹‰à¸² Level à¸–à¸¹à¸à¸£à¸°à¸šà¸¸)
        level_constraint = ""
        if level is not None and 1 <= level <= 5:
            level_constraint = self._get_level_constraint_prompt(level)
            logger.info(f"Applying Level Constraint: {level_constraint}")
        
        # ðŸŸ¢ 3. à¸£à¸§à¸¡ Prompt à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”
        instruction_prompt = (
            f"à¹‚à¸›à¸£à¸”à¸ˆà¸±à¸”à¸­à¸±à¸™à¸”à¸±à¸šà¸„à¸§à¸²à¸¡à¹€à¸à¸µà¹ˆà¸¢à¸§à¸‚à¹‰à¸­à¸‡à¸‚à¸­à¸‡à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡ Statement à¹€à¸«à¸¥à¹ˆà¸²à¸™à¸µà¹‰à¸à¸±à¸šà¸«à¸¥à¸±à¸à¸à¸²à¸™à¸—à¸µà¹ˆà¹à¸ªà¸”à¸‡à¸–à¸¶à¸‡ ({augmentation_keywords}). "
            f"à¹‚à¸”à¸¢à¸žà¸´à¸ˆà¸²à¸£à¸“à¸²à¸§à¹ˆà¸²à¸«à¸¥à¸±à¸à¸à¸²à¸™à¸™à¸µà¹‰à¸¡à¸µà¸„à¸§à¸²à¸¡à¸Šà¸±à¸”à¹€à¸ˆà¸™à¹ƒà¸™à¸à¸²à¸£à¸ªà¸™à¸±à¸šà¸ªà¸™à¸¸à¸™à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡ Statement à¹ƒà¸™à¸”à¹‰à¸²à¸™ '{self.enabler_id.upper()}' à¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆ. "
            f"{level_constraint}"
            f"à¸«à¸¥à¸±à¸à¸à¸²à¸™: "
        )
        
        query = f"{instruction_prompt}{base_query[:1000]}"
        logger.info(f"Using {self.enabler_id.upper()} Augmented Query for RAG: '{query[:120]}...'")

        # ===================================================================================================
        
        # ðŸ“Œ Note: self.statement_retriever.invoke(query) à¸ˆà¸°à¸—à¸³à¸à¸²à¸£ Rerank à¸ à¸²à¸¢à¹ƒà¸™
        retrieved_statements: List[LcDocument] = self.statement_retriever.invoke(query)

        suggested_mappings = []
        for i, doc in enumerate(retrieved_statements):
            score = float(doc.metadata.get('relevance_score', 0.0))
            if score < similarity_threshold:
                # à¸«à¸¢à¸¸à¸”à¹€à¸¡à¸·à¹ˆà¸­à¸„à¸°à¹à¸™à¸™à¸•à¹ˆà¸³à¸à¸§à¹ˆà¸² Threshold
                break
            
            # à¸”à¸¶à¸‡à¸£à¸²à¸¢à¸¥à¸°à¹€à¸­à¸µà¸¢à¸” Statement
            details = self._get_statement_detail(doc.page_content)
            
            if details:
                # à¸™à¸±à¸šà¹€à¸‰à¸žà¸²à¸° suggestion à¸—à¸µà¹ˆà¸œà¹ˆà¸²à¸™ threshold
                if len(suggested_mappings) < suggestion_limit:
                    suggested_mappings.append({
                        "suggestion_rank": i + 1,
                        "score": score,
                        "statement_key": details["statement_key"],
                        "sub_level": details["sub_level"],
                        "statement_text": details["statement_text"],
                        "justification": f"à¸„à¸§à¸²à¸¡à¸„à¸¥à¹‰à¸²à¸¢à¸—à¸²à¸‡à¸„à¸§à¸²à¸¡à¸«à¸¡à¸²à¸¢à¸ªà¸¹à¸‡: à¹€à¸™à¸·à¹‰à¸­à¸«à¸²à¸«à¸¥à¸±à¸à¸à¸²à¸™à¹€à¸à¸µà¹ˆà¸¢à¸§à¸‚à¹‰à¸­à¸‡à¸à¸±à¸šà¸«à¸±à¸§à¸‚à¹‰à¸­: '{details['sub_level']}'"
                    })

        return suggested_mappings

    def process_directory(self,
                          directory: str,
                          output_file: str = "results/merged_results.json",
                          level: Optional[int] = None, # ðŸ‘ˆ à¸£à¸±à¸šà¸„à¹ˆà¸² Level à¹€à¸‚à¹‰à¸²à¸¡à¸²
                          top_k: int = 7,
                          threshold: float = 0.9900,
                          suggestion_limit: int = 3):
        """
        Process all supported files in a directory and save merged results into a single JSON
        """
        os.makedirs(os.path.dirname(output_file), exist_ok=True)

        merged_results = {}
        # ðŸ› ï¸ à¸›à¸£à¸±à¸šà¸›à¸£à¸¸à¸‡: à¹€à¸žà¸´à¹ˆà¸¡ .jpg, .jpeg, .png à¹ƒà¸™à¸£à¸²à¸¢à¸à¸²à¸£à¸—à¸µà¹ˆà¸£à¸­à¸‡à¸£à¸±à¸š
        SUPPORTED_EXTENSIONS = [".pdf", ".docx", ".txt", ".xlsx", ".pptx", ".md", ".csv", ".jpg", ".jpeg", ".png"]

        for filename in os.listdir(directory):
            ext = os.path.splitext(filename)[1].lower()
            
            # ðŸ’¡ Skip directories and hidden files/temp files
            file_path = os.path.join(directory, filename)
            if os.path.isdir(file_path) or filename.startswith('.'):
                 continue

            if ext not in SUPPORTED_EXTENSIONS:
                logger.info(f"Skipping unsupported file type: {filename}")
                continue
            
            doc_id = os.path.splitext(filename)[0]
            
            logger.info(f"\n==================================================")
            logger.info(f"ðŸš€ STARTING PROCESSING: {filename}")
            logger.info(f"==================================================")

            try:
                suggested = self.process_and_suggest_mapping(
                    file_path=file_path,
                    doc_id=doc_id,
                    level=level, # ðŸ‘ˆ à¸ªà¹ˆà¸‡ Level à¹€à¸‚à¹‰à¸²à¹„à¸›
                    top_k_statements=top_k,
                    similarity_threshold=threshold,
                    suggestion_limit=suggestion_limit
                )
                merged_results[doc_id] = suggested
            except Exception as e:
                logger.error(f"âŒ Failed to process {filename}: {e}")
                merged_results[doc_id] = {"error": str(e)}

        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(merged_results, f, ensure_ascii=False, indent=2)
        logger.info(f"âœ… Merged results saved to {output_file}")


def main():
    parser = argparse.ArgumentParser(description="Evidence Mapping Generator CLI")
    # ðŸ› ï¸ 3. à¸›à¸£à¸±à¸šà¸›à¸£à¸¸à¸‡: à¹€à¸žà¸´à¹ˆà¸¡ choices à¸•à¸²à¸¡à¸£à¸²à¸¢à¸à¸²à¸£ Enabler à¸—à¸µà¹ˆà¸à¸³à¸«à¸™à¸”
    parser.add_argument("--enabler", 
                            type=str, 
                            default="KM",
                            choices=["CG", "L", "SP", "RM&IC", "SCM", "DT", "HCM", "KM", "IM", "IA"],
                            help="The core business enabler abbreviation (e.g., 'KM', 'SCM').")
    
    # ðŸ“Œ 4. à¹€à¸žà¸´à¹ˆà¸¡ Argument à¸ªà¸³à¸«à¸£à¸±à¸š Level à¹€à¸žà¸·à¹ˆà¸­à¹ƒà¸Šà¹‰à¹ƒà¸™ Constraint Prompt
    parser.add_argument("--level", 
                            type=int, 
                            choices=[1, 2, 3, 4, 5], 
                            default=None, 
                            help="Maturity Level constraint (1-5). Used to filter out higher level statements.")
                            
    parser.add_argument("--file_path", type=str, help="Path to a single evidence file")
    
    parser.add_argument("--doc_id", type=str, help="Optional Document ID to override the filename. (e.g., KM1.1L106)")
    
    parser.add_argument("--directory", type=str, help="Directory to process all evidence files")
    parser.add_argument("--output_file", type=str, default="results/merged_results.json", help="Output JSON file path")
    parser.add_argument("--top_k", type=int, default=7, help="Top K statements to retrieve")
    parser.add_argument("--threshold", type=float, default=0.9900, help="Similarity threshold") 
    parser.add_argument("--suggestion_limit", type=int, default=3, help="Max number of suggestions") 
    args = parser.parse_args()

    try:
        generator = EvidenceMappingGenerator(enabler_id=args.enabler)
    except FileNotFoundError as e:
        print(f"âŒ Error during initialization: {e}")
        return
    except Exception as e:
         print(f"âŒ General Error during initialization: {e}")
         return

    if args.file_path:
        # Process single file
        result = generator.process_and_suggest_mapping(
            file_path=args.file_path,
            doc_id=args.doc_id, 
            level=args.level, # ðŸ‘ˆ à¸ªà¹ˆà¸‡ Level à¹€à¸‚à¹‰à¸²à¹„à¸›
            top_k_statements=args.top_k,
            similarity_threshold=args.threshold,
            suggestion_limit=args.suggestion_limit
        )
        if isinstance(result, list) and result:
             print("================================================================================")
             print(f"âœ… Suggested Mappings for Evidence File '{os.path.basename(args.file_path)}' ({args.enabler}) [Level Filter: {args.level or 'None'}]:")
             print("================================================================================")
             for i, suggestion in enumerate(result):
                print(f"--- Suggestion {i + 1} (Score: {suggestion['score']:.4f}) ---")
                print(f"  Statement Key:   {suggestion['statement_key']}")
                print(f"  Sub/Level:       {suggestion['sub_level']}")
                print(f"  Statement Text:  {suggestion['statement_text']}")
                print(f"  Justification:   {suggestion['justification']}")
                print("----------------------------------------")
             print(f"Found {len(result)} suggestions (filtered by top {args.top_k}).")
        else:
             print("================================================================================")
             print(f"âœ… Suggested Mappings for Evidence File '{os.path.basename(args.file_path)}' ({args.enabler}) [Level Filter: {args.level or 'None'}]:")
             print("================================================================================")
             print("âŒ No suggested mappings found above the threshold, or an error occurred.")
    elif args.directory:
        # Process all files in directory
        generator.process_directory(
            directory=args.directory,
            output_file=args.output_file,
            level=args.level, # ðŸ‘ˆ à¸ªà¹ˆà¸‡ Level à¹€à¸‚à¹‰à¸²à¹„à¸›
            top_k=args.top_k,
            threshold=args.threshold,
            suggestion_limit=args.suggestion_limit
        )
    else:
        print("âŒ Please provide either --file_path or --directory")
        sys.exit(1)


if __name__ == "__main__":
    main()
