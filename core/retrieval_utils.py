# -------------------- core/retrieval_utils.py (FINAL FIXED VERSION) --------------------
import logging
import random 
import json   
from typing import List, Dict, Any, Optional, Union
# from langchain.schema import Document # NOTE: ‡∏ñ‡∏π‡∏Å‡∏¢‡∏Å‡πÑ‡∏õ‡∏£‡∏ß‡∏°‡πÉ‡∏ô‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡πâ‡∏ß

# NOTE: ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á VectorStoreManager, load_all_vectorstores ‡πÅ‡∏•‡∏∞ RAG Prompts ‡πÑ‡∏î‡πâ
from core.vectorstore import VectorStoreManager, load_all_vectorstores 
from core.rag_prompts import SYSTEM_ASSESSMENT_PROMPT 

# Import LLM Instance Explicitly to avoid module name conflict
from models.llm import llm as llm_instance 
from langchain.schema import SystemMessage, HumanMessage # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LLM Call
from langchain.schema import Document # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£ Import Document ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÇ‡∏Ñ‡πâ‡∏î‡∏™‡πà‡∏ß‡∏ô Retrieval ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.DEBUG)


# =================================================================
# === MOCKING LOGIC AND GLOBAL FLAGS ===
# =================================================================

_MOCK_CONTROL_FLAG = False
_MOCK_COUNTER = 0

def set_mock_control_mode(enable: bool):
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡∏¥‡∏î/‡∏õ‡∏¥‡∏î‡πÇ‡∏´‡∏°‡∏î‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô (CONTROLLED MOCK)"""
    global _MOCK_CONTROL_FLAG, _MOCK_COUNTER
    _MOCK_CONTROL_FLAG = enable
    if enable:
        # ‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï‡∏ï‡∏±‡∏ß‡∏ô‡∏±‡∏ö‡πÄ‡∏™‡∏°‡∏≠‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏¥‡∏î‡πÇ‡∏´‡∏°‡∏î‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°
        _MOCK_COUNTER = 0
        logger.info("üîë CONTROLLED MOCK Mode ENABLED. Score will be 1 for first 5 statements, then 0.")
    else:
        logger.info("‚ùå CONTROLLED MOCK Mode DISABLED.")


# =================================================================
# === RETRIEVAL FUNCTIONS ===
# NOTE: ‡πÇ‡∏Ñ‡πâ‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏´‡∏•‡∏±‡∏Å‡πÜ ‡πÅ‡∏ï‡πà‡∏ñ‡∏π‡∏Å‡∏£‡∏ß‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå
# =================================================================

def retrieve_statements(statements: List[str], doc_id: Optional[str] = None) -> Dict[str, List[Document]]:
    """
    Retrieve documents ‡∏à‡∏≤‡∏Å vectorstore ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö list ‡∏Ç‡∏≠‡∏á statements
    """
    vs_manager = VectorStoreManager()
    retriever = vs_manager.get_retriever(k=5)
    if retriever is None:
        logger.error("Retriever not initialized.")
        return {stmt: [] for stmt in statements}

    results: Dict[str, List[Document]] = {}
    for stmt in statements:
        try:
            # NOTE: ‡πÉ‡∏ä‡πâ retriever.get_relevant_documents
            docs = retriever.get_relevant_documents(stmt)
            if not docs:
                logger.warning(f"‚ö†Ô∏è No results found for statement: {stmt[:50]}...")
            results[stmt] = docs
        except Exception as e:
            logger.error(f"Retrieval failed for statement: {stmt[:50]}... Error: {e}")
            results[stmt] = []
    return results


def retrieve_context(statement: str,
                     doc_ids: Optional[List[str]] = None,
                     doc_type: str = "document",
                     top_k: int = 10,
                     final_k: int = 3) -> Dict[str, Any]:
    """
    üîç Retrieve top evidences for a given KM statement.
    """
    try:
        retriever = load_all_vectorstores(
            doc_ids=doc_ids,
            top_k=top_k,
            final_k=final_k,
            doc_type=doc_type
        )
        # NOTE: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å get_relevant_documents
        docs: List[Document] = retriever.get_relevant_documents(statement) if retriever else []

        results = []
        for d in docs:
            meta = d.metadata
            # NOTE: ‡∏Ñ‡∏ß‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ metadata ‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏Å .get()
            results.append({
                "doc_id": meta.get("doc_id"),
                "doc_type": meta.get("doc_type"),
                "chunk_index": meta.get("chunk_index"),
                "score": meta.get("relevance_score", None),
                "source": meta.get("source"),
                "content": d.page_content.strip()
            })

        logger.debug(f"‚úÖ Found {len(results)} context items for statement: '{statement[:60]}...'")
        return {"statement": statement, "context_count": len(results), "top_evidences": results}

    except Exception as e:
        logger.error(f"‚ö†Ô∏è Retrieval failed for statement='{statement[:60]}...': {e}")
        return {"statement": statement, "context_count": 0, "top_evidences": [], "error": str(e)}


def batch_retrieve_from_checklist(checklist_json_path: str, doc_type: str = "km_document") -> List[Dict[str, Any]]:
    """
    Loop ‡∏ú‡πà‡∏≤‡∏ô checklist JSON ‡πÅ‡∏•‡πâ‡∏ß retrieve context ‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö‡∏ó‡∏∏‡∏Å statement
    """
    # NOTE: ‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á Import json ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏´‡∏≤‡∏Å‡∏£‡∏±‡∏ô‡πÅ‡∏¢‡∏Å ‡πÅ‡∏ï‡πà‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å import ‡πÑ‡∏ß‡πâ module level ‡πÅ‡∏•‡πâ‡∏ß ‡∏à‡∏∂‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤
    try:
        with open(checklist_json_path, "r", encoding="utf-8") as f:
            checklist = json.load(f)
    except Exception as e:
        logger.error(f"Failed to load checklist JSON: {e}")
        return []

    results = []
    for enabler in checklist:
        for level in range(1, 6):
            level_key = f"Level_{level}_Statements"
            statements: List[str] = enabler.get(level_key, [])
            
            for stmt in statements:
                result = retrieve_context(stmt, doc_type=doc_type)
                results.append({
                    "enabler_id": enabler.get("Enabler_ID"),
                    "sub_criteria_id": enabler.get("Sub_Criteria_ID"),
                    "level": level,
                    **result
                })
    return results


# =================================================================
# === EVALUATION FUNCTION (MOCK & REAL LLM) ===
# =================================================================

def evaluate_with_llm(statement: str, context: str, standard: str) -> Dict[str, Any]:
    """
    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ LLM ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á
    """
    global _MOCK_CONTROL_FLAG, _MOCK_COUNTER
    
    # 1. MOCK CONTROL LOGIC
    if _MOCK_CONTROL_FLAG:
        _MOCK_COUNTER += 1
        
        # Logic: Pass L1, Partially Pass L2 (3 Pass, 2 Pass, 1 Fail)
        if _MOCK_COUNTER <= 3: # L1 Statements
            score = 1
            reason_text = f"MOCK: FORCED PASS (L1)"
        elif _MOCK_COUNTER in [4, 5]: # L2 Statements (2/3 Pass)
            score = 1
            reason_text = f"MOCK: FORCED PASS (L2)"
        else: # L2 S3 and all L3-L5 (Counter > 5)
            score = 0
            reason_text = f"MOCK: FORCED FAIL (L2+)"

        logger.debug(f"MOCK COUNT: {_MOCK_COUNTER} | SCORE: {score} | STMT: '{statement[:20]}...'")
        return {"score": score, "reason": reason_text}

    
    # 2. REAL LLM CALL LOGIC

    if llm_instance is None:
        logger.error("‚ùå LLM Instance is not initialized (likely failed to connect to Ollama).")
        score = random.choice([0, 1])
        reason = f"LLM Initialization Failed (Fallback to Random Score {score})"
        return {"score": score, "reason": reason}

    # 1. ‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö User Prompt (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LLM)
    user_prompt = f"""
    --- Statement ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô (‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡∏°‡∏µ) ---
    {statement}

    --- ‡πÄ‡∏Å‡∏ì‡∏ë‡πå (Standard/Rubric) ---
    {standard}

    --- ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏û‡∏ö‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏à‡∏£‡∏¥‡∏á (Context ‡∏à‡∏≤‡∏Å Semantic Search) ---
    {context if context else "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á"}

    --- ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á ---
    ‡πÇ‡∏õ‡∏£‡∏î‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó Se-AM Consultant ‡∏ß‡πà‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏û‡∏ö (Context) ‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á 
    ‡∏Å‡∏±‡∏ö Statement ‡πÅ‡∏•‡∏∞‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    
    ‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏≠‡∏ö‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö JSON ‡∏ó‡∏µ‡πà‡∏°‡∏µ key: 'score' (0 ‡∏´‡∏£‡∏∑‡∏≠ 1) ‡πÅ‡∏•‡∏∞ 'reason' ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô!
    ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: {{"score": 1, "reason": "‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô X ‡πÉ‡∏ô Context ‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô Statement Y..."}}
    """
    
    try:
        response = llm_instance.invoke([
            SystemMessage(content=SYSTEM_ASSESSMENT_PROMPT),
            HumanMessage(content=user_prompt)
        ])
        
        llm_response_content = response.content if hasattr(response, 'content') else str(response)

        # 2. Parse JSON string ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å LLM (Robust Parsing)
        if llm_response_content.strip().startswith("```json"):
            llm_response_content = llm_response_content.strip().replace("```json", "").replace("```", "")
            
        llm_output = json.loads(llm_response_content.strip())
        
        score = int(llm_output.get("score", 0)) 
        reason = llm_output.get("reason", "No reason provided by LLM.")
        
        return {"score": score, "reason": reason}

    except Exception as e:
        logger.error(f"‚ùå LLM Evaluation failed. Using RANDOM SCORE as fallback. Error: {e}")
        score = random.choice([0, 1])
        reason = f"LLM Call Failed (Fallback to Random Score {score}): {str(e)}"
        return {"score": score, "reason": reason}