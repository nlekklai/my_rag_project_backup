import logging
import random 
import json   
import time # <--- ‡πÄ‡∏û‡∏¥‡πà‡∏° import time ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà
from typing import List, Dict, Any, Optional, Union

# ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤ vectorstore ‡πÅ‡∏•‡∏∞ rag_prompts ‡∏ñ‡∏π‡∏Å import ‡πÑ‡∏î‡πâ
from core.vectorstore import VectorStoreManager, load_all_vectorstores 
from core.rag_prompts import SYSTEM_ASSESSMENT_PROMPT 

# Import LLM Instance Explicitly to avoid module name conflict
from models.llm import llm as llm_instance 
from langchain.schema import SystemMessage, HumanMessage 
from langchain.schema import Document 

logger = logging.getLogger(__name__)
# ‡∏õ‡∏£‡∏±‡∏ö level ‡πÄ‡∏õ‡πá‡∏ô DEBUG ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')


# =================================================================
# === MOCKING LOGIC AND GLOBAL FLAGS ===
# =================================================================

_MOCK_CONTROL_FLAG = False
_MOCK_COUNTER = 0

def set_mock_control_mode(enable: bool):
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡∏¥‡∏î/‡∏õ‡∏¥‡∏î‡πÇ‡∏´‡∏°‡∏î‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô (CONTROLLED MOCK)"""
    global _MOCK_CONTROL_FLAG, _MOCK_COUNTER
    _MOCK_CONTROL_FLAG = enable
    if enable:
        _MOCK_COUNTER = 0
        logger.info("üîë CONTROLLED MOCK Mode ENABLED. Score will be 1 for first 5 statements, then 0.")
    else:
        logger.info("‚ùå CONTROLLED MOCK Mode DISABLED.")


# =================================================================
# === RETRIEVAL FUNCTIONS (INCLUDING THE NEW FILTER FUNCTION) ===
# =================================================================

def retrieve_statements(statements: List[str], doc_id: Optional[str] = None) -> Dict[str, List[Document]]:
    """
    Retrieve documents ‡∏à‡∏≤‡∏Å vectorstore ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö list ‡∏Ç‡∏≠‡∏á statements
    """
    vs_manager = VectorStoreManager()
    retriever = vs_manager.get_retriever(k=5) 
    if retriever is None:
        logger.error("Retriever not initialized.")
        return {stmt: [] for stmt in statements}

    results: Dict[str, List[Document]] = {}
    for stmt in statements:
        try:
            # NOTE: ‡πÉ‡∏ä‡πâ retriever.get_relevant_documents
            docs = retriever.invoke(stmt) 
            if not docs:
                logger.warning(f"‚ö†Ô∏è No results found for statement: {stmt[:50]}...")
            results[stmt] = docs
        except Exception as e:
            logger.error(f"Retrieval failed for statement: {stmt[:50]}... Error: {e}")
            results[stmt] = []
    return results


# üö® FIXED FUNCTION: retrieve_context_with_filter
def retrieve_context_with_filter(
    query: str, 
    retriever: Any, 
    # üö® CRITICAL FIX: ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ä‡∏∑‡πà‡∏≠ Argument ‡∏ô‡∏µ‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ô enabler_assessment.py
    metadata_filter: Optional[List[str]] = None, 
) -> Dict[str, Any]:
    """
    Retrieval function ‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ (Document ID Filter)
    ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ retriever ‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß
    """
    if retriever is None:
        return {"top_evidences": []}
    
    # 1. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô)
    filter_document_ids = metadata_filter 

    # 2. Log ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á
    if filter_document_ids:
        logger.debug(f"RAG Filter Applied: {len(filter_document_ids)} documents for query: '{query[:30]}...'")

    try:
        # 3. ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ Search (‡πÉ‡∏ä‡πâ .invoke() ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ô‡πÑ‡∏î‡πâ)
        docs: List[Document] = retriever.invoke(query) 
        
        # 4. ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏î‡πâ‡∏ß‡∏¢‡∏°‡∏∑‡∏≠ (Manual Filtering)
        # ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å filter_document_ids (‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå) ‡∏ñ‡∏π‡∏Å‡∏™‡πà‡∏á‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß ‡∏à‡∏∂‡∏á‡∏Å‡∏£‡∏≠‡∏á‡∏ï‡∏≤‡∏° doc_id ‡πÉ‡∏ô metadata
        if filter_document_ids:
            logger.debug(f"Filter List Length: {len(filter_document_ids)}") 

            filtered_docs = []
            for doc in docs:
                doc_id_in_metadata = doc.metadata.get("doc_id") # ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤ doc_id

                # ‡πÇ‡∏Ñ‡πâ‡∏î‡∏Å‡∏£‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å: ‡∏ñ‡πâ‡∏≤ doc_id ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï ‡πÉ‡∏´‡πâ‡∏ô‡∏≥‡πÑ‡∏õ‡πÉ‡∏ä‡πâ
                if doc_id_in_metadata in filter_document_ids:
                    filtered_docs.append(doc)
                # üí° Debug Logs:
                # elif doc_id_in_metadata is None:
                #     logger.debug(f"Document missing 'doc_id' key. Source: {doc.metadata.get('source')}")
                # else:
                #     logger.debug(f"Doc ID mismatch: Metadata='{doc_id_in_metadata}' not in Filter.")
            
            docs = filtered_docs
            logger.debug(f"Found {len(docs)} documents after manual filtering.")
        
        # 5. ‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        top_evidences = []
        for d in docs:
            meta = d.metadata
            top_evidences.append({
                "doc_id": meta.get("doc_id"),
                "source": meta.get("source"),
                "content": d.page_content.strip()
            })
            
        return {"top_evidences": top_evidences}
        
    except Exception as e:
        logger.error(f"Error during RAG retrieval with filter: {e}")
        return {"top_evidences": []}


# =================================================================
# === EVALUATION FUNCTION (MOCK & REAL LLM) ===
# =================================================================

MAX_LLM_RETRIES = 3 

def evaluate_with_llm(statement: str, context: str, standard: str) -> Dict[str, Any]:
    """
    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ LLM ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á (‡πÄ‡∏û‡∏¥‡πà‡∏° Logic Retry)
    """
    global _MOCK_CONTROL_FLAG, _MOCK_COUNTER
    
    # 1. MOCK CONTROL LOGIC (‡πÄ‡∏î‡∏¥‡∏°)
    if _MOCK_CONTROL_FLAG:
        # ... (‡πÇ‡∏Ñ‡πâ‡∏î MOCK ‡πÄ‡∏î‡∏¥‡∏°) ...
        # ...
        logger.debug(f"MOCK COUNT: {_MOCK_COUNTER} | SCORE: {score} | STMT: '{statement[:20]}...'")
        return {"score": score, "reason": reason_text}

    
    # 2. REAL LLM CALL LOGIC
    if llm_instance is None:
        logger.error("‚ùå LLM Instance is not initialized (likely failed to connect to Ollama).")
        score = random.choice([0, 1])
        reason = f"LLM Initialization Failed (Fallback to Random Score {score})"
        return {"score": score, "reason": reason}

    # 1. ‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö User Prompt (‡πÄ‡∏î‡∏¥‡∏°)
    # ... (user_prompt remains the same) ...
    user_prompt = f"""
    --- Statement ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô (‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡∏°‡∏µ) ---
    {statement}
    
    --- ‡πÄ‡∏Å‡∏ì‡∏ë‡πå (Standard/Rubric) ---
    {standard}
    
    --- ‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏û‡∏ö‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏à‡∏£‡∏¥‡∏á (Context ‡∏à‡∏≤‡∏Å Semantic Search) ---
    {context if context else "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á"}
    
    --- ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á ---
    ‡πÇ‡∏õ‡∏£‡∏î‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó Se-AM Consultant ‡∏ß‡πà‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏û‡∏ö (Context) ‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á 
    ‡∏Å‡∏±‡∏ö Statement ‡πÅ‡∏•‡∏∞‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    
    ‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏≠‡∏ö‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö JSON ‡∏ó‡∏µ‡πà‡∏°‡∏µ key: 'score' (0 ‡∏´‡∏£‡∏∑‡∏≠ 1) ‡πÅ‡∏•‡∏∞ 'reason' ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô!
    ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: {{"score": 1, "reason": "‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô X ‡πÉ‡∏ô Context ‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô Statement Y..."}}
    """
    
    # === NEW RETRY LOOP ===
    for attempt in range(MAX_LLM_RETRIES):
        try:
            # A. ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ LLM
            response = llm_instance.invoke([
                SystemMessage(content=SYSTEM_ASSESSMENT_PROMPT),
                HumanMessage(content=user_prompt)
            ])
            
            llm_response_content = response.content if hasattr(response, 'content') else str(response)

            # B. Parse JSON string ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å LLM
            if llm_response_content.strip().startswith("```json"):
                llm_response_content = llm_response_content.strip().replace("```json", "").replace("```", "")
                
            llm_output = json.loads(llm_response_content.strip()) # üö® ‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏î Error
            
            score = int(llm_output.get("score", 0)) 
            reason = llm_output.get("reason", "No reason provided by LLM.")
            
            # ‡∏ñ‡πâ‡∏≤‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: RETURN ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ (‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏•‡∏≠‡∏á‡∏ã‡πâ‡∏≥)
            return {"score": score, "reason": reason}

        except json.JSONDecodeError as e:
            # C. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ JSON Error
            if attempt < MAX_LLM_RETRIES - 1:
                logger.warning(f"‚ùå JSON Parse Failed (Attempt {attempt + 1}/{MAX_LLM_RETRIES}). Retrying in 1s... Error: {e}")
                time.sleep(1) # ‡∏´‡∏ô‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡πà‡∏≠‡∏ô‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà
                continue
            else:
                # ‡∏ñ‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏£‡∏≠‡∏ö‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡πÅ‡∏•‡πâ‡∏ß ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏õ‡πÉ‡∏ä‡πâ Fallback
                logger.error(f"‚ùå LLM Evaluation failed after {MAX_LLM_RETRIES} attempts. JSON format failure.")
                break # ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å Loop
        
        except Exception as e:
            # D. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Error ‡∏≠‡∏∑‡πà‡∏ô‡πÜ (‡πÄ‡∏ä‡πà‡∏ô Connection Error)
            logger.error(f"‚ùå LLM Evaluation failed due to unexpected error. Error: {e}")
            break # ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å Loop ‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ Fallback ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ

    # === FALLBACK LOGIC (‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠ Loop ‡∏à‡∏ö‡∏î‡πâ‡∏ß‡∏¢ Break) ===
    logger.error("‚ùå Using RANDOM SCORE as final fallback.")
    score = random.choice([0, 1])
    reason = f"LLM Call Failed (Fallback to Random Score {score}) after {MAX_LLM_RETRIES} attempts."
    return {"score": score, "reason": reason}