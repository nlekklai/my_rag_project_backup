import logging
import random
import json
import time 
from typing import List, Dict, Any, Optional, Union
from langchain.schema import SystemMessage, HumanMessage 
from langchain.schema import Document 

# üö® IMPORT: ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤ Regex (‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ô‡∏µ‡πâ)
import re 
# üö® IMPORT: ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤ Pydantic Model ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå action_plan_schema.py
from core.action_plan_schema import ActionPlanActions 
# üö® IMPORT: ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤ Prompts
from core.rag_prompts import (
    SYSTEM_ASSESSMENT_PROMPT, 
    USER_ASSESSMENT_PROMPT, 
    ACTION_PLAN_PROMPT,
    SYSTEM_ACTION_PLAN_PROMPT,
    SYSTEM_EVIDENCE_DESCRIPTION_PROMPT, 
    EVIDENCE_DESCRIPTION_PROMPT         
) 
from core.vectorstore import VectorStoreManager, load_all_vectorstores 
from models.llm import llm as llm_instance 

logger = logging.getLogger(__name__)
# ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏Ñ‡∏∏‡∏ì‡∏£‡∏±‡∏ô‡πÉ‡∏ô‡πÇ‡∏´‡∏°‡∏î real ‡πÅ‡∏•‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ debug/info ‡∏à‡∏∂‡∏á‡πÉ‡∏ä‡πâ level INFO
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


# =================================================================
# === MOCKING LOGIC AND GLOBAL FLAGS ===
# =================================================================

_MOCK_CONTROL_FLAG = False
_MOCK_COUNTER = 0

def set_mock_control_mode(enable: bool):
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡∏¥‡∏î/‡∏õ‡∏¥‡∏î‡πÇ‡∏´‡∏°‡∏î‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô (CONTROLLED MOCK)"""
    global _MOCK_CONTROL_FLAG, _MOCK_COUNTER
    _MOCK_CONTROL_FLAG = enable
    if enable:
        _MOCK_COUNTER = 0
        logger.info("üîë CONTROLLED MOCK Mode ENABLED.")
    else:
        logger.info("‚ùå CONTROLLED MOCK Mode DISABLED.")

def retrieve_context_with_filter(
    query: str, 
    retriever: Any, 
    metadata_filter: Optional[List[str]] = None, 
) -> Dict[str, Any]:
    """Retrieves documents from the vector store, optionally filtering by document ID."""
    if retriever is None:
        return {"top_evidences": []}
    
    filter_document_ids = metadata_filter 

    try:
        docs: List[Document] = retriever.invoke(query) 
        
        # Manual Filtering
        if filter_document_ids:
            filter_set = set(filter_document_ids)
            
            filtered_docs = []
            for doc in docs:
                doc_id_in_metadata = doc.metadata.get("doc_id") 

                if doc_id_in_metadata in filter_set:
                    filtered_docs.append(doc)
            
            docs = filtered_docs
        
        # ‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        top_evidences = []
        for d in docs:
            meta = d.metadata
            top_evidences.append({
                "doc_id": meta.get("doc_id"),
                "source": meta.get("source"),
                "content": d.page_content.strip()
            })
            
        return {"top_evidences": top_evidences}
        
    except Exception as e:
        logger.error(f"Error during RAG retrieval with filter: {e}")
        return {"top_evidences": []}


# =================================================================
# === EVALUATION FUNCTION (MOCK & REAL LLM) (‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç JSON Robustness) ===
# =================================================================

MAX_LLM_RETRIES = 3 

def evaluate_with_llm(statement: str, context: str, standard: str) -> Dict[str, Any]:
    """
    Performs the LLM evaluation, extracting a score and reason, 
    with robust JSON parsing and retry logic.
    """
    global _MOCK_CONTROL_FLAG, _MOCK_COUNTER
    
    # 1. MOCK CONTROL LOGIC
    if _MOCK_CONTROL_FLAG:
        _MOCK_COUNTER += 1
        
        score = 1 if _MOCK_COUNTER <= 5 else 0
        reason_text = f"MOCK: FORCED {'PASS' if score == 1 else 'FAIL'} (Statement {_MOCK_COUNTER})"
        
        logger.debug(f"MOCK COUNT: {_MOCK_COUNTER} | SCORE: {score} | STMT: '{statement[:20]}...'")
        return {"score": score, "reason": reason_text}

    
    # 2. REAL LLM CALL LOGIC
    if llm_instance is None:
        logger.error("‚ùå LLM Instance is not initialized.")
        score = random.choice([0, 1])
        reason = f"LLM Initialization Failed (Fallback to Random Score {score})"
        return {"score": score, "reason": reason}

    # üü¢ NEW: ‡πÉ‡∏ä‡πâ PromptTemplate ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á HumanMessage Content
    user_prompt_content = USER_ASSESSMENT_PROMPT.format(
        statement=statement,
        standard=standard,
        # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô" ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà ‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ PromptTemplate
        context=context if context else "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á" 
    )
    
    # === NEW RETRY LOOP ===
    for attempt in range(MAX_LLM_RETRIES):
        try:
            # A. ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ LLM
            response = llm_instance.invoke([
                SystemMessage(content=SYSTEM_ASSESSMENT_PROMPT),
                HumanMessage(content=user_prompt_content)
            ])
            
            llm_response_content = response.content if hasattr(response, 'content') else str(response)
            
            # üõë B. FIX: ‡πÉ‡∏ä‡πâ Regex ‡πÄ‡∏û‡∏∑‡πà‡∏≠ Clean ‡πÅ‡∏•‡∏∞‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ JSON Block
            
            # 1. Clean up markdown fences (```json...```) 
            cleaned_content = llm_response_content.strip()
            if cleaned_content.startswith("```json"):
                cleaned_content = cleaned_content.replace("```json", "", 1).rstrip('`')
            elif cleaned_content.startswith("```"):
                cleaned_content = cleaned_content.replace("```", "", 1).rstrip('`')
            
            # 2. ‡πÉ‡∏ä‡πâ Regex ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏£‡∏≠‡∏á‡∏´‡∏≤ JSON block ‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå
            json_match = re.search(r'\{.*\}', cleaned_content, re.DOTALL)
            
            final_json_string = None
            if json_match:
                final_json_string = json_match.group(0)
            
            if not final_json_string:
                raise ValueError("LLM response did not contain a recognizable JSON block.")
                
            # 3. Parse JSON string
            llm_output = json.loads(final_json_string) # ‚¨ÖÔ∏è Parse string ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏Å‡∏£‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß
            
            # C. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ Key 'llm_score' ‡πÅ‡∏•‡∏∞ 'reason' ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô
            if "llm_score" in llm_output and "reason" in llm_output:
                raw_score = llm_output.get("llm_score", 0)
                # Ensure score is an integer (handling cases where LLM might return '1' as a string)
                score = int(str(raw_score)) if str(raw_score).isdigit() else 0 
                reason = llm_output.get("reason", "No reason provided by LLM.")
                
                return {
                    "llm_score": score,  # ‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà LLM ‡∏™‡∏£‡πâ‡∏≤‡∏á
                    "reason": reason,
                    "score": score       # Key ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏ö‡πÉ‡∏ä‡πâ (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ)
                } 
            
            else:
                raise ValueError("LLM response JSON is missing 'llm_score' or 'reason' keys.")

        except (json.JSONDecodeError, ValueError) as e:
            logger.warning(f"‚ùå Format/JSON Parse Failed (Attempt {attempt + 1}/{MAX_LLM_RETRIES}). Retrying in 1s... Error: {e}")
            if attempt < MAX_LLM_RETRIES - 1:
                time.sleep(1) 
                continue
            else:
                logger.error(f"‚ùå LLM Evaluation failed after {MAX_LLM_RETRIES} attempts. JSON/Format failure.")
                break 
        
        except Exception as e:
            logger.error(f"‚ùå LLM Evaluation failed due to unexpected error (Connection/Runtime). Error: {e}")
            break 

    # === FALLBACK LOGIC ===
    logger.error("‚ùå Using RANDOM SCORE as final fallback.")
    score = random.choice([0, 1])
    reason = f"LLM Call Failed (Fallback to Random Score {score}) after {MAX_LLM_RETRIES} attempts."
    return {"score": score, "reason": reason}


# =================================================================
# === ACTION PLAN GENERATION UTILITY (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á Logic) ===
# =================================================================

def _call_llm_for_json_output(prompt: str, system_prompt: str) -> str:
    """Basic LLM call for JSON output, relies on the System Prompt to enforce JSON."""
    if llm_instance is None:
        raise ConnectionError("LLM Instance is not available for Action Plan Generation.")

    response = llm_instance.invoke([
        SystemMessage(content=system_prompt),
        HumanMessage(content=prompt)
    ])
    
    llm_response_content = response.content if hasattr(response, 'content') else str(response)
    
    # Clean up markdown fences (```json...```)
    cleaned_content = llm_response_content.strip()
    if cleaned_content.startswith("```json"):
        cleaned_content = cleaned_content.replace("```json", "", 1).rstrip('`')
    elif cleaned_content.startswith("```"):
        cleaned_content = cleaned_content.replace("```", "", 1).rstrip('`')
        
    return cleaned_content.strip()

def generate_action_plan_via_llm(
    failed_statements_data: List[Dict[str, Any]], 
    sub_id: str, 
    target_level: int, 
) -> Dict[str, Any]:
    
    # 1. MOCK LOGIC
    global _MOCK_CONTROL_FLAG
    if _MOCK_CONTROL_FLAG:
        logger.warning("MOCK: Generating dummy Action Plan via MOCK Logic.")
        actions = []
        for i, data in enumerate(failed_statements_data):
            statement_id = f"L{data.get('level', target_level)} S{data.get('statement_number', i+1)}"
            failed_level = data.get('level', target_level)
            
            actions.append(ActionPlanActions.Actions.item_type( 
                Statement_ID=statement_id,
                Failed_Level=failed_level,
                Recommendation=f"MOCK: [Specific Action] ‡∏à‡∏±‡∏î‡∏ó‡∏≥ Policy '{data.get('statement_text', 'N/A')[:20]}...' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç GAP ‡∏à‡∏≤‡∏Å‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•: {data.get('llm_reasoning', 'No reason')[:20]}...",
                Target_Evidence_Type="MOCK: Policy Document (Type: Guideline)",
                Key_Metric="Policy Approved and Published"
            ).model_dump())
        
        return ActionPlanActions(
            Phase=f"1. Strategic Gap Closure (Target L{target_level})",
            Goal=f"‡∏ö‡∏£‡∏£‡∏•‡∏∏‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô Level {target_level} ‡∏Ç‡∏≠‡∏á {sub_id}",
            Actions=actions
        ).model_dump()


    # --- REAL IMPLEMENTATION (LLM Call for Action Plan) ---
    try:
        # 2. Prepare Prompt Input Variables
        failed_statements_text = []
        for data in failed_statements_data:
            stmt_num = data.get('statement_number', 'N/A')
            failed_statements_text.append(f"""
            --- STATEMENT FAILED (L{data.get('level', 'N/A')} S{stmt_num}) ---
            Statement Text: {data.get('statement_text', 'N/A')}
            Reason for Failure: {data.get('llm_reasoning', 'N/A')}
            RAG Context Found: {data.get('retrieved_context', 'No context found or saved')}
            """)
            
        statements_list_str = "\n".join(failed_statements_text)

        # 3. Format the Prompt
        llm_prompt_content = ACTION_PLAN_PROMPT.format(
            sub_id=sub_id,
            target_level=target_level,
            failed_statements_list=statements_list_str 
        )
        

        # 4. Define System Prompt with JSON Schema
        schema_dict = ActionPlanActions.model_json_schema()
        
        # üü¢ ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á: ‡πÉ‡∏ä‡πâ SYSTEM_ACTION_PLAN_PROMPT ‡πÅ‡∏•‡∏∞‡∏ú‡∏ô‡∏ß‡∏Å JSON Schema ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ
        system_prompt_content = (
            SYSTEM_ACTION_PLAN_PROMPT + # ‚¨ÖÔ∏è ‡πÉ‡∏ä‡πâ System Prompt ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
            "\n\n--- REQUIRED JSON SCHEMA (STRICTLY FOLLOW) ---\n" +
            json.dumps(schema_dict, indent=2)
        )

        # 5. CALL LLM
        llm_response_json_str = _call_llm_for_json_output(
            prompt=llm_prompt_content,
            system_prompt=system_prompt_content # ‚¨ÖÔ∏è ‡πÉ‡∏ä‡πâ system prompt ‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß
        )
        
        
        # üõë 6. FIX: ‡πÉ‡∏ä‡πâ Regex ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ JSON Block ‡∏Å‡πà‡∏≠‡∏ô Parse
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ String ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if not llm_response_json_str or not llm_response_json_str.strip():
             raise ValueError("LLM returned an empty response for Action Plan.")
             
        # ‡πÉ‡∏ä‡πâ Regex ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ JSON Block
        json_match = re.search(r'\{.*\}', llm_response_json_str.strip(), re.DOTALL)

        cleaned_content = None
        if json_match:
            cleaned_content = json_match.group(0)

        if not cleaned_content:
            logger.error("‚ùå Failed to find a valid JSON block for Action Plan using Regex.")
            raise ValueError("LLM response did not contain a recognizable JSON block for Action Plan.")

        # 7. Process and Parse
        llm_result_dict = json.loads(cleaned_content) # ‚¨ÖÔ∏è ‡πÉ‡∏ä‡πâ string ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏Å‡∏£‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß
        
        # 8. Validate and Dump
        validated_plan_model = ActionPlanActions.model_validate(llm_result_dict)

        final_action_plan_result = validated_plan_model.model_dump()
        
        return final_action_plan_result

    except Exception as e:
        logger.error(f"‚ùå Action Plan Generation Failed: {e}", exc_info=True)
        return {
             "Phase": "Error",
             "Goal": f"Failed to generate Action Plan for {sub_id} (Target L{target_level})",
             "Actions": [{
                "Statement_ID": "N/A", 
                "Failed_Level": target_level, 
                "Recommendation": f"System Error: {str(e)}", 
                "Target_Evidence_Type": "Check Logs", 
                "Key_Metric": "Error"
             }]
        }
    

# ----------------------------------------------------------------------
# === NARRATIVE REPORT GENERATION ===
# ----------------------------------------------------------------------

def generate_narrative_report_via_llm_real(prompt_text: str, system_instruction: str) -> str:
    """
    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏Å LLM API ‡∏à‡∏£‡∏¥‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡πÄ‡∏ä‡∏¥‡∏á‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö CEO
    
    :param prompt_text: Prompt ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• JSON ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏†‡∏≤‡∏¢‡πÉ‡∏ô (Human Message)
    :param system_instruction: System Instruction ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó‡πÅ‡∏•‡∏∞‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö (‡∏°‡∏≤‡∏à‡∏≤‡∏Å rag_prompts.py)
    :return: ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡πÄ‡∏ä‡∏¥‡∏á‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏î‡∏¢ LLM
    """
    # üö® ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á LLM instance (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡πÉ‡∏ô environment ‡∏à‡∏£‡∏¥‡∏á)
    try:
        from langchain.schema import SystemMessage, HumanMessage 
        # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤ llm ‡∏ñ‡∏π‡∏Å import ‡∏à‡∏≤‡∏Å models.llm
        from models.llm import llm as llm_instance 
    except ImportError:
        logger.error("‚ùå Cannot import LLM dependencies (SystemMessage, HumanMessage, llm_instance).")
        return "[ERROR: LLM Dependencies missing for real API call.]"
        
    if llm_instance is None:
        logger.error("‚ùå LLM Instance is not initialized for real API call.")
        return "[ERROR: LLM Client is not initialized for real API call.]"
        
    logger.info("Executing REAL LLM call for narrative report synthesis...")
    
    try:
        # System Message ‡∏ñ‡∏π‡∏Å‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô Argument
        
        response = llm_instance.invoke([
            SystemMessage(content=system_instruction),
            HumanMessage(content=prompt_text)
        ])
        
        # üü¢ FIX: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó Response ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á .content
        # ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô String ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á ‡∏´‡∏£‡∏∑‡∏≠ LangChain Response Object
        if hasattr(response, 'content'):
            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô LangChain/SDK Response Object
            generated_text = response.content
        elif isinstance(response, str):
            # ‡∏ñ‡πâ‡∏≤ Wrapper ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô String ‡∏ï‡∏£‡∏á‡πÜ
            generated_text = response
        else:
            # ‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å
            raise TypeError(f"LLM response type {type(response)} is not supported for content extraction.")
            
        # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å LLM
        return generated_text.strip()
        
    except Exception as e:
        logger.error(f"Real LLM API call failed during narrative report generation: {e}")
        # Fallback message
        return f"[API ERROR] Failed to generate narrative report via real LLM API: {e}"

# =================================================================
# === EVIDENCE DESCRIPTION GENERATION (NEW FUNCTION) ===
# =================================================================

def generate_evidence_description_via_llm(
    sub_id: str, 
    level: int, 
    standard: str, 
    context: str
) -> str:
    """
    Generates a narrative description for a sub-criteria level based on the aggregated context 
    using the dedicated Evidence Description Prompt.
    
    Args:
        sub_id: The ID of the sub-criteria (e.g., '6.1.1').
        level: The maturity level (e.g., 1).
        standard: The standard description for that level.
        context: The aggregated context from all successful statements in that level.
        
    Returns:
        A concise narrative description string.
    """
    if llm_instance is None:
        logger.error("‚ùå LLM Instance is not initialized for Evidence Description.")
        return "‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: LLM Client ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô"
        
    if not context.strip() or "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô" in context.lower():
        # Fallback message that encourages user to find more evidence
        return "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡∏µ‡πâ ‡∏à‡∏∂‡∏á‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏∞‡∏ö‡∏∏‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô"
        
    try:
        # 1. Prepare Human Message Content using the imported PromptTemplate
        user_prompt_content = EVIDENCE_DESCRIPTION_PROMPT.format(
            sub_id=sub_id,
            level=level,
            standard=standard,
            context=context
        )

        # 2. Invoke LLM (Pure Text Generation)
        response = llm_instance.invoke([
            SystemMessage(content=SYSTEM_EVIDENCE_DESCRIPTION_PROMPT), # System prompt for role/rules
            HumanMessage(content=user_prompt_content) # Context and question
        ])
        
        # 3. Extract Content
        generated_text = response.content if hasattr(response, 'content') else str(response)
        
        logger.info(f"‚úÖ Generated Evidence Description for {sub_id} L{level}")
        return generated_text.strip()
        
    except Exception as e:
        logger.error(f"‚ùå Error during Evidence Description generation for {sub_id} L{level}: {e}")
        return "‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏≥‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô (LLM Failure)"

def summarize_context_with_llm(context: str, sub_criteria_name: str, level: int) -> Dict[str, str]:
    """
    ‡πÉ‡∏ä‡πâ LLM ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡∏∏‡∏õ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡πÅ‡∏•‡∏∞‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
    """
    # üö® FIX 1: ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß Context ‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤ LLM (‡πÉ‡∏ä‡πâ MAX_CONTEXT_LENGTH)
    # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤ MAX_CONTEXT_LENGTH ‡∏ñ‡∏π‡∏Å‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤ Global ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ
    
    # ‡∏ñ‡πâ‡∏≤ MAX_CONTEXT_LENGTH ‡∏ñ‡∏π‡∏Å‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà‡∏≠‡∏∑‡πà‡∏ô ‡πÄ‡∏ä‡πà‡∏ô 2500 (‡∏à‡∏≤‡∏Å EnablerAssessment)
    # ‡πÉ‡∏´‡πâ‡∏™‡πà‡∏á‡∏Ñ‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô parameter ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏á‡∏ó‡∏µ‡πà‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà
    MAX_LLM_SUMMARY_CONTEXT = 3000 # ‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ï‡∏≤‡∏°‡∏Ç‡∏µ‡∏î‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏£‡∏∏‡∏õ
    
    # ‡∏ï‡∏±‡∏î Context ‡∏ñ‡πâ‡∏≤‡∏°‡∏±‡∏ô‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
    if len(context) > MAX_LLM_SUMMARY_CONTEXT:
        logger.warning(f"Context for summary L{level} is too long ({len(context)}), truncating to {MAX_LLM_SUMMARY_CONTEXT}.")
        context_to_use = context[:MAX_LLM_SUMMARY_CONTEXT]
    else:
        context_to_use = context
        
    try:
        # 1. ‡∏à‡∏±‡∏î‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° System ‡πÅ‡∏•‡∏∞ Human Prompt
        system_prompt = SYSTEM_EVIDENCE_DESCRIPTION_PROMPT
        human_prompt = EVIDENCE_DESCRIPTION_PROMPT.format(
            sub_criteria_name=sub_criteria_name,
            level=level,
            context=context_to_use # üö® ‡πÉ‡∏ä‡πâ Context ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡∏±‡∏î‡πÅ‡∏•‡πâ‡∏ß
        )
        
        # 2. ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ LLM 
        response = llm_instance.invoke([
            SystemMessage(content=system_prompt), 
            HumanMessage(content=human_prompt)
        ])
        
        # ‡∏î‡∏∂‡∏á content ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤ (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÉ‡∏ô‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏≠‡∏∑‡πà‡∏ô ‡πÜ ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏•‡πâ‡∏ß)
        response_text = response.content if hasattr(response, 'content') else str(response)
        
        return {"summary": response_text.strip()}
        
    except Exception as e:
        logger.error(f"LLM Summary generation failed: {e}")
        return {"summary": "‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏î‡∏¢ LLM"}