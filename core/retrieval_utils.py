# core/retrieval_utils.py
import logging
import random
import json, json5
import time
from typing import List, Dict, Any, Optional, Union, TypeVar, Type, Tuple
# üí° ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç #1: Messages ‡πÅ‡∏•‡∏∞ Document ‡∏ñ‡∏π‡∏Å‡∏¢‡πâ‡∏≤‡∏¢‡πÑ‡∏õ‡∏ó‡∏µ‡πà langchain_core
from langchain_core.messages import SystemMessage, HumanMessage 
from langchain_core.documents import Document as LcDocument
from pydantic import ValidationError, BaseModel
import regex as re
import hashlib

# Project imports
from core.assessment_schema import StatementAssessment, EvidenceSummary
from core.action_plan_schema import ActionPlanActions
from core.rag_prompts import (
    SYSTEM_ASSESSMENT_PROMPT,
    USER_ASSESSMENT_PROMPT,
    ACTION_PLAN_PROMPT,
    SYSTEM_ACTION_PLAN_PROMPT,
    SYSTEM_EVIDENCE_DESCRIPTION_PROMPT,
    EVIDENCE_DESCRIPTION_PROMPT
)
from core.vectorstore import (
    VectorStoreManager,
    load_all_vectorstores,
    GLOBAL_RERANKER, 
    get_global_reranker
)


T = TypeVar('T', bound=BaseModel)

try:
    from models.llm import llm as llm_instance
except Exception:
    llm_instance = None

# -------------------- Config from global_vars --------------------
try:
    from config.global_vars import (
        DEFAULT_ENABLER,
        INITIAL_TOP_K,
        FINAL_K_RERANKED,
        FINAL_K_NON_RERANKED,
    )
except ImportError as e:
    print(f"FATAL ERROR: Cannot import global_vars: {e}")
    raise

logger = logging.getLogger(__name__)

# =================================================================
# MOCKING LOGIC AND GLOBAL FLAGS
# =================================================================
_MOCK_CONTROL_FLAG = False
_MOCK_COUNTER = 0

def set_mock_control_mode(enable: bool):
    global _MOCK_CONTROL_FLAG, _MOCK_COUNTER
    _MOCK_CONTROL_FLAG = enable
    if enable:
        _MOCK_COUNTER = 0
        logger.info("üîë CONTROLLED MOCK Mode ENABLED.")
    else:
        logger.info("‚ùå CONTROLLED MOCK Mode DISABLED.")


# ------------------------------------------------------------------
# ID Normalization and Hashing
# ------------------------------------------------------------------
def _hash_stable_id_to_64_char(stable_id: str) -> str:
    return hashlib.sha256(stable_id.lower().encode('utf-8')).hexdigest()

def normalize_stable_ids(ids: List[str]) -> List[str]:
    normalized = []
    for i in ids:
        if len(i) == 64:
            normalized.append(i.lower())
        else:
            normalized.append(_hash_stable_id_to_64_char(i))
    return normalized

# =================================================================
# Document Retrieval
# =================================================================
def retrieve_context_by_doc_ids(doc_uuids: List[str], collection_name: str) -> Dict[str, Any]:
    """
    Retrieve documents by UUIDs from a specific collection (‡πÉ‡∏ä‡πâ 64-char Stable UUIDs)
    """
    if VectorStoreManager is None:
        logger.error("‚ùå VectorStoreManager is not available.")
        return {"top_evidences": []}
        
    if not doc_uuids:
        logger.warning("‚ö†Ô∏è No document UUIDs provided for retrieval.")
        return {"top_evidences": []}

    try:
        manager = VectorStoreManager()
        
        # 1. Normalize ID ‡πÄ‡∏õ‡πá‡∏ô 64-char Stable UUIDs
        normalized_uuids = normalize_stable_ids(doc_uuids)
        
        # 2. ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Vector Store
        docs: List[LcDocument] = manager.get_documents_by_id(normalized_uuids, doc_type=collection_name)

        # 3. ‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        top_evidences = []
        for d in docs:
            meta = d.metadata
            top_evidences.append({
                "doc_id": meta.get("doc_id"),              # 64-char Stable UUID
                "doc_type": meta.get("doc_type"),
                "chunk_uuid": meta.get("chunk_uuid"),      # ID ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡∏≠‡∏á Chunk
                "source": meta.get("source") or meta.get("doc_source"), # ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå/‡∏ó‡∏µ‡πà‡∏°‡∏≤
                "content": d.page_content.strip(),
                "chunk_index": meta.get("chunk_index")
            })

        logger.info(f"‚úÖ Successfully retrieved {len(top_evidences)} evidences by UUIDs from collection '{collection_name}'.")
        return {"top_evidences": top_evidences}

    except Exception as e:
        logger.error(f"Error during UUID-based retrieval: {e}", exc_info=True)
        return {"top_evidences": []}


# ------------------------------------------------------------------
# RAG Retrieval with optional hard filter (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á rerank)
# ------------------------------------------------------------------
def retrieve_context_with_filter(
    query: str, 
    doc_type: str, 
    enabler: str, 
    stable_doc_ids: Optional[list] = None, 
    top_k_reranked: int = FINAL_K_RERANKED, 
    disable_semantic_filter: bool = False,
    allow_fallback: bool = False
) -> dict:

    try:
        manager = VectorStoreManager()
        collection_name = f"{doc_type}_{(enabler or DEFAULT_ENABLER).lower()}" \
                          if doc_type.lower() == "evidence" else doc_type.lower()
        vectorstore = manager._load_chroma_instance(collection_name)
        if not vectorstore:
            logger.error(f"‚ùå Vectorstore '{collection_name}' not found.")
            return {"top_evidences": []}

        # Hard filter
        where_clause = None
        if stable_doc_ids:
            stable_doc_ids_normalized = [doc_id.lower() for doc_id in stable_doc_ids]
            collection = getattr(vectorstore, "_collection", None)
            if collection:
                try:
                    test_results = collection.get(
                        where={"stable_doc_uuid": {"$in": stable_doc_ids_normalized}},
                        include=["metadatas"]
                    )
                    if len(test_results.get("ids", [])) > 0:
                        where_clause = {"stable_doc_uuid": {"$in": stable_doc_ids_normalized}}
                except Exception as e:
                    logger.error(f"Hard filter failed: {e}")
                    if not allow_fallback:
                        return {"top_evidences": []}

        # Base retriever
        search_kwargs = {"k": INITIAL_TOP_K}
        if where_clause:
            search_kwargs["filter"] = where_clause
        base_retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs=search_kwargs)

        # -------------------------
        # Rerank + truncate
        # -------------------------
        documents = base_retriever.invoke(query)
        if not disable_semantic_filter:
            # reranker = get_reranking_compressor(top_n=top_k_reranked)
            reranker = get_global_reranker(top_k_reranked)
            if reranker and hasattr(reranker, "rerank"):
                try:
                    documents = reranker.rerank(query, documents)
                    logger.info(f"RAG Retrieval (Rerank) truncated to top {top_k_reranked}.")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Rerank failed: {e}. Using base retriever + truncate.")
                    documents = documents[:top_k_reranked]
            else:
                logger.warning("‚ö†Ô∏è Rerank unavailable. Using base retriever + truncate.")
                documents = documents[:top_k_reranked]
        else:
            # Non-rerank
            documents = documents[:top_k_reranked]
            logger.info(f"RAG Retrieval (Non-Rerank) truncated to top {top_k_reranked}.")

        # Format results
        top_evidences = []
        for doc in documents:
            if not isinstance(doc, LcDocument):
                continue
            metadata = doc.metadata or {}
            if not metadata.get("source"):
                uuid_ref = metadata.get("stable_doc_uuid", "")[:8]
                metadata["source"] = f"Unknown_Source_{uuid_ref}" if uuid_ref else "Unknown_Source"
            if "page_label" not in metadata and "page" in metadata:
                metadata["page_label"] = str(metadata["page"])
            if "chunk_index" not in metadata and "chunk_id" in metadata:
                metadata["chunk_index"] = metadata.get("chunk_id")
            if not metadata.get("file_name"):
                metadata["file_name"] = metadata.get("source")
            top_evidences.append({
                "content": doc.page_content,
                "metadata": metadata
            })

        return {"top_evidences": top_evidences}

    except Exception as e:
        logger.error(f"Error in retrieve_context_with_filter: {e}", exc_info=True)
        return {"top_evidences": []}


    
# ------------------------------------------------------------------
# Robust JSON Extraction
# ------------------------------------------------------------------# 

def _robust_extract_json(text: str) -> Optional[Any]:
    """
    Attempts to extract a complete and valid JSON object from text.
    """
    if not text:
        return None

    cleaned_text = text.strip()

    # 1. ‡∏•‡∏ö code fences (```json, ```, ‡∏Ø‡∏•‡∏Ø)
    cleaned_text = re.sub(r'^\s*```(?:json)?\s*', '', cleaned_text, flags=re.MULTILINE | re.IGNORECASE)
    cleaned_text = re.sub(r'\s*```\s*$', '', cleaned_text, flags=re.MULTILINE)
    
    # ‡∏•‡∏ö‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ "json" ‡∏´‡∏£‡∏∑‡∏≠ "output" ‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤
    cleaned_text = re.sub(r'^\s*(?:json|output|result)\s*', '', cleaned_text, flags=re.IGNORECASE)

    # 2. ‡∏•‡∏≠‡∏á‡πÇ‡∏´‡∏•‡∏î‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
    try:
        return json.loads(cleaned_text)
    except json.JSONDecodeError:
        pass # ‡∏ñ‡πâ‡∏≤‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ‡∏ó‡∏≥‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ
    
    # -----------------------------------------------------------
    # ‚úÖ FIX: Logic ‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ JSON Object ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° Narrative
    # -----------------------------------------------------------
    try:
        # 2.1 ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏Ç‡∏≠‡∏á '{' ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å ‡πÅ‡∏•‡∏∞ '}' ‡∏ï‡∏±‡∏ß‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
        
        # ‡∏•‡∏≠‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Object JSON (Dictionary)
        start_index = cleaned_text.find('{')
        end_index = cleaned_text.rfind('}')
        
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏ö Object, ‡∏•‡∏≠‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ List JSON (Array)
        if start_index == -1 and end_index == -1:
            start_index = cleaned_text.find('[')
            end_index = cleaned_text.rfind(']')

        if start_index != -1 and end_index != -1 and end_index > start_index:
            # ‡∏ï‡∏±‡∏î‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ß‡∏á‡πÄ‡∏•‡πá‡∏ö/‡∏ß‡∏á‡πÄ‡∏•‡πá‡∏ö‡πÄ‡∏´‡∏•‡∏µ‡πà‡∏¢‡∏°‡πÄ‡∏õ‡∏¥‡∏î‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å‡πÅ‡∏•‡∏∞‡∏õ‡∏¥‡∏î‡∏ï‡∏±‡∏ß‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
            json_candidate = cleaned_text[start_index : end_index + 1]
            
            # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÇ‡∏´‡∏•‡∏î JSON ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡∏±‡∏î‡∏°‡∏≤
            return json.loads(json_candidate)

    except (json.JSONDecodeError, Exception) as e:
        logger.debug(f"JSON extraction failed even with robust search: {e}")
        return None

    # ‡∏´‡∏≤‡∏Å‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á
    return None

def _normalize_keys(data: Union[Dict, List, Any]) -> Union[Dict, List, Any]:
    """
    Helper function to recursively normalize common LLM output keys 
    (‡πÄ‡∏ä‡πà‡∏ô 'llm_score' -> 'score' ‡πÅ‡∏•‡∏∞ 'reasoning' -> 'reason'). 
    ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Action Plan ‡πÅ‡∏•‡∏∞ Summary ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å
    """
    if isinstance(data, dict):
        normalized_data = {}
        # üéØ Mapping: (Key ‡∏ó‡∏µ‡πà LLM ‡∏ï‡∏≠‡∏ö‡∏ú‡∏¥‡∏î) -> (Key ‡∏ó‡∏µ‡πà Pydantic Schema ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£)
        key_mapping = {
            'llm_score': 'score',
            'reasoning': 'reason',          # üü¢ FIX: ‡πÅ‡∏õ‡∏•‡∏á 'reasoning' ‡πÄ‡∏õ‡πá‡∏ô 'reason'
            'llm_reasoning': 'reason',
            'assessment_reason': 'reason', 
            'comment': 'reason',

            # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Action Plan:
            'actions': 'Actions',
            'phase': 'Phase',
            'goal': 'Goal'
        }
        
        for k, v in data.items():
            k_lower = k.lower()
            normalized_key = k
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á key
            for old_k, new_k in key_mapping.items():
                if k_lower == old_k:
                    normalized_key = new_k
                    break
            
            # ‡πÉ‡∏ä‡πâ normalized_key ‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ã‡πâ‡∏≥
            normalized_data[normalized_key] = _normalize_keys(v)
                 
        return normalized_data
    elif isinstance(data, list):
        return [_normalize_keys(item) for item in data]
    return data


def parse_llm_json_response(llm_response_text: str, pydantic_schema: Type[T]) -> Union[T, List[T]]:
    """
    ‡∏î‡∏∂‡∏á JSON ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° LLM, Normalize Key ‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏î‡πâ‡∏ß‡∏¢ Pydantic Schema.
    (‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Action Plan ‡πÅ‡∏•‡∏∞ Summary ‡∏ó‡∏µ‡πà‡∏°‡∏±‡∏Å‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ JSON/Key)
    """
    raw_data = _robust_extract_json(llm_response_text)
    
    if raw_data is None:
        raise ValueError("Could not robustly extract valid JSON from LLM response.")
    
    # üü¢ NEW STEP: Normalize Keys ‡∏Å‡πà‡∏≠‡∏ô Validate (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Action Plan)
    raw_data = _normalize_keys(raw_data) 
        
    try:
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Schema ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô List
        if hasattr(pydantic_schema, '__origin__') and pydantic_schema.__origin__ is list:
            item_schema = pydantic_schema.__args__[0]
            if not isinstance(raw_data, list):
                raw_data = [raw_data]
            
            validated_list = [item_schema.model_validate(item) for item in raw_data]
            return validated_list
        else:
            # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Schema ‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß
            if isinstance(raw_data, list):
                if raw_data:
                    logger.warning(f"Expected single object for {pydantic_schema.__name__}, but received a list. Using first element.")
                    raw_data = raw_data[0]
                else:
                    raise ValueError("Received empty list when expecting a single object.")

            validated_model = pydantic_schema.model_validate(raw_data)
            return validated_model
            
    except ValidationError as e:
        logger.error(f"Pydantic Validation Error for {pydantic_schema.__name__}: {e}")
        raise ValueError(f"Pydantic validation failed for schema {pydantic_schema.__name__}. Details: {e}")
    except Exception as e:
        logger.error(f"An unexpected error occurred during Pydantic validation: {e}")
        raise ValueError(f"An unexpected error occurred during JSON validation: {e}")


# ------------------------------------------------------------------
# üü¢ ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: extract_uuids_from_llm_response
# ------------------------------------------------------------------

# Regex ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ UUIDs ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ (‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö 8-4-4-4-12)
UUID_PATTERN = re.compile(r'[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}', re.IGNORECASE)

def extract_uuids_from_llm_response(
    text: str, 
    key_hint: Optional[List[str]] = None # üü¢ ‡∏£‡∏±‡∏ö key_hint
) -> List[str]:
    """
    Attempts to extract UUIDs from text, supporting both plain text search and 
    JSON key search (when key_hint is provided).
    """
    unique_uuids = set()
    cleaned_text = text.strip()

    # 1. ‡∏•‡∏≠‡∏á Deserialize ‡πÄ‡∏õ‡πá‡∏ô JSON (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô Key ‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏)
    try:
        data: Dict[str, Any] = json.loads(cleaned_text)
        
        # 2. ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô‡∏Ñ‡∏µ‡∏¢‡πå‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ key_hint)
        if key_hint:
            for key in key_hint:
                if key in data:
                    uuids_list = data[key]
                    if isinstance(uuids_list, list):
                        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô List ‡∏Ç‡∏≠‡∏á String (‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà LLM ‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô ["uuid1", "uuid2"])
                        for item in uuids_list:
                            if isinstance(item, str) and UUID_PATTERN.fullmatch(item):
                                unique_uuids.add(item)
                    elif isinstance(uuids_list, str):
                         # ‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡∏Ñ‡∏µ‡∏¢‡πå‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏õ‡πá‡∏ô string ‡∏ó‡∏µ‡πà‡∏°‡∏µ UUIDs ‡∏Ñ‡∏±‡πà‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡∏≤‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á
                         found_in_string = UUID_PATTERN.findall(uuids_list)
                         unique_uuids.update(found_in_string)
        
        # 3. ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ key_hint ‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏û‡∏ö ‡πÉ‡∏´‡πâ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ UUIDs ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° JSON ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏ô Field ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö key_hint)
        # (‡πÉ‡∏ä‡πâ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö Fallback Search ‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà key_hint ‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå ‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô‡∏≠‡∏∑‡πà‡∏ô)
        all_text = json.dumps(data)
        found_in_string = UUID_PATTERN.findall(all_text)
        unique_uuids.update(found_in_string)

    except json.JSONDecodeError:
        # 4. ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà JSON ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ‡∏´‡∏£‡∏∑‡∏≠‡πÅ‡∏Ñ‡πà‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤ ‡πÉ‡∏´‡πâ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ UUIDs ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
        found_in_string = UUID_PATTERN.findall(cleaned_text)
        unique_uuids.update(found_in_string)
        
    return list(unique_uuids)

# =================================================================
MAX_LLM_RETRIES = 3
# =================================================================

def evaluate_with_llm(
    statement: str,
    context: str,
    standard: str,
    enabler_name: Optional[str] = None,
    **kwargs
) -> Dict[str, Any]:
    """
    ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô Statement ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö Standard ‡∏Ç‡∏≠‡∏á Enabler ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Evidence Context ‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏ö‡∏ö RAG
    
    ‚úÖ FIX: ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡πÉ‡∏ä‡πâ Logic ‡πÄ‡∏î‡∏¥‡∏° (‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤ JSON ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á) ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á Field
    """
    global _MOCK_CONTROL_FLAG, _MOCK_COUNTER

    # -------------------- MOCK MODE --------------------
    if _MOCK_CONTROL_FLAG:
        _MOCK_COUNTER += 1
        score = 1 if _MOCK_COUNTER <= 9 else 0
        reason_text = f"MOCK: FORCED {'PASS' if score == 1 else 'FAIL'} (Statement {_MOCK_COUNTER})"
        is_pass = score >= 1
        status_th = "‡∏ú‡πà‡∏≤‡∏ô" if is_pass else "‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô"
        return {
            "score": score,
            "reason": reason_text,
            "pass_status": is_pass,
            "status_th": status_th,
            "enabler": enabler_name or "N/A"
        }

    # -------------------- FALLBACK CASE --------------------
    if llm_instance is None:
        score = random.choice([0, 1])
        reason = f"LLM Initialization Failed (Fallback to Random Score {score})"
        is_pass = score >= 1
        status_th = "‡∏ú‡πà‡∏≤‡∏ô" if is_pass else "‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô"
        return {
            "score": score,
            "reason": reason,
            "pass_status": is_pass,
            "status_th": status_th,
            "enabler": enabler_name or "N/A"
        }

    # -------------------- GENERATE USER PROMPT --------------------
    user_prompt_content = USER_ASSESSMENT_PROMPT.format(
        enabler_name=enabler_name or DEFAULT_ENABLER,
        level=f"Level {kwargs.get('level', 'N/A')}",
        statement=statement,
        standard=standard,
        context=context if context else "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á"
    )

    # -------------------- CALL LLM --------------------
    for attempt in range(MAX_LLM_RETRIES):
        try:
            response = llm_instance.invoke(
                [SystemMessage(content=SYSTEM_ASSESSMENT_PROMPT), HumanMessage(content=user_prompt_content)],
                **({'format': 'json'} if hasattr(llm_instance, 'model_params') and 'format' in llm_instance.model_params else {})
            )

            llm_response_content = response.content if hasattr(response, 'content') else str(response)
            llm_output = _robust_extract_json(llm_response_content)

            if not llm_output or not isinstance(llm_output, dict):
                raise ValueError("LLM response did not contain a recognizable JSON block.")

            # üéØ FIX: ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Key Nomalization ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö StatementAssessment ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á 
            # (‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å Logic ‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏î‡∏µ‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏≤‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ Pydantic Error ‡πÉ‡∏´‡∏°‡πà)
            
            # 1. ‡∏î‡∏∂‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô: ‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á 'llm_score' ‡πÅ‡∏•‡∏∞ 'score'
            raw_score = llm_output.get("llm_score", llm_output.get("score"))
            
            # 2. ‡∏î‡∏∂‡∏á‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•: ‡∏£‡∏±‡∏ö‡∏à‡∏≤‡∏Å Field ‡∏ó‡∏µ‡πà LLM ‡∏ä‡∏≠‡∏ö‡∏ï‡∏≠‡∏ö (reason, reasoning, llm_reasoning)
            reason_text = (
                llm_output.get("reason") or 
                llm_output.get("reasoning") or 
                llm_output.get("llm_reasoning") or
                llm_output.get("assessment_reason")
            )
            
            score_int = int(str(raw_score)) if raw_score is not None else 0

            # 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á Dict ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà Pydantic Schema StatementAssessment ‡∏ô‡πà‡∏≤‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ (score, reason)
            validated_data = {"score": score_int, "reason": reason_text}
            
            try:
                # 4. Validate (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Error ‡∏£‡πâ‡∏≤‡∏¢‡πÅ‡∏£‡∏á)
                StatementAssessment.model_validate(validated_data)
            except ValidationError as ve:
                 logger.warning(f"Pydantic Validation Warning for StatementAssessment (Minor): {ve}")
                 # ‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï‡πÉ‡∏´‡πâ‡∏ú‡πà‡∏≤‡∏ô‡πÑ‡∏õ‡∏ï‡πà‡∏≠ ‡∏´‡∏≤‡∏Å Field ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç (score, reason) ‡∏ñ‡∏π‡∏Å‡∏î‡∏∂‡∏á‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß
                 pass
            
            is_pass = score_int >= 1
            status_th = "‡∏ú‡πà‡∏≤‡∏ô" if is_pass else "‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô"

            validated_data.update({
                "pass_status": is_pass,
                "status_th": status_th,
                "enabler": enabler_name or "N/A"
            })
            return validated_data

        except Exception as e:
            logger.warning(f"LLM Evaluation failed (Attempt {attempt+1}/{MAX_LLM_RETRIES}): {e}")
            if attempt < MAX_LLM_RETRIES - 1:
                time.sleep(1)
                continue

    # -------------------- FALLBACK RESULT --------------------
    score = random.choice([0, 1])
    reason = f"LLM Call Failed after max retries (Fallback to Random Score {score})"
    is_pass = score >= 1
    status_th = "‡∏ú‡πà‡∏≤‡∏ô" if is_pass else "‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô"
    return {
        "score": score,
        "reason": reason,
        "pass_status": is_pass,
        "status_th": status_th,
        "enabler": enabler_name or "N/A"
    }

# =================================================================
# Narrative & Evidence Summary
# =================================================================
# ... (‡πÇ‡∏Ñ‡πâ‡∏î generate_narrative_report_via_llm_real ‡πÄ‡∏î‡∏¥‡∏°) ...
# ... (‡πÇ‡∏Ñ‡πâ‡∏î generate_evidence_description_via_llm ‡πÄ‡∏î‡∏¥‡∏°) ...

def generate_narrative_report_via_llm_real(prompt_text: str, system_instruction: str) -> str:
    if llm_instance is None:
        return "[ERROR: LLM Client is not initialized for real API call.]"
    try:
        response = llm_instance.invoke([SystemMessage(content=system_instruction), HumanMessage(content=prompt_text)])
        return (response.content if hasattr(response, 'content') else str(response)).strip()
    except Exception as e:
        return f"[API ERROR] Failed to generate narrative report: {e}"


def summarize_context_with_llm(context: str, sub_criteria_name: str, level: int, sub_id: str, schema: Any) -> Dict[str, str]:
    MAX_LLM_SUMMARY_CONTEXT = 5000
    MAX_RETRIES = 3 
    
    # 1Ô∏è‚É£ ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
    context_to_use = context[:MAX_LLM_SUMMARY_CONTEXT] if len(context) > MAX_LLM_SUMMARY_CONTEXT else context
    
    default_error_response = {
        "summary": f"‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏î‡∏¢ LLM (Level {level})", 
        "suggestion_for_next_level": "N/A - ‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Raw LLM Output ‡∏´‡∏£‡∏∑‡∏≠ LLM Service"
    }

    # 2Ô∏è‚É£ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö LLM instance
    try:
        if llm_instance is None:
            logger.error("LLM instance is None. Cannot run summary generation.")
            default_error_response["summary"] = "LLM Service ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô"
            default_error_response["suggestion_for_next_level"] = "‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ LLM Service."
            return default_error_response
    except NameError:
        logger.error("Global 'llm_instance' variable not defined or accessible.")
        default_error_response["summary"] = "LLM Instance Error (NameError)"
        return default_error_response

    # 3Ô∏è‚É£ Retry Loop
    for attempt in range(MAX_RETRIES):
        try:
            # 3.1 ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Schema JSON ‡πÅ‡∏•‡∏∞ Prompt
            schema_dict = EvidenceSummary.model_json_schema()
            system_prompt_content = (
                SYSTEM_EVIDENCE_DESCRIPTION_PROMPT
                + "\n\n--- REQUIRED JSON SCHEMA ---\n"
                + json.dumps(schema_dict, indent=2, ensure_ascii=False)
                + "\n\nüß† IMPORTANT: Respond ONLY in valid JSON format following the schema above."
            )
            
            human_prompt = EVIDENCE_DESCRIPTION_PROMPT.format(
                standard=sub_criteria_name,
                level=level,
                context=context_to_use, 
                sub_id=sub_id
            )
            
            # 3.2 ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å LLM 
            llm_full_response = _fetch_llm_response(
                prompt=human_prompt, 
                system_prompt=system_prompt_content, 
                max_retries=1
            )
            
            # ‚úÖ Debug Raw Output
            logger.debug(f"üß© RAW LLM OUTPUT (Attempt {attempt+1}/{MAX_RETRIES}):\n{llm_full_response}")

            # 3.3 ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏° parse
            validated_summary_model = parse_llm_json_response(llm_full_response, EvidenceSummary)
            
            # ‚úÖ ‡∏´‡∏≤‡∏Å LLM ‡∏ï‡∏≠‡∏ö‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô JSON ‡∏´‡∏£‡∏∑‡∏≠ parse ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‚Üí fallback
            if validated_summary_model is None:
                logger.warning("‚ö†Ô∏è LLM output not parsed correctly. Falling back to raw text summary.")
                return {
                    "summary": llm_full_response[:2000],  # ‡∏ï‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ log overflow
                    "suggestion_for_next_level": "LLM ‡∏ï‡∏≠‡∏ö‡∏ô‡∏≠‡∏Å schema ‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö prompt ‡∏´‡∏£‡∏∑‡∏≠ schema definition"
                }

            # 3.4 ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ‚úÖ
            return validated_summary_model.model_dump()

        except Exception as e:
            logger.warning(f"LLM Summary failed (Attempt {attempt+1}/{MAX_RETRIES}): {e}")
            if attempt < MAX_RETRIES - 1:
                time.sleep(1)
                continue

            # 3.5 ‡∏•‡πâ‡∏°‡∏Ñ‡∏£‡∏ö‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á
            logger.error(f"Error in summarize_context_with_llm: Failed after {MAX_RETRIES} attempts. Details: {e}", exc_info=True)
            default_error_response["suggestion_for_next_level"] = str(e)
            return default_error_response

    # 4Ô∏è‚É£ fallback ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
    return default_error_response

def generate_evidence_description_via_llm(*args, **kwargs) -> str:
    logger.warning("generate_evidence_description_via_llm is deprecated. Use summarize_context_with_llm instead.")
    return "‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏ñ‡∏π‡∏Å‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô ‡πÇ‡∏õ‡∏£‡∏î‡πÉ‡∏ä‡πâ summarize_context_with_llm ‡πÅ‡∏ó‡∏ô"


def _fetch_llm_response(prompt: str, system_prompt: str, max_retries: int = 1) -> str:
    """
    ‡∏î‡∏∂‡∏á Raw Response ‡∏à‡∏≤‡∏Å LLM API
    """
    if llm_instance is None:
        raise ConnectionError("LLM Instance is not available.")
        
    config_params = {"temperature": 0.0}
    
    # üü¢ ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ JSON format ‡∏´‡∏≤‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ
    if hasattr(llm_instance, 'model_params') and 'format' in llm_instance.model_params:
         config_params.update({'format': 'json'})
         
    # üéØ FIX 2: ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Local LLM ‡∏ö‡∏≤‡∏á‡∏ï‡∏±‡∏ß (‡πÄ‡∏ä‡πà‡∏ô Ollama) ‡∏≠‡∏≤‡∏à‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏™‡πà‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÉ‡∏ô System Prompt ‡πÄ‡∏û‡∏¥‡πà‡∏°
    system_prompt_with_json_ins = system_prompt + "\n\n--- IMPORTANT RULE ---\nOutput MUST be a single, valid JSON object that strictly adheres to the schema. DO NOT include any narrative text, introduction, or markdown fences (```json, ```) outside the JSON object itself."

    for attempt in range(max_retries):
        try:
            response = llm_instance.invoke(
                [SystemMessage(content=system_prompt_with_json_ins), HumanMessage(content=prompt)], 
                config=config_params 
            )
            # Return raw content string
            return (response.content if hasattr(response, 'content') else str(response)).strip()
        except Exception as e:
            logger.warning(f"LLM API call failed (Attempt {attempt+1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                time.sleep(1)
                continue
            raise Exception("LLM call failed after max retries for raw response retrieval.")
    return ""


def clean_llm_response(raw: str) -> str:
    """Clean hidden / zero-width chars, strip whitespace"""
    if not isinstance(raw, str):
        return ""
    cleaned = re.sub(r'[\u200b\u200c\u200d\uFEFF]', '', raw)
    cleaned = cleaned.strip()
    return cleaned

def create_structured_action_plan(
    failed_statements_data: list,
    sub_id: str,
    enabler: str,
    target_level: int,
    max_retries: int = 2,
) -> list:
    """
    Robust Action Plan generator from assessment results (LLM)
    - Cleans hidden characters
    - Retries LLM calls
    - Fallback to placeholder if parsing fails
    """
    global _MOCK_CONTROL_FLAG

    if _MOCK_CONTROL_FLAG:
        return [{"Mock": "Action Plan"}]

    # -------------------------
    # 1. Prepare failed statements text
    # -------------------------
    failed_statements_text = []
    for data in failed_statements_data:
        stmt_num = data.get("statement_number", "N/A")
        failed_level = data.get("level", "N/A")
        statement_id = f"L{failed_level}_S{stmt_num}"

        fields = ["evidence_statement_text", "rubric_standard_text", "llm_reasoning", "retrieved_context"]
        texts = []
        for f in fields:
            t = data.get(f, data.get("reason", "N/A") if f=="llm_reasoning" else "N/A")
            if isinstance(t, str):
                t = clean_llm_response(t)
            texts.append(t)
        evidence_statement_text, rubric_standard_text, reason_for_failure, retrieved_context = texts

        failed_statements_text.append(f"""
--- STATEMENT FAILED (Core Business Enabler: {enabler}) ---
Statement ID: {statement_id}
Evidence_statement: {evidence_statement_text}
Rubric_standard: {rubric_standard_text}
Failed Level: {failed_level}
Reason for Failure: {reason_for_failure}
Evidence Snippet: {retrieved_context}
**IMPORTANT: Use '{statement_id}' for 'Statement_ID' in Action Plan.**
""")
    statements_list_str = "\n".join(failed_statements_text)

    # -------------------------
    # 2. Prepare system prompt
    # -------------------------
    try:
        schema_dict = ActionPlanActions.model_json_schema()
        schema_json = json.dumps(schema_dict, indent=2, ensure_ascii=False)
    except Exception:
        schema_json = "{}"
    system_prompt_content = SYSTEM_ACTION_PLAN_PROMPT + "\n\n--- REQUIRED JSON SCHEMA ---\n" + schema_json

    # -------------------------
    # 3. Call LLM with retries
    # -------------------------
    final_error = None
    llm_full_response = ""

    for attempt in range(max_retries + 1):
        try:
            llm_prompt_content = ACTION_PLAN_PROMPT.format(
                sub_id=sub_id,
                target_level=target_level,
                failed_statements_list=statements_list_str
            )

            llm_full_response = _fetch_llm_response(
                prompt=llm_prompt_content,
                system_prompt=system_prompt_content,
                max_retries=1
            )
            cleaned_response = clean_llm_response(llm_full_response)

            # Try normal JSON parse first
            try:
                validated_data = parse_llm_json_response(cleaned_response, ActionPlanActions)
            except Exception:
                # fallback: use json5 (more tolerant)
                validated_data = parse_llm_json_response(json5.loads(cleaned_response), ActionPlanActions)

            if validated_data is None:
                raise ValueError("JSON parsing returned None unexpectedly.")

            logger.info(f"üéâ Action Plan created successfully on Attempt {attempt+1}!")
            return validated_data.model_dump()

        except Exception as e:
            final_error = str(e)
            logger.warning(f"‚ö†Ô∏è Attempt {attempt+1}/{max_retries+1} failed: {e}")
            logger.error(f"--- RAW LLM RESPONSE START ---\n{llm_full_response}\n--- RAW LLM RESPONSE END ---")
            if attempt == max_retries:
                logger.error(f"[ERROR] Failed to generate Action Plan via LLM for {sub_id}: {final_error}")

    # -------------------------
    # 4. Fallback
    # -------------------------
    fallback_plan = {
        "Phase": f"Action Plan Error - L{target_level}",
        "Goal": f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á Action Plan ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {sub_id} ‡πÑ‡∏î‡πâ",
        "Actions": [
            {
                "Statement_ID": "LLM_ERROR_FALLBACK",
                "Failed_Level": target_level,
                "Recommendation": f"LLM Output ‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á JSON Schema: {final_error}",
                "Target_Evidence_Type": "System Check",
                "Key_Metric": "LLM output validation passed",
                "Steps": [
                    {
                        "Step": "1",
                        "Description": f"‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Response ‡∏ó‡∏µ‡πà‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {llm_full_response[:200]}...",
                        "Responsible": "System",
                        "Tools_Templates": "N/A",
                        "Verification_Outcome": "System review required"
                    }
                ]
            }
        ]
    }
    return [fallback_plan]
