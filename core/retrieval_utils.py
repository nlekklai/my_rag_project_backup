#core/retrieval_utils.py
import logging
import random
import json
import time 
from typing import List, Dict, Any, Optional, Union
from langchain.schema import SystemMessage, HumanMessage 
from langchain.schema import Document 

# üö® IMPORT: ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤ Regex (‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ô‡∏µ‡πâ)
import re 
# üö® IMPORT: ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤ Pydantic Model ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå action_plan_schema.py
from core.action_plan_schema import ActionPlanActions 
# üü¢ NEW: ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤ Pydantic Model ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå assessment_schema.py
from core.assessment_schema import StatementAssessment, EvidenceSummary
# üö® IMPORT: ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤ Prompts
from core.rag_prompts import (
    SYSTEM_ASSESSMENT_PROMPT, 
    USER_ASSESSMENT_PROMPT, 
    ACTION_PLAN_PROMPT,
    SYSTEM_ACTION_PLAN_PROMPT,
    SYSTEM_EVIDENCE_DESCRIPTION_PROMPT, 
    EVIDENCE_DESCRIPTION_PROMPT         
) 
from core.vectorstore import VectorStoreManager, load_all_vectorstores 
from models.llm import llm as llm_instance 

logger = logging.getLogger(__name__)
# ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏Ñ‡∏∏‡∏ì‡∏£‡∏±‡∏ô‡πÉ‡∏ô‡πÇ‡∏´‡∏°‡∏î real ‡πÅ‡∏•‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ debug/info ‡∏à‡∏∂‡∏á‡πÉ‡∏ä‡πâ level INFO
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


# =================================================================
# === MOCKING LOGIC AND GLOBAL FLAGS ===
# =================================================================

_MOCK_CONTROL_FLAG = False
_MOCK_COUNTER = 0

def set_mock_control_mode(enable: bool):
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡∏¥‡∏î/‡∏õ‡∏¥‡∏î‡πÇ‡∏´‡∏°‡∏î‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô (CONTROLLED MOCK)"""
    global _MOCK_CONTROL_FLAG, _MOCK_COUNTER
    _MOCK_CONTROL_FLAG = enable
    if enable:
        _MOCK_COUNTER = 0
        logger.info("üîë CONTROLLED MOCK Mode ENABLED.")
    else:
        logger.info("‚ùå CONTROLLED MOCK Mode DISABLED.")

def retrieve_context_with_filter(
    query: str, 
    retriever: Any, 
    metadata_filter: Optional[List[str]] = None, 
) -> Dict[str, Any]:
    """Retrieves documents from the vector store, optionally filtering by document ID."""
    if retriever is None:
        return {"top_evidences": []}
    
    filter_document_ids = metadata_filter 

    try:
        docs: List[Document] = retriever.invoke(query) 
        
        # Manual Filtering
        if filter_document_ids:
            filter_set = set(filter_document_ids)
            
            filtered_docs = []
            for doc in docs:
                doc_id_in_metadata = doc.metadata.get("doc_id") 

                if doc_id_in_metadata in filter_set:
                    filtered_docs.append(doc)
            
            docs = filtered_docs
        
        # ‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        top_evidences = []
        for d in docs:
            meta = d.metadata
            top_evidences.append({
                "doc_id": meta.get("doc_id"),
                "source": meta.get("source"),
                "content": d.page_content.strip()
            })
            
        return {"top_evidences": top_evidences}
        
    except Exception as e:
        logger.error(f"Error during RAG retrieval with filter: {e}")
        return {"top_evidences": []}


# =================================================================
# === EVALUATION FUNCTION (MOCK & REAL LLM) (‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç JSON Robustness) ===
# =================================================================

MAX_LLM_RETRIES = 3 

def evaluate_with_llm(statement: str, context: str, standard: str) -> Dict[str, Any]:
    """
    Performs the LLM evaluation, extracting a score and reason, 
    with robust JSON parsing and retry logic.
    """
    global _MOCK_CONTROL_FLAG, _MOCK_COUNTER
    
    # 1. MOCK CONTROL LOGIC
    if _MOCK_CONTROL_FLAG:
        _MOCK_COUNTER += 1
        
        # MOCK LOGIC: ‡πÉ‡∏´‡πâ Level 1-3 (9 statements) ‡∏ú‡πà‡∏≤‡∏ô ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö highest_full_level=3
        score = 1 if _MOCK_COUNTER <= 9 else 0
        reason_text = f"MOCK: FORCED {'PASS' if score == 1 else 'FAIL'} (Statement {_MOCK_COUNTER})"
        
        # üü¢ FIX: Calculate Pass Status for MOCK
        is_pass = score >= 1
        status_th = "‡∏ú‡πà‡∏≤‡∏ô" if is_pass else "‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô"

        logger.debug(f"MOCK COUNT: {_MOCK_COUNTER} | SCORE: {score} | STMT: '{statement[:20]}...'")
        
        # üü¢ FIX: Return the full data structure
        return {
            "score": score, 
            "reason": reason_text,
            "pass_status": is_pass, # ‚¨ÖÔ∏è ‡πÄ‡∏û‡∏¥‡πà‡∏° pass status
            "status_th": status_th  # ‚¨ÖÔ∏è ‡πÄ‡∏û‡∏¥‡πà‡∏° status ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
        }

    
    # 2. REAL LLM CALL LOGIC
    if llm_instance is None:
        logger.error("‚ùå LLM Instance is not initialized.")
        score = random.choice([0, 1])
        reason = f"LLM Initialization Failed (Fallback to Random Score {score})"
        
        # Fallback Logic
        is_pass = score >= 1
        status_th = "‡∏ú‡πà‡∏≤‡∏ô" if is_pass else "‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô"
        
        return {
            "score": score, 
            "reason": reason,
            "pass_status": is_pass,
            "status_th": status_th
        }

    # üü¢ NEW: ‡πÉ‡∏ä‡πâ PromptTemplate ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á HumanMessage Content
    user_prompt_content = USER_ASSESSMENT_PROMPT.format(
        statement=statement,
        standard=standard,
        # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô" ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà ‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ PromptTemplate
        context=context if context else "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á" 
    )
    
    # === NEW RETRY LOOP ===
    for attempt in range(MAX_LLM_RETRIES):
        try:
            # A. ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ LLM
            response = llm_instance.invoke([
                SystemMessage(content=SYSTEM_ASSESSMENT_PROMPT),
                HumanMessage(content=user_prompt_content)
            ])
            
            llm_response_content = response.content if hasattr(response, 'content') else str(response)
            
            # üõë B. FIX: ‡πÉ‡∏ä‡πâ Regex ‡πÄ‡∏û‡∏∑‡πà‡∏≠ Clean ‡πÅ‡∏•‡∏∞‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ JSON Block
            
            # 1. Clean up markdown fences (```json...```) 
            cleaned_content = llm_response_content.strip()
            if cleaned_content.startswith("```json"):
                cleaned_content = cleaned_content.replace("```json", "", 1).rstrip('`')
            elif cleaned_content.startswith("```"):
                cleaned_content = cleaned_content.replace("```", "", 1).rstrip('`')
            
            # 2. ‡πÉ‡∏ä‡πâ Regex ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏£‡∏≠‡∏á‡∏´‡∏≤ JSON block ‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå
            # NOTE: ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏°‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ key 'llm_score'
            json_match = re.search(r'\{.*\}', cleaned_content, re.DOTALL)
            
            final_json_string = None
            if json_match:
                final_json_string = json_match.group(0)
            
            if not final_json_string:
                raise ValueError("LLM response did not contain a recognizable JSON block.")
                
            # 3. Parse JSON string
            llm_output = json.loads(final_json_string) # ‚¨ÖÔ∏è Parse string ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏Å‡∏£‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß
            
            # 4. üü¢ NEW: Validate against StatementAssessment Pydantic Schema
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ Key 'score' ‡∏´‡∏£‡∏∑‡∏≠ 'llm_score' ‡πÅ‡∏•‡∏∞ 'reason' ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô
            raw_score = llm_output.get("llm_score") or llm_output.get("score")
            reason = llm_output.get("reason")
            
            if raw_score is not None and reason is not None:
                # ‡∏™‡∏£‡πâ‡∏≤‡∏á Dict ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ Pydantic Key
                validated_data = {
                    "score": int(str(raw_score)) if str(raw_score).isdigit() else 0,
                    "reason": reason
                }
                
                # 5. ‡πÉ‡∏ä‡πâ Pydantic Model Validate (‡πÑ‡∏î‡πâ score ‡πÅ‡∏•‡∏∞ reason)
                validated_assessment = StatementAssessment.model_validate(validated_data)
                
                # üü¢ FIX: Calculate Pass Status based on score
                final_score = validated_assessment.score
                # Assumption: score >= 1 is a Pass
                is_pass = final_score >= 1 
                status_th = "‡∏ú‡πà‡∏≤‡∏ô" if is_pass else "‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô"
                
                # üü¢ FIX: Return the complete assessment data, including calculated pass status
                result_data = validated_assessment.model_dump()
                result_data["pass_status"] = is_pass # ‚¨ÖÔ∏è ‡πÄ‡∏û‡∏¥‡πà‡∏° pass status
                result_data["status_th"] = status_th  # ‚¨ÖÔ∏è ‡πÄ‡∏û‡∏¥‡πà‡∏° status ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
                
                return result_data
            
            else:
                raise ValueError("LLM response JSON is missing 'score' or 'reason' keys.")

        except (json.JSONDecodeError, ValueError) as e:
            logger.warning(f"‚ùå Format/JSON Parse Failed (Attempt {attempt + 1}/{MAX_LLM_RETRIES}). Retrying in 1s... Error: {e}")
            if attempt < MAX_LLM_RETRIES - 1:
                time.sleep(1) 
                continue
            else:
                logger.error(f"‚ùå LLM Evaluation failed after {MAX_LLM_RETRIES} attempts. JSON/Format failure.")
                break 
        
        except Exception as e:
            logger.error(f"‚ùå LLM Evaluation failed due to unexpected error (Connection/Runtime). Error: {e}")
            break 

    # === FALLBACK LOGIC ===
    logger.error("‚ùå Using RANDOM SCORE as final fallback.")
    score = random.choice([0, 1])
    reason = f"LLM Call Failed (Fallback to Random Score {score}) after {MAX_LLM_RETRIES} attempts."
    
    # Fallback Logic
    is_pass = score >= 1
    status_th = "‡∏ú‡πà‡∏≤‡∏ô" if is_pass else "‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô"
    
    # üü¢ Fallback ‡∏Ñ‡∏ß‡∏£‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö StatementAssessment.model_dump() + pass status
    return {
        "score": score, 
        "reason": reason,
        "pass_status": is_pass, # ‚¨ÖÔ∏è ‡πÄ‡∏û‡∏¥‡πà‡∏° pass status
        "status_th": status_th  # ‚¨ÖÔ∏è ‡πÄ‡∏û‡∏¥‡πà‡∏° status ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
    }


# =================================================================
# === ACTION PLAN GENERATION UTILITY (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á Logic) ===
# =================================================================

def _call_llm_for_json_output(prompt: str, system_prompt: str) -> str:
    """Basic LLM call for JSON output, relies on the System Prompt to enforce JSON."""
    if llm_instance is None:
        raise ConnectionError("LLM Instance is not available for Action Plan Generation.")

    response = llm_instance.invoke([
        SystemMessage(content=system_prompt),
        HumanMessage(content=prompt)
    ])
    
    llm_response_content = response.content if hasattr(response, 'content') else str(response)
    
    # Clean up markdown fences (```json...```)
    cleaned_content = llm_response_content.strip()
    if cleaned_content.startswith("```json"):
        cleaned_content = cleaned_content.replace("```json", "", 1).rstrip('`')
    elif cleaned_content.startswith("```"):
        cleaned_content = cleaned_content.replace("```", "", 1).rstrip('`')
        
    return cleaned_content.strip()

def generate_action_plan_via_llm(
    failed_statements_data: List[Dict[str, Any]], 
    sub_id: str, 
    target_level: int, 
) -> Dict[str, Any]:
    
    # 1. MOCK LOGIC
    global _MOCK_CONTROL_FLAG
    if _MOCK_CONTROL_FLAG:
        logger.warning("MOCK: Generating dummy Action Plan via MOCK Logic.")
        actions = []
        for i, data in enumerate(failed_statements_data):
            statement_id = f"L{data.get('level', target_level)} S{data.get('statement_number', i+1)}"
            failed_level = data.get('level', target_level)
            
            # NOTE: ‡πÉ‡∏ä‡πâ model_dump() ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á dict ‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö ActionPlanActions.Actions.item_type
            actions.append(ActionPlanActions.Actions.item_type( 
                Statement_ID=statement_id,
                Failed_Level=failed_level,
                Recommendation=f"MOCK: [Specific Action] ‡∏à‡∏±‡∏î‡∏ó‡∏≥ Policy '{data.get('statement_text', 'N/A')[:20]}...' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç GAP ‡∏à‡∏≤‡∏Å‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•: {data.get('llm_reasoning', 'No reason')[:20]}...",
                Target_Evidence_Type="MOCK: Policy Document (Type: Guideline)",
                Key_Metric="Policy Approved and Published"
            ).model_dump())
        
        return ActionPlanActions(
            Phase=f"1. Strategic Gap Closure (Target L{target_level})",
            Goal=f"‡∏ö‡∏£‡∏£‡∏•‡∏∏‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô Level {target_level} ‡∏Ç‡∏≠‡∏á {sub_id}",
            Actions=actions
        ).model_dump()


    # --- REAL IMPLEMENTATION (LLM Call for Action Plan) ---
    try:
        # 2. Prepare Prompt Input Variables
        failed_statements_text = []
        for data in failed_statements_data:
            stmt_num = data.get('statement_number', 'N/A')
            failed_statements_text.append(f"""
            --- STATEMENT FAILED (L{data.get('level', 'N/A')} S{stmt_num}) ---
            Statement Text: {data.get('statement_text', 'N/A')}
            Reason for Failure: {data.get('llm_reasoning', 'N/A')}
            RAG Context Found: {data.get('retrieved_context', 'No context found or saved')}
            """)
            
        statements_list_str = "\n".join(failed_statements_text)

        # 3. Format the Prompt
        llm_prompt_content = ACTION_PLAN_PROMPT.format(
            sub_id=sub_id,
            target_level=target_level,
            failed_statements_list=statements_list_str 
        )
        

        # 4. Define System Prompt with JSON Schema
        schema_dict = ActionPlanActions.model_json_schema()
        
        # üü¢ ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á: ‡πÉ‡∏ä‡πâ SYSTEM_ACTION_PLAN_PROMPT ‡πÅ‡∏•‡∏∞‡∏ú‡∏ô‡∏ß‡∏Å JSON Schema ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ
        system_prompt_content = (
            SYSTEM_ACTION_PLAN_PROMPT + # ‚¨ÖÔ∏è ‡πÉ‡∏ä‡πâ System Prompt ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
            "\n\n--- REQUIRED JSON SCHEMA (STRICTLY FOLLOW) ---\n" +
            json.dumps(schema_dict, indent=2)
        )

        # 5. CALL LLM
        llm_response_json_str = _call_llm_for_json_output(
            prompt=llm_prompt_content,
            system_prompt=system_prompt_content # ‚¨ÖÔ∏è ‡πÉ‡∏ä‡πâ system prompt ‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß
        )
        
        
        # üõë 6. FIX: ‡πÉ‡∏ä‡πâ Regex ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ JSON Block ‡∏Å‡πà‡∏≠‡∏ô Parse
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ String ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if not llm_response_json_str or not llm_response_json_str.strip():
             raise ValueError("LLM returned an empty response for Action Plan.")
             
        # ‡πÉ‡∏ä‡πâ Regex ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ JSON Block
        json_match = re.search(r'\{.*\}', llm_response_json_str.strip(), re.DOTALL)

        cleaned_content = None
        if json_match:
            cleaned_content = json_match.group(0)

        if not cleaned_content:
            logger.error("‚ùå Failed to find a valid JSON block for Action Plan using Regex.")
            raise ValueError("LLM response did not contain a recognizable JSON block for Action Plan.")

        # 7. Process and Parse
        llm_result_dict = json.loads(cleaned_content) # ‚¨ÖÔ∏è ‡πÉ‡∏ä‡πâ string ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏Å‡∏£‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß
        
        # 8. Validate and Dump
        validated_plan_model = ActionPlanActions.model_validate(llm_result_dict)

        final_action_plan_result = validated_plan_model.model_dump()
        
        return final_action_plan_result

    except Exception as e:
        logger.error(f"‚ùå Action Plan Generation Failed: {e}", exc_info=True)
        return {
             "Phase": "Error",
             "Goal": f"Failed to generate Action Plan for {sub_id} (Target L{target_level})",
             "Actions": [{
                "Statement_ID": "N/A", 
                "Failed_Level": target_level, 
                "Recommendation": f"System Error: {str(e)}", 
                "Target_Evidence_Type": "Check Logs", 
                "Key_Metric": "Error"
             }]
        }
    

# ----------------------------------------------------------------------
# === NARRATIVE REPORT GENERATION ===
# ----------------------------------------------------------------------

def generate_narrative_report_via_llm_real(prompt_text: str, system_instruction: str) -> str:
    """
    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏Å LLM API ‡∏à‡∏£‡∏¥‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡πÄ‡∏ä‡∏¥‡∏á‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö CEO
    
    :param prompt_text: Prompt ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• JSON ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏†‡∏≤‡∏¢‡πÉ‡∏ô (Human Message)
    :param system_instruction: System Instruction ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó‡πÅ‡∏•‡∏∞‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö (‡∏°‡∏≤‡∏à‡∏≤‡∏Å rag_prompts.py)
    :return: ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡πÄ‡∏ä‡∏¥‡∏á‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏î‡∏¢ LLM
    """
    # üö® ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á LLM instance (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡πÉ‡∏ô environment ‡∏à‡∏£‡∏¥‡∏á)
    try:
        from langchain.schema import SystemMessage, HumanMessage 
        # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤ llm ‡∏ñ‡∏π‡∏Å import ‡∏à‡∏≤‡∏Å models.llm
        from models.llm import llm as llm_instance 
    except ImportError:
        logger.error("‚ùå Cannot import LLM dependencies (SystemMessage, HumanMessage, llm_instance).")
        return "[ERROR: LLM Dependencies missing for real API call.]"
        
    if llm_instance is None:
        logger.error("‚ùå LLM Instance is not initialized for real API call.")
        return "[ERROR: LLM Client is not initialized for real API call.]"
        
    logger.info("Executing REAL LLM call for narrative report synthesis...")
    
    try:
        # System Message ‡∏ñ‡∏π‡∏Å‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô Argument
        
        response = llm_instance.invoke([
            SystemMessage(content=system_instruction),
            HumanMessage(content=prompt_text)
        ])
        
        # üü¢ FIX: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó Response ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á .content
        # ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô String ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á ‡∏´‡∏£‡∏∑‡∏≠ LangChain Response Object
        if hasattr(response, 'content'):
            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô LangChain/SDK Response Object
            generated_text = response.content
        elif isinstance(response, str):
            # ‡∏ñ‡πâ‡∏≤ Wrapper ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô String ‡∏ï‡∏£‡∏á‡πÜ
            generated_text = response
        else:
            # ‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å
            raise TypeError(f"LLM response type {type(response)} is not supported for content extraction.")
            
        # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å LLM
        return generated_text.strip()
        
    except Exception as e:
        logger.error(f"Real LLM API call failed during narrative report generation: {e}")
        # Fallback message
        return f"[API ERROR] Failed to generate narrative report via real LLM API: {e}"

# =================================================================
# === EVIDENCE DESCRIPTION GENERATION (NEW FUNCTION) ===
# =================================================================

def summarize_context_with_llm(context: str, sub_criteria_name: str, level: int, sub_id: str, schema: Any) -> Dict[str, str]:
    """
    ‡πÉ‡∏ä‡πâ LLM ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡∏∏‡∏õ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡πÅ‡∏•‡∏∞‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î 
    ‡πÅ‡∏•‡∏∞ Validate ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏î‡πâ‡∏ß‡∏¢ EvidenceSummary Schema.
    """
    
    # üö® FIX 1: ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß Context ‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤ LLM
    MAX_LLM_SUMMARY_CONTEXT = 3000 
    
    if llm_instance is None:
        return {"summary": "‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: LLM Client ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô", "suggestion_for_next_level": "N/A"}
        
    context_to_use = context
    if len(context) > MAX_LLM_SUMMARY_CONTEXT:
        logger.warning(f"Context for summary L{level} is too long ({len(context)}), truncating to {MAX_LLM_SUMMARY_CONTEXT}.")
        context_to_use = context[:MAX_LLM_SUMMARY_CONTEXT]
        
    # üö® FIX 2: ‡∏õ‡∏£‡∏±‡∏ö System Prompt ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö JSON Output ‡∏ï‡∏≤‡∏° EvidenceSummary
    schema_dict = EvidenceSummary.model_json_schema()
    system_prompt_content = (
        SYSTEM_EVIDENCE_DESCRIPTION_PROMPT + 
        "\n\n--- REQUIRED JSON SCHEMA (STRICTLY FOLLOW) ---\n" +
        json.dumps(schema_dict, indent=2)
    )

    try:
        # 1. ‡∏à‡∏±‡∏î‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Human Prompt
        human_prompt = EVIDENCE_DESCRIPTION_PROMPT.format(
            standard=sub_criteria_name,
            level=level,
            context=context_to_use,
            sub_id=sub_id # üö® FIX 2: ‡πÄ‡∏û‡∏¥‡πà‡∏° sub_id ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ format
        )
        
        # 2. ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ LLM 
        llm_response_json_str = _call_llm_for_json_output(
            prompt=human_prompt,
            system_prompt=system_prompt_content
        )
        
        # 3. ‡πÉ‡∏ä‡πâ Regex ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ JSON Block
        json_match = re.search(r'\{.*\}', llm_response_json_str.strip(), re.DOTALL)
        
        cleaned_content = None
        if json_match:
            cleaned_content = json_match.group(0)

        if not cleaned_content:
            logger.error("‚ùå Failed to find a valid JSON block for Evidence Summary using Regex.")
            raise ValueError("LLM response did not contain a recognizable JSON block for Evidence Summary.")

        # 4. Process and Parse
        llm_result_dict = json.loads(cleaned_content) 
        
        # 5. Validate against EvidenceSummary Schema
        validated_summary_model = EvidenceSummary.model_validate(llm_result_dict)
        
        logger.info(f"‚úÖ Generated Evidence Summary for {sub_criteria_name} L{level}")
        
        # 6. Return the validated Dict
        return validated_summary_model.model_dump()
        
    except Exception as e:
        logger.error(f"LLM Summary generation failed: {e}", exc_info=True)
        return {"summary": "‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏î‡∏¢ LLM", "suggestion_for_next_level": f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å LLM: {str(e)}"}
    
def generate_evidence_description_via_llm(*args, **kwargs) -> str:
    """
    Deprecated: ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏ñ‡∏π‡∏Å‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏î‡πâ‡∏ß‡∏¢ summarize_context_with_llm ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Pydantic Schema
    """
    logger.warning("generate_evidence_description_via_llm is deprecated. Using summarize_context_with_llm instead.")
    # Fallback/Error message based on the old usage pattern (if accidentally called)
    return "‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏ñ‡∏π‡∏Å‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô ‡πÇ‡∏õ‡∏£‡∏î‡πÉ‡∏ä‡πâ summarize_context_with_llm ‡πÅ‡∏ó‡∏ô"