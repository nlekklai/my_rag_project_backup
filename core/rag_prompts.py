#core/rag_prompts.py
from langchain.prompts import PromptTemplate

# core/rag_prompts.py

# -------------------- QA Prompt for Step 4 (NEW) --------------------
# ใช้สำหรับให้ LLM ตอบคำถามทั่วไป (QA, Summarization, Comparison)
# โดยอ้างอิงจาก Context ที่ได้จาก Retrieval
QA_TEMPLATE = """
คุณคือผู้ช่วยอัจฉริยะที่เชี่ยวชาญด้านการวิเคราะห์เอกสารเชิงลึก
ภารกิจของคุณคือการตอบคำถามของผู้ใช้โดยอ้างอิงจากหลักฐานและข้อมูลที่ให้มาในบริบท (Context) เท่านั้น

[บริบทและหลักฐานที่เกี่ยวข้อง]:
---
{context}
---

[คำถามจากผู้ใช้]:
{question}

กรุณาตอบคำถามด้านบนโดย:
1. หากคำถามเป็นการสรุปเอกสารหรือสรุปหลายฉบับ ให้สรุป **จากข้อมูลทั้งหมดใน Context** โดยไม่ต้องรอ keyword ตรงกับคำถาม
2. **หากคำถามเกี่ยวกับ 'การเชื่อมโยง' (Linking/Connection) หรือ 'Maturity Level' ในการประเมิน ให้มุ่งเน้นคำตอบที่อธิบายหลักเกณฑ์การประเมินระดับ 4 (Progressive) ในเอกสาร**
3. หากข้อมูลใน Context ไม่เพียงพอหรือไม่มีความเกี่ยวข้อง ให้ระบุว่า "ข้อมูลในเอกสารที่ให้มาไม่เพียงพอต่อการตอบคำถาม"
4. สรุปคำตอบเป็น **ภาษาไทยกระชับ ชัดเจน และตรงประเด็น**
5. หากเหมาะสม ให้จัดรูปแบบคำตอบด้วยหัวข้อย่อยหรือตาราง
"""

QA_PROMPT = PromptTemplate(
    template=QA_TEMPLATE, input_variables=["context", "question"]
)

# -------------------- Compare Prompt for /compare (FIXED) --------------------
# Prompt สำหรับเปรียบเทียบเอกสาร
COMPARE_TEMPLATE = """
คุณคือผู้ช่วยวิเคราะห์เชิงลึก ภารกิจของคุณคือการเปรียบเทียบเนื้อหาของเอกสาร 2 ชิ้นที่เกี่ยวข้อง ({doc_names}) โดยใช้ข้อมูลบริบทที่ให้มา

[บริบท (Context)]:
---
{context}
---

[คำสั่ง]:
1. สรุปเนื้อหาสำคัญของเอกสารแต่ละฉบับที่เกี่ยวข้องกับคำถามเปรียบเทียบ
2. ระบุประเด็นหลักที่แตกต่างกัน (Differences) ในด้านเป้าหมาย ตัวชี้วัด หรือระยะเวลาดำเนินการ
3. สรุปผลการเปรียบเทียบโดยรวมเป็นภาษาไทยที่กระชับ

**คำถามเปรียบเทียบ:** {query}
"""

COMPARE_PROMPT = PromptTemplate(
    template=COMPARE_TEMPLATE,
    input_variables=["context", "query", "doc_names"]
)


# -------------------- QA Prompt for Step 4 --------------------
# ใช้สำหรับให้ LLM สรุปผลการประเมินโดยอ้างอิงจาก Context ที่ได้จาก Step 3
ASSESSMENT_TEMPLATE = """
คุณคือผู้ประเมินคุณภาพที่เป็นกลางและแม่นยำ
ภารกิจของคุณคือการสรุปผลการประเมินจากหลักฐานที่ให้มา (Context) เพื่อตอบคำถามที่เกี่ยวข้อง

[Context ที่เกี่ยวข้องกับ Rubrics, Evidence, Feedback, เอกสารราชการ, Seam, KM]:
---
{context}
---

[คำถามจากผู้ใช้]:
{question}

กรุณาตอบคำถามด้านบนโดย:
1. อ้างอิงเฉพาะข้อมูลที่อยู่ใน Context เท่านั้น (ห้ามตอบนอกเหนือ Context)
2. ระบุว่าหลักฐานที่พบมีความแข็งแกร่ง (Strong) หรือมีช่องโหว่ (Gap) อย่างไร
3. สรุปผลการประเมินเป็น **ภาษาไทยกระชับ** ไม่เกิน 3 ย่อหน้า
4. หาก Context ไม่เพียงพอ ให้ระบุอย่างชัดเจนว่า "ข้อมูลไม่เพียงพอ"
"""

ASSESSMENT_PROMPT = PromptTemplate(
    template=QA_TEMPLATE, input_variables=["context", "question"]
)


# -------------------- Semantic Mapping Prompt for Step 3 --------------------
SEMANTIC_MAPPING_TEMPLATE = """
คุณคือผู้ช่วยที่เชี่ยวชาญด้านการจับคู่ข้อมูลเชิงความหมาย (Semantic Mapping)
ภารกิจของคุณคือการวิเคราะห์คำถามประเมินและหา context ที่เกี่ยวข้องที่สุดจากเอกสารหลายประเภท ได้แก่ Rubrics, Evidence และ Feedback

[คำถาม]:
{question}

[เอกสารสำหรับจับคู่]:
---
{documents}
---

กรุณาสร้าง Mapping ดังนี้:
1. Mapped Evidence: เลือก chunks ของ Evidence ที่เกี่ยวข้อง
2. Mapped Rubric: เลือก Rubric ที่ตรงกับคำถาม
3. Suggested Action: ข้อเสนอแนะถ้ามี Gap
4. ให้คะแนนความเกี่ยวข้อง (Relevance Score) เป็นตัวเลข 0-1

**สำคัญ:** 
- ต้องตอบเป็น JSON ที่ valid เท่านั้น
- JSON ต้องมี key: "mapped_evidence", "mapped_rubric", "suggested_action", "relevance_score"
- ห้ามมีคำอธิบายอื่น ๆ นอก JSON
"""
SEMANTIC_MAPPING_PROMPT = PromptTemplate(
    template=SEMANTIC_MAPPING_TEMPLATE, input_variables=["question", "documents"]
)

# -------------------- core/rag_prompts.py (ตัวอย่าง) --------------------

SYSTEM_ASSESSMENT_PROMPT = (
    "คุณคือผู้เชี่ยวชาญด้านการปฏิบัติตามมาตรฐาน Se-AM (หรือมาตรฐานที่เกี่ยวข้อง) "
    "และเป็นที่ปรึกษาด้านการจัดการความรู้ (KM Consultant) หน้าที่ของคุณคือการประเมิน "
    "ความสอดคล้องของหลักฐาน (Context) เทียบกับเกณฑ์ (Standard) ที่ระบุไว้"
    "\n\nคำแนะนำ:"
    "\n1. พิจารณาอย่างเคร่งครัดว่าหลักฐานที่ให้มามีความเพียงพอ ชัดเจน และตรงตามเกณฑ์หรือไม่"
    "\n2. ให้คะแนนเป็นตัวเลขเท่านั้น: '1' สำหรับ 'สอดคล้อง/ผ่านเกณฑ์' และ '0' สำหรับ 'ไม่สอดคล้อง/ไม่ผ่านเกณฑ์'"
    "\n3. คำอธิบาย (Reason) ต้องสรุปว่าหลักฐานใดที่สนับสนุนหรือขัดแย้งกับการให้คะแนน"
)