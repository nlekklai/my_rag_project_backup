import os
import shutil
from typing import Set, List
# FIXED: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å ingest_all_files ‡πÄ‡∏õ‡πá‡∏ô process_document ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö
from core.ingest import process_document, delete_document, DATA_DIR 

# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô
EXCLUDED_FOLDERS: Set[str] = {'evidence', 'qa', 'rubrics', 'feedback'}

def get_top_level_files(data_dir: str, exclude_dirs: Set[str]) -> List[str]:
    """‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠ Doc ID (‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏•‡∏±‡∏Å) ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå data/ ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô"""
    file_ids = []
    for item in os.listdir(data_dir):
        path = os.path.join(data_dir, item)
        # ‡∏Ç‡πâ‡∏≤‡∏°‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô
        if os.path.isdir(path) and item in exclude_dirs:
            continue
        
        # ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏•‡∏±‡∏Å‡πÄ‡∏õ‡πá‡∏ô Doc ID
        if os.path.isfile(path):
            file_id = os.path.splitext(item)[0]
            file_ids.append(file_id)
    return file_ids

def run_batch_ingestion():
    """
    Ingests all files directly in the data/ folder, excluding specific subdirectories.
    **Now running sequentially to avoid segmentation faults on M-series chips (MPS concurrency issues).**
    """
    print("--- Starting Batch Ingestion of Top-Level Documents (Sequential Mode) ---")
    print(f"Skipping documents in subdirectories: {EXCLUDED_FOLDERS}")
    
    files_to_clean_ids = get_top_level_files(DATA_DIR, EXCLUDED_FOLDERS)

    # 1. CRITICAL: Clean up old vectorstores for top-level documents first
    print("\nüßπ Cleaning up old Vector Stores for top-level documents...")
    cleaned_count = 0
    for doc_id in files_to_clean_ids:
        vectorstore_path = os.path.join("vectorstore", doc_id)
        if os.path.exists(vectorstore_path):
            shutil.rmtree(vectorstore_path)
            print(f"  - Deleted old vector store folder: {doc_id}")
            cleaned_count += 1
    
    if cleaned_count == 0:
        print("  - No existing vector stores found to clean.")
    print("üßπ Cleanup complete.")
    
    # 2. Sequential Ingestion
    print("\nüöÄ Starting sequential document processing...")
    results = []
    
    for doc_id in files_to_clean_ids:
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏ï‡πá‡∏°‡∏à‡∏≤‡∏Å Doc ID
        full_filename = next(
            (item for item in os.listdir(DATA_DIR) 
             if os.path.isfile(os.path.join(DATA_DIR, item)) and os.path.splitext(item)[0] == doc_id),
            None
        )
        
        if full_filename:
            file_path = os.path.join(DATA_DIR, full_filename)
            print(f"  - Ingesting '{full_filename}'...")
            try:
                # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å process_document ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á (Single-Threaded)
                processed_id = process_document(
                    file_path, 
                    full_filename, 
                    collection_id=doc_id # Explicitly pass the correct ID
                )
                results.append({"file": full_filename, "doc_id": processed_id, "status": "processed"})
            except Exception as e:
                print(f"  ‚ùå Error processing {full_filename}: {e}")
                results.append({"file": full_filename, "doc_id": doc_id, "status": "failed"})


    # 3. Summary
    print("\n--- Ingestion Summary ---")
    for result in results:
        status_icon = "‚úÖ" if result['status'] == 'processed' else "‚ùå"
        print(f"{status_icon} {result['file']} -> Doc ID: {result['doc_id']} ({result['status']})")
        
    if not results:
        print("‚ÑπÔ∏è No top-level files found to process.")
        
    print("\n--- Batch Ingestion Complete ---")


if __name__ == "__main__":
    run_batch_ingestion()
