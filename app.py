# app.py (Full Code - Fixed robustness for missing 'file_path')

from fastapi import FastAPI, APIRouter, HTTPException, UploadFile, File, Form, BackgroundTasks, Query, Path
from pydantic import BaseModel, Field
from typing import List, Dict, Optional, Any, Union
import os
from datetime import datetime, timezone
import logging
import json
from langchain.schema import Document as LcDocument, SystemMessage, HumanMessage 
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse
from starlette.concurrency import run_in_threadpool
from contextlib import asynccontextmanager
from typing import Tuple
from core.retrieval_utils import (
        retrieve_context_by_doc_ids, 
        parse_llm_json_response,
        load_all_vectorstores,
        retrieve_context_with_filter
    )

# üü¢ ‡πÄ‡∏û‡∏¥‡πà‡∏° Imports ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö RAG Core Logic ‡πÅ‡∏•‡∏∞ Reranker

# ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡∏•‡∏≤‡∏™ Ranker ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Final Rerank
from flashrank import Ranker 

try:
    from core.rag_prompts import QA_PROMPT, COMPARE_PROMPT, SYSTEM_QA_INSTRUCTION, SYSTEM_COMPARE_INSTRUCTION 
    
    # üü¢ FIX: ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤ SUPPORTED_DOC_TYPES, DocInfo, ‡πÅ‡∏•‡∏∞‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏à‡∏≤‡∏Å core.ingest
    from core.ingest import (
        process_document, 
        list_documents, 
        delete_document_by_uuid, 
        DATA_DIR, 
        SUPPORTED_TYPES, 
        DocInfo, 
        SUPPORTED_DOC_TYPES # <--- ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡∏à‡∏≤‡∏Å ingest ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
    )
    
    from core.vectorstore import (
        vectorstore_exists, 
        get_vectorstore_path, 
        VectorStoreManager, 
        MultiDocRetriever, 
        NamedRetriever, 
        _get_collection_name,
        INITIAL_TOP_K, 
        FINAL_K_RERANKED
    )
    
    from langchain.chains import RetrievalQA
    from models.llm import llm as llm_instance 
    from core.run_assessment import run_assessment_process 
    from core.evidence_mapping_generator import EvidenceMappingGenerator
    
except ImportError as e:
    # üî¥ ‡∏´‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏î ImportError ‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤‡∏Ç‡∏≤‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏≠‡∏∑‡πà‡∏ô‡∏ô‡∏≠‡∏Å‡πÄ‡∏´‡∏ô‡∏∑‡∏≠‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏•‡πâ‡∏ß
    print(f"‚ùå FATAL ERROR: Core module import failed. Missing file: {e}")
    class TempImportError(Exception): pass 
    raise TempImportError(f"CRITICAL: Missing core files for non-mock operation. Check your imports and project structure. Error: {e}") 


# -----------------------------
# --- Logging Setup ---
# -----------------------------
logger = logging.getLogger("ingest")
logger.setLevel(logging.INFO)
ch = logging.StreamHandler()
ch.setLevel(logging.INFO)
formatter = logging.Formatter("%(asctime)s | %(levelname)s | %(message)s")
ch.setFormatter(formatter)
logger.addHandler(ch)

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# -----------------------------
# --- Global Constants ---
# -----------------------------
VECTORSTORE_DIR = "vectorstore"
REF_DATA_DIR = "ref_data" 
DEFAULT_ENABLER = "KM"
SUPPORTED_ENABLERS = ["CG", "L", "SP", "RM&IC", "SCM", "DT", "HCM", "KM", "IM", "IA"]

# (Add this near other Pydantic model definitions in app.py)
from langchain_core.pydantic_v1 import BaseModel as LangchainBaseModel # Import Langchain Pydantic v1
from langchain_core.output_parsers import JsonOutputParser

class LLMComparisonMetric(LangchainBaseModel):
    """Schema for a single comparison point returned by the LLM."""
    metric: str = Field(..., description="The key metric or area being compared (e.g., 'Required Documents', 'Fee Structure', 'Eligibility')")
    doc1: str = Field(..., description="The value or description of this metric in Document 1")
    doc2: str = Field(..., description="The value or description of this metric in Document 2")
    delta: float = Field(..., description="The quantitative change between doc1 and doc2 (e.g., 5.0, -2.5, 0). Use 0 if not quantitative.")
    remark: Optional[str] = Field(None, description="Any additional brief remark or explanation.")
    
# Update CompareRequest to include 'query'
class CompareRequest(BaseModel):
    # ‚úÖ FIX: ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Default ‡πÄ‡∏õ‡πá‡∏ô None ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Pydantic ‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö null/omitted field
    doc1_uuid: Optional[str] = None
    doc2_uuid: Optional[str] = None
    
    # ‚úÖ FIX: ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Default ‡πÄ‡∏õ‡πá‡∏ô None
    doc_type_list: Optional[List[str]] = None
    
    # ‚úÖ FIX: ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Default ‡πÄ‡∏õ‡πá‡∏ô None
    query: Optional[str] = None
# -----------------------------
# --- Helper Functions (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ JSON Files) ---
# -----------------------------


def get_ref_data_path(enabler: str, data_type: str) -> str:
    """‡∏™‡∏£‡πâ‡∏≤‡∏á path ‡πÄ‡∏ï‡πá‡∏°‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå JSON ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Enabler ‡πÅ‡∏•‡∏∞ Data Type ‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏"""
    enabler = enabler.lower()
    
    if data_type == 'statements':
        filename = f"{enabler}_evidence_statements_checklist.json"
    elif data_type == 'rubrics':
        filename = f"{enabler}_rating_criteria_rubric.json"
    elif data_type == 'mapping':
        filename = f"{enabler}_evidence_mapping.json"
    elif data_type == 'weighting':
        filename = f"{enabler}_scoring_level_fractions.json"
    else:
        raise ValueError(f"Invalid data_type: {data_type}")
        
    return os.path.join(REF_DATA_DIR, filename) 

def load_ref_data_file(filepath: str) -> Any:
    """‡∏≠‡πà‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• JSON ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î"""
    if not os.path.exists(filepath):
        if any(keyword in filepath for keyword in ['statements', 'mapping', 'rubric']):
            return []
        return {}
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            return json.load(f)
    except json.JSONDecodeError:
        logger.error(f"Failed to decode JSON from {filepath}")
        raise HTTPException(status_code=500, detail=f"Invalid JSON format in {filepath}")
    except Exception as e:
        logger.error(f"Error loading file {filepath}: {e}")
        raise HTTPException(status_code=500, detail=f"Error loading file: {filepath}")

def save_ref_data_file(filepath: str, data: Any):
    """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• JSON ‡∏Å‡∏•‡∏±‡∏ö‡∏•‡∏á‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå"""
    os.makedirs(os.path.dirname(filepath), exist_ok=True) 
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


# -----------------------------
# --- Lifespan (‡πÅ‡∏ó‡∏ô on_event) ---
# -----------------------------
@asynccontextmanager
async def lifespan(app: FastAPI):
    """Lifespan manager ‡πÅ‡∏ó‡∏ô @app.on_event ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö startup/shutdown"""
    # --- Startup ---
    os.makedirs(DATA_DIR, exist_ok=True)
    os.makedirs(VECTORSTORE_DIR, exist_ok=True)
    os.makedirs(REF_DATA_DIR, exist_ok=True) 
    logging.info(f"‚úÖ Data directory '{DATA_DIR}', vectorstore '{VECTORSTORE_DIR}', and ref_data '{REF_DATA_DIR}' ensured.")

    yield  # <-- Application runs here

    # --- Shutdown ---
    logging.info("üõë Application shutdown complete.")

# -----------------------------
# --- FastAPI Initialization ---
# -----------------------------
app = FastAPI(
    title="Assessment RAG API",
    description="API for RAG-based document assessment and analysis.",
    lifespan=lifespan
)

# -----------------------------
# --- CORS ---
# -----------------------------
origins = ["http://localhost:5173", "http://127.0.0.1:5173", "*"]
app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# -----------------------------
# --- Pydantic Models ---
# -----------------------------

class UploadResponse(BaseModel):
    doc_id: str = Field(..., description="Unique ID of the ingested document.") # Changed from uuid to doc_id
    status: str = Field(..., description="Status of the upload and processing.")
    filename: str = Field(..., description="Original name of the uploaded file.")
    doc_type: str = Field(..., description="Document type.")
    upload_date: Optional[str] = Field(None, description="ISO formatted upload date.") # Added for list endpoint consistency
    message: Optional[str] = Field(None, description="Optional message/error.")
    
class AssessmentRequest(BaseModel):
    enabler: str = "KM"
    sub_criteria_id: str = "all"
    mode: str = "real"
    filter_mode: bool = False
    export_results: bool = False
    
class AssessmentRecord(BaseModel):
    record_id: str
    enabler: str
    sub_criteria_id: str
    mode: str
    timestamp: str
    
    status: str = "RUNNING" 

    overall_score: Optional[float] = None
    highest_full_level: Optional[int] = None
    export_path: Optional[str] = None

class RefDataPayload(BaseModel):
    data: Dict | List 

    
# Global List of Assessment Records (in-memory for demo/simple environment)
ASSESSMENT_HISTORY: List[AssessmentRecord] = []

# -----------------------------
# --- Assessment Endpoints ---
# -----------------------------

@app.get("/list-collections/")
async def debug_list_collections():
    """Returns a list of collection names that the server can see in the vectorstore directory."""
    try:
        manager = VectorStoreManager() 
        collections = manager.get_all_collection_names() 
        return {"available_collections": collections, "status": "Success", "vectorstore_dir": manager._base_path}
    except Exception as e:
        return {"available_collections": [], "status": f"Error: Failed to initialize VectorStoreManager or access path: {str(e)}"}

@app.post("/api/assess")
async def run_assessment_task(request: AssessmentRequest, background_tasks: BackgroundTasks):
    record_id = os.urandom(8).hex()
    
    initial_record = AssessmentRecord(
        record_id=record_id,
        enabler=request.enabler.upper(),
        sub_criteria_id=request.sub_criteria_id,
        mode=request.mode,
        timestamp=datetime.now(timezone.utc).isoformat(),
        status="RUNNING" 
    )
    ASSESSMENT_HISTORY.append(initial_record)
    
    background_tasks.add_task(_background_assessment_runner, record_id, request)
    
    return {"status": "accepted", "record_id": record_id, "message": "Assessment started in background. Check /api/assess/history for status."}

# -----------------------------
# --- Assessment History Endpoint ---
# -----------------------------
@app.get("/api/assess/history", response_model=List[AssessmentRecord])
async def get_assessment_history(enabler: Optional[str] = None): 
    
    filtered_history = ASSESSMENT_HISTORY
    
    if enabler:
        enabler_upper = enabler.upper()
        
        filtered_history = [
            record for record in ASSESSMENT_HISTORY 
            if record.enabler.upper() == enabler_upper
        ]
        
    return sorted(filtered_history, key=lambda r: r.timestamp, reverse=True)


@app.get("/api/assess/results/{record_id}")
async def get_assessment_results(record_id: str):
    record = next((r for r in ASSESSMENT_HISTORY if r.record_id == record_id), None)
    if not record:
        raise HTTPException(status_code=404, detail="Assessment record not found.")

    if record.export_path and os.path.exists(record.export_path):
        return FileResponse(record.export_path, media_type="application/json", filename=os.path.basename(record.export_path))
    
    raise HTTPException(status_code=404, detail="Full assessment data not available for this record.")


# -----------------------------
# --- Reference Data Endpoints ---
# -----------------------------

# R1: GET /api/ref_data/{enabler}
@app.get("/api/ref_data/{enabler}")
async def get_all_reference_data(enabler: str):
    """
    R1: ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Reference Data ‡∏ó‡∏±‡πâ‡∏á 4 ‡∏ä‡∏ô‡∏¥‡∏î (Statements, Rubrics, Mapping, Weighting) 
    ‡∏Ç‡∏≠‡∏á Enabler ‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏‡πÉ‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
    """
    enabler = enabler.lower()
    data = {}
    
    def load_data_safe(data_type: str):
        filepath = get_ref_data_path(enabler, data_type)
        data[data_type] = load_ref_data_file(filepath)

    try:
        await run_in_threadpool(lambda: load_data_safe('statements'))
        await run_in_threadpool(lambda: load_data_safe('rubrics'))
        await run_in_threadpool(lambda: load_data_safe('mapping'))
        await run_in_threadpool(lambda: load_data_safe('weighting'))
        
        data['enabler'] = enabler.upper()

        return data
    except HTTPException:
        raise 
    except Exception as e:
        logger.error(f"Error loading all ref data for {enabler}: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to load all reference data for {enabler}")


# R2: POST /api/ref_data/{enabler}/{data_type}
@app.post("/api/ref_data/{enabler}/{data_type}")
async def save_reference_data(enabler: str, data_type: str, payload: RefDataPayload):
    """
    R2: ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Reference Data (Statements, Rubrics, Mapping, ‡∏´‡∏£‡∏∑‡∏≠ Weighting) 
    """
    enabler = enabler.lower()
    
    if data_type not in ['statements', 'rubrics', 'mapping', 'weighting']:
        raise HTTPException(status_code=400, detail="Invalid data_type. Must be one of: statements, rubrics, mapping, weighting.")
        
    filepath = get_ref_data_path(enabler, data_type)
    
    try:
        await run_in_threadpool(lambda: save_ref_data_file(filepath, payload.data))
        logger.info(f"Saved {data_type} for {enabler} to {filepath}")
        return {"status": "success", "enabler": enabler.upper(), "data_type": data_type}
    except Exception as e:
        logger.error(f"Error saving {data_type} for {enabler}: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to save {data_type} data.")


# R3: POST /api/ref_data/auto_map/{enabler}
@app.post("/api/ref_data/auto_map/{enabler}")
async def trigger_auto_mapping(enabler: str, background_tasks: BackgroundTasks):
    """
    R3: Trigger Background Task ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ Auto Mapping/LLM Generation 
    """
    enabler = enabler.lower()
    
    background_tasks.add_task(_background_auto_mapper, enabler)
    
    return {"status": "accepted", "enabler": enabler.upper(), "message": "Auto Mapping process started in background."}


# -----------------------------
# --- Background Runner Logic ---
# -----------------------------

def _background_assessment_runner(record_id: str, request: AssessmentRequest):
    logger.info(f"Processing background assessment for {record_id}...")
    
    record = next((r for r in ASSESSMENT_HISTORY if r.record_id == record_id), None)
    if not record:
        logger.error(f"FATAL: Initial record {record_id} not found in history list. Exiting runner.")
        return 
        
    try:
        final_summary = run_assessment_process(
            enabler=request.enabler,
            sub_criteria_id=request.sub_criteria_id,
            mode=request.mode,
            filter_mode=request.filter_mode,
            export=True 
        )
        
        record.overall_score = final_summary['Overall']['overall_maturity_score']
        
        sub_id_for_level = request.sub_criteria_id if request.sub_criteria_id != 'all' else list(final_summary['SubCriteria_Breakdown'].keys())[0] if final_summary['SubCriteria_Breakdown'] else None
        record.highest_full_level = final_summary['SubCriteria_Breakdown'].get(sub_id_for_level, {}).get('highest_full_level', 0) if sub_id_for_level else 0
        
        record.export_path = final_summary.get("export_path_used")
        record.status = "COMPLETED" 
        
        record.timestamp = datetime.now(timezone.utc).isoformat()
        
        logger.info(f"Assessment {record_id} completed successfully. Score: {record.overall_score:.2f}")

    except Exception as e:
        logger.error(f"Assessment task {record_id} failed: {e}")
        record.overall_score = -1.0
        record.highest_full_level = -1
        record.status = "FAILED"
        record.timestamp = datetime.now(timezone.utc).isoformat()

# -----------------------------
# --- Auto Mapping Background Runner ---
# -----------------------------
def _background_auto_mapper(enabler: str):
    logger.info(f"Starting Auto Mapping for {enabler}...")
    
    try:
        generator = EvidenceMappingGenerator(enabler_id=enabler.upper())
        new_mapping_data = generator.generate_full_mapping_data() 
        
        filepath = get_ref_data_path(enabler, 'mapping')
        save_ref_data_file(filepath, new_mapping_data) 
        
        logger.info(f"Auto Mapping for {enabler} completed and saved successfully.")

    except Exception as e:
        logger.error(f"Auto Mapping task for {enabler} failed: {e}")
        
        
# -----------------------------
# --- Uploads & Document Endpoints (Using Bracket Notation for DocInfo) ---
# -----------------------------
@app.get("/api/uploads/document", response_model=List[UploadResponse])
async def list_uploads_document_only():
    """
    Endpoint ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö GET /api/uploads/document 
    """
    return await list_uploads_by_type("document") 


@app.get("/api/documents", response_model=List[UploadResponse])
async def get_documents():
    return await list_all_uploads() 

@app.get("/api/uploads/list", response_model=List[UploadResponse])
async def list_all_uploads():
    """
    ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏à‡∏≥‡∏Å‡∏±‡∏î doc_type (‡πÉ‡∏ä‡πâ DocInfo Dict)
    """
    doc_data: Dict[str, DocInfo] | List[DocInfo] = await run_in_threadpool(lambda: list_documents(doc_types=None))
    
    uploads: List[UploadResponse] = []
    
    if not isinstance(doc_data, dict):
        logger.error(f"API Error: list_documents returned {type(doc_data).__name__}. Expected dict.")
        
        # üü¢ FIX: ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô list ‡πÉ‡∏´‡πâ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô dict ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ 'doc_id' ‡πÄ‡∏õ‡πá‡∏ô key
        if isinstance(doc_data, list):
            doc_data = {item['doc_id']: item for item in doc_data if isinstance(item, dict) and 'doc_id' in item}
        else:
            return uploads # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏ó‡∏±‡πâ‡∏á dict ‡πÅ‡∏•‡∏∞ list ‡∏Å‡πá‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á

    if not isinstance(doc_data, dict):
        # ‡∏´‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏î‡πâ‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á
        return uploads

    for uuid, doc_info in doc_data.items():
        
        # üü¢ FIX: ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡πÅ‡∏•‡∏∞‡∏•‡∏≠‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏£‡∏∞‡∏ö‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô try/except ‡∏ó‡∏µ‡πà‡πÅ‡∏¢‡∏Å‡∏Å‡∏±‡∏ô
        file_name = doc_info.get('filename', 'Unknown')
        file_path = doc_info.get('file_path')
        upload_date_iso = datetime.now(timezone.utc).isoformat() # Default

        if file_path:
            try:
                timestamp = os.path.getmtime(file_path) 
                upload_datetime = datetime.fromtimestamp(timestamp, tz=timezone.utc)
                upload_date_iso = upload_datetime.isoformat()
            except Exception as e:
                # Log warning when the actual file is missing or inaccessible
                logger.warning(f"Failed to get modification time for {file_name} ({file_path}). Error: {e}")
        else:
             # Log warning when 'file_path' metadata is missing 
             logger.warning(f"Metadata missing 'file_path' for document {uuid} ({file_name}). Using current timestamp.")
            
        uploads.append(UploadResponse(
            doc_id=uuid,
            # üü¢ FIX: ‡πÉ‡∏ä‡πâ file_name ‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏°‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
            filename=file_name,
            file_type=os.path.splitext(file_name)[1], 
            status="Ingested" if doc_info['chunk_count'] > 0 else "Pending",
            upload_date=upload_date_iso
        ))
        
    uploads.sort(key=lambda x: x.filename)
    return uploads

@app.delete("/api/documents/{doc_id}")
async def remove_document(doc_id: str, doc_type: str = Query("document", description="Document type collection name"), enabler: Optional[str] = Query(None)):
    """‡∏•‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å Vectorstore ‡πÅ‡∏•‡∏∞ Mapping ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ UUID, doc_type ‡πÅ‡∏•‡∏∞ enabler (‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô evidence)"""
    try:
        # NOTE: delete_document_by_uuid ‡πÉ‡∏ô core/ingest.py ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö stable_doc_uuid, doc_type, ‡πÅ‡∏•‡∏∞ enabler
        await run_in_threadpool(lambda: delete_document_by_uuid(stable_doc_uuid=doc_id, doc_type=doc_type, enabler=enabler))
        return {"status": "ok", "doc_id": doc_id, "doc_type": doc_type, "enabler": enabler}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

# -----------------------------
# --- Upload Endpoints ---
# -----------------------------
@app.post("/upload", response_model=UploadResponse)
async def upload_file(file: UploadFile = File(...), source_name: Optional[str] = Form(None)):
    os.makedirs(DATA_DIR, exist_ok=True)
    folder = os.path.join(DATA_DIR, "document")
    os.makedirs(folder, exist_ok=True)
    file_path = os.path.join(folder, file.filename)
    
    with open(file_path, "wb") as f:
        f.write(await file.read())
        
    doc_id = await run_in_threadpool(lambda: process_document(file_path=file_path, file_name=file.filename, doc_type="document")) 
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£ Ingest (‡πÉ‡∏ä‡πâ doc_id ‡πÅ‡∏•‡∏∞ doc_type)
    status = "Ingested" if await run_in_threadpool(lambda: vectorstore_exists(doc_id=doc_id, doc_type="document", enabler=None, base_path=VECTORSTORE_DIR)) else "Pending"
    
    return UploadResponse(
        status=status,
        doc_id=doc_id,
        filename=file.filename,
        file_type=os.path.splitext(file.filename)[1],
        upload_date=datetime.now(timezone.utc).isoformat()
    )

@app.post("/upload/{doc_type}", response_model=UploadResponse)
async def upload_document(
    doc_type: str = Path(..., description=f"The type of document to upload. Must be one of: {SUPPORTED_DOC_TYPES}"),
    file: UploadFile = File(..., description="The document file to upload."),
    background_tasks: BackgroundTasks = None,
) -> UploadResponse:
    """
    Uploads a document for ingestion into the RAG system. Processing is done in the background.
    """
    if doc_type not in SUPPORTED_DOC_TYPES:
        raise HTTPException(
            status_code=400, 
            detail=f"Invalid doc_type: {doc_type}. Must be one of {SUPPORTED_DOC_TYPES}"
        )

    # 1. Save the file and start process (Mocking the UUID generation in process_document)
    timestamp = datetime.now(timezone.utc).strftime("%Y%m%d%H%M%S")
    sanitized_filename = "".join(c for c in file.filename if c.isalnum() or c in ('.', '_', '-')).rstrip()
    if not sanitized_filename:
        sanitized_filename = f"uploaded_file_{timestamp}"
    
    temp_file_path = os.path.join(DATA_DIR, f"{timestamp}_{sanitized_filename}")
    os.makedirs(DATA_DIR, exist_ok=True)
    
    # A mock doc_id is needed for the immediate response, the actual one will be saved by the background task
    mock_doc_id = f"temp-{timestamp}-{os.urandom(4).hex()}" 

    try:
        contents = await file.read()
        await run_in_threadpool(lambda: open(temp_file_path, "wb").write(contents))
        
        # 2. Add the document processing to background tasks
        # NOTE: The process_document implementation must handle saving the final DocInfo with the correct UUID/Doc_ID.
        # üõë FIX: Updated to pass 'file_name' and 'stable_doc_uuid' to match the core/ingest.py signature.
        background_tasks.add_task(
            process_document, 
            file_path=temp_file_path, 
            file_name=file.filename, # Re-introducing file name under the correct parameter name
            stable_doc_uuid=mock_doc_id, # Use mock ID as placeholder for the required 64-char hash
            doc_type=doc_type
        )
        
        # 3. Respond immediately
        return UploadResponse(
            doc_id=mock_doc_id, # Return a mock/temp ID
            status="pending", # Use 'pending' for consistency with background process
            filename=file.filename,
            doc_type=doc_type,
            upload_date=datetime.now(timezone.utc).isoformat(),
            message="Document accepted for background processing."
        )

    except Exception as e:
        logger.error(f"Error during file upload process: {e}")
        if os.path.exists(temp_file_path):
            os.remove(temp_file_path)
        raise HTTPException(status_code=500, detail=f"Failed to process upload: {e}")

# ----------------------------------------------------
# --- List Documents Endpoint (RESTORED) ---
# ----------------------------------------------------
@app.get("/api/uploads/{doc_type}", response_model=List[UploadResponse]) 
async def list_uploads_by_type(doc_type: str) -> List[UploadResponse]:
    """
    ‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô doc_type ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î ‡∏´‡∏£‡∏∑‡∏≠ 'all'
    """
    
    doc_type_to_fetch = [doc_type] if doc_type.lower() != 'all' else SUPPORTED_DOC_TYPES
    
    # Fetch document metadata asynchronously
    doc_data: Union[Dict[str, DocInfo], List[DocInfo]] = await run_in_threadpool(
        lambda: list_documents(doc_types=doc_type_to_fetch)
    )
    
    uploads: List[UploadResponse] = []
    
    doc_map = {}
    
    # üü¢ Handle both dict and list return types from list_documents
    if isinstance(doc_data, dict):
        doc_map = doc_data
    elif isinstance(doc_data, list):
         # Convert list of DocInfo (dicts) to a map keyed by doc_id (the stable UUID)
         doc_map = {item.get('doc_id') or item.get('stable_doc_uuid'): item for item in doc_data if isinstance(item, dict) and ('doc_id' in item or 'stable_doc_uuid' in item)}
    else:
        # Handle unexpected type gracefully (should return empty but log error)
        logger.error(f"API Error: list_documents returned unexpected type: {type(doc_data).__name__}. Returning empty list.")
        return uploads

    requested_doc_type = doc_type.lower()
    
    for doc_id, doc_info in doc_map.items():
        
        actual_doc_type = doc_info.get('doc_type', '').lower()
        
        # ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
        if requested_doc_type != 'all' and actual_doc_type != requested_doc_type: 
            continue
            
        file_name = doc_info.get('filename', 'Unknown')
        file_path = doc_info.get('file_path')
        upload_date_iso = datetime.now(timezone.utc).isoformat() # Default

        # Determine Upload Date from File Metadata
        if file_path and os.path.exists(file_path):
            try:
                timestamp = os.path.getmtime(file_path) 
                upload_datetime = datetime.fromtimestamp(timestamp, tz=timezone.utc)
                upload_date_iso = upload_datetime.isoformat()
            except Exception as e:
                logger.warning(f"Failed to get modification time for {file_name}. Error: {e}")
        
        # Determine Status
        status = "Pending"
        chunk_count = doc_info.get('chunk_count', 0)
        
        # üü¢ ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏ï‡∏£‡∏£‡∏Å‡∏∞‡πÉ‡∏´‡∏°‡πà: ‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏Ç‡∏≠‡∏á Chunk ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å
        ingestion_status = doc_info.get('ingestion_status', '').lower()
        
        if ingestion_status == 'failed':
             status = "Failed"
        elif ingestion_status == 'processing':
             status = "Processing"
        elif chunk_count > 0:
            # üí° ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ chunks ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ Failed/Processing ‡πÉ‡∏´‡πâ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô Ingested
            status = "Ingested"
        # ‡∏´‡∏≤‡∏Å status_lower ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á (‡πÄ‡∏ä‡πà‡∏ô ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• status) ‡πÅ‡∏•‡∏∞ chunk_count ‡πÄ‡∏õ‡πá‡∏ô 0 ‡∏à‡∏∞‡∏Ñ‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô "Pending" ‡πÑ‡∏ß‡πâ
             
        uploads.append(UploadResponse(
            doc_id=doc_id,
            filename=file_name,
            doc_type=actual_doc_type, 
            status=status,
            upload_date=upload_date_iso,
            message=doc_info.get('error_message') # Include error message if failed
        ))
        
    uploads.sort(key=lambda x: x.filename)
    
    return uploads

# -----------------------------
# --- Upload File Deletion, Download ---
# -----------------------------
@app.delete("/upload/{doc_type}/{file_id}")
async def delete_upload_by_id(doc_type: str, file_id: str):
    """
    ‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞ Chunks ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å Vector Store ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ ID/UUID
    """
    
    target_doc_type = doc_type
        
    try:
        await run_in_threadpool(lambda: delete_document_by_uuid(doc_id=file_id, doc_type=target_doc_type))
        logger.info(f"Successfully initiated deletion for Doc ID: {file_id}")
        return {"status": f"Document {file_id} deletion initiated."}
    except FileNotFoundError:
        raise HTTPException(status_code=404, detail=f"File not found for ID: {file_id}")
    except Exception as e:
        logger.error(f"Error deleting file {file_id}: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to delete document: {e}")

@app.get("/upload/{doc_type}/{file_id}")
async def download_upload(doc_type: str, file_id: str):
    
    # üìå NOTE: list_documents ‡∏ñ‡∏π‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà‡∏î‡πâ‡∏ß‡∏¢ ‡πÅ‡∏•‡∏∞‡∏≠‡∏≤‡∏à‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô list
    doc_data: Dict[str, DocInfo] | List[DocInfo] = await run_in_threadpool(lambda: list_documents(doc_types=[doc_type]))
    
    # üü¢ FIX: ‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô dict ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏≤ filepath
    if not isinstance(doc_data, dict) and isinstance(doc_data, list):
         doc_data = {item['doc_id']: item for item in doc_data if isinstance(item, dict) and 'doc_id' in item}
         
    file_name_to_download = None
    filepath = None
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ doc_data ‡πÄ‡∏õ‡πá‡∏ô dict ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏Å .items()
    if isinstance(doc_data, dict):
        for uuid, info in doc_data.items():
            if uuid == file_id:
                # üü¢ FIX: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô info['file_name'] ‡πÄ‡∏õ‡πá‡∏ô info['filename']
                file_name_to_download = info.get('filename') 
                filepath = info.get('file_path') # ‡πÉ‡∏ä‡πâ .get() ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
                break
            
    if not file_name_to_download:
        raise HTTPException(status_code=404, detail="Document ID not found in mapping.")
        
    if not filepath or not os.path.exists(filepath):
        raise HTTPException(status_code=404, detail="File not found on disk.")
        
    return FileResponse(filepath, filename=file_name_to_download)


# -----------------------------
# --- Ingest Endpoint ---
# -----------------------------
class IngestRequest(BaseModel):
    doc_ids: List[str]
    doc_type: Optional[str] = "document"
    enabler: Optional[str] = None # NEW: ‡πÄ‡∏û‡∏¥‡πà‡∏° enabler

@app.post("/ingest")
async def ingest_documents(request: IngestRequest):
    """
    ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß‡∏ï‡∏≤‡∏° doc_ids (Stable UUIDs)
    """
    results = []
    
    # Note: Folder check removed as files are stored in DATA_DIR regardless of doc_type sub-folder structure.
    # folder = os.path.join(DATA_DIR, request.doc_type)
    # if not os.path.isdir(folder):
    #      return {"status": "failed", "error": f"Document type folder not found: {folder}"}

    # üìå NOTE: list_documents ‡∏ñ‡∏π‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà‡∏î‡πâ‡∏ß‡∏¢ ‡πÅ‡∏•‡∏∞‡∏≠‡∏≤‡∏à‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô list
    doc_data: Dict[str, DocInfo] | List[DocInfo] = await run_in_threadpool(lambda: list_documents(doc_types=[request.doc_type]))
    
    # üü¢ FIX: ‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô dict ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏î‡πâ‡∏ß‡∏¢ doc_id
    # Ensure doc_data is a dictionary mapping doc_id to DocInfo
    if isinstance(doc_data, list):
         doc_data = {item.get('doc_id') or item.get('stable_doc_uuid'): item for item in doc_data if isinstance(item, dict) and ('doc_id' in item or 'stable_doc_uuid' in item)}
         
    if not isinstance(doc_data, dict):
        return {"status": "failed", "error": "Failed to retrieve document list from DocInfo."}

    for doc_id in request.doc_ids:
        
        info = doc_data.get(doc_id)
        if not info:
             results.append({"doc_id": doc_id, "result": "failed", "error": f"Document ID '{doc_id}' not found in DocInfo mapping. Was the file uploaded via /upload first?"})
             continue
        
        # üü¢ FIX: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô info['file_name'] ‡πÄ‡∏õ‡πá‡∏ô info['filename'] (‡∏ï‡∏≤‡∏° UploadResponse model)
        file_name = info.get('filename')
        file_path = info.get('file_path')
        
        if not file_name or not file_path:
             results.append({"doc_id": doc_id, "result": "failed", "error": f"Document ID '{doc_id}' metadata missing filename or file_path."})
             continue


        file_extension = os.path.splitext(file_name)[1].lower()
        if file_extension not in SUPPORTED_TYPES:
            results.append({"doc_id": doc_id, "result": "failed", "error": f"Unsupported file type: {file_extension}. Supported types are: {', '.join(SUPPORTED_TYPES)}"})
            continue
        
        if not os.path.exists(file_path):
            results.append({"doc_id": doc_id, "result": "failed", "error": f"File path not found on disk: {file_path}. The file may have been manually deleted."})
            continue

        logger.info(f"Attempting to re-ingest file: {file_path}")

        try:
            # NOTE: process_document ‡πÉ‡∏ô core/ingest.py ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö enabler
            await run_in_threadpool(
                process_document,
                file_path=file_path, 
                file_name=file_name, 
                doc_type=request.doc_type, 
                base_path=VECTORSTORE_DIR,
                stable_doc_uuid=doc_id, 
                enabler=request.enabler # <--- ‡∏™‡πà‡∏á enabler
            )
            
        except Exception as e:
            logger.error(f"Error while processing document {doc_id}: {e}", exc_info=True)
            results.append({"doc_id": doc_id, "result": "failed", "error": str(e)})
            continue
        
        # Check for vectorstore existence after processing
        if await run_in_threadpool(lambda: vectorstore_exists(doc_id=doc_id, doc_type=request.doc_type, enabler=request.enabler, base_path=VECTORSTORE_DIR)):
            results.append({"doc_id": doc_id, "result": "success"})
        else:
            logger.warning(f"Vectorstore not found for {doc_id} after processing.")
            results.append({"doc_id": doc_id, "result": "failed", "error": "Vectorstore not found after processing"})

    return {"status": "completed", "results": results}


# --- ‡πÇ‡∏°‡πÄ‡∏î‡∏• (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°) ---
class QuerySource(BaseModel):
    source_id: str = Field(..., description="UUID of the source document.")
    file_name: str = Field(..., description="Original file name.")
    chunk_text: str = Field(..., description="The content of the retrieved chunk.")
    chunk_id: str = Field(..., description="The internal ID of the chunk.")
    score: float = Field(..., description="Relevance score of the chunk.")

class QueryResponse(BaseModel):
    answer: str = Field(..., description="The final answer generated by the LLM based on the query and retrieved context.")
    sources: List[QuerySource] = Field(..., description="List of document chunks used as context.")

# --- Endpoint ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á ---
# --- Endpoint ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á (Final Version) ---
@app.post("/query", response_model=QueryResponse)
async def query_llm(
    background_tasks: BackgroundTasks,
    question: str = Form(...),
    conversation_id: Optional[str] = Form(None),
    doc_ids: Optional[List[str]] = Form(None),
    doc_types: Optional[List[str]] = Form(None)
):
    if not question:
        raise HTTPException(status_code=400, detail="Missing required field: question")

    doc_ids = doc_ids or []
    doc_types = doc_types or []

    # üü¢ ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Enabler (‡πÉ‡∏ä‡πâ KM)
    evidence_enabler = DEFAULT_ENABLER
    
    logger.info(
    f"üß† /query received: question='{question[:60]}...', doc_ids={doc_ids}, doc_types={doc_types}"
    )

    if llm_instance is None:
        raise HTTPException(status_code=503, detail="LLM service is not available.")

    # 1Ô∏è‚É£ Retrieve documents (‡πÉ‡∏ä‡πâ retrieve_context_with_filter ‡∏ß‡∏ô‡∏ã‡πâ‡∏≥)
    
    # 1. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Reranker (‡πÉ‡∏ä‡πâ Flashrank) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏Ç‡πâ‡∏≤‡∏° Collection
    try:
        FINAL_K_VALUE = FINAL_K_RERANKED
        ranker = Ranker()
    except Exception as e:
        logger.error(f"Failed to initialize Flashrank: {e}. Cannot perform final rerank.")
        # ‡∏´‡∏≤‡∏Å Reranker ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß ‡∏ñ‡∏∑‡∏≠‡πÄ‡∏õ‡πá‡∏ô Critical error
        raise HTTPException(status_code=500, detail="Reranking service initialization failed.")
    
    all_chunks_raw: List[LcDocument] = []
    
    # 2. ‡∏ß‡∏ô‡∏ã‡πâ‡∏≥ (Iterate) ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô‡∏ó‡∏∏‡∏Å doc_types ‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏™‡πà‡∏á‡∏°‡∏≤
    valid_doc_types = doc_types if doc_types else ["document"] # ‡πÉ‡∏ä‡πâ "document" ‡πÄ‡∏õ‡πá‡∏ô default
    
    for d_type in valid_doc_types:
        try:
            # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Enabler (‡πÉ‡∏ä‡πâ KM ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö 'evidence' ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô)
            current_enabler = evidence_enabler if d_type.lower() == "evidence" else None
            
            # üö® FIX: ‡πÉ‡∏ä‡πâ d_type ‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á doc_ids ‡πÄ‡∏õ‡πá‡∏ô Hard Filter
            retrieved_result = await run_in_threadpool(
                lambda: retrieve_context_with_filter(
                    query=question,
                    doc_type=d_type,
                    enabler=current_enabler,
                    stable_doc_ids=doc_ids, # Hard Filter
                    top_k_reranked=100 # ‡∏î‡∏∂‡∏á‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏≤‡∏Å‡∏û‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Final Rerank
                )
            )
            
            # ‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
            if isinstance(retrieved_result, list):
                all_chunks_raw.extend(retrieved_result)
            elif isinstance(retrieved_result, dict) and 'documents' in retrieved_result:
                 all_chunks_raw.extend(retrieved_result.get('documents', []))
            
            logger.info(f"Retrieved {len(all_chunks_raw)} raw chunks (current type: {d_type}).")


        except Exception as e:
            logger.error(f"Retrieval error for doc_type='{d_type}': {e}", exc_info=True)
            # Log error ‡πÅ‡∏•‡∏∞‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡πà‡∏≠‡πÑ‡∏õ

    logger.info(f"Total raw chunks retrieved across all types: {len(all_chunks_raw)}")
    
    # 3. Final Rerank ‡πÅ‡∏•‡∏∞ Truncate ‡∏Ç‡πâ‡∏≤‡∏° Collections
    if not all_chunks_raw:
        retrieved_docs_lc = []
    else:
        # ‡πÅ‡∏õ‡∏•‡∏á LcDocument ‡πÄ‡∏õ‡πá‡∏ô dict format ‡∏ó‡∏µ‡πà Flashrank ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
        flashrank_docs = [{
            "id": i,
            "text": doc.page_content, 
            "meta": doc.metadata
        } for i, doc in enumerate(all_chunks_raw)]
        
        # Rerank ‡∏î‡πâ‡∏ß‡∏¢ Flashrank
        reranked_results = ranker.rank(question, flashrank_docs)

        # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Top K ‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô LcDocument
        top_reranked_results = reranked_results[:FINAL_K_VALUE]
        retrieved_docs_lc: List[LcDocument] = []

        for result in top_reranked_results:
            original_doc = all_chunks_raw[result['id']]
            # Inject relevance score ‡∏à‡∏≤‡∏Å Flashrank
            original_doc.metadata["score"] = result['relevance_score']
            retrieved_docs_lc.append(original_doc)
            
    logger.info(f"Final retrieved documents after cross-collection Reranking: {len(retrieved_docs_lc)}")
    
    # 4. Handle Empty Result (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Error 500)
    if not retrieved_docs_lc:
        return QueryResponse(
            answer="‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å ‡πÇ‡∏õ‡∏£‡∏î‡∏•‡∏≠‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡∏∑‡πà‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì",
            sources=[]
        )
    
    # üü¢ DEBUG: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Ç‡∏≠‡∏á Top K Chunks
    for i, doc in enumerate(retrieved_docs_lc):
        file_name = doc.metadata.get('file_name', doc.metadata.get('source', 'N/A'))
        logger.critical(f"üîç CHUNK {i+1} SOURCE: {file_name} | Doc ID: {doc.metadata.get('doc_id', 'N/A')} | Score: {doc.metadata.get('score', 0.0):.4f}")
        logger.critical(f"   CONTENT SNIPPET: {doc.page_content[:500].replace('\n', ' ')}...")
        
    # --------------

    # 2Ô∏è‚É£ ‡∏™‡∏£‡πâ‡∏≤‡∏á context ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LLM
    context_chunks = []
    for i, doc in enumerate(retrieved_docs_lc):
        source_name = doc.metadata.get('file_name', doc.metadata.get('source', 'N/A'))
        context_chunks.append(
            f"**Source {i+1} (Doc: {source_name}):**\n{doc.page_content}"
        )
    context_text = "\n\n---\n\n".join(context_chunks)

    human_message_content = QA_PROMPT.format(context=context_text, question=question)
    messages = [
        SystemMessage(content=SYSTEM_QA_INSTRUCTION),
        HumanMessage(content=human_message_content)
    ]

    # 3Ô∏è‚É£ ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å LLM ‡∏à‡∏£‡∏¥‡∏á
    try:
        llm_answer = await run_in_threadpool(lambda: llm_instance.invoke(messages))
        llm_answer = llm_answer.content if hasattr(llm_answer, 'content') else str(llm_answer)
    except Exception as e:
        logger.error(f"LLM generation error: {e}", exc_info=True)
        llm_answer = "‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÇ‡∏î‡∏¢ AI"

    # 4Ô∏è‚É£ ‡∏™‡∏£‡πâ‡∏≤‡∏á sources ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö response
    final_sources = []
    for doc in retrieved_docs_lc:
        final_sources.append(QuerySource(
            source_id=doc.metadata.get('doc_id', 'N/A'),
            file_name=doc.metadata.get('file_name', doc.metadata.get('source', 'N/A')), 
            chunk_text=doc.page_content,
            chunk_id=doc.metadata.get('chunk_uuid', 'N/A'), 
            score=doc.metadata.get('score', 0.0) # score ‡∏°‡∏≤‡∏à‡∏≤‡∏Å Reranker
        ))

    return QueryResponse(answer=llm_answer, sources=final_sources)


# ----------------------------------------------------
# --- Compare Endpoint ---
# ----------------------------------------------------

@app.post("/compare")
async def compare_documents(
    doc1: str = Form(...),
    doc2: str = Form(...),
    query: str = Form(...),
    doc_types: List[str] = Form(None)
):
    """
    ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏™‡∏≠‡∏á‡∏â‡∏ö‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ LLM
    """
    all_doc_ids = [doc1, doc2]
    final_doc_type_list = doc_types if doc_types else SUPPORTED_DOC_TYPES
    try:
        context_docs = await run_in_threadpool(lambda: retrieve_context_by_doc_ids(
            doc_ids=all_doc_ids, 
            k=99999, 
            doc_types=final_doc_type_list
        ))
    except Exception as e:
        logger.error(f"Retrieval error during comparison: {e}")
        raise HTTPException(status_code=500, detail="RAG retrieval failed during comparison setup.")

    doc1_text = "\n".join([d.page_content for d in context_docs if d.metadata.get('doc_id') == doc1])
    doc2_text = "\n".join([d.page_content for d in context_docs if d.metadata.get('doc_id') == doc2])
    
    skipped = False
    if not doc1_text or not doc2_text:
        skipped = True
        logger.warning(f"Skipping comparison: Missing content for Doc1 ({len(doc1_text)} chars) or Doc2 ({len(doc2_text)} chars).")
        return {
            "result": {"metrics": []}, 
            "skipped": True
        }

    final_query = query if query else "Key differences and similarities."
    
    human_message_content = COMPARE_PROMPT.format(
        doc1_content=doc1_text[:20000], 
        doc2_content=doc2_text[:20000], 
        query=final_query
    )
    
    messages = [
        SystemMessage(content=SYSTEM_COMPARE_INSTRUCTION),
        HumanMessage(content=human_message_content)
    ]
    
    def call_llm_safe(messages_list: List[Any]) -> str:
        """Helper to invoke LLM and return raw content (expected to be JSON string)."""
        res = llm_instance.invoke(messages_list) 
        if hasattr(res, 'content'): 
            return res.content.strip()
        return str(res).strip()

    logger.info(f"Calling LLM for comparison with query: {final_query}")
    json_delta_string = await run_in_threadpool(lambda: call_llm_safe(messages))
    
    # ‚úÖ FIX: ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ parse_llm_json_response ‡∏à‡∏≤‡∏Å core.retrieval_utils
    try:
        metrics_data = parse_llm_json_response(json_delta_string, List[LLMComparisonMetric])
    except Exception as e:
        logger.error(f"Failed to parse LLM comparison JSON: {e}. Raw response: {json_delta_string[:500]}...")
        metrics_data = []
        
    # 6. Final response structure
    return {
        "result": {
            "metrics": metrics_data
        },
        "skipped": skipped
    }

class HealthCheckResponse(BaseModel):
    status: str
    message: str
    timestamp: str

@app.get("/health", response_model=HealthCheckResponse)
def health_check():
    return HealthCheckResponse(
        status="ok",
        message="RAG service is operational.",
        timestamp=datetime.now(timezone.utc).isoformat()
    )
