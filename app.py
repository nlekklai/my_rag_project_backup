# app.py (Full Code - Fixed robustness for missing 'file_path')

from fastapi import FastAPI, APIRouter, HTTPException, UploadFile, File, Form, BackgroundTasks, Query
from pydantic import BaseModel, Field
from typing import List, Dict, Optional, Any
import os
from datetime import datetime, timezone
import time
import logging
import json
from langchain.schema import Document, SystemMessage, HumanMessage 
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse
from starlette.concurrency import run_in_threadpool
from contextlib import asynccontextmanager
from typing import Tuple

# --- Core Imports (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡πÉ‡∏ô‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå) ---
try:
    from core.rag_prompts import QA_PROMPT, COMPARE_PROMPT, SYSTEM_QA_INSTRUCTION, SYSTEM_COMPARE_INSTRUCTION 
    
    # üü¢ FIX: ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤ SUPPORTED_DOC_TYPES, DocInfo, ‡πÅ‡∏•‡∏∞‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏à‡∏≤‡∏Å core.ingest
    from core.ingest import (
        process_document, 
        list_documents, 
        delete_document_by_uuid, 
        DATA_DIR, 
        SUPPORTED_TYPES, 
        DocInfo, 
        SUPPORTED_ENABLERS,
        SUPPORTED_DOC_TYPES # <--- ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡∏à‡∏≤‡∏Å ingest ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
    )
    
    from core.vectorstore import (
        vectorstore_exists, 
        get_vectorstore_path, 
        VectorStoreManager, 
        MultiDocRetriever, 
        NamedRetriever, 
        _get_collection_name,
        INITIAL_TOP_K, 
        FINAL_K_RERANKED
    )
    
    from langchain.chains import RetrievalQA
    from models.llm import llm as llm_instance 
    from core.run_assessment import run_assessment_process 
    from core.evidence_mapping_generator import EvidenceMappingGenerator
    
except ImportError as e:
    # üî¥ ‡∏´‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏î ImportError ‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤‡∏Ç‡∏≤‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏≠‡∏∑‡πà‡∏ô‡∏ô‡∏≠‡∏Å‡πÄ‡∏´‡∏ô‡∏∑‡∏≠‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏•‡πâ‡∏ß
    print(f"‚ùå FATAL ERROR: Core module import failed. Missing file: {e}")
    class TempImportError(Exception): pass 
    raise TempImportError(f"CRITICAL: Missing core files for non-mock operation. Check your imports and project structure. Error: {e}") 


# -----------------------------
# --- Logging Setup ---
# -----------------------------
logger = logging.getLogger("ingest")
logger.setLevel(logging.INFO)
ch = logging.StreamHandler()
ch.setLevel(logging.INFO)
formatter = logging.Formatter("%(asctime)s | %(levelname)s | %(message)s")
ch.setFormatter(formatter)
logger.addHandler(ch)

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# -----------------------------
# --- Global Constants ---
# -----------------------------
VECTORSTORE_DIR = "vectorstore"
REF_DATA_DIR = "ref_data" 

# (Add this near other Pydantic model definitions in app.py)
from langchain_core.pydantic_v1 import BaseModel as LangchainBaseModel # Import Langchain Pydantic v1
from langchain_core.output_parsers import JsonOutputParser

class LLMComparisonMetric(LangchainBaseModel):
    """Schema for a single comparison point returned by the LLM."""
    metric: str = Field(..., description="The key metric or area being compared (e.g., 'Required Documents', 'Fee Structure', 'Eligibility')")
    doc1: str = Field(..., description="The value or description of this metric in Document 1")
    doc2: str = Field(..., description="The value or description of this metric in Document 2")
    delta: float = Field(..., description="The quantitative change between doc1 and doc2 (e.g., 5.0, -2.5, 0). Use 0 if not quantitative.")
    remark: Optional[str] = Field(None, description="Any additional brief remark or explanation.")
    
# Update CompareRequest to include 'query'
class CompareRequest(BaseModel):
    # ‚úÖ FIX: ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Default ‡πÄ‡∏õ‡πá‡∏ô None ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Pydantic ‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö null/omitted field
    doc1_uuid: Optional[str] = None
    doc2_uuid: Optional[str] = None
    
    # ‚úÖ FIX: ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Default ‡πÄ‡∏õ‡πá‡∏ô None
    doc_type_list: Optional[List[str]] = None
    
    # ‚úÖ FIX: ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Default ‡πÄ‡∏õ‡πá‡∏ô None
    query: Optional[str] = None
# -----------------------------
# --- Helper Functions (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ JSON Files) ---
# -----------------------------

def get_ref_data_path(enabler: str, data_type: str) -> str:
    """‡∏™‡∏£‡πâ‡∏≤‡∏á path ‡πÄ‡∏ï‡πá‡∏°‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå JSON ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Enabler ‡πÅ‡∏•‡∏∞ Data Type ‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏"""
    enabler = enabler.lower()
    
    if data_type == 'statements':
        filename = f"{enabler}_evidence_statements_checklist.json"
    elif data_type == 'rubrics':
        filename = f"{enabler}_rating_criteria_rubric.json"
    elif data_type == 'mapping':
        filename = f"{enabler}_evidence_mapping.json"
    elif data_type == 'weighting':
        filename = f"{enabler}_scoring_level_fractions.json"
    else:
        raise ValueError(f"Invalid data_type: {data_type}")
        
    return os.path.join(REF_DATA_DIR, filename) 

def load_ref_data_file(filepath: str) -> Any:
    """‡∏≠‡πà‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• JSON ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î"""
    if not os.path.exists(filepath):
        if any(keyword in filepath for keyword in ['statements', 'mapping', 'rubric']):
            return []
        return {}
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            return json.load(f)
    except json.JSONDecodeError:
        logger.error(f"Failed to decode JSON from {filepath}")
        raise HTTPException(status_code=500, detail=f"Invalid JSON format in {filepath}")
    except Exception as e:
        logger.error(f"Error loading file {filepath}: {e}")
        raise HTTPException(status_code=500, detail=f"Error loading file: {filepath}")

def save_ref_data_file(filepath: str, data: Any):
    """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• JSON ‡∏Å‡∏•‡∏±‡∏ö‡∏•‡∏á‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå"""
    os.makedirs(os.path.dirname(filepath), exist_ok=True) 
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


# -----------------------------
# --- Lifespan (‡πÅ‡∏ó‡∏ô on_event) ---
# -----------------------------
@asynccontextmanager
async def lifespan(app: FastAPI):
    """Lifespan manager ‡πÅ‡∏ó‡∏ô @app.on_event ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö startup/shutdown"""
    # --- Startup ---
    os.makedirs(DATA_DIR, exist_ok=True)
    os.makedirs(VECTORSTORE_DIR, exist_ok=True)
    os.makedirs(REF_DATA_DIR, exist_ok=True) 
    logging.info(f"‚úÖ Data directory '{DATA_DIR}', vectorstore '{VECTORSTORE_DIR}', and ref_data '{REF_DATA_DIR}' ensured.")

    yield  # <-- Application runs here

    # --- Shutdown ---
    logging.info("üõë Application shutdown complete.")

# -----------------------------
# --- FastAPI Initialization ---
# -----------------------------
app = FastAPI(
    title="Assessment RAG API",
    description="API for RAG-based document assessment and analysis.",
    lifespan=lifespan
)

# -----------------------------
# --- CORS ---
# -----------------------------
origins = ["http://localhost:5173", "http://127.0.0.1:5173", "*"]
app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# -----------------------------
# --- Pydantic Models ---
# -----------------------------
class UploadResponse(BaseModel):
    doc_id: str
    filename: str
    file_type: str
    status: str
    upload_date: str
    
class AssessmentRequest(BaseModel):
    enabler: str = "KM"
    sub_criteria_id: str = "all"
    mode: str = "real"
    filter_mode: bool = False
    export_results: bool = False
    
class AssessmentRecord(BaseModel):
    record_id: str
    enabler: str
    sub_criteria_id: str
    mode: str
    timestamp: str
    
    status: str = "RUNNING" 

    overall_score: Optional[float] = None
    highest_full_level: Optional[int] = None
    export_path: Optional[str] = None

class RefDataPayload(BaseModel):
    data: Dict | List 
    
# Global List of Assessment Records (in-memory for demo/simple environment)
ASSESSMENT_HISTORY: List[AssessmentRecord] = []

# -----------------------------
# --- Helper: Setup MultiDocRetriever ---
# -----------------------------
def _setup_multi_retriever(doc_type_list: List[str], enabler: Optional[str] = None, filter_doc_ids: Optional[List[str]] = None) -> MultiDocRetriever:
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ MultiDocRetriever ‡πÇ‡∏î‡∏¢‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Enabler ‡πÅ‡∏•‡∏∞ Doc Type ‡∏´‡∏•‡∏≤‡∏¢‡∏ä‡∏ô‡∏¥‡∏î
    """
    # VectorStoreManager ‡∏ñ‡∏π‡∏Å‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö path/config ‡πÅ‡∏ï‡πà Retriver ‡∏ñ‡∏π‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏™‡∏°‡∏≠
    manager = VectorStoreManager()
    retrievers_list: List[NamedRetriever] = []
    
    for doc_type in doc_type_list:
        doc_type_lower = doc_type.lower()
        
        # Logic ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Evidence (‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ Enabler)
        if doc_type_lower == "evidence":
            if not enabler:
                logger.warning("‚ö†Ô∏è Skipping 'evidence': No enabler specified.")
                continue
            collection_name = _get_collection_name(doc_type_lower, enabler)
        
        # Logic ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Doc Type ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
        elif doc_type_lower in SUPPORTED_DOC_TYPES: # ‡πÉ‡∏ä‡πâ SUPPORTED_DOC_TYPES ‡∏à‡∏≤‡∏Å core.ingest
            collection_name = _get_collection_name(doc_type_lower, None)
        else:
             logger.warning(f"‚ö†Ô∏è Skipping unsupported doc_type: {doc_type_lower}")
             continue
             
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Collection ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏û‡∏¥‡πà‡∏°
        # NOTE: ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å vectorstore_exists ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏∏ enabler ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö evidence
        if doc_type_lower == "evidence":
             exists = vectorstore_exists(doc_id="N/A", doc_type=doc_type_lower, enabler=enabler, base_path=VECTORSTORE_DIR)
        else:
             exists = vectorstore_exists(doc_id="N/A", doc_type=doc_type_lower, enabler=None, base_path=VECTORSTORE_DIR)
             
        if not exists:
             logger.warning(f"‚ö†Ô∏è Vectorstore collection '{collection_name}' not found on disk. Skipping.")
             continue
             
        retrievers_list.append(
            NamedRetriever(
                doc_id=doc_type_lower, 
                doc_type=collection_name, 
                top_k=INITIAL_TOP_K,
                final_k=FINAL_K_RERANKED
            )
        )
        logger.info(f"Adding RAG source: {collection_name}")


    if not retrievers_list:
        raise ValueError("No valid document sources configured for RAG based on input types/enabler.")

    # 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á MultiDocRetriever
    multidoc_retriever = MultiDocRetriever(
        retrievers_list=retrievers_list,
        k_per_doc=INITIAL_TOP_K,
        doc_ids_filter=filter_doc_ids 
    )
    return multidoc_retriever


# -----------------------------
# --- Assessment Endpoints ---
# -----------------------------

@app.get("/list-collections/")
async def debug_list_collections():
    """Returns a list of collection names that the server can see in the vectorstore directory."""
    try:
        manager = VectorStoreManager() 
        collections = manager.get_all_collection_names() 
        return {"available_collections": collections, "status": "Success", "vectorstore_dir": manager._base_path}
    except Exception as e:
        return {"available_collections": [], "status": f"Error: Failed to initialize VectorStoreManager or access path: {str(e)}"}

@app.post("/api/assess")
async def run_assessment_task(request: AssessmentRequest, background_tasks: BackgroundTasks):
    record_id = os.urandom(8).hex()
    
    initial_record = AssessmentRecord(
        record_id=record_id,
        enabler=request.enabler.upper(),
        sub_criteria_id=request.sub_criteria_id,
        mode=request.mode,
        timestamp=datetime.now(timezone.utc).isoformat(),
        status="RUNNING" 
    )
    ASSESSMENT_HISTORY.append(initial_record)
    
    background_tasks.add_task(_background_assessment_runner, record_id, request)
    
    return {"status": "accepted", "record_id": record_id, "message": "Assessment started in background. Check /api/assess/history for status."}

# -----------------------------
# --- Assessment History Endpoint ---
# -----------------------------
@app.get("/api/assess/history", response_model=List[AssessmentRecord])
async def get_assessment_history(enabler: Optional[str] = None): 
    
    filtered_history = ASSESSMENT_HISTORY
    
    if enabler:
        enabler_upper = enabler.upper()
        
        filtered_history = [
            record for record in ASSESSMENT_HISTORY 
            if record.enabler.upper() == enabler_upper
        ]
        
    return sorted(filtered_history, key=lambda r: r.timestamp, reverse=True)


@app.get("/api/assess/results/{record_id}")
async def get_assessment_results(record_id: str):
    record = next((r for r in ASSESSMENT_HISTORY if r.record_id == record_id), None)
    if not record:
        raise HTTPException(status_code=404, detail="Assessment record not found.")

    if record.export_path and os.path.exists(record.export_path):
        return FileResponse(record.export_path, media_type="application/json", filename=os.path.basename(record.export_path))
    
    raise HTTPException(status_code=404, detail="Full assessment data not available for this record.")


# -----------------------------
# --- Reference Data Endpoints ---
# -----------------------------

# R1: GET /api/ref_data/{enabler}
@app.get("/api/ref_data/{enabler}")
async def get_all_reference_data(enabler: str):
    """
    R1: ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Reference Data ‡∏ó‡∏±‡πâ‡∏á 4 ‡∏ä‡∏ô‡∏¥‡∏î (Statements, Rubrics, Mapping, Weighting) 
    ‡∏Ç‡∏≠‡∏á Enabler ‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏‡πÉ‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
    """
    enabler = enabler.lower()
    data = {}
    
    def load_data_safe(data_type: str):
        filepath = get_ref_data_path(enabler, data_type)
        data[data_type] = load_ref_data_file(filepath)

    try:
        await run_in_threadpool(lambda: load_data_safe('statements'))
        await run_in_threadpool(lambda: load_data_safe('rubrics'))
        await run_in_threadpool(lambda: load_data_safe('mapping'))
        await run_in_threadpool(lambda: load_data_safe('weighting'))
        
        data['enabler'] = enabler.upper()

        return data
    except HTTPException:
        raise 
    except Exception as e:
        logger.error(f"Error loading all ref data for {enabler}: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to load all reference data for {enabler}")


# R2: POST /api/ref_data/{enabler}/{data_type}
@app.post("/api/ref_data/{enabler}/{data_type}")
async def save_reference_data(enabler: str, data_type: str, payload: RefDataPayload):
    """
    R2: ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Reference Data (Statements, Rubrics, Mapping, ‡∏´‡∏£‡∏∑‡∏≠ Weighting) 
    """
    enabler = enabler.lower()
    
    if data_type not in ['statements', 'rubrics', 'mapping', 'weighting']:
        raise HTTPException(status_code=400, detail="Invalid data_type. Must be one of: statements, rubrics, mapping, weighting.")
        
    filepath = get_ref_data_path(enabler, data_type)
    
    try:
        await run_in_threadpool(lambda: save_ref_data_file(filepath, payload.data))
        logger.info(f"Saved {data_type} for {enabler} to {filepath}")
        return {"status": "success", "enabler": enabler.upper(), "data_type": data_type}
    except Exception as e:
        logger.error(f"Error saving {data_type} for {enabler}: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to save {data_type} data.")


# R3: POST /api/ref_data/auto_map/{enabler}
@app.post("/api/ref_data/auto_map/{enabler}")
async def trigger_auto_mapping(enabler: str, background_tasks: BackgroundTasks):
    """
    R3: Trigger Background Task ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ Auto Mapping/LLM Generation 
    """
    enabler = enabler.lower()
    
    background_tasks.add_task(_background_auto_mapper, enabler)
    
    return {"status": "accepted", "enabler": enabler.upper(), "message": "Auto Mapping process started in background."}


# -----------------------------
# --- Background Runner Logic ---
# -----------------------------

def _background_assessment_runner(record_id: str, request: AssessmentRequest):
    logger.info(f"Processing background assessment for {record_id}...")
    
    record = next((r for r in ASSESSMENT_HISTORY if r.record_id == record_id), None)
    if not record:
        logger.error(f"FATAL: Initial record {record_id} not found in history list. Exiting runner.")
        return 
        
    try:
        final_summary = run_assessment_process(
            enabler=request.enabler,
            sub_criteria_id=request.sub_criteria_id,
            mode=request.mode,
            filter_mode=request.filter_mode,
            export=True 
        )
        
        record.overall_score = final_summary['Overall']['overall_maturity_score']
        
        sub_id_for_level = request.sub_criteria_id if request.sub_criteria_id != 'all' else list(final_summary['SubCriteria_Breakdown'].keys())[0] if final_summary['SubCriteria_Breakdown'] else None
        record.highest_full_level = final_summary['SubCriteria_Breakdown'].get(sub_id_for_level, {}).get('highest_full_level', 0) if sub_id_for_level else 0
        
        record.export_path = final_summary.get("export_path_used")
        record.status = "COMPLETED" 
        
        record.timestamp = datetime.now(timezone.utc).isoformat()
        
        logger.info(f"Assessment {record_id} completed successfully. Score: {record.overall_score:.2f}")

    except Exception as e:
        logger.error(f"Assessment task {record_id} failed: {e}")
        record.overall_score = -1.0
        record.highest_full_level = -1
        record.status = "FAILED"
        record.timestamp = datetime.now(timezone.utc).isoformat()

# -----------------------------
# --- Auto Mapping Background Runner ---
# -----------------------------
def _background_auto_mapper(enabler: str):
    logger.info(f"Starting Auto Mapping for {enabler}...")
    
    try:
        generator = EvidenceMappingGenerator(enabler_id=enabler.upper())
        new_mapping_data = generator.generate_full_mapping_data() 
        
        filepath = get_ref_data_path(enabler, 'mapping')
        save_ref_data_file(filepath, new_mapping_data) 
        
        logger.info(f"Auto Mapping for {enabler} completed and saved successfully.")

    except Exception as e:
        logger.error(f"Auto Mapping task for {enabler} failed: {e}")
        
        
# -----------------------------
# --- Uploads & Document Endpoints (Using Bracket Notation for DocInfo) ---
# -----------------------------
@app.get("/api/uploads/document", response_model=List[UploadResponse])
async def list_uploads_document_only():
    """
    Endpoint ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö GET /api/uploads/document 
    """
    return await list_uploads_by_type("document") 


@app.get("/api/uploads/{doc_type}", response_model=List[UploadResponse]) 
async def list_uploads_by_type(doc_type: str):
    """
    ‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô doc_type ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î (‡πÉ‡∏ä‡πâ DocInfo Dict)
    """
    
    doc_data: Dict[str, DocInfo] | List[DocInfo] = await run_in_threadpool(lambda: list_documents(doc_types=[doc_type]))
    
    uploads: List[UploadResponse] = []
    
    if not isinstance(doc_data, dict):
        logger.error(
            f"API Error: list_documents for doc_type='{doc_type}' returned {type(doc_data).__name__}. Expected dict."
        )
        # üü¢ FIX: ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô list ‡πÉ‡∏´‡πâ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô dict ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ 'doc_id' ‡πÄ‡∏õ‡πá‡∏ô key
        if isinstance(doc_data, list):
            doc_data = {item['doc_id']: item for item in doc_data if isinstance(item, dict) and 'doc_id' in item}
        else:
            return uploads # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏ó‡∏±‡πâ‡∏á dict ‡πÅ‡∏•‡∏∞ list ‡∏Å‡πá‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á
        
    if not isinstance(doc_data, dict):
        # ‡∏´‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏î‡πâ‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á
        return uploads

    for uuid, doc_info in doc_data.items():
        
        if doc_info['doc_type'] != doc_type: 
            continue
            
        # üü¢ FIX: ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡πÅ‡∏•‡∏∞‡∏•‡∏≠‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏£‡∏∞‡∏ö‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô try/except ‡∏ó‡∏µ‡πà‡πÅ‡∏¢‡∏Å‡∏Å‡∏±‡∏ô
        file_name = doc_info.get('filename', 'Unknown')
        file_path = doc_info.get('file_path')
        upload_date_iso = datetime.now(timezone.utc).isoformat() # Default

        if file_path:
            try:
                timestamp = os.path.getmtime(file_path) 
                upload_datetime = datetime.fromtimestamp(timestamp, tz=timezone.utc)
                upload_date_iso = upload_datetime.isoformat()
            except Exception as e:
                # Log warning when the actual file is missing or inaccessible
                logger.warning(f"Failed to get modification time for {file_name} ({file_path}). Error: {e}")
        else:
             # Log warning when 'file_path' metadata is missing (the root cause of the previous error)
             logger.warning(f"Metadata missing 'file_path' for document {uuid} ({file_name}). Using current timestamp.")

            
        uploads.append(UploadResponse(
            doc_id=uuid,
            # üü¢ FIX: ‡πÉ‡∏ä‡πâ file_name ‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏°‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
            filename=file_name,
            file_type=os.path.splitext(file_name)[1], 
            status="Ingested" if doc_info['chunk_count'] > 0 else "Pending",
            upload_date=upload_date_iso
        ))
        
    uploads.sort(key=lambda x: x.filename)
    
    return uploads

@app.get("/api/documents", response_model=List[UploadResponse])
async def get_documents():
    return await list_all_uploads() 

@app.get("/api/uploads/list", response_model=List[UploadResponse])
async def list_all_uploads():
    """
    ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏à‡∏≥‡∏Å‡∏±‡∏î doc_type (‡πÉ‡∏ä‡πâ DocInfo Dict)
    """
    doc_data: Dict[str, DocInfo] | List[DocInfo] = await run_in_threadpool(lambda: list_documents(doc_types=None))
    
    uploads: List[UploadResponse] = []
    
    if not isinstance(doc_data, dict):
        logger.error(f"API Error: list_documents returned {type(doc_data).__name__}. Expected dict.")
        
        # üü¢ FIX: ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô list ‡πÉ‡∏´‡πâ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô dict ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ 'doc_id' ‡πÄ‡∏õ‡πá‡∏ô key
        if isinstance(doc_data, list):
            doc_data = {item['doc_id']: item for item in doc_data if isinstance(item, dict) and 'doc_id' in item}
        else:
            return uploads # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏ó‡∏±‡πâ‡∏á dict ‡πÅ‡∏•‡∏∞ list ‡∏Å‡πá‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á

    if not isinstance(doc_data, dict):
        # ‡∏´‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏î‡πâ‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á
        return uploads

    for uuid, doc_info in doc_data.items():
        
        # üü¢ FIX: ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡πÅ‡∏•‡∏∞‡∏•‡∏≠‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏£‡∏∞‡∏ö‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô try/except ‡∏ó‡∏µ‡πà‡πÅ‡∏¢‡∏Å‡∏Å‡∏±‡∏ô
        file_name = doc_info.get('filename', 'Unknown')
        file_path = doc_info.get('file_path')
        upload_date_iso = datetime.now(timezone.utc).isoformat() # Default

        if file_path:
            try:
                timestamp = os.path.getmtime(file_path) 
                upload_datetime = datetime.fromtimestamp(timestamp, tz=timezone.utc)
                upload_date_iso = upload_datetime.isoformat()
            except Exception as e:
                # Log warning when the actual file is missing or inaccessible
                logger.warning(f"Failed to get modification time for {file_name} ({file_path}). Error: {e}")
        else:
             # Log warning when 'file_path' metadata is missing 
             logger.warning(f"Metadata missing 'file_path' for document {uuid} ({file_name}). Using current timestamp.")
            
        uploads.append(UploadResponse(
            doc_id=uuid,
            # üü¢ FIX: ‡πÉ‡∏ä‡πâ file_name ‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏°‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
            filename=file_name,
            file_type=os.path.splitext(file_name)[1], 
            status="Ingested" if doc_info['chunk_count'] > 0 else "Pending",
            upload_date=upload_date_iso
        ))
        
    uploads.sort(key=lambda x: x.filename)
    return uploads

@app.delete("/api/documents/{doc_id}")
async def remove_document(doc_id: str, doc_type: str = Query("document", description="Document type collection name"), enabler: Optional[str] = Query(None)):
    """‡∏•‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å Vectorstore ‡πÅ‡∏•‡∏∞ Mapping ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ UUID, doc_type ‡πÅ‡∏•‡∏∞ enabler (‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô evidence)"""
    try:
        # NOTE: delete_document_by_uuid ‡πÉ‡∏ô core/ingest.py ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö stable_doc_uuid, doc_type, ‡πÅ‡∏•‡∏∞ enabler
        await run_in_threadpool(lambda: delete_document_by_uuid(stable_doc_uuid=doc_id, doc_type=doc_type, enabler=enabler))
        return {"status": "ok", "doc_id": doc_id, "doc_type": doc_type, "enabler": enabler}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

# -----------------------------
# --- Upload Endpoints ---
# -----------------------------
@app.post("/upload", response_model=UploadResponse)
async def upload_file(file: UploadFile = File(...), source_name: Optional[str] = Form(None)):
    os.makedirs(DATA_DIR, exist_ok=True)
    folder = os.path.join(DATA_DIR, "document")
    os.makedirs(folder, exist_ok=True)
    file_path = os.path.join(folder, file.filename)
    
    with open(file_path, "wb") as f:
        f.write(await file.read())
        
    doc_id = await run_in_threadpool(lambda: process_document(file_path=file_path, file_name=file.filename, doc_type="document")) 
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£ Ingest (‡πÉ‡∏ä‡πâ doc_id ‡πÅ‡∏•‡∏∞ doc_type)
    status = "Ingested" if await run_in_threadpool(lambda: vectorstore_exists(doc_id=doc_id, doc_type="document", enabler=None, base_path=VECTORSTORE_DIR)) else "Pending"
    
    return UploadResponse(
        status=status,
        doc_id=doc_id,
        filename=file.filename,
        file_type=os.path.splitext(file.filename)[1],
        upload_date=datetime.now(timezone.utc).isoformat()
    )

@app.post("/upload/{doc_type}", response_model=UploadResponse)
async def upload_file_type(doc_type: str, file: UploadFile = File(...), enabler: Optional[str] = Form(None)):
    folder = os.path.join(DATA_DIR, doc_type)
    os.makedirs(folder, exist_ok=True)
    file_path = os.path.join(folder, file.filename)

    with open(file_path, "wb") as f:
        f.write(await file.read())

    try:
        # NOTE: process_document ‡πÉ‡∏ô core/ingest.py ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö enabler 
        doc_id = await run_in_threadpool(lambda: process_document(file_path=file_path, file_name=file.filename, doc_type=doc_type, enabler=enabler))
        
    except Exception as e:
        logger.error(f"Failed to process {file.filename} as {doc_type}: {e}")
        raise HTTPException(status_code=500, detail=f"File processing failed: {e}")

    # ‡πÉ‡∏ä‡πâ doc_id ‡πÅ‡∏•‡∏∞ doc_type/enabler ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö vectorstore
    status = "Ingested" if await run_in_threadpool(lambda: vectorstore_exists(doc_id=doc_id, doc_type=doc_type, enabler=enabler, base_path=VECTORSTORE_DIR)) else "Pending"

    return UploadResponse(
        status=status,
        doc_id=doc_id,
        filename=file.filename,
        file_type=os.path.splitext(file.filename)[1],
        upload_date=datetime.now(timezone.utc).isoformat()
    )

# -----------------------------
# --- Upload File Deletion, Download ---
# -----------------------------
@app.delete("/upload/{doc_type}/{file_id}")
async def delete_upload(doc_type: str, file_id: str, enabler: Optional[str] = Query(None)):
    
    # üìå NOTE: list_documents ‡∏ñ‡∏π‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà‡∏î‡πâ‡∏ß‡∏¢ ‡πÅ‡∏•‡∏∞‡∏≠‡∏≤‡∏à‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô list
    doc_data: Dict[str, DocInfo] | List[DocInfo] = await run_in_threadpool(lambda: list_documents(doc_types=[doc_type]))
    
    # üü¢ FIX: ‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô dict ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏≤ filepath
    if not isinstance(doc_data, dict) and isinstance(doc_data, list):
         doc_data = {item['doc_id']: item for item in doc_data if isinstance(item, dict) and 'doc_id' in item}
    
    filepath = None
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ doc_data ‡πÄ‡∏õ‡πá‡∏ô dict ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏Å .items()
    if isinstance(doc_data, dict):
        for uuid, info in doc_data.items():
            if uuid == file_id:
                filepath = info.get('file_path') # ‡πÉ‡∏ä‡πâ .get() ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
                break
    
    if filepath and os.path.exists(filepath):
        await run_in_threadpool(lambda: os.remove(filepath))
        logger.info(f"Original file deleted: {filepath}")
    else:
        logger.warning(f"Original file for doc_id '{file_id}' not found. Proceeding with vectorstore deletion.")
        
    try:
        # NOTE: delete_document_by_uuid ‡πÉ‡∏ô core/ingest.py ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö enabler
        await run_in_threadpool(lambda: delete_document_by_uuid(stable_doc_uuid=file_id, doc_type=doc_type, enabler=enabler))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to delete vectorstore/mapping for {file_id}: {e}")

    return {"status": "deleted", "doc_id": file_id}

@app.get("/upload/{doc_type}/{file_id}")
async def download_upload(doc_type: str, file_id: str):
    
    # üìå NOTE: list_documents ‡∏ñ‡∏π‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà‡∏î‡πâ‡∏ß‡∏¢ ‡πÅ‡∏•‡∏∞‡∏≠‡∏≤‡∏à‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô list
    doc_data: Dict[str, DocInfo] | List[DocInfo] = await run_in_threadpool(lambda: list_documents(doc_types=[doc_type]))
    
    # üü¢ FIX: ‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô dict ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏≤ filepath
    if not isinstance(doc_data, dict) and isinstance(doc_data, list):
         doc_data = {item['doc_id']: item for item in doc_data if isinstance(item, dict) and 'doc_id' in item}
         
    file_name_to_download = None
    filepath = None
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ doc_data ‡πÄ‡∏õ‡πá‡∏ô dict ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏Å .items()
    if isinstance(doc_data, dict):
        for uuid, info in doc_data.items():
            if uuid == file_id:
                # üü¢ FIX: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô info['file_name'] ‡πÄ‡∏õ‡πá‡∏ô info['filename']
                file_name_to_download = info.get('filename') 
                filepath = info.get('file_path') # ‡πÉ‡∏ä‡πâ .get() ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
                break
            
    if not file_name_to_download:
        raise HTTPException(status_code=404, detail="Document ID not found in mapping.")
        
    if not filepath or not os.path.exists(filepath):
        raise HTTPException(status_code=404, detail="File not found on disk.")
        
    return FileResponse(filepath, filename=file_name_to_download)


# -----------------------------
# --- Ingest Endpoint ---
# -----------------------------
class IngestRequest(BaseModel):
    doc_ids: List[str]
    doc_type: Optional[str] = "document"
    enabler: Optional[str] = None # NEW: ‡πÄ‡∏û‡∏¥‡πà‡∏° enabler

@app.post("/ingest")
async def ingest_documents(request: IngestRequest):
    """
    ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß‡∏ï‡∏≤‡∏° doc_ids (Stable UUIDs)
    """
    results = []
    
    folder = os.path.join(DATA_DIR, request.doc_type)
    if not os.path.isdir(folder):
         return {"status": "failed", "error": f"Document type folder not found: {folder}"}

    # üìå NOTE: list_documents ‡∏ñ‡∏π‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà‡∏î‡πâ‡∏ß‡∏¢ ‡πÅ‡∏•‡∏∞‡∏≠‡∏≤‡∏à‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô list
    doc_data: Dict[str, DocInfo] | List[DocInfo] = await run_in_threadpool(lambda: list_documents(doc_types=[request.doc_type]))
    
    # üü¢ FIX: ‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô dict ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏î‡πâ‡∏ß‡∏¢ doc_id
    if not isinstance(doc_data, dict) and isinstance(doc_data, list):
         doc_data = {item['doc_id']: item for item in doc_data if isinstance(item, dict) and 'doc_id' in item}
         
    if not isinstance(doc_data, dict):
        return {"status": "failed", "error": "Failed to retrieve document list from DocInfo."}

    for doc_id in request.doc_ids:
        
        info = doc_data.get(doc_id)
        if not info:
             results.append({"doc_id": doc_id, "result": "failed", "error": f"Document ID '{doc_id}' not found in DocInfo mapping. Was the file uploaded via /upload first?"})
             continue
        
        # üü¢ FIX: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô info['file_name'] ‡πÄ‡∏õ‡πá‡∏ô info['filename']
        file_name = info['filename']
        file_path = info['file_path']

        file_extension = os.path.splitext(file_name)[1].lower()
        if file_extension not in SUPPORTED_TYPES:
            results.append({"doc_id": doc_id, "result": "failed", "error": f"Unsupported file type: {file_extension}. Supported types are: {', '.join(SUPPORTED_TYPES)}"})
            continue
        
        if not os.path.exists(file_path):
            results.append({"doc_id": doc_id, "result": "failed", "error": f"File path not found on disk: {file_path}. The file may have been manually deleted."})
            continue

        logger.info(f"Attempting to re-ingest file: {file_path}")

        try:
            # NOTE: process_document ‡πÉ‡∏ô core/ingest.py ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö enabler
            await run_in_threadpool(
                process_document,
                file_path=file_path, 
                file_name=file_name, 
                doc_type=request.doc_type, 
                base_path=VECTORSTORE_DIR,
                stable_doc_uuid=doc_id, 
                enabler=request.enabler # <--- ‡∏™‡πà‡∏á enabler
            )
            
        except Exception as e:
            logger.error(f"Error while processing document {doc_id}: {e}", exc_info=True)
            results.append({"doc_id": doc_id, "result": "failed", "error": str(e)})
            continue
        
        if await run_in_threadpool(lambda: vectorstore_exists(doc_id=doc_id, doc_type=request.doc_type, enabler=request.enabler, base_path=VECTORSTORE_DIR)):
            results.append({"doc_id": doc_id, "result": "success"})
        else:
            logger.warning(f"Vectorstore not found for {doc_id} after processing.")
            results.append({"doc_id": doc_id, "result": "failed", "error": "Vectorstore not found after processing"})

    return {"status": "completed", "results": results}

# -----------------------------
# --- Query Endpoint (Full Multi Doc/Type Support) ---
# -----------------------------
@app.post("/query")
async def query_endpoint(
    question: str = Form(...),
    doc_ids: Optional[List[str]] = Query(None),
    doc_types: Optional[str] = Form(None), 
    enabler: Optional[str] = Form(None) # NEW: ‡∏£‡∏±‡∏ö enabler
):
    
    import json
    skipped = []
    output = {
        "question": question,
        "doc_ids": [],
        "doc_types": [],
        "answer": "",
        "skipped": skipped,
        "enabler": enabler.upper() if enabler else None
    }

    # 1Ô∏è‚É£ Parse doc_types
    if doc_types:
        doc_type_list = [dt.strip() for dt in doc_types.split(",") if dt.strip()]
    else:
        doc_type_list = ["document", "evidence"]
    output['doc_types'] = doc_type_list

    # 2Ô∏è‚É£ Parse doc_ids
    uuid_list = [uid.strip() for uid in doc_ids if uid] if doc_ids else None

    # Helper: format context
    def format_context(docs):
        context_sections = []
        for i, d in enumerate(docs, 1):
            doc_name = d.metadata.get("doc_id", f"Document {i}")
            doc_type = d.metadata.get("doc_type", "N/A")
            # ‡πÉ‡∏ä‡πâ metadata.get("retriever_source") ‡∏ã‡∏∂‡πà‡∏á‡∏Ñ‡∏∑‡∏≠ Collection Name 
            source = d.metadata.get("retriever_source", doc_type) 
            context_sections.append(f"[{doc_name} ({source})]\n{d.page_content}")
        return "\n\n".join(context_sections)

    # Helper: LLM call
    def call_llm_safe(messages_list: List[Any]) -> str:
        res = llm_instance.invoke(messages_list)
        if isinstance(res, dict) and "result" in res:
            return res["result"]
        elif hasattr(res, "content"):
            return res.content.strip()
        elif isinstance(res, str):
            return res.strip()
        return str(res).strip()

    try:
        if not doc_type_list:
            raise ValueError("Must specify at least one document type for RAG.")

        # -----------------------------
        # Load MultiRetriever (‡πÉ‡∏ä‡πâ Helper ‡πÉ‡∏´‡∏°‡πà)
        # -----------------------------
        multi_retriever = await run_in_threadpool(
            _setup_multi_retriever,
            doc_type_list=doc_type_list,
            enabler=enabler,
            filter_doc_ids=uuid_list 
        )

        # -----------------------------
        # Perform Retrieval
        # -----------------------------
        all_docs = await run_in_threadpool(lambda: multi_retriever.invoke(question))

        if not all_docs:
            raise ValueError("No relevant content could be retrieved from the selected documents or collections.")

        docs_for_question = all_docs 

        output['doc_ids'] = list({d.metadata.get("doc_id") for d in docs_for_question if d.metadata.get("doc_id")})

        # -----------------------------
        # Build context + prompt
        # -----------------------------
        context_text = format_context(docs_for_question)
        human_message_content = QA_PROMPT.format(context=context_text, question=question)
        messages = [
            SystemMessage(content=SYSTEM_QA_INSTRUCTION),
            HumanMessage(content=human_message_content)
        ]

        # -----------------------------
        # Call LLM
        # -----------------------------
        answer_text = await run_in_threadpool(lambda: call_llm_safe(messages))
        output['answer'] = answer_text

    except ValueError as e:
        output['answer'] = f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {str(e)}"
        output['error'] = str(e)
    except Exception as e:
        output['answer'] = f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏Ñ‡∏≤‡∏î‡∏Ñ‡∏¥‡∏î‡πÉ‡∏ô‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• RAG: {str(e)}"
        output['error'] = str(e)

    # Flatten JSON output from LLM if possible (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°)
    if answer_text:
        answer_text = answer_text.strip()
        if answer_text.startswith("{") and answer_text.endswith("}"):
            try:
                llm_json = json.loads(answer_text)
                flattened_answer = []

                if 'summary' in llm_json and llm_json['summary']:
                    flattened_answer.append("üìå Summary:\n" + llm_json['summary'])
                if 'details' in llm_json and llm_json['details']:
                    for d in llm_json['details']:
                        # NOTE: doc_name ‡πÉ‡∏ô LLM output ‡∏°‡∏±‡∏Å‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô UUID ‡∏´‡∏£‡∏∑‡∏≠‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå
                        flattened_answer.append(f"üìÑ {d.get('doc_name','')}: {d.get('text','')}") 
                if 'comparison' in llm_json and llm_json['comparison']:
                    flattened_answer.append("‚öñÔ∏è Comparison:")
                    for k,v in llm_json['comparison'].items():
                        flattened_answer.append(f"{k}: {v}")
                if 'search_results' in llm_json and llm_json['search_results']:
                    flattened_answer.append("üîç Search Results:")
                    for r in llm_json['search_results']:
                        flattened_answer.append(f"{r.get('doc_name','')}: {r.get('text','')}")

                if flattened_answer:
                    output['answer'] = "\n\n".join(flattened_answer)

            except Exception as e:
                if 'error' not in output:
                    output['error'] = f"JSON Parsing Error: {str(e)}"

    return output

# -----------------------------------------------------------------------------
# üìå API Endpoint
# -----------------------------------------------------------------------------
@app.post("/compare")
async def compare_two_documents(request: CompareRequest):
    
    # üìå ‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö UUIDs ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á ‡∏´‡∏•‡∏±‡∏á Pydantic ‡∏ú‡πà‡∏≤‡∏ô
    if not request.doc1_uuid or not request.doc2_uuid:
        logger.error(f"Comparison failed: Missing required UUIDs. Doc1: {request.doc1_uuid}, Doc2: {request.doc2_uuid}")
        raise HTTPException(status_code=400, detail="Both doc1_uuid and doc2_uuid must be selected and provided.")

    doc1_uuid = request.doc1_uuid
    doc2_uuid = request.doc2_uuid
    
    # ‚úÖ ‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤ final_doc_type_list ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Default ‡∏´‡∏≤‡∏Å‡πÄ‡∏õ‡πá‡∏ô None ‡∏´‡∏£‡∏∑‡∏≠ Empty List
    final_doc_type_list = request.doc_type_list if request.doc_type_list and len(request.doc_type_list) > 0 else ['document']
    
    # ‚úÖ ‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤ final_query ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Default ‡∏´‡∏≤‡∏Å request.query ‡πÄ‡∏õ‡πá‡∏ô None
    default_query = "‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Ç‡∏≠‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á‡∏â‡∏ö‡∏±‡∏ö‡∏ô‡∏µ‡πâ"
    final_query = request.query if request.query else default_query
    
    # 1. Load VectorStoreManager
    try:
        # NOTE: manager ‡∏ï‡πâ‡∏≠‡∏á‡∏ñ‡∏π‡∏Å initialize ‡∏à‡∏≤‡∏Å doc_type_list ‡∏ó‡∏µ‡πà‡∏Å‡∏£‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß
        # üí° ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å: ‡πÉ‡∏ä‡πâ doc_type_list ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠ parameter ‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
        manager = VectorStoreManager(doc_type_list=final_doc_type_list) 
    except Exception as e:
        logger.error(f"Failed to initialize VectorStoreManager for comparison: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to initialize RAG system: {e}")

    # 2. Retrieve Documents by UUID
    
    # 2.1 Fetch document 1
    doc1_info = await run_in_threadpool(lambda: manager.get_doc_info_by_uuid(doc1_uuid))
    if not doc1_info:
        raise HTTPException(status_code=404, detail=f"Document 1 with UUID {doc1_uuid} not found.")
    doc1_chunks, doc1_text, skipped1 = await run_in_threadpool(
        lambda: manager.retrieve_all_text_for_uuid(doc1_uuid, doc1_info['doc_type'], doc1_info.get('enabler'))
    )
    doc1_name = doc1_info['filename']

    # 2.2 Fetch document 2
    doc2_info = await run_in_threadpool(lambda: manager.get_doc_info_by_uuid(doc2_uuid))
    if not doc2_info:
        raise HTTPException(status_code=404, detail=f"Document 2 with UUID {doc2_uuid} not found.")
    doc2_chunks, doc2_text, skipped2 = await run_in_threadpool(
        lambda: manager.retrieve_all_text_for_uuid(doc2_uuid, doc2_info['doc_type'], doc2_info.get('enabler'))
    )
    doc2_name = doc2_info['filename']
    
    skipped = skipped1 + skipped2

    # 3. Combine Text and setup structured output
    doc_names_formatted = f"{doc1_name} ‡πÅ‡∏•‡∏∞ {doc2_name}"
    context_text = f"--- Document 1: {doc1_name} ---\n{doc1_text}\n\n--- Document 2: {doc2_name} ---\n{doc2_text}"
    
    # Use JsonOutputParser to get the format instructions for the LLM
    # NOTE: List[LLMComparisonMetric] ‡∏ï‡πâ‡∏≠‡∏á‡∏ñ‡∏π‡∏Å Import ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô Pydantic Model ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
    parser = JsonOutputParser(pydantic_object=List[LLMComparisonMetric]) 
    json_format_instruction = parser.get_format_instructions()
    
    # 4. Format Prompt (COMPARE_PROMPT must accept {json_format_instruction})
    human_message_content = COMPARE_PROMPT.format(
        context=context_text, 
        query=final_query, # ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô final_query ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡πâ‡∏ß
        doc_names=doc_names_formatted,
        json_format_instruction=json_format_instruction # Pass JSON format instruction
    )

    # 5. Setup Messages
    messages = [
        SystemMessage(content=SYSTEM_COMPARE_INSTRUCTION), # ‚úÖ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: ‡πÉ‡∏ä‡πâ SYSTEM_COMPARE_INSTRUCTION ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏•‡πâ‡∏ß
        HumanMessage(content=human_message_content)
    ]
    
    def call_llm_safe(messages_list: List[Any]) -> str:
        """Helper to invoke LLM and return raw content (expected to be JSON string)."""
        res = llm_instance.invoke(messages_list) 
        if hasattr(res, 'content'): 
            return res.content.strip()
        return str(res).strip()

    # 6. Call LLM to get the structured JSON string
    logger.info(f"Calling LLM for comparison with query: {final_query}")
    json_delta_string = await run_in_threadpool(lambda: call_llm_safe(messages))

    # 7. Final response structure
    return {
        "result": {
            "metrics": [
                {
                    "metric": final_query, # ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô final_query ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡πâ‡∏ß
                    "doc1": doc1_text, 
                    "doc2": doc2_text, 
                    "delta": json_delta_string, # JSON string of List[LLMComparisonMetric]
                    "remark": f"Comparison generated from doc_types: {', '.join(final_doc_type_list)}. LLM Query: {final_query}"
                }
            ]
        },
        "skipped": skipped
    }



# -----------------------------
# --- Evidence Mapping Endpoint (Completed) ---
# -----------------------------
@app.post("/map-evidence/")
async def map_evidence(file: UploadFile, enabler_id):
    raise HTTPException(status_code=501, detail="Not Implemented")